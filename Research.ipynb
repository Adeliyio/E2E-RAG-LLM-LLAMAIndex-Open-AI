{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (0.10.26)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.2.1)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.11)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.26 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.10.26)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.14)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.13)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.26->llama_index) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.15 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (0.1.15)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.26->llama_index) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.24.0)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index) (0.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.26->llama_index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.26->llama_index) (2.6.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.26->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.26->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.9.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.24.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.26->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.26->llama_index) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.26->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.26->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.26->llama_index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.26->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.26->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.26->llama_index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.26->llama_index) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.26->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.26->llama_index) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\rag1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.26->llama_index) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents= SimpleDirectoryReader(\"Data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='cfeab761-f997-4b6e-bb7c-e9de0e262ffc', embedding=None, metadata={'page_label': 'Cover', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d961d7a2-fbea-48e2-a947-41614ce620b8', embedding=None, metadata={'page_label': 'FM-1', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Deep Learning with TensorFlow \\nand Keras\\nThird Edition\\nBuild and deploy supervised, unsupervised, deep, and reinforcement \\nlearning models\\nAmita Kapoor\\nAntonio Gulli\\nSujit Pal\\nBIRMINGHAM—MUMBAI', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b239e89f-e4f5-4e1d-bbe3-d073d85c057f', embedding=None, metadata={'page_label': 'FM-2', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Deep Learning with TensorFlow and Keras\\nThird Edition\\nCopyright © 2022 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \\nany form or by any means, without the prior written permission of the publishe r, except in the case of brief \\nquotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \\npresented. Howeve r, the information contained in this book is sol d without warrant y, ei ther express or \\nimplied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \\ndamages caused or alleged to have been caused directly or indirectly by this book.\\nPackt Publishing ha s endeavored to provide trademark information about all of the companies and products \\nmentioned in this book by the appropriate use of capitals. Howeve r, Packt Publishing cannot guarantee the \\naccuracy of this information.\\nLead Senior Publishing Product Manager: Tusha r Gu pta\\nAcquisition Editor – Peer Reviews:  Gaurav Gavas\\nProject Editor: Namrata Katare\\nContent Development Editor: Bhavesh Amin\\nCopy Editor:  Safis Editing\\nTechnical Editor: Aniket Shetty\\nProofreader:  Safis Editing\\nIndexer:  Rekha Nair\\nPresentation Designer:  Ganesh Bhadwalkar\\nFirst published: April 2017\\nSecond edition: December 2019\\nThird edition: October 2022\\nProduction reference: 1300922\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-80323-291-1\\nwww.packt.com', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='949fa68d-6b21-45cd-ad76-1b0b957075bf', embedding=None, metadata={'page_label': 'FM-3', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword\\nApproachable, well-written, with a great balance between \\ntheory and practice. A very enjoyable introduction to machine \\nlearning for software developers.\\nFrançois Chollet,\\nCreator of Keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c643add-b7a4-46f5-b7da-1a613592cf44', embedding=None, metadata={'page_label': 'FM-4', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contributors\\nAbout the authors\\nAmita Kapoor  taught and supervised research in the field of neural networks and artificial intelligence \\nfor 20+ years as an Associate Professor at the University of Delhi. At present, she works as an independent \\nAI consultant and provides her expertise to various organizations working in the field of AI and EdTech.\\nFirst and foremost, I am thankful to the readers of this book. It is your encouragement via messages and \\nemails that motivate me to give my best. I am extremely thankful to my co-authors, Antonio Gulli and Sujit \\nPal, for sharing their vast experience with me in writing this book. I am thankful to the entire Packt team \\nfor the effort they put in since the inception of this book and the reviewers who painstakingly went through \\nthe content and verified the code; their comments and suggestions helped improve the book.\\nLast but not the least, I am thankful to my teachers for their faith in me, my colleagues at the University of \\nDelhi for their love and support, my friends for continuously motivating me, and my family members for \\ntheir patience and love.\\nA part of the royalties of the book are donated.\\nAntonio Gulli  has a passion for establishing and managing global technological talent for innovation \\nand execution. His core expertise is in cloud computing, deep learning, and search engines. Currently, \\nAntonio works for Google in the Cloud Office of the CTO in Zurich, working on Search, Cloud Infra, \\nSovereignty, and Conversational AI. Previously, he served as a founding member of the Office of the \\nCTO in the EMEA. Earlier on, he served as Google Warsaw Site Director Leader, growing the site to 450+ \\nengineers fully focused on cloud managing teams in GCE, Kubernetes, Serverless, Borg, and Console.\\nSo far, Antonio has been lucky enough to gain professional experience in five countries in Europe and \\nto manage teams in six countries in EMEA and the U.S:\\n• In Amsterdam, as Vice President for Elsevier, a leading scientific publisher.\\n• In London, as Principal Engineer for Bing Search, Microsoft.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='895afcba-bdca-4661-b391-16202338a7be', embedding=None, metadata={'page_label': 'FM-5', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='• In Italy and the U.K, as CTO, Europe for Ask.com.\\n• In Poland, the U.K, and Switzerland with Google.\\nAntonio has co-invented a number of technologies for search, smart energy, and AI with 11 patents \\nissued (21 applied) and published several books on coding and machine learning also translated \\ninto Japanese and Chinese. He speaks Spanish, English, and Italian and is currently learning Polish \\nand French. Antonio is a proud father of Two boys, Lorenzo, 21 and Leonardo, 16, and a little queen, \\nAurora, 11.\\nI want to thank my sons, Lorenzo and Leonardo, and my daughter, Aurora, for being the motivation behind \\nmy perseverance. Also, I want to thank my partner, Nina, for being the North Star of my life in recent years.\\nSujit Pal  is a Technology Research Director at Elsevier Labs, an advanced technology group within the \\nReed-Elsevier Group of companies. His interests include semantic search, natural language processing, \\nmachine learning, and deep learning. At Elsevier, he has worked on several initiatives involving search \\nquality measurement and improvement, image classification and duplicate detection, and annotation \\nand ontology development for medical and scientific corpora.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c97fe43-ccaa-4d08-9adc-4e8c47502e2e', embedding=None, metadata={'page_label': 'FM-6', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='About the reviewer\\nRaghav Bali  is a seasoned data science professional with over a decade’s experience in the research \\nand development of large-scale solutions in finance, digital experience, IT infrastructure, and \\nhealthcare for giants such as Intel, American Express, UnitedHealth Group, and Delivery Hero. He is \\nan innovator with 7+ patents, a published author of multiple well-received books (including Hands-On \\nTransfer Learning with Python), has peer reviewed papers, and is a regular speaker in leading conferences \\non topics in the areas of machine learning, deep learning, computer vision, NLP, generative models, \\nand augmented reality.\\nI would like to take this opportunity to congratulate the authors on yet another amazing book. Thanks to \\nPackt for bringing me on board as a reviewer for this book, particularly Namrata, Saby, and Tushar for all \\ntheir support and assistance and for being so receptive throughout the review process. And finally, I’d like \\nto thank my wife, family, and colleagues for all the support and patience.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a40d46c-42c8-456b-b00f-1da581a1ba96', embedding=None, metadata={'page_label': 'vii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents\\nPreface   xxiii\\nChapter 1: Neural Network Foundations with TF   1\\nWhat is TensorFlow (TF)?  ����������������������������������������������������������������������������������������������������������  1\\nWhat is Keras?  ��������������������������������������������������������������������������������������������������������������������������  3\\nIntroduction to neural networks  �����������������������������������������������������������������������������������������������  3\\nPerceptron  �������������������������������������������������������������������������������������������������������������������������������  4\\nOur first example of TensorFlow code • 4\\nMulti-layer perceptron: our first example of a network  ��������������������������������������������������������������  5\\nProblems in training the perceptron and solution • 6\\nActivation function: sigmoid • 7\\nActivation function: tanh • 7\\nActivation function: ReLU • 7\\nTwo additional activation functions: ELU and Leaky ReLU • 8\\nActivation functions • 9\\nIn short: what are neural networks after all? • 10\\nA real example: recognizing handwritten digits  �����������������������������������������������������������������������  10\\nOne hot-encoding (OHE) • 10\\nDefining a simple neural net in TensorFlow • 11\\nRunning a simple TensorFlow net and establishing a baseline • 15\\nImproving the simple net in TensorFlow with hidden layers • 16\\nFurther improving the simple net in TensorFlow with dropout • 19\\nTesting different optimizers in TensorFlow • 22\\nIncreasing the number of epochs • 27\\nControlling the optimizer learning rate • 28\\nIncreasing the number of internal hidden neurons • 28\\nIncreasing the size of batch computation • 30', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='271f8621-4c36-41d9-89d3-fa6d44ee53b6', embedding=None, metadata={'page_label': 'viii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents viii\\nSummarizing experiments run to recognizing handwritten digits • 31\\nRegularization ������������������������������������������������������������������������������������������������������������������������  31\\nAdopting regularization to avoid overfitting • 31\\nUnderstanding batch normalization • 33\\nPlaying with Google Colab: CPUs, GPUs, and TPUs  �������������������������������������������������������������������  33\\nSentiment analysis  �����������������������������������������������������������������������������������������������������������������  36\\nHyperparameter tuning and AutoML • 38\\nPredicting output  �������������������������������������������������������������������������������������������������������������������  39\\nA practical overview of backpropagation  ���������������������������������������������������������������������������������  39\\nWhat have we learned so far?  ��������������������������������������������������������������������������������������������������  41\\nToward a deep learning approach  �������������������������������������������������������������������������������������������  41\\nSummary  �������������������������������������������������������������������������������������������������������������������������������  42\\nReferences  �����������������������������������������������������������������������������������������������������������������������������  42\\nChapter 2: Regression and Classification   43\\nWhat is regression?  ����������������������������������������������������������������������������������������������������������������  43\\nPrediction using linear regression  ������������������������������������������������������������������������������������������  44\\nSimple linear regression • 45\\nMultiple linear regression • 48\\nMultivariate linear regression • 49\\nNeural networks for linear regression  �������������������������������������������������������������������������������������  49\\nSimple linear regression using TensorFlow Keras • 49\\nMultiple and multivariate linear regression using the TensorFlow Keras API • 53\\nClassification tasks and decision boundaries  ���������������������������������������������������������������������������  58\\nLogistic regression • 59\\nLogistic regression on the MNIST dataset • 60\\nSummary  �������������������������������������������������������������������������������������������������������������������������������  64\\nReferences  �����������������������������������������������������������������������������������������������������������������������������  64\\nChapter 3: Convolutional Neural Networks   65\\nDeep convolutional neural networks  ���������������������������������������������������������������������������������������  66\\nLocal receptive fields • 66\\nShared weights and bias • 67\\nA mathematical example • 67\\nConvNets in TensorFlow • 68\\nPooling layers • 68\\nMax pooling • 68', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8d0172e3-710e-4981-9542-e76b06e54451', embedding=None, metadata={'page_label': 'ix', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents ix\\nAverage pooling • 69\\nConvNets summary • 69\\nAn example of DCNN: LeNet  ����������������������������������������������������������������������������������������������������  69\\nLeNet code in TF • 70\\nUnderstanding the power of deep learning • 77\\nRecognizing CIFAR-10 images with deep learning  ��������������������������������������������������������������������  78\\nImproving the CIFAR-10 performance with a deeper network • 82\\nImproving the CIFAR-10 performance with data augmentation • 84\\nPredicting with CIFAR-10 • 87\\nVery deep convolutional networks for large-scale image recognition  ����������������������������������������  88\\nRecognizing cats with a VGG16 network • 90\\nUtilizing the tf.Keras built-in VGG16 net module • 90\\nRecycling pre-built deep learning models for extracting features • 91\\nDeep Inception V3 for transfer learning  ����������������������������������������������������������������������������������  93\\nOther CNN architectures  ��������������������������������������������������������������������������������������������������������  95\\nAlexNet • 95\\nResidual networks • 95\\nHighwayNets and DenseNets • 96\\nXception • 97\\nStyle transfer  �������������������������������������������������������������������������������������������������������������������������  99\\nContent distance • 100\\nStyle distance • 101\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  102\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  102\\nChapter 4: Word Embeddings   103\\nWord embedding ‒ origins and fundamentals  ������������������������������������������������������������������������  104\\nDistributed representations  ��������������������������������������������������������������������������������������������������  105\\nStatic embeddings  ����������������������������������������������������������������������������������������������������������������  106\\nWord2Vec • 106\\nGloVe • 109\\nCreating your own embeddings using Gensim  �����������������������������������������������������������������������  110\\nExploring the embedding space with Gensim  ������������������������������������������������������������������������  111\\nUsing word embeddings for spam detection  ��������������������������������������������������������������������������  114\\nGetting the data • 115\\nMaking the data ready for use • 115\\nBuilding the embedding matrix • 117', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fed8a211-9ce3-4d9e-adb9-35b4731b150f', embedding=None, metadata={'page_label': 'x', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents x\\nDefining the spam classifier • 118\\nTraining and evaluating the model • 120\\nRunning the spam detector • 121\\nNeural embeddings – not just for words  ��������������������������������������������������������������������������������  122\\nItem2Vec • 122\\nnode2vec • 123\\nCharacter and subword embeddings  �������������������������������������������������������������������������������������  128\\nDynamic embeddings  �����������������������������������������������������������������������������������������������������������  129\\nSentence and paragraph embeddings  ������������������������������������������������������������������������������������  131\\nLanguage model-based embeddings  ��������������������������������������������������������������������������������������  132\\nUsing BERT as a feature extractor • 134\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  135\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  136\\nChapter 5: Recurrent Neural Networks   139\\nThe basic RNN cell  ���������������������������������������������������������������������������������������������������������������  140\\nBackpropagation through time (BPTT) • 142\\nVanishing and exploding gradients • 143\\nRNN cell variants  �����������������������������������������������������������������������������������������������������������������  144\\nLong short-term memory (LSTM) • 144\\nGated recurrent unit (GRU) • 146\\nPeephole LSTM • 147\\nRNN variants  ������������������������������������������������������������������������������������������������������������������������  147\\nBidirectional RNNs • 147\\nStateful RNNs • 148\\nRNN topologies  ��������������������������������������������������������������������������������������������������������������������  149\\nExample ‒ One-to-many – Learning to generate text • 150\\nExample ‒ Many-to-one – Sentiment analysis • 157\\nExample ‒ Many-to-many – POS tagging • 163\\nEncoder-decoder architecture – seq2seq  �������������������������������������������������������������������������������  172\\nExample ‒ seq2seq without attention for machine translation • 173\\nAttention mechanism  �����������������������������������������������������������������������������������������������������������  182\\nExample ‒ seq2seq with attention for machine translation • 184\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  189\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  190', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aea0ca4f-6569-4199-bdc7-364dec192b58', embedding=None, metadata={'page_label': 'xi', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xi\\nChapter 6: Transformers   193\\nArchitecture  �������������������������������������������������������������������������������������������������������������������������  194\\nKey intuitions • 195\\nPositional encoding • 195\\nAttention • 195\\nSelf-attention • 198\\nMulti-head (self-)attention • 198\\nHow to compute attention • 198\\nEncoder-decoder architecture • 200\\nResidual and normalization layers • 200\\nAn overview of the transformer architecture • 200\\nTraining • 204\\nTransformers’ architectures  �������������������������������������������������������������������������������������������������  204\\nCategories of transformers • 204\\nDecoder or autoregressive • 205\\nEncoder or autoencoding • 205\\nSeq2seq • 205\\nMultimodal • 205\\nRetrieval • 205\\nAttention • 205\\nFull versus sparse • 206\\nLSH attention • 206\\nLocal attention • 206\\nPretraining  ��������������������������������������������������������������������������������������������������������������������������  206\\nEncoder pretraining • 206\\nDecoder pretraining • 206\\nEncoder-decoder pretraining • 206\\nA taxonomy for pretraining tasks • 207\\nAn overview of popular and well-known models  ��������������������������������������������������������������������  207\\nBERT • 207\\nGPT-2 • 209\\nGPT-3 • 209\\nReformer • 210\\nBigBird • 211\\nTransformer-XL • 212\\nXLNet • 213', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d7169e54-f865-4e02-8db8-46ab2b62e7ff', embedding=None, metadata={'page_label': 'xii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xii\\nRoBERTa • 213\\nALBERT • 214\\nStructBERT • 214\\nT5 and MUM • 215\\nELECTRA • 216\\nDeBERTa • 217\\nThe Evolved Transformer and MEENA • 217\\nLaMDA  • 219\\nSwitch Transformer • 221\\nRETRO • 222\\nPathways and PaLM • 223\\nImplementation  �������������������������������������������������������������������������������������������������������������������  223\\nTransformer reference implementation: An example of translation • 224\\nHugging Face • 242\\nGenerating text  • 242\\nAutoselecting a model and autotokenization • 244\\nNamed entity recognition • 245\\nSummarization • 246\\nFine-tuning • 248\\nTFHub • 250\\nEvaluation  ����������������������������������������������������������������������������������������������������������������������������  252\\nQuality • 252\\nGLUE • 252\\nSuperGLUE • 253\\nSQuAD • 254\\nRACE • 255\\nNLP-progress • 255\\nSize • 256\\nLarger doesn’t always mean better • 257\\nCost of serving • 257\\nOptimization  ������������������������������������������������������������������������������������������������������������������������  257\\nQuantization • 257\\nWeight pruning • 257\\nDistillation • 258\\nCommon pitfalls: dos and don’ts  �������������������������������������������������������������������������������������������  258\\nDos • 258\\nDon’ts • 259', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66dcbb1c-38b5-4158-8266-f536dd3f84ec', embedding=None, metadata={'page_label': 'xiii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xiii\\nThe future of transformers  ���������������������������������������������������������������������������������������������������  259\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  260\\nChapter 7: Unsupervised Learning   261\\nPrincipal component analysis  �����������������������������������������������������������������������������������������������  261\\nPCA on the MNIST dataset • 262\\nTensorFlow Embedding API • 264\\nK-means clustering ���������������������������������������������������������������������������������������������������������������  266\\nK-means in TensorFlow • 267\\nVariations in k-means • 270\\nSelf-organizing maps  ������������������������������������������������������������������������������������������������������������  271\\nColour mapping using a SOM • 273\\nRestricted Boltzmann machines  ��������������������������������������������������������������������������������������������  278\\nReconstructing images using an RBM • 279\\nDeep belief networks • 283\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  284\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  284\\nChapter 8: Autoencoders   287\\nIntroduction to autoencoders  �����������������������������������������������������������������������������������������������  287\\nVanilla autoencoders  ������������������������������������������������������������������������������������������������������������  289\\nTensorFlow Keras layers ‒ defining custom layers • 290\\nReconstructing handwritten digits using an autoencoder • 292\\nSparse autoencoder  ��������������������������������������������������������������������������������������������������������������  295\\nDenoising autoencoders  �������������������������������������������������������������������������������������������������������  297\\nClearing images using a denoising autoencoder • 298\\nStacked autoencoder  ������������������������������������������������������������������������������������������������������������  301\\nConvolutional autoencoder for removing noise from images • 301\\nA TensorFlow Keras autoencoder example ‒ sentence vectors • 306\\nVariational autoencoders  ������������������������������������������������������������������������������������������������������  314\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  320\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  320\\nChapter 9: Generative Models   321\\nWhat is a GAN?  ���������������������������������������������������������������������������������������������������������������������  322\\nMNIST using GAN in TensorFlow • 324', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='383b4d01-37fa-4a4b-a798-94c97537dbc8', embedding=None, metadata={'page_label': 'xiv', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xiv\\nDeep convolutional GAN (DCGAN)  ����������������������������������������������������������������������������������������  329\\nDCGAN for MNIST digits • 330\\nSome interesting GAN architectures  ��������������������������������������������������������������������������������������  339\\nSRGAN • 339\\nCycleGAN • 340\\nInfoGAN • 342\\nCool applications of GANs  �����������������������������������������������������������������������������������������������������  344\\nCycleGAN in TensorFlow  �������������������������������������������������������������������������������������������������������  348\\nFlow-based models for data generation  ���������������������������������������������������������������������������������  356\\nDiffusion models for data generation  ������������������������������������������������������������������������������������  358\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  359\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  360\\nChapter 10: Self-Supervised Learning   361\\nPrevious work  ����������������������������������������������������������������������������������������������������������������������  362\\nSelf-supervised learning  �������������������������������������������������������������������������������������������������������  363\\nSelf-prediction  ���������������������������������������������������������������������������������������������������������������������  363\\nAutoregressive generation • 364\\nPixelRNN • 364\\nImage GPT (IPT) • 364\\nGPT-3 • 365\\nXLNet • 365\\nWaveNet • 366\\nWaveRNN • 366\\nMasked generation • 366\\nBERT • 366\\nStacked denoising autoencoder • 367\\nContext autoencoder • 367\\nColorization • 369\\nInnate relationship prediction • 369\\nRelative position • 369\\nSolving jigsaw puzzles • 370\\nRotation • 370\\nHybrid self-prediction • 370\\nVQ-VAE • 371\\nJukebox • 371\\nDALL-E • 371', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc3a4ca2-6cd2-4c59-8de5-030d5b5f5236', embedding=None, metadata={'page_label': 'xv', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xv\\nVQ-GAN • 372\\nContrastive learning  �������������������������������������������������������������������������������������������������������������  373\\nTraining objectives • 373\\nContrastive loss • 374\\nTriplet loss • 374\\nN-pair loss • 374\\nLifted structural loss • 375\\nNCE loss • 375\\nInfoNCE loss • 375\\nSoft nearest neighbors loss • 376\\nInstance transformation • 376\\nSimCLR • 376\\nBarlow Twins • 377\\nBYOL • 378\\nFeature clustering • 378\\nDeepCluster • 379\\nSwAV • 379\\nInterCLR • 379\\nMultiview coding • 380\\nAMDIM • 380\\nCMC • 381\\nMultimodal models • 381\\nCLIP • 381\\nCodeSearchNet • 383\\nData2Vec • 383\\nPretext tasks  ������������������������������������������������������������������������������������������������������������������������  384\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  384\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  384\\nChapter 11: Reinforcement Learning   389\\nAn introduction to RL  �����������������������������������������������������������������������������������������������������������  389\\nRL lingo • 391\\nDeep reinforcement learning algorithms • 393\\nHow does the agent choose its actions, especially when untrained? • 394\\nHow does the agent maintain a balance between exploration and exploitation? • 394\\nHow to deal with the highly correlated input state space • 394\\nHow to deal with the problem of moving targets • 395', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67dee5c6-deed-480d-8d4f-50f5601b42f6', embedding=None, metadata={'page_label': 'xvi', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xvi\\nReinforcement success in recent years • 395\\nSimulation environments for RL  �������������������������������������������������������������������������������������������  396\\nAn introduction to OpenAI Gym  ��������������������������������������������������������������������������������������������  397\\nRandom agent playing Breakout • 401\\nWrappers in Gym • 403\\nDeep Q-networks  ������������������������������������������������������������������������������������������������������������������  406\\nDQN for CartPole • 407\\nDQN to play a game of Atari • 412\\nDQN variants • 415\\nDouble DQN • 416\\nDueling DQN • 416\\nRainbow • 418\\nDeep deterministic policy gradient  ���������������������������������������������������������������������������������������  418\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  420\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  420\\nChapter 12: Probabilistic TensorFlow   423\\nTensorFlow Probability  ��������������������������������������������������������������������������������������������������������  423\\nTensorFlow Probability distributions  ������������������������������������������������������������������������������������  427\\nUsing TFP distributions • 428\\nCoin Flip Example • 428\\nNormal distribution • 431\\nBayesian networks • 434\\nHandling uncertainty in predictions using TensorFlow Probability • 437\\nAleatory uncertainty • 438\\nEpistemic uncertainty • 438\\nCreating a synthetic dataset • 438\\nBuilding a regression model using TensorFlow • 439\\nProbabilistic neural networks for aleatory uncertainty • 440\\nAccounting for the epistemic uncertainty • 442\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  443\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  444\\nChapter 13: An Introduction to AutoML   445\\nWhat is AutoML?  ������������������������������������������������������������������������������������������������������������������  445\\nAchieving AutoML  ����������������������������������������������������������������������������������������������������������������  446\\nAutomatic data preparation  ��������������������������������������������������������������������������������������������������  447', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad29b83a-08f3-4b62-bc2f-fa1d11c0d356', embedding=None, metadata={'page_label': 'xvii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xvii\\nAutomatic feature engineering  ����������������������������������������������������������������������������������������������  447\\nAutomatic model generation  �������������������������������������������������������������������������������������������������  448\\nAutoKeras  ����������������������������������������������������������������������������������������������������������������������������  450\\nGoogle Cloud AutoML and Vertex AI  ��������������������������������������������������������������������������������������  451\\nUsing the Google Cloud AutoML Tables solution • 451\\nUsing the Google Cloud AutoML Text solution • 463\\nUsing the Google Cloud AutoML Video solution • 466\\nCost • 470\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  471\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  471\\nChapter 14: The Math Behind Deep Learning   473\\nHistory  ��������������������������������������������������������������������������������������������������������������������������������  473\\nSome mathematical tools ������������������������������������������������������������������������������������������������������  474\\nVectors • 474\\nDerivatives and gradients everywhere • 474\\nGradient descent • 476\\nChain rule • 477\\nA few differentiation rules • 477\\nMatrix operations • 478\\nActivation functions  �������������������������������������������������������������������������������������������������������������  478\\nDerivative of the sigmoid • 478\\nDerivative of tanh • 479\\nDerivative of ReLU • 479\\nBackpropagation  ������������������������������������������������������������������������������������������������������������������  480\\nForward step • 482\\nBackstep • 483\\nCase 1: From hidden layer to output layer • 484\\nCase 2: From hidden layer to hidden layer • 485\\nCross entropy and its derivative • 489\\nBatch gradient descent, stochastic gradient descent, and mini-batch • 490\\nBatch gradient descent • 491\\nStochastic gradient descent • 491\\nMini-batch gradient descent • 491\\nThinking about backpropagation and ConvNets • 491\\nThinking about backpropagation and RNNs • 492\\nA note on TensorFlow and automatic differentiation  ��������������������������������������������������������������  495', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7333dfa5-53af-4cd5-8f13-bab5e33cd428', embedding=None, metadata={'page_label': 'xviii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xviii\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  495\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  496\\nChapter 15: Tensor Processing Unit   499\\nC/G/T processing units  ���������������������������������������������������������������������������������������������������������  499\\nCPUs and GPUs • 499\\nTPUs • 500\\nFour generations of TPUs, plus Edge TPU  ������������������������������������������������������������������������������  501\\nFirst generation TPU • 501\\nSecond generation TPU • 504\\nThird generation TPU • 505\\nFourth generation TPUs • 506\\nEdge TPU • 506\\nTPU performance  �����������������������������������������������������������������������������������������������������������������  507\\nHow to use TPUs with Colab  ��������������������������������������������������������������������������������������������������  509\\nChecking whether TPUs are available • 509\\nKeras MNIST TPU end-to-end training • 510\\nUsing pretrained TPU models  �����������������������������������������������������������������������������������������������  511\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  513\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  514\\nChapter 16: Other Useful Deep Learning Libraries   515\\nHugging Face  �����������������������������������������������������������������������������������������������������������������������  515\\nOpenAI  ��������������������������������������������������������������������������������������������������������������������������������  517\\nOpenAI GPT-3 API • 517\\nOpenAI DALL-E 2 • 518\\nOpenAI Codex • 519\\nPyTorch  �������������������������������������������������������������������������������������������������������������������������������  520\\nONNX  ����������������������������������������������������������������������������������������������������������������������������������  522\\nH2O �ai  ���������������������������������������������������������������������������������������������������������������������������������  522\\nH2O AutoML  • 523\\nAutoML using H2O • 523\\nH2O model explainability • 526\\nPartial dependence plots • 526\\nVariable importance heatmap • 527\\nModel correlation • 528\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  529', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='881509d3-750e-4092-829a-155214ea6f74', embedding=None, metadata={'page_label': 'xix', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xix\\nChapter 17: Graph Neural Networks   531\\nGraph basics  ������������������������������������������������������������������������������������������������������������������������  532\\nGraph machine learning  �������������������������������������������������������������������������������������������������������  532\\nGraph convolutions – the intuition behind GNNs  �������������������������������������������������������������������  533\\nCommon graph layers �����������������������������������������������������������������������������������������������������������  534\\nGraph convolution network • 535\\nGraph attention network • 535\\nGraphSAGE (sample and aggregate) • 536\\nGraph isomorphism network • 536\\nCommon graph applications  �������������������������������������������������������������������������������������������������  537\\nNode classification • 537\\nGraph classification • 541\\nLink prediction • 545\\nGraph customizations  �����������������������������������������������������������������������������������������������������������  551\\nCustom layers and message passing • 551\\nCustom graph dataset • 554\\nSingle graphs in datasets • 554\\nSet of multiple graphs in datasets • 557\\nFuture directions  ������������������������������������������������������������������������������������������������������������������  560\\nHeterogeneous graphs • 560\\nTemporal Graphs • 560\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  561\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  561\\nChapter 18: Machine Learning Best Practices   563\\nThe need for best practices  ���������������������������������������������������������������������������������������������������  563\\nData best practices  ���������������������������������������������������������������������������������������������������������������  564\\nFeature selection • 565\\nFeatures and data • 565\\nAugmenting textual data • 567\\nModel best practices  �������������������������������������������������������������������������������������������������������������  568\\nBaseline models • 569\\nPretrained models, model APIs, and AutoML • 569\\nModel evaluation and validation • 570\\nModel improvements • 571\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  572', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60bedb2a-3df2-47c8-bcdd-94983aff276d', embedding=None, metadata={'page_label': 'xx', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xx\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  572\\nChapter 19: TensorFlow 2 Ecosystem   575\\nTensorFlow Hub  �������������������������������������������������������������������������������������������������������������������  576\\nUsing pretrained models for inference • 577\\nTensorFlow Datasets �������������������������������������������������������������������������������������������������������������  580\\nLoad a TFDS dataset • 581\\nBuilding data pipelines using TFDS • 583\\nTensorFlow Lite  �������������������������������������������������������������������������������������������������������������������  585\\nQuantization • 585\\nFlatBuffers • 586\\nMobile converter • 586\\nMobile optimized interpreter • 586\\nSupported platforms • 587\\nArchitecture • 587\\nUsing TensorFlow Lite • 588\\nA generic example of an application • 588\\nUsing GPUs and accelerators • 589\\nAn example of an application • 589\\nPretrained models in TensorFlow Lite  �����������������������������������������������������������������������������������  591\\nImage classification • 592\\nObject detection • 594\\nPose estimation • 594\\nSmart reply • 594\\nSegmentation • 594\\nStyle transfer • 594\\nText classification • 595\\nLarge language models • 596\\nA note about using mobile GPUs • 596\\nAn overview of federated learning at the edge  ������������������������������������������������������������������������  597\\nTensorFlow FL APIs • 599\\nTensorFlow �js  �����������������������������������������������������������������������������������������������������������������������  600\\nVanilla TensorFlow.js • 600\\nConverting models • 607\\nPretrained models • 607\\nNode.js • 609\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  610', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54a50110-ffd2-4436-a694-0f1d3ee53105', embedding=None, metadata={'page_label': 'xxi', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xxi\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  611\\nChapter 20: Advanced Convolutional Neural Networks   613\\nComposing CNNs for complex tasks  ��������������������������������������������������������������������������������������  613\\nClassification and localization • 614\\nSemantic segmentation • 615\\nObject detection • 616\\nInstance segmentation • 619\\nApplication zoos with tf �Keras and TensorFlow Hub  ���������������������������������������������������������������  621\\nKeras Applications • 621\\nTensorFlow Hub • 621\\nAnswering questions about images (visual Q&A)  ��������������������������������������������������������������������  622\\nCreating a DeepDream network  ��������������������������������������������������������������������������������������������  625\\nInspecting what a network has learned  ���������������������������������������������������������������������������������  629\\nVideo  �����������������������������������������������������������������������������������������������������������������������������������  630\\nClassifying videos with pretrained nets in six different ways • 630\\nText documents  ��������������������������������������������������������������������������������������������������������������������  631\\nUsing a CNN for sentiment analysis • 632\\nAudio and music  �������������������������������������������������������������������������������������������������������������������  634\\nDilated ConvNets, WaveNet, and NSynth • 635\\nA summary of convolution operations  �����������������������������������������������������������������������������������  639\\nBasic CNNs • 639\\nDilated convolution • 640\\nTransposed convolution • 640\\nSeparable convolution • 640\\nDepthwise convolution • 640\\nDepthwise separable convolution • 641\\nCapsule networks  �����������������������������������������������������������������������������������������������������������������  641\\nWhat is the problem with CNNs? • 641\\nWhat is new with capsule networks? • 641\\nSummary  �����������������������������������������������������������������������������������������������������������������������������  643\\nReferences  ���������������������������������������������������������������������������������������������������������������������������  643\\nOther Books You May Enjoy   647\\nIndex   651', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='593291cf-205e-4cd0-9b95-6430e2481f48', embedding=None, metadata={'page_label': 'xxii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18c70277-d375-4a44-8ea5-c9f552b5536c', embedding=None, metadata={'page_label': 'xxiii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface\\nDeep Learning with TensorFlow and Keras, Third Edition, is a concise yet thorough introduction to modern \\nneural networks, artificial intelligence, and deep learning technologies designed especially for software \\nengineers and data scientists. The book is the natural follow-up of the books Deep Learning with Keras \\n[1] and TensorFlow 1.x Deep Learning Cookbook [2] previously written by the same authors.\\nThis book provides a very detailed panorama of the evolution of learning technologies over the past six \\nyears. The book presents dozens of working deep neural networks coded in Python using TensorFlow \\n2.x, a modular network library based on Keras-like APIs [1].\\nArtificial Intelligence ( AI) lays the ground for everything this book discusses. Machine Learning ( ML) \\nis a branch of AI, and Deep Learning (DL ) is in turn a subset of ML. This section will briefly discuss \\nthese three concepts, which you will regularly encounter throughout the rest of this book.\\nAI denotes any activity where machines mimic intelligent behaviors typically shown by humans. \\nMore formally, it is a research field in which machines aim to replicate cognitive capabilities such as \\nlearning behaviors, proactive interaction with the environment, inference and deduction, computer \\nvision, speech recognition, problem-solving, knowledge representation, and perception. AI builds on \\nelements of computer science, mathematics, and statistics, as well as psychology and other sciences \\nstudying human behaviors. There are multiple strategies for building AI. During the 1970s and 1980s, \\n“expert” systems became extremely popular. The goal of these systems was to solve complex problems \\nby representing the knowledge with a large number of manually defined if-then rules. This approach \\nworked for small problems on very specific domains, but it was not able to scale up for larger problems \\nand multiple domains. Later, AI focused more and more on methods based on statistical methods \\nthat are part of ML.\\nML is a subdiscipline of AI that focuses on teaching computers how to learn without the need to be \\nprogrammed for specific tasks. The key idea behind ML is that it is possible to create algorithms that \\nlearn from, and make predictions on, data. There are three different broad categories of ML:\\n• Supervised learning, in which the machine is presented with input data and the desired output, \\nand the goal is to learn from those training examples in such a way that meaningful predictions \\ncan be made for data that the machine has never observed before.\\n• Unsupervised learning, in which the machine is presented with input data only, and the \\nmachine has to subsequently find some meaningful structure by itself, with no external \\nsupervision or input.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43324c1b-720d-41a8-97be-a1863e0137bb', embedding=None, metadata={'page_label': 'xxiv', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxiv\\n• Reinforcement learning, in which the machine acts as an agent, interacting with the \\nenvironment. The machine is provided with “rewards” for behaving in a desired manner, and \\n“penalties” for behaving in an undesired manner. The machine attempts to maximize rewards \\nby learning to develop its behavior accordingly.\\nDL took the world by storm in 2012. During that year, the ImageNet 2012 challenge was launched with \\nthe goal of predicting the content of photographs using a subset of a large hand-labeled dataset. A \\ndeep learning model named AlexNet achieved a top-5 error rate of 15.3%, a significant improvement \\nwith respect to previous state-of-the-art results. According to the Economist, Suddenly people started \\nto pay attention, not just within the AI community but across the technology industry as a whole.\\nThat was only the beginning. Today, DL techniques are successfully applied in heterogeneous domains \\nincluding, but not limited to, healthcare, environment, green energy, computer vision, text analysis, \\nmultimedia, finance, retail, gaming, simulation, industry, robotics, and self-driving cars. In each of \\nthese domains, DL techniques can solve problems with a level of accuracy that was not possible using \\nprevious methods.\\nLooking back at the past eight years, it is fascinating and exciting to see the extent of the contributions \\nthat DL has made to science and industry. There is no reason to believe that the next eight years will \\nsee any less contribution; indeed, as the field of DL continues to advance, we anticipate that we’ll see \\neven more exciting and fascinating contributions provided by DL.\\nThis book introduces you to the magic of deep learning. We will start with simple models and \\nprogressively will introduce increasingly sophisticated models. The approach will always be hands-\\non, with a healthy dose of code to work with.\\nWho this book is for\\nIf you are a data scientist with experience in ML or an AI programmer with some exposure to neural \\nnetworks, you will find this book a useful entry point to DL with TensorFlow. If you are a software \\nengineer with a growing interest in the DL tsunami, you will find this book a foundational platform \\nto broaden your knowledge on the topic. Basic knowledge of Python is required for this book.\\nWhat this book covers\\nChapter 1, Neural Network Foundations with TF, is where we learn the basics of TensorFlow, an open-\\nsource library developed by Google for machine learning and deep learning. In addition, we introduce \\nthe basics of neural networks and deep learning, two areas of machine learning that had incredible \\ngrowth during the last few years. The idea behind this chapter is to provide all the tools needed to do \\nbasic but fully hands-on deep learning.\\nChapter 2, Regression and Classification, focuses on the fundamental tasks in ML techniques: regression \\nand classification. We will learn how to use TensorFlow to build simple, multiple, and multivariate \\nregression models. We will use logistic regression to solve a multi-class classification problem.\\nChapter 3, Convolutional Neural Networks, covers how to use deep learning ConvNets for recognizing \\nMNIST handwritten characters with high accuracy. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff98d220-7e91-4b03-b54f-0fc3439d0d78', embedding=None, metadata={'page_label': 'xxv', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxv\\nWe use the CIFAR 10 dataset to build a deep learning classifier with 10 categories, and the ImageNet \\ndataset to build an accurate classifier with 1,000 categories. In addition, we investigate how to use \\nlarge deep learning networks such as VGG16 and very deep networks such as InceptionV3. We will \\nconclude with a discussion on transfer learning\\nChapter 4,  Word Embeddings, is where we describe the origins of and theory behind distributed \\nrepresentations and word embeddings and chart the progress of word embeddings from static word-\\nbased embeddings more dynamic and expressive embeddings based on sentences and paragraphs. \\nWe also explore how the idea of word embeddings can be extended to include non-word sequences \\nas well, such as nodes in a graph or user sessions in a web application. The chapter also contains \\nmultiple examples of using word embeddings of various kinds.\\nChapter 5, Recurrent Neural Networks, describes an important architectural subclass of neural networks \\nthat are optimized for handling sequence data such as natural language or time series. We describe \\nthe important architectures in this genre, such as LSTM  (Long Short-Term Memory) and GRU  (Gated \\nRecurrent Unit) and show how they can be extended to handle bidirectional states and states across \\nbatches. We also provide examples of using RNNs with various topologies for specific tasks, such as \\ngenerating text, sentiment analysis, and part-of-speech tagging. We also describe the popular seq2seq \\narchitecture, which uses a pair of RNNs in an encoder-decoder pipeline to solve a variety of NLP tasks.\\nChapter 6, Transformers, covers transformers, a deep learning architecture that has revolutionized \\nthe traditional natural language processing field. We start by reviewing the key intuitions behind \\nthe architecture and various categories of transformers, together with a deep dive into the most \\npopular models. Then, we focus on implementations both based on the vanilla architecture and on \\npopular libraries, such as Hugging Face and TensorFlow Hub. After that, we briefly discuss evaluation, \\noptimization, and some of the best practices commonly adopted when using transformers. The last \\nsection is devoted to reviewing how transformers can be used to perform computer vision tasks, a \\ntotally different domain from NLP. That requires a careful definition of the attention mechanism. In \\nthe end, attention is all you need! And at the core of attention, there is nothing more than the cosine \\nsimilarity between vectors.\\nChapter 7, Unsupervised Learning, delves into unsupervised learning models. It will cover techniques \\nrequired for clustering and dimensionality reduction like PCA, k-means, and self-organized maps. \\nIt will go into the details of Boltzmann machines and their implementation using TensorFlow. The \\nconcepts covered will be extended to build Restricted Boltzmann Machines (RBMs ).\\nChapter 8, Autoencoders, describes autoencoders, a class of neural networks that attempt to recreate the \\ninput as its target. It will cover different varieties of autoencoders like sparse autoencoders, convolutional \\nautoencoders, and denoising autoencoders. The chapter will train a denoising autoencoder to remove \\nnoise from input images. It will demonstrate how autoencoders can be used to create MNIST digits. \\nIt will also cover the steps involved in building an LSTM autoencoder to generate sentence vectors. \\nFinally, we will learn how to build a variational autoencoder to generate images.\\nChapter 9, Generative Models, focuses on Generative Adversarial Networks (GANs ). We start with the \\nfirst proposed GAN model and use it to forge MNIST characters. The chapter shows you how to use \\ndeep convolutional GANs to create celebrity images. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4d786e9-fdb1-4ab5-9c5d-a2add3a1ec1a', embedding=None, metadata={'page_label': 'xxvi', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxvi\\nThe chapter discusses the various GAN architectures like SRGAN, InfoGAN, and CycleGAN. The \\nchapter covers a range of cool GAN applications. Finally, the chapter concludes with a TensorFlow \\nimplementation of CycleGAN to convert winter-summer images.\\nChapter 10, Self-Supervised Learning, provides an overview of various strategies used for self-supervised \\nlearning in computer vision, audio, and natural language processing. It covers self-prediction through \\nstrategies such as autoregressive generation, masked generation, relationship prediction, and hybrids \\nof these approaches. It also covers contrastive learning, a popular technique for self-supervised \\nlearning, and its application to various pretext tasks in various application domains.\\nChapter 11, Reinforcement Learning, focuses on reinforcement learning, covering the Q-learning \\nalgorithm and the Bellman equation. The chapter covers discounted rewards, exploration and \\nexploitation, and discount factors. It explains policy-based and model-based reinforcement learning. \\nWe will build a Deep Q-learning Network (DQN ) to play an Atari game. And finally, we learn how to \\ntrain agents using the policy gradient algorithm.\\nChapter 12, Probabilistic TensorFlow, introduces TensorFlow Probability, the library built over TensorFlow \\nto perform probabilistic reasoning and statistical analysis. The chapter demonstrates how to use \\nTensorFlow Probability to generate synthetic data. We will build Bayesian networks and perform \\ninference. The chapter also introduces the concept of uncertainty, aleatory and epistemic, and how \\nto calculate the uncertainty of your trained models.\\nChapter 13, An Introduction to AutoML, introduces AutoML, whose goal is to enable domain experts \\nwho are unfamiliar with machine learning technologies to use ML techniques easily. We will go \\nthrough a practical exercise using Google Cloud Platform and do quite a bit of hands-on work after \\nbriefly discussing the fundamentals. The chapter covers automatic data preparation, automatic feature \\nengineering, and automatic model generation. Then, we introduce AutoKeras and Google Cloud AutoML \\nwith its multiple solutions for table, vision, text, translation, and video processing.\\nChapter 14, The Math Behind Deep Learning, covers the math behind deep learning. This topic is quite \\nadvanced and not necessarily required for practitioners. However, it is recommended reading to \\nunderstand what is going on “under the hood” when we play with neural networks. We start with a \\nhistorical introduction, and then we will review the high school concept of derivatives and gradients \\nand introduce the gradient descent and backpropagation algorithms commonly used to optimize \\ndeep learning networks.\\nChapter 15, Tensor Processing Unit, discusses TPUs. TPUs are very special ASIC chips developed at \\nGoogle for executing neural network mathematical operations in an ultra-fast manner. The core of the \\ncomputation is a systolic multiplier that computes multiple dot products (row * column) in parallel, thus \\naccelerating the computation of basic deep learning operations. Think of a TPU as a special-purpose \\nco-processor for deep learning that is focused on matrix or tensor operations. We will review the four \\ngenerations of TPUs so far, plus an additional Edge TPU for IoT.\\nChapter 16, Other Useful Deep Learning Libraries, introduces other deep learning frameworks. We will \\nexplore Hugging Face, OpenAI’s GPT3, and DALL-E 2. The chapter introduces another very popular \\ndeep learning framework, PyTorch. We also cover H2O.ai and its AutoML module. The chapter also \\nbriefly discusses the ONNX open-source format for deep learning models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82ec3eea-65fb-4978-8536-c06f8fecebc4', embedding=None, metadata={'page_label': 'xxvii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Preface xxvii\\nChapter 17, Graph Neural Networks, introduces graphs and graph machine learning, with particular \\nemphasis on graph neural networks and the popular Deep Graph Library ( DGL ). We describe the \\ntheory behind various commonly used graph layers used in GNNs (and available in DGL) and provide \\nexamples of GNNs used for node classification, link prediction, and graph classification. We also show \\nhow to work with your own graph dataset and customize graph layers to create novel GNN architectures. \\nWe then cover more cutting-edge advances in the field of Graph ML, such as heterogeneous graphs \\nand temporal graphs.\\nChapter 18 , Machine Learning Best Practices , focuses on strategies and practices to follow to get the \\nbest model in training and production. The chapter discusses the best practices from two different \\nperspectives: the best practices for the data and the best practices with respect to models.\\nChapter 19, TensorFlow 2 Ecosystem, lays out the different components of the TensorFlow ecosystem. \\nWe introduce TensorFlow Hub, a repository for pretrained deep learning models. The chapter talks \\nabout TensorFlow Datasets – a collection of ready-to-use datasets. We will also talk about TensorFlow \\nLite and TensorFlow JS – the framework for mobile and embedded systems and the web. Finally, the \\nchapter talks about federated learning, a decentralized machine learning framework.\\nChapter 20, Advanced Convolutional Neural Networks, shows more advanced uses for convolutional \\nneural networks ( CNNs). We will explore how CNNs can be applied within the areas of computer vision, \\nvideo, textual documents, audio, and music. We’ll conclude with a section summarizing convolution \\noperations.\\nDownload the example code files\\nThe code bundle for the book is hosted on GitHub at https://packt.link/dltf . We also have \\nother code bundles from our rich catalog of books and videos available at https://github.com/\\nPacktPublishing/ . Check them out!\\nDownload the color images\\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. You \\ncan download it here: https://static.packt-cdn.com/downloads/9781803232911_ColorImages.pdf .\\nConventions used\\nThere are a number of text conventions used throughout this book.\\nCodeInText : Indicates code words in the text, database table names, folder names, filenames, file \\nextensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “Each neuron \\ncan be initialized with specific weights via the 'kernel_initializer'  parameter.”\\nA block of code is set as follows:\\n# Build the model.\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n            input_shape=(RESHAPED,),\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6a862fb-aed9-4531-971a-c76f9ed6b7c6', embedding=None, metadata={'page_label': 'xxviii', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Preface xxviii\\n            name= 'dense_layer' , \\n            activation= 'softmax' ))\\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items \\nare highlighted:\\n# Build the model.\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n            input_shape=(RESHAPED,),\\n            name='dense_layer' , \\n            activation= 'softmax' ))\\nAny command-line input or output is written as follows:\\npip install gym\\nBold : Indicates a new term, an important word, or words that you see on the screen. For instance, \\nwords in menus or dialog boxes appear in the text like this. For example: “A Deep Convolutional Neural \\nNetwork (DCNN) consists of many neural network layers.”\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: Email feedback@packtpub.com  and mention the book’s title in the subject of your \\nmessage. If you have questions about any aspect of this book, please email us at questions@packtpub.\\ncom.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. \\nIf you have found a mistake in this book, we would be grateful if you reported this to us. Please visit \\nhttp://www.packtpub.com/submit-errata , click Submit Errata, and fill in the form.\\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would \\nbe grateful if you would provide us with the location address or website name. Please contact us at \\ncopyright@packtpub.com  with a link to the material.Warnings or important notes appear like this.\\nTips and tricks appear like this.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eb6fad68-1bd3-4786-b105-cb7065c33b54', embedding=None, metadata={'page_label': 'xxix', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxix\\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you are \\ninterested in either writing or contributing to a book, please visit http://authors.packtpub.com .\\nReferences\\n1. Deep Learning with Keras: Implementing deep learning models and neural networks with the power \\nof Python, Paperback – 26 Apr 2017, Antonio Gulli, Sujit Pal\\n2. TensorFlow 1.x Deep Learning Cookbook: Over 90 unique recipes to solve artificial-intelligence driven \\nproblems with Python, Antonio Gulli, Amita Kapoor\\nShare your thoughts\\nOnce you’ve read Deep Learning with TensorFlow and Keras, Third Edition, we’d love to hear your thoughts! \\nPlease click here to go straight to the Amazon review page  for this book and share your feedback.\\nYour review is important to us and the tech community and will help us make sure we’re delivering \\nexcellent quality content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86190e58-4326-42a4-83c1-06d74a25c450', embedding=None, metadata={'page_label': 'xxx', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0356c1b7-f1dc-4c46-8c09-59a2931f08b2', embedding=None, metadata={'page_label': '1', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1\\nNeural Network Foundations with \\nTF\\nIn this chapter, we learn the basics of TensorFlow, an open-source library developed by Google for \\nmachine learning and deep learning. In addition, we introduce the basics of neural networks and \\ndeep learning, two areas of machine learning that have had incredible Cambrian growth during the \\nlast few years. The idea behind this chapter is to provide all the tools needed to do basic but fully \\nhands-on deep learning.\\nWe will learn:\\n• What TensorFlow and Keras are\\n• An introduction to neural networks\\n• What the perceptron and multi-layer perceptron are\\n• A real example: recognizing handwritten digits\\nLet’s begin!\\nWhat is TensorFlow (TF)?\\nTensorFlow is a powerful open-source software library developed by the Google Brain Team for deep \\nneural networks, the topic covered in this book. It was first made available under the Apache 2.0 License \\nin November 2015 and has since grown rapidly; as of May 2022, its GitHub repository ( https://github.\\ncom/tensorflow/tensorflow ) has more than 129,000 commits, with roughly 3,100 contributors. This \\nin itself provides a measure of the popularity of TensorFlow.All the code files for this chapter can be found at https://packt.link/dltfchp1 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f46abb1-9931-4d44-a6dc-ee98126b1825', embedding=None, metadata={'page_label': '2', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 2\\nLet us first learn what exactly TensorFlow is and why it is so popular among deep neural network \\nresearchers and engineers. Google calls it “an open-source software library for machine intelligence,” \\nbut since there are so many other deep learning libraries like PyTorch ( https://pytorch.org/ ), Caffe \\n(https://caffe.berkeleyvision.org/ ), and MXNet ( https://mxnet.apache.org/ ), what makes \\nTensorFlow special? Most other deep learning libraries, like TensorFlow, have auto-differentiation \\n(a useful mathematical tool used for optimization), many are open-source platforms. Most of them \\nsupport the CPU/GPU option, have pretrained models, and support commonly used NN architectures \\nlike recurrent neural networks, convolutional neural networks, and deep belief networks. So, what \\nelse is there in TensorFlow? Let me list the top features:\\n• It works with all popular languages such as Python, C++, Java, R, and Go. TensorFlow provides \\nstable Python and C++ APIs, as well as a non-guaranteed backward-compatible API for other \\nlanguages. \\n• Keras – a high-level neural network API that has been integrated with TensorFlow (in 2.0 Keras \\nbecame the standard API for interacting with TensorFlow). This API specifies how software \\ncomponents should interact.\\n• TensorFlow allows model deployment and ease of use in production.\\n• Most importantly, TensorFlow has very good community support. \\nThe number of stars on GitHub (see Figure 1.1) is a measure of popularity for all open-source projects. \\nAs of May 2022, TensorFlow, Keras, and PyTorch have 165K, 55K, and 56K stars respectively, which \\nmakes TensorFlow the most popular framework for machine learning:\\nFigure 1.1: Number of stars for various deep learning projects on GitHub', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6bc8ee49-fded-4908-868b-8005863485a1', embedding=None, metadata={'page_label': '3', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 3\\nWhat is Keras?\\nKeras is a beautiful API for composing building blocks to create and train deep learning models. Keras \\ncan be integrated with multiple deep learning engines including Google TensorFlow, Microsoft CNTK, \\nAmazon MXNet, and Theano. Starting with TensorFlow 2.0, Keras, the API developed by François \\nChollet, has been adopted as the standard high-level API, largely simplifying coding and making \\nprogramming more intuitive.\\nIntroduction to neural networks\\nArtificial neural networks (briefly, “nets” or ANNs) represent a class of machine learning models \\nloosely inspired by studies about the central nervous systems of mammals. Each ANN is made up \\nof several interconnected “neurons,” organized in “layers.” Neurons in one layer pass messages to \\nneurons in the next layer (they “fire,” in jargon terms) and this is how the network computes things. \\nInitial studies were started in the early 1950s with the introduction of the “perceptron” [1], a two-layer \\nnetwork used for simple operations, and further expanded in the late 1960s with the introduction of \\nthe “back-propagation” algorithm used for efficient multi-layer network training (according to [2] and \\n[3]). Some studies argue that these techniques have roots dating further back than normally cited [4].\\nNeural networks were a topic of intensive academic studies up until the 1980s, at which point other \\nsimpler approaches became more relevant. However, there has been a resurgence of interest since \\nthe mid 2000s, mainly thanks to three factors: a breakthrough fast learning algorithm proposed by G. \\nHinton [3], [5], and [6]; the introduction of GPUs around 2011 for massive numeric computation; and \\nthe availability of big collections of data for training.\\nThese improvements opened the route for modern “deep learning,” a class of neural networks \\ncharacterized by a significant number of layers of neurons that are able to learn rather sophisticated \\nmodels, based on progressive levels of abstraction. People began referring to it as “deep” when it \\nstarted utilizing 3–5 layers a few years ago. Now, networks with more than 200 layers are commonplace!\\nThis learning via progressive abstraction resembles vision models that have evolved over millions \\nof years within the human brain. Indeed, the human visual system is organized into different layers. \\nFirst, our eyes are connected to an area of the brain named the visual cortex (V1), which is located \\nin the lower posterior part of our brain. This area is common to many mammals and has the role of \\ndiscriminating basic properties like small changes in visual orientation, spatial frequencies, and colors.\\nIt has been estimated that V1 consists of about 140 million neurons, with tens of billions of connections \\nbetween them. V1 is then connected to other areas, V2, V3, V4, V5, and V6 doing progressively more \\ncomplex image processing and recognition of more sophisticated concepts, such as shapes, faces, \\nanimals, and many more. It has been estimated that there are ~16 billion human cortical neurons and \\nabout 10–25% of the human cortex is devoted to vision [7]. Deep learning has taken some inspiration \\nfrom this layer-based organization of the human visual system: early artificial neuron layers learn \\nbasic properties of images while deeper layers learn more sophisticated concepts.\\nThis book covers several major aspects of neural networks by providing working nets in TensorFlow. \\nSo, let’s start!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb72711c-2492-4281-8418-e841447b864b', embedding=None, metadata={'page_label': '4', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 4\\nPerceptron\\nThe “perceptron” is a simple algorithm that, given an input vector x  of m  values (x 1, x2,..., and x m), \\noften called input features or simply features, outputs either a 1 (“yes”) or a 0 (“no”). Mathematically, \\nwe define a function:\\n𝑓𝑓(𝑥𝑥)={1     𝑤𝑤𝑥𝑥 +𝑏𝑏𝑏𝑏\\n𝑏     𝑜𝑜𝑜𝑜 ℎ𝑒𝑒𝑒𝑒𝑤𝑤𝑒𝑒𝑒𝑒𝑒𝑒   \\nWhere w is a vector of weights, 𝑤𝑤𝑤𝑤𝑤   is the dot product 𝛴𝛴𝑗𝑗𝑗𝑗𝑚𝑚𝑤𝑤𝑗𝑗𝑥𝑥𝑗𝑗 , and b is the bias. If you remember \\nelementary geometry, wx + b defines a boundary hyperplane that changes position according to the \\nvalues assigned to w and b. Note that a hyperplane is a subspace whose dimension is one fewer than \\nthat of its ambient space. See (Figure 1.2) for an example:\\nFigure 1.2: An example of a hyperplane\\nIn other words, this is a very simple but effective algorithm! For example, given three input features, \\nthe amounts of red, green, and blue in a color, the perceptron could try to decide whether the color \\nis “white” or not.\\nNote that the perceptron cannot express a “maybe” answer. It can answer “yes” (1) or “no” (0) if we \\nunderstand how to define w  and b . This is the “training” process that will be discussed in the following \\nsections.\\nOur first example of TensorFlow code\\nThere are three ways of creating a model in tf.keras : sequential API, functional API, and model \\nsubclassing. In this chapter, we will use the simplest one, Sequential() , while the other two are \\ndiscussed in Chapter 2, Regression and Classification. A Sequential()  model is a linear pipeline (a \\nstack) of neural network layers. This code fragment defines a single layer with 10 artificial neurons \\nthat expect 784 input variables (also known as features). Note that the net is “dense,” meaning that \\neach neuron in a layer is connected to all neurons located in the previous layer, and to all the neurons \\nin the following layer:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nNB_CLASSES = 10\\nRESHAPED = 784\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n            input_shape=(RESHAPED,), kernel_initializer= 'zeros',\\n            name= 'dense_layer' , activation= 'softmax' ))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fcfe6e58-399f-47cd-96ed-77aa3170c040', embedding=None, metadata={'page_label': '5', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 1 5\\nEach neuron can be initialized with specific weights via the 'kernel_initializer'  parameter. There \\nare a few choices, the most common of which are listed below: \\n• random_uniform : Weights are initialized to uniformly random small values in the range (-0.05, \\n0.05).\\n• random_normal : Weights are initialized according to a Gaussian distribution, with zero mean \\nand a small standard deviation of 0.05. For those of you who are not familiar with a Gaussian \\ndistribution, think about a symmetric “bell curve” shape.\\n• zero : All weights are initialized to zero.\\nA full list is available online ( https://www.tensorflow.org/api_docs/python/tf/keras/\\ninitializers ).\\nMulti-layer perceptron: our first example of a network\\nIn this chapter, we present our first example of a network with multiple dense layers. Historically, \\n“perceptron” was the name given to the model having one single linear layer, and as a consequence, if \\nit has multiple layers, we call it a Multi-Layer Perceptron (MLP ). Note that the input and the output \\nlayers are visible from the outside, while all the other layers in the middle are hidden – hence the \\nname hidden layers. In this context, a single layer is simply a linear function and the MLP is therefore \\nobtained by stacking multiple single layers one after the other:\\nFigure 1.3: An example of multiple layer perceptron\\nIn Figure 1.3 each node in the first hidden layer receives an input and “fires” (0,1) according to the \\nvalues of the associated linear function. Then the output of the first hidden layer is passed to the \\nsecond layer where another linear function is applied, the results of which are passed to the final \\noutput layer consisting of one single neuron. It is interesting to note that this layered organization \\nvaguely resembles the organization of the human vision system, as we discussed earlier.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d44bb28b-9152-4dd2-bc2d-c2be009a7eda', embedding=None, metadata={'page_label': '6', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 6\\nProblems in training the perceptron and solution\\nLet’s consider a single neuron; what are the best choices for the weight w and the bias b? Ideally, we \\nwould like to provide a set of training examples and let the computer adjust the weight and the bias \\nin such a way that the errors produced in the output are minimized.\\nIn order to make this a bit more concrete, let’s suppose that we have a set of images of cats and another \\nseparate set of images not containing cats. Suppose that each neuron receives input from the value of \\na single pixel in the images. While the computer processes those images, we would like our neuron to \\nadjust its weights and its bias so that we have fewer and fewer images wrongly recognized.\\nThis approach seems very intuitive, but it requires that a small change in the weights (or the bias) \\ncauses only a small change in the outputs. Think about it: if we have a big output jump, we cannot \\nlearn progressively. After all, kids learn little by little. Unfortunately, the perceptron does not show \\nthis “little-by-little” behavior. A perceptron is either a 0 or 1, and that’s a big jump that will not help \\nin learning (see Figure 1.4):\\n \\nFigure 1.4: Example of a perceptron – either a 0 or 1\\nWe need something different, something smoother. We need a function that progressively changes \\nfrom 0 to 1 with no discontinuity. Mathematically, this means that we need a continuous function \\nthat allows us to compute the derivative. You might remember that in mathematics the derivative is \\nthe amount by which a function is changing at a given point. For functions with input given by real \\nnumbers, the derivative is the slope of the tangent line at a point on a graph. Later in this chapter we \\nwill see why derivatives are important for learning, when we will talk about gradient descent. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6ab8560-7ad8-4fb3-8f4b-4b71d7680f3f', embedding=None, metadata={'page_label': '7', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 7\\nActivation function: sigmoid\\nThe sigmoid function, defined as 𝜎𝜎(𝑥𝑥)=1\\n1+𝑒𝑒−𝑥𝑥  and represented in the image below, has small output \\nchanges in the range (0, 1) when the input varies in the range (−∞,∞) . Mathematically the function \\nis continuous. A typical sigmoid function is represented in Figure 1.5:\\nFigure 1.5: A sigmoid function with output in the range (0,1)\\nA neuron can use the sigmoid for computing the nonlinear function 𝜎𝜎(𝑧𝑧𝑧𝑤𝑤𝑤𝑤+𝑏𝑏) . Note that if z = wx \\n+ b is very large and positive, then 𝑒𝑒−𝑧𝑧→0  so 𝜎𝜎𝜎𝜎𝜎𝜎 𝜎 𝜎  , while if z = wx + b is very large and negative, \\nthen 𝑒𝑒−𝑧𝑧→∞   so 𝜎𝜎𝜎𝜎𝜎𝜎 𝜎 𝜎  . In other words, a neuron with sigmoid activation has a behavior similar to \\nthe perceptron, but the changes are gradual and output values such as 0.5539 or 0.123191 are perfectly \\nlegitimate. In this sense a sigmoid neuron can answer “maybe.”\\nActivation function: tanh\\nAnother useful activation function is tanh. It is defined as tanh (𝑧𝑧)=𝑒𝑒𝑧𝑧 − 𝑒𝑒−𝑧𝑧\\n𝑒𝑒𝑧𝑧 + 𝑒𝑒−𝑧𝑧  whose shape is shown in \\nFigure 1.6. Its outputs range from -1 to 1:\\nFigure 1.6: Tanh activation function\\nActivation function: ReLU\\nThe “sigmoid” is not the only kind of smooth activation function used for neural networks. Recently, \\na very simple function named ReLU  (REctified Linear Unit) became very popular because it helps \\naddress some problems of optimizations observed with sigmoids. We will discuss these problems in \\nmore detail when we talk about vanishing gradient in Chapter 5, Recurrent Neural Networks. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f594e58-84c5-4185-88b6-057df2b3cce9', embedding=None, metadata={'page_label': '8', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 8\\nA ReLU is simply defined as f(x) = max(0, x) and the nonlinear function is represented in Figure 1.7. As \\nwe can see, the function is zero for negative values and it grows linearly for positive values. The ReLU \\nis also very simple to implement (generally three instructions are enough), while the sigmoid is a few \\norders of magnitude more. This helps to squeeze the neural networks onto an early GPU:\\nFigure 1.7: A ReLU function\\nTwo additional activation functions: ELU and Leaky ReLU\\nSigmoid and ReLU are not the only activation functions used for learning.\\nExponential Linear Unit (ELU ) is defined as 𝑓𝑓(𝛼𝛼𝛼 𝛼𝛼)={𝛼𝛼(𝑒𝑒𝑥𝑥−1)   if  𝛼𝛼𝑥 𝑥 \\n𝛼𝛼                    if   𝛼𝛼 𝑥 𝑥  for 𝛼𝛼𝛼𝛼   and  its plot is \\nrepresented in Figure 1.8:\\n \\nFigure 1.8: An ELU function', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e2f07c3-24bb-458c-983f-35a2d2e82a5a', embedding=None, metadata={'page_label': '9', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 9\\nLeakyReLU is defined as 𝑓𝑓(𝛼𝛼𝛼 𝛼𝛼)={𝛼𝛼𝛼𝛼    if  𝛼𝛼 𝑥 𝑥  \\n𝛼𝛼      if   𝛼𝛼 𝑥 𝑥 for 𝛼𝛼𝛼𝛼   and its plot is represented in Figure 1.9:\\nFigure 1.9: A LeakyReLU function\\nBoth the functions allow small updates if x is negative, which might be useful in certain conditions.\\nActivation functions\\nSigmoid, Tanh, ELU, Leaky ReLU, and ReLU are generally called activation functions in neural network \\njargon. In the gradient descent section, we will see that those gradual changes typical of sigmoid and \\nReLU functions are the basic building blocks to developing a learning algorithm that adapts little by \\nlittle, by progressively reducing the mistakes made by our nets. An example of using the activation \\nfunction σ  with the (x 1, x2,..., xm) input vector, the (w 1, w2,..., wm) weight vector, the b bias, and the Σ  \\nsummation is given in Figure 1.10 (note that TensorFlow supports many activation functions, a full \\nlist of which is available online):\\nFigure 1.10: An example of an activation function applied after a linear function ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85996984-e006-4b15-812a-497d399aa83e', embedding=None, metadata={'page_label': '10', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 10\\nIn short: what are neural networks after all?\\nIn one sentence, machine learning models are a way to compute a function that maps some inputs to \\ntheir corresponding outputs. The function is nothing more than a number of addition and multiplication \\noperations. However, when combined with a nonlinear activation and stacked in multiple layers, these \\nfunctions can learn almost anything [8]. We also need a meaningful metric capturing what we want \\nto optimize (this being the so-called loss function that we will cover later in the book), enough data \\nto learn from, and sufficient computational power.\\nNow, it might be beneficial to stop one moment and ask ourselves what “learning” really is? Well, \\nwe can say for our purposes that learning is essentially a process aimed at generalizing established \\nobservations [9] to predict future results. So, in short, this is exactly the goal we want to achieve with \\nneural networks.\\nA real example: recognizing handwritten digits\\nIn this section we will build a network that can recognize handwritten numbers. To achieve this goal, \\nwe use MNIST ( http://yann.lecun.com/exdb/mnist/ ), a database of handwritten digits made up of a \\ntraining set of 60,000 examples, and a test set of 10,000 examples. The training examples are annotated \\nby humans with the correct answer. For instance, if the handwritten digit is the number “3,” then 3 is \\nsimply the label associated with that example.\\nIn machine learning, when a dataset with correct answers is available, we say that we can perform a \\nform of supervised learning. In this case we can use training examples for improving our net. Testing \\nexamples also have the correct answer associated with each digit. In this case, however, the idea is to \\npretend that the label is unknown, let the network do the prediction, and then later on reconsider the \\nlabel to evaluate how well our neural network has learned to recognize digits. Unsurprisingly, testing \\nexamples are just used to test the performance of our net.\\nEach MNIST image is in grayscale and consists of 28 x 28 pixels. A subset of these images of numbers \\nis shown in Figure 1.11:\\n \\nFigure 1.11: A collection of MNIST images\\nOne hot-encoding (OHE)\\nWe will use OHE as a simple tool to encode information used inside neural networks. In many \\napplications, it is convenient to transform categorical (non-numerical) features into numerical \\nvariables. For instance, the categorical feature digit with value d in [0–9] can be encoded into a binary \\nvector with 10 positions, which has always a 0 value except the d - th position where a 1 is present. For \\nexample, the digit 3 can be encoded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ded39bd-6ec8-4db4-b135-d8f56cca255f', embedding=None, metadata={'page_label': '11', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 11\\nThis type of representation is called One-Hot-Encoding (OHE ) or sometimes simply one-hot, and is \\nvery common in data mining when the learning algorithm is specialized in dealing with numerical \\nfunctions.\\nDefining a simple neural net in TensorFlow\\nIn this section we use TensorFlow to define a network that recognizes MNIST handwritten digits. We \\nstart with a very simple neural network and then progressively improve it.\\nFollowing Keras’ style, TensorFlow provides suitable libraries ( https://www.tensorflow.org/api_\\ndocs/python/tf/keras/datasets ) for loading the dataset and splits it into training sets, X_train , \\nused for fine-tuning our net, and test sets, X_test , used for assessing the performance. Later in the \\nchapter, we are going to formally define what a training set, a validation set, and a test set are. For now, \\nwe just need to know that a training set is the dataset used to let our neural network learn from data \\nexamples. Data is converted into float32  to use 32-bit precision when training a neural network and \\nnormalized to the range [0,1]. In addition, we load the true labels into Y_train  and Y_test  respectively, \\nand perform one-hot encoding on them. Let’s see the code.\\nFor now, do not focus too much on understanding why certain parameters have specific assigned \\nvalues, as these choices will be discussed throughout the rest of the book. Intuitively, an epoch defines \\nhow long the training should last, BATCH_SIZE  is the number of samples you feed in your network at \\na time, and the validation sample is the amount of data reserved for checking or proving the validity \\nof the training process. The reason why we picked EPOCHS = 200 , BATCH_SIZE = 128 , VALIDATION_\\nSPLIT=0.2 , and N_HIDDEN = 128  will be clearer later in this chapter when we will explore different \\nvalues and discuss hyperparameters optimization. Let’s see our first code fragment of a neural network  \\nin TensorFlow. Reading is intuitive but you will find a detailed explanation in the upcoming pages:\\nimport tensorflow as tf\\nimport numpy as np\\nfrom tensorflow import keras\\n# Network and training parameters.\\nEPOCHS = 200\\nBATCH_SIZE = 128\\nVERBOSE = 1\\nNB_CLASSES = 10   # number of outputs = number of digits\\nN_HIDDEN = 128\\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\\n# Loading MNIST dataset.\\n# verify\\n# You can verify that the split between train and test is 60,000, and 10,000 \\nrespectively. \\n# Labels have one-hot representation.is automatically applied\\nmnist = keras.datasets.mnist', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4116c327-e712-4c0e-997d-bdb304387349', embedding=None, metadata={'page_label': '12', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 12\\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\\n# X_train is 60000 rows of 28x28 values; we  --> reshape it to 60000 x 784.\\nRESHAPED = 784\\n#\\nX_train = X_train.reshape( 60000, RESHAPED)\\nX_test = X_test.reshape( 10000, RESHAPED)\\nX_train = X_train.astype( 'float32' )\\nX_test = X_test.astype( 'float32' )\\n# Normalize inputs to be within in [0, 1].\\nX_train /= 255\\nX_test /= 255\\nprint(X_train.shape[ 0], 'train samples' )\\nprint(X_test.shape[ 0], 'test samples' )\\n# One-hot representation of the labels.\\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\\nYou can see from the above code that the input layer has a neuron associated to each pixel in the image \\nfor a total of 28 x 28=784 neurons, one for each pixel in the MNIST images.\\nTypically, the values associated with each pixel are normalized in the range [0,1] (which means that \\nthe intensity of each pixel is divided by 255, the maximum intensity value). The output can be one of \\nten classes, with one class for each digit.\\nThe final layer is a single neuron with the activation function 'softmax' , which is a generalization of \\nthe sigmoid function. As discussed earlier, a sigmoid function output is in the range (0, 1) when the \\ninput varies in the range (−∞,∞) . Similarly, a softmax “squashes” a K-dimensional vector of arbitrary \\nreal values into a K-dimensional vector of real values in the range (0, 1), so that they all add up to 1. \\nIn our case, it aggregates ten answers provided by the previous layer with ten neurons. What we have \\njust described is implemented with the following code:\\n# Build the model.\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n            input_shape=(RESHAPED,),\\n            name= 'dense_layer' , \\n            activation= 'softmax' ))\\nOnce we define the model, we have to compile it so that it can be executed by TensorFlow. There are \\na few choices to be made during compilation. Firstly, we need to select an optimizer, which is the \\nspecific algorithm used to update weights while we train our model. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed68a4ef-0bcd-4e89-91ac-4152f5deb9bb', embedding=None, metadata={'page_label': '13', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 13\\nA complete list of optimizers is at https://www.tensorflow.org/api_docs/python/tf/keras/\\noptimizers . Second, we need to select an objective function, which is used by the optimizer to navigate \\nthe space of weights (frequently objective functions are called either loss functions or cost functions and \\nthe process of optimization is defined as a process of loss minimization). Third, we need to evaluate \\nthe trained model. \\nSome common choices for objective functions (a complete list of loss functions is at https://www.\\ntensorflow.org/api_docs/python/tf/keras/losses ) are: \\n• mse, which defines the mean squared error between the predictions and the true values. \\nMathematically if d is a vector of predictions and y is the vector of n observed values, then \\n𝑀𝑀𝑀𝑀𝑀𝑀=1\\n𝑛𝑛∑(𝑑𝑑𝑑𝑑𝑑)2 𝑛𝑛\\n𝑖𝑖𝑖1  . Note that this objective function is the average of all the mistakes made \\nin each prediction. If a prediction is far off from the true value, then this distance is made more \\nevident by the squaring operation. In addition, the square can add up the error regardless of \\nwhether a given value is positive or negative.\\n• binary_crossentropy , which defines the binary logarithmic loss. Suppose that our \\nmodel predicts p  while the target is c, then the binary cross-entropy is defined as \\n𝐿𝐿(𝑝𝑝,𝑐𝑐)=−𝑐𝑐ln(𝑝𝑝)−(1−𝑐𝑐)ln(1−𝑝𝑝) . Note that this objective function is suitable for binary \\nlabels prediction.\\n• categorical_crossentropy , which defines the multiclass logarithmic loss. Categorical \\ncross-entropy compares the distribution of the predictions with the true distribution, with \\nthe probability of the true class set to 1 and 0 for the other classes. If the true class is c and the \\nprediction is y, then the categorical cross-entropy is defined as:\\n𝐿𝐿(𝑐𝑐,𝑝𝑝)=−∑𝑐𝑐 𝑖𝑖ln(𝑝𝑝𝑖𝑖)\\n𝑖𝑖 \\nOne way to think about multi-class logarithm loss is to consider the true class represented as \\na one-hot encoded vector, and the closer the model’s outputs are to that vector, the lower the \\nloss. Note that this objective function is suitable for multi-class label predictions. It is also the \\ndefault choice with softmax activation. A complete list of loss functions is at https://www.\\ntensorflow.org/api_docs/python/tf/keras/losses .\\nSome common choices for metrics (a complete list of metrics is at https://www.tensorflow.org/\\napi_docs/python/tf/keras/metrics ) are:\\n• Accuracy, defined as the proportion of correct predictions with respect to the total number \\nof predictions.\\n• Precision, defined as the proportion of correct positive predictions with respect to the number \\nof correct and incorrect positive predictions.\\n• Recall, defined as the proportion of correct positive predictions with respect to the actual \\nnumber of positive predictions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04eebe03-084b-4c75-9c9f-ee619bc51af8', embedding=None, metadata={'page_label': '14', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 14\\nA complete list of metrics is at https://www.tensorflow.org/api_docs/python/tf/keras/metrics . \\nMetrics are similar to objective functions, with the only difference being that they are not used for \\ntraining a model, only for evaluating the model. However, it is important to understand the difference \\nbetween metrics and objective functions. As discussed, the loss function is used to optimize your \\nnetwork. This is the function minimized by the selected optimizer. Instead, a metric is used to judge \\nthe performance of your network. This is only for you to run an evaluation, and it should be separated \\nfrom the optimization process. On some occasions, it would be ideal to directly optimize for a specific \\nmetric. However, some metrics are not differentiable with respect to their inputs, which precludes \\nthem from being used directly.\\nWhen compiling a model in TensorFlow, it is possible to select the optimizer, the loss function, and \\nthe metric used together with a given model:\\n# Compiling the model.\\nmodel.compile(optimizer= 'SGD', \\n              loss= 'categorical_crossentropy' ,\\n              metrics=[ 'accuracy' ])\\nStochastic Gradient Descent (SGD ) is a particular kind of optimization algorithm used to reduce the \\nmistakes made by neural networks after each training epoch. We will review SGD and other optimization \\nalgorithms in the next chapters. Once the model is compiled, it can then be trained with the fit()  \\nmethod, which specifies a few parameters:\\n• epochs  is the number of times the model is exposed to the training set. At each iteration the \\noptimizer tries to adjust the weights so that the objective function is minimized.\\n• batch_size  is the number of training instances observed before the optimizer performs a \\nweight update; there are usually many batches per epoch.\\nTraining a model in TensorFlow is very simple:\\n# Training the model.\\nmodel.fit(X_train, Y_train,\\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\\nNote that we’ve reserved part of the training set for validation. The key idea is that we reserve a part \\nof the training data for measuring the performance on the validation while training. This is a good \\npractice to follow for any machine learning task, and one that we will adopt in all of our examples. \\nPlease note that we will return to validation later in this chapter when we will talk about overfitting.\\nOnce the model is trained, we can evaluate it on the test set that contains new examples never seen \\nby the model during the training phase. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f5ef7f2-91e7-4f7a-b421-5a5204a0e97c', embedding=None, metadata={'page_label': '15', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 15\\nNote that, of course, the training set and the test set are rigorously separated. There is no point in \\nevaluating a model on an example that was already used for training. In TF we can use the method \\nevaluate(X_test, Y_test)  to compute the test_loss  and the test_acc :\\n#evaluate the model\\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\\nprint(\\'Test accuracy:\\' , test_acc)\\nCongratulations! You have just defined your first neural network in TensorFlow. A few lines of code \\nand your computer should be able to recognize handwritten numbers. Let’s run the code and see \\nwhat the performance is.\\nRunning a simple TensorFlow net and establishing a baseline\\nSo, let’s see what happens when we run the code: \\nModel: \"sequential\"\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #    \\n=================================================================\\ndense_layer (Dense)         (None, 10)                7850       \\n                                                                 \\n=================================================================\\nTotal params: 7,850\\nTrainable params: 7,850\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 48000 samples, validate on 12000 samples\\nEpoch 1/200\\n48000/48000 [==============================] - 1s 31us/sample - loss: 2.1276 - \\naccuracy: 0.2322 - val_loss: 1.9508 - val_accuracy: 0.3908\\nEpoch 2/200\\n48000/48000 [==============================] - 1s 23us/sample - loss: 1.8251 - \\naccuracy: 0.5141 - val_loss: 1.6848 - val_accuracy: 0.6277\\nEpoch 3/200\\n48000/48000 [==============================] - 1s 25us/sample - loss: 1.5992 - \\naccuracy: 0.6531 - val_loss: 1.4838 - val_accuracy: 0.7150\\nEpoch 4/200\\n48000/48000 [==============================] - 1s 27us/sample - loss: 1.4281 - \\naccuracy: 0.7115 - val_loss: 1.3304 - val_accuracy: 0.7551\\nEpoch 5/200', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51f26c2f-a964-4489-afa7-ed9be9308ec7', embedding=None, metadata={'page_label': '16', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 16\\nFirst the net architecture is dumped and we can see the different types of layers used, their output shape, \\nhow many parameters (i.e., how many weights) they need to optimize, and how they are connected. \\nThen, the network is trained on 48K samples, and 12K are reserved for validation. Once the neural \\nmodel is built, it is then tested on 10K samples. For now we won’t go into the internals of how the \\ntraining happens, but we can see that the program runs for 200 iterations and each time accuracy \\nimproves. When the training ends, we test our model on the test set and we achieve about 89.96% \\naccuracy on the training dataset, 90.70% on validation, and 90.71% on test: \\nEpoch 199/200\\n48000/48000 [==============================] - 1s 22us/sample - loss: 0.3684 - \\naccuracy: 0.8995 - val_loss: 0.3464 - val_accuracy: 0.9071\\nEpoch 200/200\\n48000/48000 [==============================] - 1s 23us/sample - loss: 0.3680 - \\naccuracy: 0.8996 - val_loss: 0.3461 - val_accuracy: 0.9070\\n10000/10000 [==============================] - 1s 54us/sample - loss: 0.3465 - \\naccuracy: 0.9071\\nTest accuracy: 0.9071\\nThis means that nearly 1 in 10 images are incorrectly classified. We can certainly do better than that.\\nImproving the simple net in TensorFlow with hidden layers\\nOkay, we have a baseline of accuracy of 89.96% on the training dataset, 90.70% on validation, and \\n90.71% on test. It is a good starting point, but we can improve it. Let’s see how.\\nAn initial improvement is to add additional layers to our network because these additional neurons \\nmight intuitively help to learn more complex patterns in the training data. In other words, additional \\nlayers add more parameters, potentially allowing a model to memorize more complex patterns. So, \\nafter the input layer, we have a first dense layer with N_HIDDEN  neurons and an activation function \\n'relu' . This additional layer is considered hidden because it is not directly connected either with the \\ninput or with the output. After the first hidden layer we have a second hidden layer, again with N_HIDDEN  \\nneurons, followed by an output layer with ten neurons, each one of which will fire when the relative \\ndigit is recognized. The following code defines this new network:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n# Network and training.\\nEPOCHS = 50\\nBATCH_SIZE = 128\\nVERBOSE = 1\\nNB_CLASSES = 10   # number of outputs = number of digits\\nN_HIDDEN = 128\\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0921c2dc-033d-4e2d-8948-f7fc953d8c1a', embedding=None, metadata={'page_label': '17', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 1 17\\n# Loading MNIST dataset.\\n# Labels have one-hot representation.\\nmnist = keras.datasets.mnist\\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\\n# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\\nRESHAPED = 784\\n#\\nX_train = X_train.reshape( 60000, RESHAPED)\\nX_test = X_test.reshape( 10000, RESHAPED)\\nX_train = X_train.astype( 'float32' )\\nX_test = X_test.astype( 'float32' )\\n# Normalize inputs to be within in [0, 1].\\nX_train, X_test = X_train / 255.0, X_test / 255.0\\nprint(X_train.shape[ 0], 'train samples' )\\nprint(X_test.shape[ 0], 'test samples' )\\n# Labels have one-hot representation.\\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\\n# Build the model.\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(N_HIDDEN,\\n             input_shape=(RESHAPED,),\\n             name= 'dense_layer' , activation= 'relu'))\\nmodel.add(keras.layers.Dense(N_HIDDEN,\\n             name= 'dense_layer_2' , activation= 'relu'))\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n             name= 'dense_layer_3' , activation= 'softmax' ))\\n# Summary of the model.\\nmodel.summary()\\n# Compiling the model.\\nmodel.compile(optimizer= 'SGD', \\n              loss= 'categorical_crossentropy' ,\\n              metrics=[ 'accuracy' ])\\n# Training the model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9fd133a-4bc9-4d78-8a05-2f15643bd094', embedding=None, metadata={'page_label': '18', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 18\\nmodel.fit(X_train, Y_train,\\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\\n# Evaluating the model.\\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\\nprint('Test accuracy:' , test_acc)\\nNote that to_categorical(Y_train, NB_CLASSES)  converts the array Y_train  into a matrix with as \\nmany  columns as there are classes. The number of rows stays the same. So, for instance, if we have:\\n> labels\\narray([0, 2, 1, 2, 0])\\nthen:\\nto_categorical(labels)\\narray([[ 1.,  0.,  0.],\\n       [ 0.,  0.,  1.],\\n       [ 0.,  1.,  0.],\\n       [ 0.,  0.,  1.],\\n       [ 1.,  0.,  0.]], dtype=float32)\\nLet’s run the code and see what results we get with this multi-layer network: \\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #    \\n=================================================================\\ndense_layer (Dense)         (None, 128)               100480     \\n                                                                 \\ndense_layer_2 (Dense)       (None, 128)               16512      \\n                                                                 \\ndense_layer_3 (Dense)       (None, 10)                1290       \\n                                                                 \\n=================================================================\\nTotal params: 118,282\\nTrainable params: 118,282\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 48000 samples, validate on 12000 samples\\nEpoch 1/50\\n48000/48000 [==============================] - 3s 63us/sample - loss: 2.2507 - \\naccuracy: 0.2086 - val_loss: 2.1592 - val_accuracy: 0.3266\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83a78cb9-1863-47fc-865b-317c7fcd1a59', embedding=None, metadata={'page_label': '19', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 19\\nThe previous output shows the initial steps of the run while the following output shows the conclusion. \\nNot bad. As seen in the following output, by adding two hidden layers we reached 90.81% on the training \\ndataset, 91.40% on validation, and 91.18% on test. This means that we have increased accuracy on \\nthe test dataset with respect to the previous network, and we have reduced the number of iterations \\nfrom 200 to 50. That’s good, but we want more.\\nIf you want, you can play by yourself and see what happens if you add only one hidden layer instead \\nof two or if you add more than two layers. I leave this experiment as an exercise:\\nEpoch 49/50\\n48000/48000 [==============================] - 1s 30us/sample - loss: 0.3347 - \\naccuracy: 0.9075 - val_loss: 0.3126 - val_accuracy: 0.9136\\nEpoch 50/50\\n48000/48000 [==============================] - 1s 28us/sample - loss: 0.3326 - \\naccuracy: 0.9081 - val_loss: 0.3107 - val_accuracy: 0.9140\\n10000/10000 [==============================] - 0s 40us/sample - loss: 0.3164 - \\naccuracy: 0.9118\\nTest accuracy: 0.9118\\nNote that improvement stops (or it become almost imperceptible) after a certain number of epochs. \\nIn machine learning this is a phenomenon called convergence.\\nFurther improving the simple net in TensorFlow with dropout\\nNow our baseline is 90.81% on training set, 91.40% on validation, and 91.18% on test. A second \\nimprovement is very simple. We decide to randomly drop – with the DROPOUT  probability – some of \\nthe values propagated inside our internal dense network of hidden layers during training. In machine \\nlearning this is a well-known form of regularization. Surprisingly enough, this idea of randomly \\ndropping a few values can improve our performance. The idea behind this improvement is that random \\ndropouts force the network to learn redundant patterns that are useful for better generalization:\\nimport tensorflow as tf\\nimport numpy as np\\nfrom tensorflow import keras\\n# Network and training.\\nEPOCHS = 200\\nBATCH_SIZE = 128\\nVERBOSE = 1\\nNB_CLASSES = 10   # number of outputs = number of digits\\nN_HIDDEN = 128\\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\\nDROPOUT = 0.3\\n# Loading MNIST dataset.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b31598bb-f48f-4d7f-80da-8ccabeaf1ff1', embedding=None, metadata={'page_label': '20', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Neural Network Foundations with TF 20\\n# Labels have one-hot representation.\\nmnist = keras.datasets.mnist\\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\\n# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\\nRESHAPED = 784\\n#\\nX_train = X_train.reshape( 60000, RESHAPED)\\nX_test = X_test.reshape( 10000, RESHAPED)\\nX_train = X_train.astype( 'float32' )\\nX_test = X_test.astype( 'float32' )\\n# Normalize inputs within [0, 1].\\nX_train, X_test = X_train / 255.0, X_test / 255.0\\nprint(X_train.shape[ 0], 'train samples' )\\nprint(X_test.shape[ 0], 'test samples' )\\n# One-hot representations for labels.\\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\\n# Building the model.\\nmodel = tf.keras.models.Sequential()\\nmodel.add(keras.layers.Dense(N_HIDDEN,\\n              input_shape=(RESHAPED,),\\n              name= 'dense_layer' , activation= 'relu'))\\nmodel.add(keras.layers.Dropout(DROPOUT))\\nmodel.add(keras.layers.Dense(N_HIDDEN,\\n              name= 'dense_layer_2' , activation= 'relu'))\\nmodel.add(keras.layers.Dropout(DROPOUT))\\nmodel.add(keras.layers.Dense(NB_CLASSES,\\n              name= 'dense_layer_3' , activation= 'softmax' ))\\n# Summary of the model.\\nmodel.summary()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5400d7f-ea54-4027-bd03-92f9351c9ee1', embedding=None, metadata={'page_label': '21', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 1 21\\n# Compiling the model.\\nmodel.compile(optimizer= 'SGD', \\n              loss= 'categorical_crossentropy' ,\\n              metrics=[ 'accuracy' ])\\n# Training the model.\\nmodel.fit(X_train, Y_train,\\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\\n# Evaluating the model.\\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\\nprint('Test accuracy:' , test_acc)\\nLet’s run the code for 200 iterations as before and we see that this net achieves an accuracy of 91.70% \\non training, 94.42% on validation, and 94.15% on testing:\\nEpoch 199/200\\n48000/48000 [==============================] - 2s 45us/sample - loss: 0.2850 - \\naccuracy: 0.9177 - val_loss: 0.1922 - val_accuracy: 0.9442\\nEpoch 200/200\\n48000/48000 [==============================] - 2s 42us/sample - loss: 0.2845 - \\naccuracy: 0.9170 - val_loss: 0.1917 - val_accuracy: 0.9442\\n10000/10000 [==============================] - 1s 61us/sample - loss: 0.1927 - \\naccuracy: 0.9415\\nTest accuracy: 0.9415\\nNote that it has been frequently observed that networks with random dropouts in internal hidden \\nlayers can “generalize” better on unseen examples contained in test sets. Intuitively we can consider \\nthis phenomenon as each neuron becoming more capable because it knows it cannot depend on \\nits neighbors. Also, it forces information to be stored in a redundant way. During testing there is no \\ndropout, so we are now using all our highly tuned neurons. In short, it is generally a good approach \\nto test how a net performs when some dropout function is adopted.\\nBesides that, note that training accuracy should still be above test accuracy; otherwise, we might be \\nnot training for long enough. This is the case in our example and therefore we should increase the \\nnumber of epochs. However, before performing this attempt we need to introduce a few other concepts  \\nthat allow the training to converge faster. Let’s talk about optimizers.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d9720ec-677d-4ec1-b5f3-7a77a92aff63', embedding=None, metadata={'page_label': '22', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 22\\nTesting different optimizers in TensorFlow\\nNow that we have defined and used a network, it is useful to start developing some intuition about \\nhow networks are trained, using an analogy. Let us focus on one popular training technique known as \\ngradient descent ( GD). Imagine a generic cost function C (w) in one single variable w  like in Figure 1.12:\\nFigure 1.12: An example of GD optimization\\nGD can be seen as a hiker who needs to navigate down a steep slope and aims to enter a ditch. The slope \\nrepresents the function C while the ditch represents the minimum C min. The hiker has a starting point \\nw0. The hiker moves little by little; imagine that there is almost zero visibility, so the hiker cannot see \\nwhere to go automatically, and they proceed in a zigzag. At each step r, the gradient is the direction \\nof maximum increase.\\nMathematically this direction is the value of the partial derivative 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕  evaluated at point w r, reached \\nat step r. Therefore, by taking the opposite direction −𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕(𝑤𝑤𝑟𝑟)  the hiker can move toward the ditch.\\nAt each step the hiker can decide how big a stride to take before the next stop. This is the so-called  \\n“learning rate” 𝜂𝜂𝜂≥𝜂0  in GD jargon. Note that if 𝜂𝜂  is too small, then the hiker will move slowly. However, \\nif 𝜂𝜂  is too high, then the hiker will possibly miss the ditch by stepping over it.\\nNow you should remember that a sigmoid is a continuous function and it is possible to compute the \\nderivative. It can be proven that the sigmoid 𝜎𝜎(𝑥𝑥)=1\\n1+𝑒𝑒−𝑥𝑥  has the derivative 𝑑𝑑𝑑𝑑(𝑥𝑥𝑥\\n𝑑𝑑(𝑥𝑥𝑥= 𝜎𝜎(𝜎𝜎𝑥(1−𝜎𝜎(𝜎𝜎𝑥𝑥 .\\nReLU is not differentiable at 0. We can however extend the first derivative at 0 to a function over the \\nwhole domain by defining it to be either a 0 or 1. \\nThe piecewise derivative of ReLU 𝑦𝑦= max\\u2061(0,𝑥𝑥)  is 𝑑𝑑𝑑𝑑\\n𝑑𝑑𝑑𝑑={0    𝑥𝑥 𝑥 0\\n1    𝑥𝑥 𝑥 0  . \\nOnce we have the derivative, it is possible to optimize the nets with a GD technique. TensorFlow \\ncomputes the derivative on our behalf so we don’t need to worry about implementing or computing it.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cadc568-43d6-4361-a24c-5a9a1f475135', embedding=None, metadata={'page_label': '23', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 1 23\\nA neural network is essentially a composition of multiple derivable functions with thousands and \\nsometimes millions of parameters. Each network layer computes a function, the error of which should \\nbe minimized in order to improve the accuracy observed during the learning phase. When we discuss \\nbackpropagation, we will discover that the minimization game is a bit more complex than our toy \\nexample. However, it is still based on the same intuition of descending a slope to reach a ditch.\\nTensorFlow implements a fast variant of GD known as Stochastic Gradient Descent (SGD ) and many \\nmore advanced optimization techniques such as RMSProp and Adam. RMSProp and Adam include the \\nconcept of momentum (a velocity component) in addition to the acceleration component that SGD \\nhas. This allows faster convergence at the cost of more computation. Think about a hiker who starts to \\nmove in one direction and then decides to change direction but remembers previous choices. It can be \\nproven that momentum helps accelerate SGD in the relevant direction and dampens oscillations [10]. \\nSGD was our default choice so far. So now let’s try the other two. It is very simple; we just need to \\nchange a few lines:\\n# Compiling the model.\\nmodel.compile(optimizer= 'RMSProp' , \\n              loss= 'categorical_crossentropy' , metrics=[ 'accuracy' ])\\nThat’s it. Let’s test it. \\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #    \\n=================================================================\\ndense_layer (Dense)         (None, 128)               100480     \\n                                                                 \\ndropout_2 (Dropout)         (None, 128)               0          \\n                                                                 \\ndense_layer_2 (Dense)       (None, 128)               16512      \\n                                                                 \\ndropout_3 (Dropout)         (None, 128)               0          \\n                                                                 \\ndense_layer_3 (Dense)       (None, 10)                1290       \\n                                                                 \\n=================================================================\\nTotal params: 118,282\\nTrainable params: 118,282\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 48000 samples, validate on 12000 samples\\nEpoch 1/10\\n48000/48000 [==============================] - 2s 48us/sample - loss: 0.4715 - \\naccuracy: 0.8575 - val_loss: 0.1820 - val_accuracy: 0.9471\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0a24c47-96ff-4c9a-878a-33e5fec0f9f3', embedding=None, metadata={'page_label': '24', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 24\\nEpoch 2/10\\n48000/48000 [==============================] - 2s 36us/sample - loss: 0.2215 - \\naccuracy: 0.9341 - val_loss: 0.1268 - val_accuracy: 0.9361\\nEpoch 3/10\\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.1684 - \\naccuracy: 0.9497 - val_loss: 0.1198 - val_accuracy: 0.9651\\nEpoch 4/10\\n48000/48000 [==============================] - 2s 43us/sample - loss: 0.1459 - \\naccuracy: 0.9569 - val_loss: 0.1059 - val_accuracy: 0.9710\\nEpoch 5/10\\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.1273 - \\naccuracy: 0.9623 - val_loss: 0.1059 - val_accuracy: 0.9696\\nEpoch 6/10\\n48000/48000 [==============================] - 2s 36us/sample - loss: 0.1177 - \\naccuracy: 0.9659 - val_loss: 0.0941 - val_accuracy: 0.9731\\nEpoch 7/10\\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.1083 - \\naccuracy: 0.9671 - val_loss: 0.1009 - val_accuracy: 0.9715\\nEpoch 8/10\\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0971 - \\naccuracy: 0.9706 - val_loss: 0.0950 - val_accuracy: 0.9758\\nEpoch 9/10\\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0969 - \\naccuracy: 0.9718 - val_loss: 0.0985 - val_accuracy: 0.9745\\nEpoch 10/10\\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0873 - \\naccuracy: 0.9743 - val_loss: 0.0966 - val_accuracy: 0.9762\\n10000/10000 [==============================] - 1s 2ms/sample - loss: 0.0922 - \\naccuracy: 0.9764\\nTest accuracy: 0.9764\\nAs you can see, RMSProp is faster than SDG since we are able to achieve in only 10 epochs an accuracy \\nof 97.43% on the training dataset, 97.62% on validation, and 97.64% on test. That’s a significant \\nimprovement on SDG. Now that we have a very fast optimizer, let us try to increase significantly the \\nnumber of epochs up to 250, and we get 98.99% accuracy on the training dataset, 97.66% on validation, \\nand 97.77% on test:\\nEpoch 248/250\\n48000/48000 [==============================] - 2s 40us/sample - loss: 0.0506 - \\naccuracy: 0.9904 - val_loss: 0.3465 - val_accuracy: 0.9762\\nEpoch 249/250\\n48000/48000 [==============================] - 2s 40us/sample - loss: 0.0490 - \\naccuracy: 0.9905 - val_loss: 0.3645 - val_accuracy: 0.9765\\nEpoch 250/250', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='229db9f4-3ea2-406c-bfa1-b1e1f83a2e13', embedding=None, metadata={'page_label': '25', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 1 25\\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.0547 - \\naccuracy: 0.9899 - val_loss: 0.3353 - val_accuracy: 0.9766\\n10000/10000 [==============================] - 1s 58us/sample - loss: 0.3184 - \\naccuracy: 0.9779\\nTest accuracy: 0.9779\\nIt is useful to observe how accuracy increases on training and test sets when the number of epochs \\nincreases (see Figure 1.13). As you can see, these two curves touch at about 15 epochs and therefore \\nthere is no need to train further after that point:\\nFigure 1.13: An example of accuracy and loss with RMSProp\\nOkay, let’s try the other optimizer, Adam() . It’s pretty simple to implement:\\n# Compiling the model.\\nmodel.compile(optimizer= 'Adam', \\n              loss= 'categorical_crossentropy' ,\\n              metrics=[ 'accuracy' ])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bfeb0d42-44f8-4f10-b87a-c2e76a156816', embedding=None, metadata={'page_label': '26', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 26\\nAs we see, Adam()  is slightly better. With Adam we achieve 98.94% accuracy on the training dataset, \\n97.89% on validation, and 97.82% on test with 50 iterations:\\nEpoch 49/50\\n48000/48000 [==============================] - 3s 55us/sample - loss: 0.0313 - \\naccuracy: 0.9894 - val_loss: 0.0868 - val_accuracy: 0.9808\\nEpoch 50/50\\n48000/48000 [==============================] - 2s 51s/sample - loss: 0.0321 - \\naccuracy: 0.9894 - val_loss: 0.0983 - val_accuracy: 0.9789\\n10000/10000 [==============================] - 1s 66us/step - loss: 0.0964 - \\naccuracy: 0.9782\\nTest accuracy: 0.9782\\nOne more time, let’s plot how accuracy increases on training and test sets when the number of epochs \\nincreases (see Figure 1.14). You’ll notice that by choosing Adam as an optimizer we are able to stop \\nafter just about 12 epochs or steps:\\nFigure 1.14: An example of accuracy and loss with Adam', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8e19171-72da-4df3-b349-92bb25f4d360', embedding=None, metadata={'page_label': '27', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 27\\nNote that this is our fifth variant and remember that our initial baseline was at 90.71% on the test \\ndataset. So far, we’ve made progressive improvements. However, gains are now more and more difficult \\nto obtain. Note that we are optimizing with a dropout of 30%. For the sake of completeness, it could be \\nuseful to report the accuracy of the test dataset for different dropout values (see Figure 1.15). In this \\nexample, we selected Adam as the optimizer. Note that the choice of optimizer isn’t a rule of thumb \\nand we can get different performance depending on the problem-optimizer combination:\\nFigure 1.15: An example of changes in accuracy for different dropout values\\nIncreasing the number of epochs\\nLet’s make another attempt and increase the number of epochs used for training from 20 to 200. \\nUnfortunately, this choice increases our computation time tenfold, yet gives us no gain. The experiment \\nis unsuccessful, but we have learned that if we spend more time learning, we will not necessarily \\nimprove the result. Learning is more about adopting smart techniques and not necessarily about the \\ntime spent in computations. Let’s keep track of our five variants in the following graph:\\nFigure 1.16: Accuracy for different models and optimizers', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3412c45e-3d6e-4ebc-97c1-db122491de12', embedding=None, metadata={'page_label': '28', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 28\\nControlling the optimizer learning rate\\nThere is another approach we can take that involves changing the learning parameter for our optimizer. \\nAs you can see in Figure 1.17, the best value reached by our three experiments [lr=0.1, lr=0.01, and \\nlr=0.001] is 0.1, which is the default learning rate for the optimizer. Good! Adam works well out of \\nthe box:\\nFigure 1.17: Accuracy for different learning rates\\nIncreasing the number of internal hidden neurons\\nYet another approach involves changing the number of internal hidden neurons. We report the \\nresults of the experiments with an increasing number of hidden neurons. We see that by increasing \\nthe complexity of the model, the runtime increases significantly because there are more and more \\nparameters to optimize. However, the gains that we are getting by increasing the size of the network \\ndecrease more and more as the network grows (see Figure 1.18, Figure 1.19, and Figure 1.20): ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f377369-fd81-447f-9106-5df6c88f8a83', embedding=None, metadata={'page_label': '29', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 29\\nFigure 1.18: Number of parameters for the increasing values of internal hidden neurons\\nOn the other hand, the time needed increases as the size of the internal network increases (see Figure \\n1.19):\\nFigure 1.19: Seconds of computation time for the increasing values of internal hidden neurons', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89be1b76-8217-4427-b4aa-40d5e951f764', embedding=None, metadata={'page_label': '30', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 30\\nNote that increasing the number of hidden neurons after a certain value can reduce the accuracy \\nbecause the network might not be able to generalize well (as shown in Figure 1.20):\\nFigure 1.20: Test accuracy for the increasing values of internal hidden neurons\\nIncreasing the size of batch computation\\nGD tries to minimize the cost function on all the examples provided in the training sets and, at the \\nsame time, for all the features provided as input. SGD is a much less expensive variant that considers \\nonly BATCH_SIZE  examples. So, let us see how it behaves when we change this parameter. As you can \\nsee, the best accuracy value is reached for a BATCH_SIZE=64  in our four experiments (see Figure 1.21):\\nFigure 1.21: Test accuracy for different batch values', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69c3e121-cde2-4015-9e9c-aa0c8ecedbad', embedding=None, metadata={'page_label': '31', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 31\\nSummarizing experiments run to recognizing handwritten digits\\nSo, let’s summarize: with five different variants, we were able to improve our performance from 90.71% \\nto 97.82%. First, we defined a simple layer network in TensorFlow. Then, we improved the performance \\nby adding some hidden layers. After that, we improved the performance on the test set by adding a \\nfew random dropouts in our network, and then by experimenting with different types of optimizers:\\nAccuracy\\nModel Training Validation Test\\nSimple 89.96% 90.70% 90.71%\\nTwo hidden layers (128) 90.81% 91.40% 91.18%\\nDropout (30%) 91.70% 94.42% 94.15% (200 epochs)\\nRMSProp 97.43% 97.62% 97.64% (10 epochs)\\nAdam 98.94% 97.89% 97.82% (10 epochs)\\nTable 1.1: Summary of experiments with various levels of accuracy\\nHowever, the next two experiments (not shown in Table 1.1) were not providing significant improvements. \\nIncreasing the number of internal neurons creates more complex models and requires more expensive \\ncomputations, but it provides only marginal gains. We have the same experience if we increase the \\nnumber of training epochs. A final experiment consisted of changing the BATCH_SIZE  for our optimizer. \\nThis also provided marginal results.\\nRegularization\\nIn this section we will review a few best practices for improving the training phase. In particular, \\nregularization and batch normalization will be discussed.\\nAdopting regularization to avoid overfitting\\nIntuitively, a good machine learning model should achieve low error on training data. Mathematically \\nthis is equivalent to minimizing the loss function on the training data given the model:\\nmin ∶  {𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙  𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 |𝑀𝑀𝑙𝑙𝑀𝑀𝑀𝑀𝑙𝑙𝑀} \\nHowever, this might not be enough. A model can become excessively complex in order to capture \\nall the relations inherently expressed by the training data. This increase in complexity might have \\ntwo negative consequences. First, a complex model might require a significant amount of time to \\nbe executed. Second, a complex model might achieve very good performance on training data but \\nperform quite badly on validation data. This is because the model is able to contrive relationships \\nbetween many parameters in the specific training context, but these relationships in fact do not exist \\nwithin a more generalized context. Causing a model to lose its ability to generalize in this manner \\nis termed “overfitting. “ Again, learning is more about generalization than memorization. Another \\nphenomenon to consider is “underfitting.” ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fc34862-782a-4d4b-956b-fa8d855614ca', embedding=None, metadata={'page_label': '32', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 32\\nThis happens when a data model cannot capture the relationship between the input and output variables \\naccurately, with a high error rate on both the training set and new unseen data:\\nFigure 1.22: Loss function and overfitting\\nAs a rule of thumb, if during the training we see that the loss increases on validation, after an initial \\ndecrease, then we have a problem of model complexity that overfits the training data.\\nIn order to solve the overfitting problem, we need a way to capture the complexity of a model, i.e. how \\ncomplex a model can be. What could the solution be? Well, a model is nothing more than a vector of \\nweights. Each weight affects the output, except for those which are zero, or very close to it. Therefore, \\nthe complexity of a model can be conveniently represented as the number of non-zero weights. In other \\nwords, if we have two models M1 and M2 achieving pretty much the same performance in terms of a \\nloss function, then we should choose the simplest model, the one which has the minimum number \\nof non-zero weights. We can use a hyperparameter λ >= 0   for controlling the importance of having \\na simple model, as in this formula:\\nmin : {𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙  𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 |𝑀𝑀𝑙𝑙𝑀𝑀𝑀𝑀𝑙𝑙𝑀}+ λ ∗ 𝑐𝑐 𝑙𝑙𝑐𝑐𝑐𝑐𝑙𝑙𝑀𝑀𝑐𝑐 𝑙𝑙𝑙𝑙𝑐𝑐 𝑙𝑀𝑀𝑙𝑙𝑀𝑀𝑀𝑀𝑙𝑙𝑀  \\nThere are three different types of regularization used in machine learning:\\n• L1 regularization (also known as LASSO). The complexity of the model is expressed as the sum \\nof the absolute values of the weights.\\n• L2 regularization (also known as Ridge). The complexity of the model is expressed as the sum \\nof the squares of the weights.\\n• ElasticNet regularization. The complexity of the model is captured by a combination of the \\ntwo techniques above. \\nNote that playing with regularization can be a good way to increase the generalization performance \\nof a network, particularly when there is an evident situation of overfitting. This set of experiments is \\nleft as an exercise to the interested reader.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='48e33784-8f35-4fe5-a461-12eb00139e67', embedding=None, metadata={'page_label': '33', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 33\\nAlso note that TensorFlow supports L1, L2, and ElasticNet regularization. A complete list of regularizers  \\nis at https://www.tensorflow.org/api_docs/python/tf/keras/regularizers . Adding regularization \\nis easy: \\nfrom tf.keras.regularizers import l2, activity_l2\\nmodel.add(Dense( 64, input_dim= 64, W_regularizer=l2( 0.01),\\n    activity_regularizer=activity_l2( 0.01)))\\nUnderstanding batch normalization\\nBatch normalization is another form of regularization and one of the most effective improvements \\nproposed during the last few years. Batch normalization enables us to accelerate training, in some \\ncases by halving the training epochs, and it offers some regularization. During training, the weights \\nin early layers naturally change and therefore the inputs of later layers can significantly change. In \\nother words, each layer must continuously re-adjust its weights to the different distribution for every \\nbatch. This may slow down the model’s training greatly. The key idea is to make layer inputs more \\nsimilar in distribution, batch after batch and epoch after epoch.\\nAnother issue is that the sigmoid activation function works very well close to zero but tends to “get \\nstuck” when values get sufficiently far away from zero. If, occasionally, neuron outputs fluctuate far \\naway from the sigmoid zero, then said neuron becomes unable to update its own weights. \\nThe other key idea is therefore to transform the layer outputs into a Gaussian distribution unit close \\nto zero. This way, layers will have significantly less variation from batch to batch. Mathematically, the \\nformula is very simple. The activation input x is centered around zero by subtracting the batch mean \\nμ  from it. Then the result is divided by σ+ϵ  , the sum of batch variance σ , and a small number ϵ  to \\nprevent division by zero. Then, we use a linear transformation y =  λ x +  β   to make sure that the \\nnormalizing effect is applied during training. \\nIn this way, λ  and β  are parameters that get optimized during the training phase in a way similar to \\nany other layer. Batch normalization has been proven to be a very effective way to increase both the \\nspeed of training and accuracy, because it helps to prevent activations becoming either too small and \\nvanishing or too big and exploding.\\nPlaying with Google Colab: CPUs, GPUs, and TPUs\\nGoogle offers a truly intuitive tool for training neural networks and for playing with TensorFlow at \\nno cost. You can find an actual Colab, which can be freely accessed, at https://colab.research.\\ngoogle.com/  and if you are familiar with Jupyter notebooks you will find a very familiar web-based \\nenvironment here. Colab stands for Colaboratory and is a Google research project created to help \\ndisseminate machine learning education and research. We will see the difference between CPUs, GPUs, \\nand TPUs in Chapter 15, Tensor Processing Unit. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2da6139-d27c-4dd6-95cf-f5f6de2ae501', embedding=None, metadata={'page_label': '34', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 34\\nFor now, it’s important to know that CPUs are generic processing units, while GPUs and TPUs are \\naccelerators, specific processing units suitable for deep learning. Let’s see how it works, starting with \\nthe screenshot shown in Figure 1.23:\\nFigure 1.23: An example of notebooks in Colab\\nBy accessing Colab, we can either check a listing of notebooks generated in the past or we can create \\na new notebook. Different versions of Python are supported.\\nWhen we create a new notebook, we can also select if we want to run it on CPUs, GPUs, or in Google’s \\nTPUs as shown in Figure 1.24: \\nFigure 1.24: Selecting the desired hardware accelerator (None, GPUs, or TPUs) – the first step', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='933593c6-eeac-4950-a260-92f77fa29dcb', embedding=None, metadata={'page_label': '35', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 35\\nBy accessing the Notebook settings option contained in the Edit menu (see Figure 1.24 and Figure \\n1.25), we can select the desired hardware accelerator (None, GPUs , or TPUs ). Google will allocate the \\nresources at no cost, although they can be withdrawn at any time, for example during periods of a \\nparticularly heavy load. In my experience, this is a very rare event, and you can access Colab pretty \\nmuch any time. However, be polite and do not do something like start mining bitcoins at no cost – you \\nwill almost certainly get evicted!\\nFigure 1.25: Selecting the desired hardware accelerator (None, GPUs, or TPUs) – the second step\\nThe next step is to insert  your code (see Figure 1.26 ) in the appropriate Colab notebook cells and \\nvoila! You are good to go. Execute the code and happy deep learning without the hassle of buying very \\nexpensive hardware to start your experiments! Figure 1.26 contains an example of code in a Google \\nnotebook:\\nFigure 1.26: An example of code in a notebook', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bdf77e6f-3ad4-47e8-ab40-360956ef40a4', embedding=None, metadata={'page_label': '36', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 36\\nSentiment analysis\\nWhat is the code we used to test Colab? It is an example of sentiment analysis developed on top of the \\nIMDB dataset. The IMDB dataset contains the text of 50,000 movie reviews from the Internet Movie \\nDatabase. Each review is either positive or negative (for example, thumbs up or thumbs down). The \\ndataset is split into 25,000 reviews for training and 25,000 reviews for testing. Our goal is to build a \\nclassifier that can predict the binary judgment given the text. We can easily load IMDB via tf.keras  \\nand the sequences of words in the reviews have been converted to sequences of integers, where each \\ninteger represents a specific word in a dictionary. We also have a convenient way of padding sentences \\nto max_len , so that we can use all sentences, whether short or long, as inputs to a neural network with \\nan input vector of fixed size: \\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models, preprocessing\\nimport tensorflow_datasets as tfds\\nmax_len = 200\\nn_words = 10000\\ndim_embedding = 256\\nEPOCHS = 20\\nBATCH_SIZE = 500\\ndef load_data ():\\n    # Load data.\\n    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_\\nwords)\\n    # Pad sequences with max_len.\\n    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\\n    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\\n    return (X_train, y_train), (X_test, y_test)\\nNow let’s build a model. We are going to use a few layers that will be explained in detail in Chapter 4, \\nWord Embeddings. For now, let’s assume that the embedding()  layer will map the sparse space of words \\ncontained in the reviews into a denser space. This will make computation easier. In addition, we will \\nuse a GlobalMaxPooling1D()  layer, which takes the maximum value of either feature vector from each \\nof the n_words  features. In addition, we have two Dense()  layers. The last one is made up of a single \\nneuron with a sigmoid activation function for making the final binary estimation:\\ndef build_model ():\\n    model = models.Sequential()\\n    # Input: - eEmbedding Layer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5aaadc99-e7b3-495b-9bdd-1a0b91c1640d', embedding=None, metadata={'page_label': '37', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 37\\n    # The model will take as input an integer matrix of size (batch, input_\\nlength).\\n    # The model will output dimension (input_length, dim_embedding).\\n    # The largest integer in the input should be no larger\\n    # than n_words (vocabulary size).\\n    model.add(layers.Embedding(n_words, \\n        dim_embedding, input_length=max_len))\\n    model.add(layers.Dropout( 0.3))\\n    # Takes the maximum value of either feature vector from each of the n_words \\nfeatures.\\n    model.add(layers.GlobalMaxPooling1D())\\n    model.add(layers.Dense( 128, activation= \\'relu\\'))\\n    model.add(layers.Dropout( 0.5))\\n    model.add(layers.Dense( 1, activation= \\'sigmoid\\' ))\\n    return model\\nNow we need to train our model and this piece of code is very similar to what we have done with \\nMNIST. Let’s see:\\n(X_train, y_train), (X_test, y_test) = load_data()\\nmodel = build_model()\\nmodel.summary()\\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\" ,\\n metrics = [ \"accuracy\" ]\\n)\\nscore = model.fit(X_train, y_train,\\n epochs = EPOCHS,\\n batch_size = BATCH_SIZE,\\n validation_data = (X_test, y_test)\\n)\\nscore = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\\nprint(\"\\\\nTest score:\" , score[ 0])\\nprint(\\'Test accuracy:\\' , score[ 1])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7914f3d7-8b7f-4be3-b34b-ffafe748aa54', embedding=None, metadata={'page_label': '38', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 38\\nLet’s see the network and then run a few iterations:\\n___________________________________________________________________\\nLayer (type)                  Output Shape              Param #    \\n===================================================================\\nembedding (Embedding)         (None, 200, 256)          2560000    \\n                                                                   \\ndropout (Dropout)             (None, 200, 256)          0          \\n                                                                   \\nglobal_max_pooling1d (Global  (None, 256)               0          \\n                                                                   \\ndense (Dense)                 (None, 128)               32896      \\n                                                                   \\ndropout_1 (Dropout)           (None, 128)               0          \\n                                                                   \\ndense_1 (Dense)               (None, 1)                 129        \\n                                                                   \\n===================================================================\\nTotal params: 2,593,025\\nTrainable params: 2,593,025\\nNon-trainable params: 0\\nAs shown in the following output, we reach accuracy of 85%, which is not bad at all for a simple network:\\nEpoch 20/20\\n25000/25000 [==============================] - 23s 925ms/sample - loss: 0.0053 \\n- accuracy: 0.9991 - val_loss: 0.4993 - val_accuracy: 0.8503\\n25000/25000 [==============================] - 2s 74us/sample - loss: 0.4993 - \\naccuracy: 0.88503\\nTest score: 0.4992710727453232\\nTest accuracy: 0.85028\\nThe next section is devoted to tuning hyperparameters and AutoML.\\nHyperparameter tuning and AutoML\\nThe experiments defined above give some opportunities for fine-tuning a net. However, what works \\nfor this example will not necessarily work for other examples. For a given neural network, there are \\nindeed multiple parameters that can be optimized (such as the number of hidden neurons, batch size, \\nnumber of epochs, and many more according to the complexity of the net itself). These parameters \\nare called “hyperparameters” to distinguish them from the parameters of the network itself, i.e. the \\nvalues of the weights and biases.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc23fc03-d15e-44f4-91d0-f70d1d7c54c8', embedding=None, metadata={'page_label': '39', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 39\\nHyperparameter tuning is the process of finding the optimal combination of those hyperparameters \\nthat minimize cost functions. The key idea is that if we have n hyperparameters, then we can imagine \\nthat they define a space with n dimensions, and the goal is to find the point in this space that corresponds \\nto an optimal value for the cost function. One way to achieve this goal is to create a grid in this space \\nand systematically check the value assumed by the cost function for each grid vertex. In other words, \\nthe hyperparameters are divided into buckets and different combinations of values are checked via \\na brute-force approach.\\nIf you think that this process of fine-tuning the hyperparameters is manual and expensive then you \\nare absolutely right! However, during the last few years, we have seen significant results in AutoML, \\na set of research techniques aimed at both automatically tuning hyperparameters and searching \\nautomatically for optimal network architecture. We will discuss more about this in Chapter 13, An \\nIntroduction to AutoML. \\nPredicting output\\nOnce a net is trained, it can of course be used for making predictions. In TensorFlow, this is very \\nsimple. We can use this method:\\n# Making predictions.\\npredictions = model.predict(X)\\nFor a given input, several types of output can be computed including a method model.evaluate()  used \\nto compute the loss values, a method model.predict_classes()  used to compute category outputs, \\nand a method model.predict_proba()  used to compute class probabilities.\\nA practical overview of backpropagation\\nMulti-layer perceptrons learn from training data through a process called backpropagation. In this \\nparagraph we will give an intuition while more details are in Chapter 14, The Math Behind Deep Learning. \\nThe process can be described as a way of progressively correcting mistakes as soon as they are detected. \\nLet’s see how this works.\\nRemember that each neural network layer has an associated set of weights that determine the output \\nvalues for a given set of inputs. Additionally, remember that a neural network can have multiple \\nhidden layers.\\nAt the beginning, all the weights have some random assignment. Then the neural network is activated \\nfor each input in the training set: values are propagated forward from the input stage through the \\nhidden stages to the output stage where a prediction is made. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9759ead6-ba95-4d4a-9424-3a7ed71bf046', embedding=None, metadata={'page_label': '40', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 40\\nNote that we keep Figure 1.27 below simple by only representing a few values with green dotted lines \\nbut in reality, all the values are propagated forward through the network:\\nFigure 1.27: Forward step in backpropagation\\nSince we know the true observed value in the training set, it is possible to calculate the error made in \\nthe prediction. The key intuition for backtracking is to propagate the error back (see Figure 1.28), using \\nan appropriate optimizer algorithm such as a GD to adjust the neural network weights with the goal \\nof reducing the error (again for the sake of simplicity only a few error values are represented here):\\nFigure 1.28: Backward step in backpropagation\\nThe process of forward propagation  from input to output and backward propagation of errors is repeated \\nseveral times until the error gets below a predefined threshold. The whole process is represented in \\nFigure 1.29:\\nFigure 1.29: Forward propagation and backward propagation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71a6a79a-aa0f-459f-b379-0f66c2609d5f', embedding=None, metadata={'page_label': '41', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 41\\nThe features represent the input, and the labels are used here to drive the learning process. The model \\nis updated in such a way that the loss function is progressively minimized. In a neural network, what \\nreally matters is not the output of a single neuron but the collective weights adjusted in each layer. \\nTherefore, the network progressively adjusts its internal weights in such a way that the prediction \\nincreases the number of correctly forecasted labels. Of course, using the right set of features and \\nhaving quality labeled data is fundamental to minimizing the bias during the learning process.\\nWhat have we learned so far?\\nIn this chapter, we have learned the basics of neural networks. More specifically, we have learned what \\na perceptron is and what a multi-layer perceptron is, how to define neural networks in TensorFlow, \\nhow to progressively improve metrics once a good baseline is established, and how to fine-tune the \\nhyperparameter space. In addition to that, we also have a good idea of useful activation functions \\n(sigmoid and ReLU) available, and how to train a network with backpropagation algorithms based on \\neither GD, SGD, or more sophisticated approaches, such as Adam and RMSProp.\\nToward a deep learning approach\\nWhile playing with handwritten digit recognition, we came to the conclusion that the closer we get to \\nan accuracy of 99%, the more difficult it is to improve. If we want more improvement, we definitely \\nneed a new idea. What are we missing? Think about it.\\nThe fundamental intuition is that in our examples so far, we are not making use of the local spatial \\nstructure of images, which means we will use the fact that an image can be described as a matrix with \\ndata locality. In particular, this piece of code transforms the bitmap representing each written digit into \\na flat vector where the local spatial structure (the fact that some pixels are closer to each other) is gone:\\n# X_train is 60000 rows of 28x28 values; we  --> reshape it as in 60000 x 784.\\nX_train = X_train.reshape( 60000, 784)\\nX_test = X_test.reshape( 10000, 784)\\nHowever, this is not how our brain works. Remember that our vision is based on multiple cortex levels, \\neach one recognizing more and more structured information while still preserving the locality. First, \\nwe see single pixels, then from that, we recognize simple geometric forms and then more and more \\nsophisticated elements such as objects, faces, human bodies, animals, and so on. \\nIn Chapter 3, we will see that a particular type of deep learning network known as the Convolutional \\nNeural Network (CNN) has been developed by taking into account both the idea of preserving the \\nlocal spatial structure in images (and more generally in any type of information that has a spatial \\nstructure) and the idea of learning via progressive levels of abstraction: with one layer you can only \\nlearn simple patterns; with more than one layer you can learn multiple patterns. Before discussing \\nCNN, we need to discuss some aspects of TensorFlow architecture and have a practical introduction \\nto a few additional machine learning concepts.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77c43de6-a718-4456-aeee-f20bb14a5fad', embedding=None, metadata={'page_label': '42', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Neural Network Foundations with TF 42\\nSummary\\nIn this chapter we learned what TensorFlow and Keras are and introduced neural networks with the \\nperceptron and the multi-layer perceptron. Then, we saw a real example of recognizing handwritten \\ndigits with several optimizations.\\nThe next chapter is devoted to regression and classification.\\nReferences\\n1. Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization \\nin the brain. Psychol. Rev, vol. 65, pp. 386–408.\\n2. Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proc. IEEE, vol. \\n78, pp. 1550–1560.\\n3. Hinton, G. E., Osindero, S., and Teh, Y. W . (2006). A fast learning algorithm for deep belief nets. \\nNeural Comput, vol. 18, pp. 1527–1554.\\n4. Schmidhuber, J. (2015). Deep learning in neural networks: an overview. Neural Networks\\u202f: Off. J. \\nInt. Neural Netw. Soc., vol. 61, pp. 85–117.\\n5. Leven, S. (1996). The roots of backpropagation: From ordered derivatives to neural networks and \\npolitical forecasting. Neural Networks, vol. 9.\\n6. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-\\npropagating errors. Nature, vol. 323.\\n7. Herculano-Houzel, S. (2009). The human brain in numbers: a linearly scaled-up primate brain . \\nFront. Hum. Neurosci., vol. 3.\\n8. Hornick, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal \\napproximators. Neural Networks Volume 2, Issue 5. Pages 359–366.\\n9. Vapnik, V . N. (2013). The nature of statistical learning theory.\\n10. Sutskever, I., Martens, J., Dahl, G., Hinton, G., (2013). On the importance of initialization and \\nmomentum in deep learning. 30th International Conference on Machine Learning, ICML.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e37a8ef8-d68e-497c-9d64-9fcf364c960c', embedding=None, metadata={'page_label': '43', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2\\nRegression and Classification\\nRegression and classification are two fundamental tasks ubiquitously present in almost all machine \\nlearning applications. They find application in varied fields ranging from engineering, physical science, \\nbiology, and the financial market, to the social sciences. They are the fundamental tools in the hands \\nof statisticians and data scientists. In this chapter, we will cover the following topics:\\n• Regression\\n• Classification\\n• Difference between classification and regression\\n• Linear regression\\n• Different types of linear regression\\n• Classification using the TensorFlow Keras API\\n• Applying linear regression to estimate the price of a house\\n• Applying logistic regression to identify handwritten digits\\nLet us first start with understanding what regression really is.\\nWhat is regression?\\nRegression is normally the first algorithm that people in machine learning work with. It allows us to \\nmake predictions from data by learning about the relationship between a given set of dependent and \\nindependent variables. It has its use in almost every field; anywhere that has an interest in drawing \\nrelationships between two or more things will find a use for regression.All the code files for this chapter can be found at https://packt.link/dltfchp2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef01f249-36bf-4a26-8fe0-9536a86421d6', embedding=None, metadata={'page_label': '44', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 44\\nConsider the case of house price estimation. There are many factors that can have an impact on the \\nhouse price: the number of rooms, the floor area, the locality, the availability of amenities, the parking \\nspace, and so on. Regression analysis can help us in finding the mathematical relationship between \\nthese factors and the house price.\\nLet us imagine a simpler world where only the area of the house determines its price. Using regression, \\nwe could determine the relationship between the area of the house (independent variable: these are \\nthe variables that do not depend upon any other variables) and its price (dependent variable: these \\nvariables depend upon one or more independent variables). Later, we could use this relationship to \\npredict the price of any house, given its area. To learn more about dependent and independent variables \\nand how to identify them, you can refer to this post: https://medium.com/deeplearning-concepts-\\nand-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db . \\nIn machine learning, the independent variables are normally input into the model and the dependent \\nvariables are output from our model.\\nDepending upon the number of independent variables, the number of dependent variables, and the \\nrelationship type, we have many different types of regression. There are two important components of \\nregression: the relationship between independent and dependent variables, and the strength of impact \\nof different independent variables on dependent variables. In the following section, we will learn in \\ndetail about the widely used linear regression technique.\\nPrediction using linear regression\\nLinear regression is one of the most widely known modeling techniques. Existing for more than \\n200 years, it has been explored from almost all possible angles. Linear regression assumes a linear \\nrelationship between the input variable (X ) and the output variable (Y ). The basic idea of linear  \\nregression is building a model, using training data that can predict the output given the input, such \\nthat the predicted output 𝑌𝑌̂  is as near the observed training output Y for the input X. It involves finding \\na linear equation for the predicted value 𝑌𝑌̂  of the form:\\n𝑌𝑌̂=𝑊𝑊𝑇𝑇𝑋𝑋𝑋𝑋𝑋  \\nwhere 𝑋𝑋𝑋{x1,x2,...,xn}  are the n input variables, and 𝑊𝑊 𝑊 𝑊𝑊𝑊 1,𝑊𝑊2,...,𝑊𝑊𝑛𝑛}  are the linear coefficients, \\nwith b as the bias term. We can also expand the preceding equation to:\\n𝑌𝑌̂=∑𝑥𝑥 𝑖𝑖𝑤𝑤𝑖𝑖𝑛𝑛\\n𝑖𝑖𝑖𝑖+𝑏𝑏 \\nThe bias term allows our regression model to provide an output even in the absence of any input; it \\nprovides us with an option to shift our data for a better fit. The error between the observed values (Y) \\nand predicted values ( 𝑌𝑌̂ ) for an input sample i is: \\n𝑒𝑒𝑖𝑖= 𝑌𝑌 𝑖𝑖−𝑌𝑌̂𝑖𝑖 \\nThe goal is to find the best estimates for the coefficients W and bias b, such that the error between \\nthe observed values Y and the predicted values 𝑌𝑌̂  is minimized. Let’s go through some examples to \\nbetter understand this.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='376e781e-e9fd-443a-bdf9-be5d05ae5db8', embedding=None, metadata={'page_label': '45', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 45\\nSimple linear regression\\nIf we consider only one independent variable and one dependent variable, what we get is a simple \\nlinear regression. Consider the case of house price prediction, defined in the preceding section; the \\narea of the house ( A) is the independent variable, and the price ( Y) of the house is the dependent \\nvariable. We want to find a linear relationship between predicted price 𝑌𝑌̂  and A, of the form:\\n𝑌𝑌̂= 𝐴𝐴𝐴𝐴𝐴 𝐴𝐴𝐴  \\nwhere b is the bias term. Thus, we need to determine W and b, such that the error between the price \\nY and the predicted price 𝑌𝑌̂  is minimized. The standard method used to estimate W and b is called \\nthe method of least squares, that is, we try to minimize the sum of the square of errors (S). For the \\npreceding case, the expression becomes:\\n𝑆𝑆(𝑊𝑊𝑊𝑊𝑊)= ∑(𝑌𝑌 𝑖𝑖−𝑌𝑌̂𝑖𝑖)2=∑(𝑌𝑌𝑖𝑖−𝐴𝐴𝑖𝑖𝑊𝑊−𝑊𝑊)2𝑁𝑁\\n𝑖𝑖𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nWe want to estimate the regression coefficients, W and b, such that S is minimized. We use the fact \\nthat the derivative of a function is 0 at its minima to get these two equations:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕= −2∑ (𝑌𝑌𝑖𝑖−𝐴𝐴𝑖𝑖𝜕𝜕−𝑊𝑊)𝐴𝐴𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖=0 \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕= −2∑ (𝑌𝑌𝑖𝑖−𝐴𝐴𝑖𝑖𝑊𝑊−𝜕𝜕)𝑁𝑁\\n𝑖𝑖𝑖𝑖=0 \\nThese two equations can be solved to find the two unknowns. To do so, we first expand the summation \\nin the second equation: \\n∑𝑌𝑌𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖−∑𝐴𝐴 𝑖𝑖𝑊𝑊𝑁𝑁\\n𝑖𝑖𝑖𝑖−∑𝑏𝑏𝑁𝑁\\n𝑖𝑖𝑖𝑖=0 \\nTake a look at the last term on the left-hand side; it just sums up a constant N time. Thus, we can \\nrewrite it as:\\n∑𝑌𝑌𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖−𝑊𝑊∑𝑊𝑊𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖−𝑁𝑁𝑁𝑁=0 \\nReordering the terms, we get: \\n𝑏𝑏𝑏1\\n𝑁𝑁∑𝑌𝑌𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖−𝑊𝑊\\n𝑁𝑁∑𝐴𝐴𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nThe two terms on the right-hand side can be replaced by 𝑌𝑌̅ , the average price (output), and 𝐴𝐴̅ , the \\naverage area (input), respectively, and thus we get: \\n𝑏𝑏𝑏𝑏𝑏̅−𝑊𝑊𝑊𝑊̅ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='311000ad-1265-4124-869f-3d81a8aad282', embedding=None, metadata={'page_label': '46', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Regression and Classification 46\\nIn a similar fashion, we expand the partial differential equation of S with respect to weight W: \\n∑(𝑌𝑌𝑖𝑖𝐴𝐴𝑖𝑖−𝑊𝑊𝐴𝐴𝑖𝑖2−𝑏𝑏𝐴𝐴𝑖𝑖)=0𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nSubstitute the expression for the bias term b: \\n∑(𝑌𝑌𝑖𝑖𝐴𝐴𝑖𝑖−𝑊𝑊𝐴𝐴𝑖𝑖2−(𝑌𝑌̅−𝑊𝑊𝐴𝐴̅)𝐴𝐴𝑖𝑖)=0𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nReordering the equation:\\n∑(𝑌𝑌𝑖𝑖𝐴𝐴𝑖𝑖−𝑌𝑌̅𝐴𝐴𝑖𝑖)−𝑊𝑊∑(𝐴𝐴𝑖𝑖2−𝐴𝐴̅𝐴𝐴𝑖𝑖)𝑁𝑁\\n𝑖𝑖𝑖𝑖=0𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nPlaying around with the mean definition, we can get from this the value of weight W as: \\n𝑊𝑊𝑊∑𝑌𝑌𝑖𝑖(𝐴𝐴𝑖𝑖−𝐴𝐴̅)𝑁𝑁\\n𝑖𝑖𝑖𝑖\\n∑(𝐴𝐴𝑖𝑖−𝐴𝐴̅)2 𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nwhere 𝑌𝑌̅  and 𝐴𝐴̅  are the average price and area, respectively. Let us try this on some simple sample data:\\n1. We import the necessary modules. It is a simple example, so we’ll be using only NumPy, pandas, \\nand Matplotlib:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n2. Next, we generate random data with a linear relationship. To make it more realistic, we also \\nadd a random noise element. You can see the two variables (the cause, area , and the effect, \\nprice ) follow a positive linear dependence:\\n#Generate a random data\\nnp.random.seed( 0)\\narea = 2.5 * np.random.randn( 100) + 25\\nprice = 25 * area + 5 + np.random.randint( 20,50, size = len(area))\\ndata = np.array([area, price])\\ndata = pd.DataFrame(data = data.T, columns=[ 'area','price' ])\\nplt.scatter(data[ 'area'], data[ 'price'])\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8815ad3-b503-4d48-9f99-510a3add2a3f', embedding=None, metadata={'page_label': '47', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 47\\nFigure 2.1: Scatter plot between the area of the house and its price\\n3. Now, we calculate the two regression coefficients using the equations we defined. You can see \\nthe result is very much near the linear relationship we have simulated:\\nW = sum(price*(area-np.mean(area))) / sum((area-np.mean(area))** 2)\\nb = np.mean(price) - W*np.mean(area)\\nprint(\"The regression coefficients are\" , W,b)\\n-----------------------------------------------\\nThe regression coefficients are 24.815544052284988 43.4989785533412\\n4. Let us now try predicting the new prices using the obtained weight and bias values:\\ny_pred = W * area + b\\n5. Next, we plot the predicted prices along with the actual price. You can see that predicted prices \\nfollow a linear relationship with the area:\\nplt.plot(area, y_pred, color= \\'red\\',label=\"Predicted Price\" )\\nplt.scatter(data[ \\'area\\'], data[ \\'price\\'], label= \"Training Data\" )\\nplt.xlabel( \"Area\")\\nplt.ylabel( \"Price\")\\nplt.legend()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f04f9fb3-680f-441e-a2d2-018f77fd8bae', embedding=None, metadata={'page_label': '48', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 48\\nFigure 2.2: Predicted values vs the actual price\\nFrom Figure 2.2, we can see that the predicted values follow the same trend as the actual house prices.\\nMultiple linear regression\\nThe preceding example was simple, but that is rarely the case. In most problems, the dependent \\nvariables depend upon multiple independent variables. Multiple linear regression finds a linear \\nrelationship between the many independent input variables (X) and the dependent output variable \\n(Y), such that they satisfy the predicted Y value of the form:\\n𝑌𝑌̂=𝑊𝑊𝑇𝑇𝑋𝑋𝑋𝑋𝑋  \\nwhere 𝑋𝑋 𝑋 𝑋𝑋 1,𝑋2,...,𝑋n}  are the n independent input variables, and 𝑊𝑊 𝑊 𝑊𝑊𝑊 1,𝑊𝑊2,..., 𝑊𝑊𝑛𝑛} are the linear \\ncoefficients, with b as the bias term.\\nAs before, the linear coefficients W s are estimated using the method of least squares, that is, minimizing \\nthe sum of squared differences between predicted values ( 𝑌𝑌̂ ) and observed values (Y). Thus, we try to \\nminimize the loss function (also called squared error, and if we divide by n , it is the mean squared error):\\n𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑙 𝑙𝑙𝑙𝑙 𝑖𝑖−𝑙𝑙̂𝑖𝑖)\\n𝑖𝑖2\\n \\nwhere the sum is over all the training samples.\\nAs you might have guessed, now, instead of two, we will have n+1 equations, which we will need to \\nsimultaneously solve. An easier alternative will be to use the TensorFlow Keras API. We will learn \\nshortly how to use the TensorFlow Keras API to perform the task of regression.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7eaac1da-f58e-4551-92cc-ac24a6b979f7', embedding=None, metadata={'page_label': '49', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 49\\nMultivariate linear regression\\nThere can be cases where the independent variables affect more than one dependent variable. For \\nexample, consider the case where we want to predict a rocket’s speed and its carbon dioxide emission \\n– these two will now be our dependent variables, and both will be affected by the sensors reading the \\nfuel amount, engine type, rocket body, and so on. This is a case of multivariate linear regression. \\nMathematically, a multivariate regression model can be represented as: \\n𝑌𝑌̂𝑖𝑖𝑖𝑖=𝑤𝑤0𝑖𝑖+∑𝑤𝑤 𝑘𝑘𝑖𝑖𝑥𝑥𝑖𝑖𝑘𝑘𝑝𝑝\\n𝑘𝑘𝑘𝑘 \\nwhere 𝑖𝑖𝑖[1,…,𝑛𝑛]  and 𝑗𝑗𝑗[1,…,𝑚𝑚] . The term 𝑌𝑌̂𝑖𝑖𝑖𝑖  represents the jth predicted output value corresponding \\nto the ith input sample, w  represents the regression coefficients, and x ik is the kth feature of the ith \\ninput sample. The number of equations needed to solve in this case will now be n x m. While we can \\nsolve these equations using matrices, the process will be computationally expensive as it will involve \\ncalculating the inverse and determinants. An easier way would be to use the gradient descent with \\nthe sum of least square error as the loss function and to use one of the many optimizers that the \\nTensorFlow API includes.\\nIn the next section, we will delve deeper into the TensorFlow Keras API, a versatile higher-level API \\nto develop your model with ease.\\nNeural networks for linear regression\\nIn the preceding sections, we used mathematical expressions for calculating the coefficients of a linear \\nregression equation. In this section, we will see how we can use the neural networks to perform the \\ntask of regression and build a neural network model using the TensorFlow Keras API.\\nBefore performing regression using neural networks, let us first review what a neural network is. \\nSimply speaking, a neural network is a network of many artificial neurons. From Chapter 1, Neural \\nNetwork Foundations with TF, we know that the simplest neural network, the (simple) perceptron, can \\nbe mathematically represented as:\\n𝑦𝑦 𝑦 𝑦𝑦𝑦(𝑊𝑊𝑇𝑇𝑥𝑥𝑥𝑥𝑥) \\nwhere f is the activation function. Consider, if we have f  as a linear function, then the above expression \\nis similar to the expression of linear regression that we learned in the previous section. In other \\nwords, we can say that a neural network, which is also called a function approximator, is a generalized  \\nregressor. Let us try to build a neural network simple regressor next using the TensorFlow Keras API.\\nSimple linear regression using TensorFlow Keras\\nIn the first chapter, we learned about how to build a model in TensorFlow Keras. Here, we will use the \\nsame Sequential  API to build a single-layered perceptron (fully connected neural network) using the \\nDense  class. We will continue with the same problem, that is, predicting the price of a house given \\nits area:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a52d8ffe-3099-4693-a56a-412287be1e11', embedding=None, metadata={'page_label': '50', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 50\\n1. We start with importing the packages we will need. Notice the addition of the Keras  module \\nand the Dense  layer in importing packages:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport tensorflow.keras as K\\nfrom tensorflow.keras.layers import Dense\\n2. Next, we generate the data, as in the previous case:\\n#Generate a random data\\nnp.random.seed( 0)\\narea = 2.5 * np.random.randn( 100) + 25\\nprice = 25 * area + 5 + np.random.randint( 20,50, size = len(area))\\ndata = np.array([area, price])\\ndata = pd.DataFrame(data = data.T, columns=[ \\'area\\',\\'price\\' ])\\nplt.scatter(data[ \\'area\\'], data[ \\'price\\'])\\nplt.show()\\n3. The input to neural networks should be normalized; this is because input gets multiplied with \\nweights, and if we have very large numbers, the result of multiplication will be large, and soon \\nour metrics may cross infinity (the largest number your computer can handle):\\ndata = (data - data. min()) / (data. max() - data. min())  #Normalize\\n4. Let us now build the model; since it is a simple linear regressor, we use a Dense  layer with \\nonly one unit:\\nmodel = K.Sequential([\\n                      Dense( 1, input_shape = [ 1,], activation= None)\\n])\\nmodel.summary()\\nModel: \"sequential\"\\n____________________________________________________________\\n Layer (type)           Output Shape              Param #   \\n============================================================\\n dense (Dense)          (None, 1)                 2         \\n                                                            \\n============================================================\\nTotal params: 2\\nTrainable params: 2\\nNon-trainable params: 0\\n____________________________________________________________', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a750c3a-d132-478d-bde4-ab5ff0990c5b', embedding=None, metadata={'page_label': '51', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 51\\n5. To train a model, we will need to define the loss function and optimizer. The loss function defines \\nthe quantity that our model tries to minimize, and the optimizer decides the minimization \\nalgorithm we are using. Additionally, we can also define metrics, which is the quantity we want \\nto log as the model is trained. We define the loss function, optimizer  (see Chapter 1, Neural \\nNetwork Foundations with TF), and metrics using the compile  function:\\nmodel.compile(loss='mean_squared_error' , optimizer= 'sgd')\\n6. Now that model is defined, we just need to train it using the fit function. Observe that we are \\nusing a batch_size  of 32 and splitting the data into training and validation datasets using the \\nvalidation_spilt  argument of the fit function: \\nmodel.fit(x=data[ 'area'],y=data[ 'price'], epochs= 100, batch_size= 32, \\nverbose= 1, validation_split= 0.2)\\nmodel.fit(x=data['area'],y=data['price'], epochs=100, batch_size=32, \\nverbose=1, validation_split=0.2)\\nEpoch 1/100\\n3/3 [==============================] - 0s 78ms/step - loss: 1.2643 - val_\\nloss: 1.4828\\nEpoch 2/100\\n3/3 [==============================] - 0s 13ms/step - loss: 1.0987 - val_\\nloss: 1.3029\\nEpoch 3/100\\n3/3 [==============================] - 0s 13ms/step - loss: 0.9576 - val_\\nloss: 1.1494\\nEpoch 4/100\\n3/3 [==============================] - 0s 16ms/step - loss: 0.8376 - val_\\nloss: 1.0156\\nEpoch 5/100\\n3/3 [==============================] - 0s 15ms/step - loss: 0.7339 - val_\\nloss: 0.8971\\nEpoch 6/100\\n3/3 [==============================] - 0s 16ms/step - loss: 0.6444 - val_\\nloss: 0.7989\\nEpoch 7/100\\n3/3 [==============================] - 0s 14ms/step - loss: 0.5689 - val_\\nloss: 0.7082\\n.\\n.\\n.\\nEpoch 96/100\\n3/3 [==============================] - 0s 22ms/step - loss: 0.0827 - val_\\nloss: 0.0755\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6560357-1098-4b5f-b5fa-b138d08a9b7b', embedding=None, metadata={'page_label': '52', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 52\\nEpoch 97/100\\n3/3 [==============================] - 0s 17ms/step - loss: 0.0824 - val_\\nloss: 0.0750\\nEpoch 98/100\\n3/3 [==============================] - 0s 14ms/step - loss: 0.0821 - val_\\nloss: 0.0747\\nEpoch 99/100\\n3/3 [==============================] - 0s 21ms/step - loss: 0.0818 - val_\\nloss: 0.0740\\nEpoch 100/100\\n3/3 [==============================] - 0s 15ms/step - loss: 0.0815 - val_\\nloss: 0.0740\\n<keras.callbacks.History at 0x7f7228d6a790>\\n7. Well, you have successfully trained a neural network to perform the task of linear regression. \\nThe mean squared error after training for 100 epochs is 0.0815 on training data and 0.074 on \\nvalidation data. We can get the predicted value for a given input using the predict  function: \\ny_pred = model.predict(data[ \\'area\\'])\\n8. Next, we plot a graph of the predicted and the actual data: \\nplt.plot(data[ \\'area\\'], y_pred, color= \\'red\\',label=\"Predicted Price\" )\\nplt.scatter(data[ \\'area\\'], data[ \\'price\\'], label= \"Training Data\" )\\nplt.xlabel( \"Area\")\\nplt.ylabel( \"Price\")\\nplt.legend()\\n9. Figure 2.3 shows the plot between the predicted data and the actual data. You can see that, just \\nlike the linear regressor, we have got a nice linear fit:\\nFigure 2.3: Predicted price vs actual price ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66dbacb5-c215-4b8c-9a4a-c29c31865235', embedding=None, metadata={'page_label': '53', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 53\\n10. In case you are interested in knowing the coefficients W and b, we can do it by printing the \\nweights of the model using model.weights :\\n[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, \\nnumpy=array([[-0.33806288]], dtype=float32)>,\\n<tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, \\nnumpy=array([0.68142694], dtype=float32)>]\\nWe can see from the result above that our coefficients are W= 0.69  and bias b= 0.127 . Thus, using \\nlinear regression, we can find a linear relationship between the house price and its area. In the next \\nsection, we explore multiple and multivariate linear regression using the TensorFlow Keras API. \\nMultiple and multivariate linear regression using the TensorFlow \\nKeras API\\nThe example in the previous section had only one independent variable, the area  of the house, and \\none dependent variable, the price of the house. However, problems in real life are not that simple; we \\nmay have more than one independent variable, and we may need to predict more than one dependent \\nvariable. As you must have realized from the discussion on multiple and multivariate regression, they \\ninvolve solving multiple equations. We can make our tasks easier by using the Keras API for both tasks.\\nAdditionally, we can have more than one neural network layer, that is, we can build a deep neural \\nnetwork. A deep neural network is like applying multiple function approximators:\\n𝑓𝑓(𝑥𝑥)= 𝑓𝑓 𝐿𝐿(𝑓𝑓𝐿𝐿𝐿𝐿(…𝑓𝑓 𝐿(𝑥𝑥)…)) \\nwith 𝑓𝑓𝐿𝐿  being the function at layer L . From the expression above, we can see that if f  was a linear function, \\nadding multiple layers of a neural network was not useful; however, using a non-linear activation \\nfunction (see Chapter 1, Neural Network Foundations with TF, for more details) allows us to apply neural \\nnetworks to the regression problems where dependent and independent variables are related in some \\nnon-linear fashion. In this section, we will use a deep neural network, built using TensorFlow Keras, \\nto predict the fuel efficiency of a car, given its number of cylinders, displacement, acceleration, and \\nso on. The data we use is available from the UCI ML repository (Blake, C., & Merz, C. (1998), the UCI \\nrepository of machine learning databases ( http://www.ics.uci.edu/~mlearn/MLRepository.html ):\\n1. We start by importing the modules that we will need. In the previous example, we normalized \\nour data using the DataFrame operations. In this example, we will make use of the Keras \\nNormalization  layer. The Normalization  layer shifts the data to a zero mean and one standard \\ndeviation. Also, since we have more than one independent variable, we will use Seaborn to \\nvisualize the relationship between different variables:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport tensorflow.keras as K\\nfrom tensorflow.keras.layers import Dense, Normalization\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad5ddc47-a5ed-4bff-bb43-44b3b4e2f8e1', embedding=None, metadata={'page_label': '54', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Regression and Classification 54\\nimport seaborn as sns\\n2. Let us first download the data from the UCI ML repo.\\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-\\nmpg/auto-mpg.data'\\ncolumn_names = [ 'mpg', 'cylinders' , 'displacement' , 'horsepower' , \\n'weight' , 'acceleration' , 'model_year' , 'origin' ]\\ndata = pd.read_csv(url, names=column_names, na_values= '?', comment= '\\\\t', \\nsep=' ', skipinitialspace= True)\\n3. The data consists of eight features: mpg, cylinders, displacement, horsepower, weight, \\nacceleration, model year, and origin. Though the origin of the vehicle can also affect the fuel \\nefficiency “mpg” (miles per gallon), we use only seven features to predict the mpg value. Also, \\nwe drop any rows with NaN values: \\ndata = data.drop( 'origin' , 1)\\nprint(data.isna(). sum())\\ndata = data.dropna()\\n4. We divide the dataset into training and test datasets. Here, we are keeping 80% of the 392 \\ndatapoints as training data and 20% as test dataset:\\ntrain_dataset = data.sample(frac= 0.8, random_state= 0)\\ntest_dataset = data.drop(train_dataset.index)\\n5. Next, we use Seaborn’s pairplot  to visualize the relationship between the different variables:\\nsns.pairplot(train_dataset[[ 'mpg', 'cylinders' , \\n'displacement' ,'horsepower' , 'weight' , 'acceleration' , 'model_year' ]], \\ndiag_kind= 'kde')\\n6. We can see that mpg (fuel efficiency) has dependencies on all the other variables, and the  \\ndependency relationship is non-linear, as none of the curves are linear:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6ce9767-24d2-4400-ae97-b7bd073d107d', embedding=None, metadata={'page_label': '55', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 55\\nFigure 2.4: Relationship among different variables of auto-mpg data\\n7. For convenience, we also separate  the variables  into input variables  and the label that we \\nwant to predict:\\ntrain_features = train_dataset.copy()\\ntest_features = test_dataset.copy() \\ntrain_labels = train_features.pop( 'mpg')\\ntest_labels = test_features.pop( 'mpg')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='487fbc2a-84d3-4eab-9be2-636f6ca0f54c', embedding=None, metadata={'page_label': '56', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Regression and Classification 56\\n8. Now, we use the Normalization layer of Keras to normalize our data. Note that while we \\nnormalized our inputs to a value with mean 0 and standard deviation 1, the output prediction \\n'mpg'  remains as it is: \\n#Normalize\\ndata_normalizer = Normalization(axis= 1)\\ndata_normalizer.adapt(np.array(train_features))\\n9. We build our model. The model has two hidden layers, with 64 and 32 neurons, respectively. \\nFor the hidden layers, we have used Rectified Linear Unit (ReLU ) as our activation function; \\nthis should help in approximating the non-linear relation between fuel efficiency and the rest \\nof the variables:\\nmodel = K.Sequential([\\n    data_normalizer,\\n    Dense( 64, activation= 'relu'),\\n    Dense( 32, activation= 'relu'),\\n    Dense( 1, activation= None)\\n])\\nmodel.summary()\\n10. Earlier, we used stochastic gradient as the optimizer; this time, we try the Adam optimizer \\n(see Chapter 1, Neural Network Foundations with TF, for more details). The loss function for the \\nregression we chose is the mean squared error again:\\nmodel.compile(optimizer= 'adam', loss='mean_squared_error' )\\n11. Next, we train the model for 100 epochs:\\nhistory = model.fit(x=train_features,y=train_labels, epochs= 100, \\nverbose= 1, validation_split= 0.2)\\n12. Cool, now that the model is trained, we can check if our model is overfitted, underfitted, or \\nproperly fitted by plotting the loss curve. Both validation loss and training loss are near each \\nother as we increase the training epochs; this suggests that our model is properly trained:\\nplt.plot(history.history[ 'loss'], label= 'loss')\\nplt.plot(history.history[ 'val_loss' ], label= 'val_loss' )\\nplt.xlabel( 'Epoch')\\nplt.ylabel( 'Error [MPG]' )\\nplt.legend()\\nplt.grid( True)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9aae9630-b265-446b-8e10-d0faea3cfd91', embedding=None, metadata={'page_label': '57', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 57\\nFigure 2.5: Model error\\n13. Let us finally compare the predicted fuel efficiency and the true fuel efficiency on the test dataset. \\nRemember that the model has not seen a test dataset ever, thus this prediction is from the \\nmodel’s ability to generalize the relationship between inputs and fuel efficiency. If the model \\nhas learned the relationship well, the two should form a linear relationship:\\ny_pred = model.predict(test_features).flatten()\\na = plt.axes(aspect= 'equal')\\nplt.scatter(test_labels, y_pred)\\nplt.xlabel( 'True Values [MPG]' )\\nplt.ylabel( 'Predictions [MPG]' )\\nlims = [ 0, 50]\\nplt.xlim(lims)\\nplt.ylim(lims)\\nplt.plot(lims, lims)\\nFigure 2.6: Plot between predicted fuel efficiency and actual values\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68d7ab60-c315-411a-9373-bd6fef275d1a', embedding=None, metadata={'page_label': '58', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Regression and Classification 58\\n14. Additionally, we can also plot the error between the predicted and true fuel efficiency:\\nerror = y_pred - test_labels\\nplt.hist(error, bins= 30)\\nplt.xlabel( 'Prediction Error [MPG]' )\\nplt.ylabel( 'Count')\\nFigure 2.7: Prediction error\\nIn case we want to make more than one prediction, that is, dealing with a multivariate regression \\nproblem, the only change would be that instead of one unit in the last dense layer, we will have as \\nmany units as the number of variables to be predicted. Consider, for example, we want to build a \\nmodel which takes into account a student’s SAT score, attendance, and some family parameters, and \\nwants to predict the GPA score for all four undergraduate years; then we will have the output layer \\nwith four units. Now that you are familiar with regression, let us move toward the classification tasks.\\nClassification tasks and decision boundaries\\nTill now, the focus of the chapter was on regression. In this section, we will talk about another \\nimportant task: the task of classification. Let us first understand the difference between regression \\n(also sometimes referred to as prediction) and classification:\\n• In classification, the data is grouped into classes/categories, while in regression, the aim is \\nto get a continuous numerical value for given data. For example, identifying the number of \\nhandwritten digits is a classification task; all handwritten digits will belong to one of the ten \\nnumbers lying between 0-9. The task of predicting the price of the house depending upon \\ndifferent input variables is a regression task.\\n• In a classification task, the model finds the decision boundaries separating one class from \\nanother. In the regression task, the model approximates a function that fits the input-output \\nrelationship.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a656788-6479-4469-b336-4b91c0c7761f', embedding=None, metadata={'page_label': '59', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 59\\n• Classification is a subset of regression; here, we are predicting classes. Regression is much \\nmore general.\\nFigure 2.8 shows how classification and regression tasks differ. In classification, we need to find a line \\n(or a plane or hyperplane in multidimensional space) separating the classes. In regression, the aim is \\nto find a line (or plane or hyperplane) that fits the given input points:\\nFigure 2.8: Classification vs regression\\nIn the following section, we will explain logistic regression, which is a very common and useful \\nclassification technique.\\nLogistic regression\\nLogistic regression is used to determine the probability of an event. Conventionally, the event is \\nrepresented as a categorical dependent variable. The probability of the event is expressed using the \\nsigmoid (or “logit”) function:\\n𝑃𝑃𝑃𝑃𝑃̂=1 | 𝑋𝑋=𝑋𝑋 𝑋=1\\n1+𝑒𝑒−𝑃𝑏𝑏𝑏𝑏𝑏𝑇𝑇𝑥𝑥𝑋 \\nThe goal now is to estimate weights 𝑊𝑊 𝑊 𝑊𝑊𝑊 1,𝑊𝑊2,...,𝑊𝑊𝑛𝑛}  and bias term b. In logistic regression, the \\ncoefficients are estimated using either the maximum likelihood estimator or stochastic gradient \\ndescent. If p is the total number of input data points, the loss is conventionally defined as a cross-\\nentropy term given by:\\n𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑙 𝑙𝑙𝑙 𝑖𝑖log(𝑙𝑙̂𝑖𝑖)+(1−𝑙𝑙𝑖𝑖)𝑝𝑝\\n1=1log(1−𝑙𝑙̂𝑖𝑖) \\nLogistic regression is used in classification problems. For example, when looking at medical data, we \\ncan use logistic regression to classify whether a person has cancer or not. If the output categorical \\nvariable has two or more levels, we can use multinomial logistic regression. Another common technique \\nused for two or more output variables is one versus all.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ac53ef7-d22c-4477-bd2a-8e53d84236d5', embedding=None, metadata={'page_label': '60', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 60\\nFor multiclass logistic regression, the cross-entropy loss function is modified as:\\n𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑙 𝑙𝑙𝑙𝑙 𝑖𝑖𝑖𝑖log𝑙𝑙̂𝑖𝑖𝑖𝑖𝑘𝑘\\n𝑖𝑖𝑗𝑗𝑝𝑝\\n𝑖𝑖𝑗𝑗 \\nwhere K is the total number of classes. You can read more about logistic regression at https://\\nen.wikipedia.org/wiki/Logistic_regression .\\nNow that you have some idea about logistic regression, let us see how we can apply it to any dataset.\\nLogistic regression on the MNIST dataset\\nNext, we will use TensorFlow Keras to classify handwritten digits using logistic regression. We will be \\nusing the MNIST ( Modified National Institute of Standards and Technology) dataset. For those working \\nin the field of deep learning, MNIST is not new, it is like the ABC of machine learning. It contains \\nimages of handwritten digits and a label for each image, indicating which digit it is. The label contains \\na value lying between 0-9 depending on the handwritten digit. Thus, it is a multiclass classification.\\nTo implement the logistic regression, we will make a model with only one dense layer. Each class \\nwill be represented by a unit in the output, so since we have 10 classes, the number of units in the \\noutput would be 10. The probability function used in the logistic regression is similar to the sigmoid \\nactivation function; therefore, we use sigmoid activation.\\nLet us build our model:\\n1. The first step is, as always, importing the modules needed. Notice that here we are using another \\nuseful layer from the Keras API, the Flatten  layer. The Flatten  layer helps us to resize the 28 \\nx 28 two-dimensional input images of the MNIST dataset into a 784 flattened array:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport tensorflow.keras as K\\nfrom tensorflow.keras.layers import Dense, Flatten\\n2. We take the input data of MNIST from the tensorflow.keras  dataset:\\n((train_data, train_labels),(test_data, test_labels)) = tf.keras.\\ndatasets.mnist.load_data()\\n3. Next, we preprocess the data. We normalize the images; the MNIST dataset images are black \\nand white images with the intensity value of each pixel lying between 0-255. We divide it by \\n255, so that now the values lie between 0-1:\\ntrain_data = train_data/np.float32( 255)\\ntrain_labels = train_labels.astype(np.int32)  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39e73747-7b94-4067-9924-c7f11b9ed6ff', embedding=None, metadata={'page_label': '61', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 61\\ntest_data = test_data/np.float32( 255)\\ntest_labels = test_labels.astype(np.int32)\\n4. Now, we define a very simple model; it has only one Dense  layer with 10 units, and it takes an \\ninput of size 784. You can see from the output of the model summary that only the Dense  layer \\nhas trainable parameters:\\nmodel = K.Sequential([\\n    Flatten(input_shape=( 28, 28)),\\n    Dense( 10, activation= \\'sigmoid\\' )\\n])\\nmodel.summary()\\nModel: \"sequential\"\\n____________________________________________________________\\n Layer (type)           Output Shape              Param #   \\n============================================================\\n flatten (Flatten)      (None, 784)               0         \\n                                                            \\n dense (Dense)          (None, 10)                7850      \\n                                                            \\n============================================================\\nTotal params: 7,850\\nTrainable params: 7,850\\nNon-trainable params: 0\\n____________________________________________________________\\n5. Since the test labels are integral values, we will use SparseCategoricalCrossentropy  loss with \\nlogits  set to True . The optimizer selected is Adam. Additionally, we also define accuracy as \\nmetrics to be logged as the model is trained. We train our model for 50 epochs, with a train-\\nvalidation split of 80:20:\\nmodel.compile(optimizer= \\'adam\\', loss=tf.keras.losses.\\nSparseCategoricalCrossentropy(from_logits= True), metrics=[ \\'accuracy\\' ])\\nhistory = model.fit(x=train_data,y=train_labels, epochs= 50, verbose= 1, \\nvalidation_split= 0.2)\\n6. Let us see how our simple model has fared by plotting the loss plot. You can see that since the \\nvalidation loss and training loss are diverging, as the training loss is decreasing, the validation \\nloss increases, thus the model is overfitting. You can improve the model performance by \\nadding hidden layers:\\nplt.plot(history.history[ \\'loss\\'], label= \\'loss\\')\\nplt.plot(history.history[ \\'val_loss\\' ], label= \\'val_loss\\' )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ce01a64-c1f7-4af8-95ba-de1d78dfa8fa', embedding=None, metadata={'page_label': '62', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression and Classification 62\\nplt.xlabel( \\'Epoch\\')\\nplt.ylabel( \\'Loss\\')\\nplt.legend()\\nplt.grid( True)\\nFigure 2.9: Loss plot\\n7. To better understand  the result, we build two utility  functions; these functions help us in \\nvisualizing the handwritten digits and the probability of the 10 units in the output:\\ndef plot_image (i, predictions_array, true_label, img):\\n    true_label, img = true_label[i], img[i]\\n    plt.grid( False)\\n    plt.xticks([])\\n    plt.yticks([])\\n    plt.imshow(img, cmap=plt.cm.binary)\\n    predicted_label = np.argmax(predictions_array)\\n    if predicted_label == true_label:\\n      color = \\'blue\\'\\n    else:\\n      color = \\'red\\'\\n    plt.xlabel( \"Pred {} Conf: {:2.0f}% True ({})\" .format(predicted_label,\\n                                  100*np.max(predictions_array),\\n                                  true_label),\\n                                  color=color)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54bd9eea-6b39-48d2-962e-39c46f17ccb1', embedding=None, metadata={'page_label': '63', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 63\\ndef plot_value_array (i, predictions_array, true_label):\\n    true_label = true_label[i]\\n    plt.grid( False)\\n    plt.xticks( range(10))\\n    plt.yticks([])\\n    thisplot = plt.bar( range(10), predictions_array,\\n    color \"#777777\" )\\n    plt.ylim([ 0, 1])\\n    predicted_label = np.argmax(predictions_array)\\n    thisplot[predicted_label].set_color( \\'red\\')\\n    thisplot[true_label].set_color( \\'blue\\')\\n8. Using these utility functions, we plot the predictions:\\npredictions = model.predict(test_data)\\ni = 56\\nplt.figure(figsize=( 10,5))\\nplt.subplot( 1,2,1)\\nplot_image(i, predictions[i], test_labels, test_data)\\nplt.subplot( 1,2,2)\\nplot_value_array(i, predictions[i],  test_labels)\\nplt.show()\\n9. The plot on the left is the image of the handwritten digit, with the predicted label, the confidence \\nin the prediction, and the true label. The image on the right shows the probability (logistic) \\noutput of the 10 units; we can see that the unit which represents the number 4 has the highest \\nprobability:\\nFigure 2.10: Predicted digit and confidence value of the prediction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e72e940-554b-4eac-8576-4334c6857ac5', embedding=None, metadata={'page_label': '64', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Regression and Classification 64\\n10. In this code, to stay true to logistic regression, we used a sigmoid activation function and only \\none Dense  layer. For better performance, adding dense layers and using softmax as the final \\nactivation function will be helpful. For example, the following model gives 97% accuracy on \\nthe validation dataset:\\nbetter_model = K.Sequential([\\n    Flatten(input_shape=( 28, 28)),\\n    Dense( 128,  activation= 'relu'),\\n    Dense( 10, activation= 'softmax' )\\n])\\nbetter_model.summary()\\nYou can experiment by adding more layers, or by changing the number of neurons in each layer, \\nand even changing the optimizer. This will give you a better understanding of how these parameters \\ninfluence the model performance.\\nSummary\\nThis chapter dealt with different types of regression algorithms. We started with linear regression \\nand used it to predict house prices for a simple one-input variable case. We built simple and multiple \\nlinear regression models using the TensorFlow Keras API. The chapter then moved toward logistic \\nregression, which is a very important and useful technique for classifying tasks. The chapter explained \\nthe TensorFlow Keras API and used it to implement both linear and logistic regression for some \\nclassical datasets. The next chapter will introduce you to convolutional neural networks, the most \\ncommercially successful neural network models for image data.\\nReferences\\nHere are some good resources if you are interested in knowing more about the concepts we’ve covered \\nin this chapter:\\n1. TensorFlow website: https://www.tensorflow.org/\\n2. Exploring bivariate numerical data: https://www.khanacademy.org/math/statistics-\\nprobability/describing-relationships-quantitative-data\\n3. Murphy, K. P. (2022). Probabilistic Machine Learning: An introduction, MIT Press.\\n4. Blake, C., & Merz, C. (1998). UCI repository of machine learning databases: http://www.ics.\\nuci.edu/~mlearn/MLRepository.html\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c4977fc-991d-430d-991a-aa47610093ac', embedding=None, metadata={'page_label': '65', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3\\nConvolutional Neural Networks\\nIn Chapter 1, Neural Network Foundations with TF, we discussed dense networks, in which each layer \\nis fully connected to the adjacent layers. We looked at one application of those dense networks in \\nclassifying the MNIST handwritten characters dataset. In that context, each pixel in the input image \\nhas been assigned to a neuron for a total of 784 (28 x 28 pixels) input neurons. However, this strategy \\ndoes not leverage the spatial structure and relationships between each image. In particular, this piece \\nof code is a dense network that transforms the bitmap representing each written digit into a flat vector \\nwhere the local spatial structure is removed. Removing the spatial structure is a problem because \\nimportant information is lost:\\n#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\\nX_train = X_train.reshape( 60000, 784)\\nX_test = X_test.reshape( 10000, 784)\\nConvolutional neural networks leverage spatial information, and they are therefore very well-suited \\nfor classifying images. These nets use an ad hoc architecture inspired by biological data taken from \\nphysiological experiments performed on the visual cortex. Biological studies show that our vision is \\nbased on multiple cortex levels, each one recognizing more and more structured information. First, \\nwe see single pixels, then from that, we recognize simple geometric forms and then more and more \\nsophisticated elements such as objects, faces, human bodies, animals, and so on.\\nConvolutional neural networks are a fascinating subject. Over a short period of time, they have shown \\nthemselves to be a disruptive technology, breaking performance records in multiple domains from text, \\nto video, to speech, going well beyond the initial image processing domain where they were originally \\nconceived. In this chapter, we will introduce the idea of convolutional neural networks (also known \\nas CNNs, DCNNs, and ConvNets), a particular type of neural network that has large importance for \\ndeep learning.\\nThis chapter covers the following topics:\\n• Deep convolutional neural networks\\n• An example of a deep convolutional neural network\\n• Recognizing CIFAR-10 images with deep learning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45ff5578-1450-44f0-b493-446f5f8cd0ec', embedding=None, metadata={'page_label': '66', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 66\\n• Very deep convolutional networks for large-scale image recognition\\n• Deep Inception V3 networks for transfer learning\\n• Other CNN architectures\\n• Style transfer\\nLet’s begin with deep convolutional neural networks.\\nDeep convolutional neural networks\\nA Deep Convolutional Neural Network (DCNN) consists of many neural network layers. Two different \\ntypes of layers, convolutional and pooling (i.e., subsampling), are typically alternated. The depth of \\neach filter increases from left to right in the network. The last stage is typically made of one or more \\nfully connected layers.\\nFigure 3.1: An example of a DCNN\\nThere are three key underlying concepts for ConvNets: local receptive fields, shared weights, and \\npooling. Let’s review them together.\\nLocal receptive fields\\nIf we want to preserve the spatial information of an image or other form of data, then it is convenient \\nto represent each image with a matrix of pixels. Given this, a simple way to encode the local structure \\nis to connect a submatrix of adjacent input neurons into one single hidden neuron belonging to the \\nnext layer. That single hidden neuron represents one local receptive field. Note that this operation \\nis named convolution, and this is where the name for this type of network is derived. You can think \\nabout convolution as the treatment of a matrix by another matrix, referred to as a kernel.All the code files for this chapter can be found at https://packt.link/dltfchp3 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68ed45a6-4fcb-4ed2-b0fd-be54aaccee71', embedding=None, metadata={'page_label': '67', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 67\\nOf course, we can encode more information by having overlapping submatrices. For instance, let’s \\nsuppose that the size of every single submatrix is 5 x 5 and that those submatrices are used with MNIST \\nimages of 28 x 28 pixels. Then we will be able to generate 24 x 24 local receptive field neurons in the \\nhidden layer. In fact, it is possible to slide the submatrices by only 23 positions before touching the \\nborders of the images. In TensorFlow, the number of pixels along one edge of the kernel, or submatrix, \\nis the kernel size, and the stride length is the number of pixels by which the kernel is moved at each \\nstep in the convolution.\\nLet’s define the feature map from one layer to another. Of course, we can have multiple feature maps \\nthat learn independently from each hidden layer. For example, we can start with 28 x 28 input neurons \\nfor processing MNIST images, and then define k feature maps of size 24 x 24 neurons each (again with \\nshape of 5 x 5) in the next hidden layer.\\nShared weights and bias\\nLet’s suppose that we want to move away from the pixel representation in a raw image, by gaining \\nthe ability to detect the same feature independently from the location where it is placed in the input \\nimage. A simple approach is to use the same set of weights and biases for all the neurons in the hidden \\nlayers. In this way, each layer will learn a set of position-independent latent features derived from \\nthe image, bearing in mind that a layer consists of a set of kernels in parallel, and each kernel only \\nlearns one feature.\\nA mathematical example\\nOne simple way to understand convolution is to think about a sliding window function applied to a \\nmatrix. In the following example, given the input matrix I and the kernel K, we get the convolved \\noutput. The 3 x 3 kernel K (sometimes called the filter or feature detector) is multiplied elementwise \\nwith the input matrix to get one cell in the output matrix. All the other cells are obtained by sliding \\nthe window over I:\\nJ\\n1 1 1 0 0\\n0 1 1 1 0\\n0 0 1 1 1\\n0 0 1 1 0\\n0 1 1 0 0K\\n1 0 1\\n0 1 0\\n1 0 1Convolved\\n4 3 4\\n2 4 3\\n2 3 4\\nIn this example, we decided to stop the sliding window as soon as we touch the borders of I (so the \\noutput is 3 x 3). Alternatively, we could have chosen to pad the input with zeros (so that the output \\nwould have been 5 x 5). This decision relates to the padding choice adopted. Note that kernel depth \\nis equal to input depth (channel).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75de9fd6-509e-42ff-ac6e-349ff7261c6d', embedding=None, metadata={'page_label': '68', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 68\\nAnother choice is about how far along we slide our sliding windows with each step. This is called \\nthe stride and it can be one or more. A larger stride generates fewer applications of the kernel and \\na smaller output size, while a smaller stride generates more output and retains more information.\\nThe size of the filter, the stride, and the type of padding are hyperparameters that can be fine-tuned \\nduring the training of the network.\\nConvNets in TensorFlow\\nIn TensorFlow, if we want to add a convolutional layer with 32 parallel features and a filter size of 3x3, \\nwe write:\\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models\\nmodel = models.Sequential()\\nmodel.add(layers.Conv2D( 32, (3, 3), activation= 'relu', input_shape=( 28, 28, \\n1)))\\nThis means that we are applying a 3x3 convolution on 28x28 images with 1 input channel (or input \\nfilters) resulting in 32 output channels (or output filters).\\nAn example of convolution is provided in Figure 3.2:\\nFigure.3.2: An example of convolution\\nPooling layers\\nLet’s suppose that we want to summarize the output of a feature map. Again, we can use the spatial \\ncontiguity of the output produced from a single feature map and aggregate the values of a sub-matrix \\ninto one single output value synthetically describing the “meaning” associated with that physical region.\\nMax pooling\\nOne easy and common choice is the so-called max pooling operator, which simply outputs the \\nmaximum activation as observed in the region. In Keras, if we want to define a max pooling layer of \\nsize 2 x 2, we write:\\nmodel.add(layers.MaxPooling2D(( 2, 2)))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4709efe-1d66-4666-a813-49fe97a8a476', embedding=None, metadata={'page_label': '69', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 69\\nAn example of the max-pooling operation is given in Figure 3.3:\\nFigure 3.3: An example of max pooling\\nAverage pooling\\nAnother choice is average pooling, which simply aggregates a region into the average values of the \\nactivations observed in that region.\\nNote that Keras implements a large number of pooling layers, and a complete list is available online \\n(see https://keras.io/layers/pooling/ ). In short, all the pooling operations are nothing more than \\na summary operation on a given region.\\nConvNets summary\\nSo far, we have described the basic concepts of ConvNets. CNNs apply convolution and pooling \\noperations in one dimension for audio and text data along the time dimension, in two dimensions \\nfor images along the (height x width) dimensions, and in three dimensions for videos along the (height \\nx width x time) dimensions. For images, sliding the filter over an input volume produces a map that \\nprovides the responses of the filter for each spatial position. \\nIn other words, a ConvNet has multiple filters stacked together that learn to recognize specific visual \\nfeatures independently from the location in the image itself. Those visual features are simple in the \\ninitial layers of the network and become more and more sophisticated deeper in the network. Training \\nof a CNN requires the identification of the right values for each filter so that an input, when passed \\nthrough multiple layers, activates certain neurons of the last layer so that it will predict the correct \\nvalues.\\nAn example of DCNN: LeNet\\nYann LeCun, who won the Turing Award, proposed [1] a family of ConvNets named LeNet, trained \\nfor recognizing MNIST handwritten characters with robustness to simple geometric transformations \\nand distortion. The core idea of LeNet is to have lower layers alternating convolution operations with \\nmax-pooling operations. The convolution operations are based on carefully chosen local receptive \\nfields with shared weights for multiple feature maps. Then, higher levels are fully connected based \\non a traditional MLP with hidden layers and softmax as the output layer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='827b57c1-c30b-4a7a-b46b-d434708b0f84', embedding=None, metadata={'page_label': '70', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 70\\nLeNet code in TF\\nTo define a LeNet in code, we use a convolutional 2D module (note that tf.keras.layers.Conv2D  is \\nan alias of tf.keras.layers.Convolution2D , so the two can be used in an interchangeable way – see \\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D ):\\nlayers.Convolution2D( 20, (5, 5), activation= 'relu', input_shape=input_shape)\\nwhere the first parameter is the number of output filters in the convolution and the next tuple is \\nthe extension of each filter. An interesting optional parameter is padding. There are two options: \\npadding='valid'  means that the convolution is only computed where the input and the filter fully \\noverlap and therefore the output is smaller than the input, while padding='same'  means that we have \\nan output that is the same  size as the input, for which the area around the input is padded with zeros.\\nIn addition, we use a MaxPooling2D  module:\\nlayers.MaxPooling2D(pool_size=( 2, 2), strides=( 2, 2))\\nwhere pool_size=(2, 2)  is a tuple of 2 integers representing the factors by which the image is vertically \\nand horizontally downscaled. So (2, 2) will halve the image in each dimension, and strides=(2, 2)  \\nis the stride used for processing.\\nNow, let us review the code. First, we import a number of modules:\\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models, optimizers\\n# network and training\\nEPOCHS = 5\\nBATCH_SIZE = 128\\nVERBOSE = 1\\nOPTIMIZER = tf.keras.optimizers.Adam()\\nVALIDATION_SPLIT= 0.90\\nIMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\\nINPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\\nNB_CLASSES = 10  # number of outputs = number of digits\\nThen we define the LeNet network:\\n#define the convnet \\ndef build(input_shape, classes):\\n    model = models.Sequential()\\nWe have a first convolutional stage with ReLU activations followed by max pooling. Our network will \\nlearn 20 convolutional filters, each one of which has a size of 5x5. The output dimension is the same as \\nthe input shape, so it will be 28 x 28. Note that since Convolutional2D  is the first stage of our pipeline, \\nwe are also required to define its input_shape . \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4a87943-6f18-4ffa-a0ab-7eca2a4746d2', embedding=None, metadata={'page_label': '71', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 71\\nThe max pooling operation implements a sliding window which slides over the layer and takes the \\nmaximum of each region with a step of two pixels both vertically and horizontally:\\n# CONV => RELU => POOL\\nmodel.add(layers.Convolution2D( 20, (5, 5), activation= \\'relu\\',\\n            input_shape=input_shape))\\nmodel.add(layers.MaxPooling2D(pool_size=( 2, 2), strides=( 2, 2)))\\nThen there is a second convolutional stage with ReLU activations, followed again by a max pooling \\nlayer. In this case, we increase the number of convolutional filters learned to 50 from the previous 20. \\nIncreasing the number of filters in deeper layers is a common technique in deep learning:\\n# CONV => RELU => POOL\\nmodel.add(layers.Convolution2D( 50, (5, 5), activation= \\'relu\\'))\\nmodel.add(layers.MaxPooling2D(pool_size=( 2, 2), strides=( 2, 2)))\\nThen we have a pretty standard flattening and a dense network of 500 neurons, followed by a softmax \\nclassifier with 10 classes:\\n# Flatten => RELU layers\\nmodel.add(layers.Flatten())\\nmodel.add(layers.Dense( 500, activation= \\'relu\\'))\\n# a softmax classifier\\nmodel.add(layers.Dense(classes, activation= \"softmax\" ))\\nreturn model\\nCongratulations, you have just defined your first deep convolutional learning network! Let’s see how \\nit looks visually:\\nFigure 3.4: Visualization of LeNet ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7851757d-3a17-4f12-83ed-769cf91554ee', embedding=None, metadata={'page_label': '72', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 72\\nNow we need some additional code for training the network, but this is very similar to what we \\ndescribed in Chapter 1, Neural Network Foundations with TF. This time we also show the code for \\nprinting the loss:\\n# data: shuffled and split between train and test sets\\n(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\\n# reshape\\nX_train = X_train.reshape(( 60000, 28, 28, 1))\\nX_test = X_test.reshape(( 10000, 28, 28, 1))\\n# normalize\\nX_train, X_test = X_train / 255.0, X_test / 255.0\\n# cast\\nX_train = X_train.astype( \\'float32\\' )\\nX_test = X_test.astype( \\'float32\\' )\\n# convert class vectors to binary class matrices\\ny_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\\ny_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\\n# initialize the optimizer and model\\nmodel = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\\nmodel.compile(loss=\"categorical_crossentropy\" , optimizer=OPTIMIZER,\\n    metrics=[ \"accuracy\" ])\\nmodel.summary()\\n# use TensorBoard, princess Aurora!\\ncallbacks = [\\n  # Write TensorBoard logs to \\'./logs\\' directory\\n  tf.keras.callbacks.TensorBoard(log_dir= \\'./logs\\' )\\n]\\n# fit \\nhistory = model.fit(X_train, y_train, \\n        batch_size=BATCH_SIZE, epochs=EPOCHS, \\n        verbose=VERBOSE, validation_split=VALIDATION_SPLIT,\\n        callbacks=callbacks)\\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\\nprint(\"\\\\nTest score:\" , score[ 0])\\nprint(\\'Test accuracy:\\' , score[ 1])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e02cd084-4643-479b-b791-761bcdcb18f0', embedding=None, metadata={'page_label': '73', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 73\\nNow let’s run the code. As you can see in Figure 3.5, the time had a significant increase, and each \\niteration in our deep net now takes ~28 seconds against ~1-2 seconds for the network defined in \\nChapter 1, Neural Network Foundations with TF. However, the accuracy reached a new peak at 99.991% \\non training, 99.91% on validation, and 99.15% on test!\\nFigure 3.5: LeNet accuracy\\nLet’s see the execution of a full run for 20 epochs:\\nModel: \"sequential_1\"\\n_____________________________________________________________________\\nLayer (type)                    Output Shape              Param #    \\n=====================================================================\\nconv2d_2 (Conv2D)               (None, 24, 24, 20)        520        \\n                                                                     \\nmax_pooling2d_2 (MaxPooling  2D) (None, 12, 12, 20)       0          \\n                                                                     \\nconv2d_3 (Conv2D)               (None, 8, 8, 50)          25050      \\n                                                                     \\nmax_pooling2d_3 (MaxPooling  2D) (None, 4, 4, 50)         0          \\n                                                                     \\nflatten   (Flatten)             (None, 800)               0          \\n                                                                     \\ndense   (Dense)                 (None, 500)               400500     \\n                                                                     \\ndense_1 (Dense)                 (None, 10)                5010    ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a4058df4-f0b8-4319-a99f-345a9745c46c', embedding=None, metadata={'page_label': '74', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 74\\n                                                                     \\n=====================================================================\\nTotal params: 431,080\\nTrainable params: 431,080\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 48000 samples, validate on 12000 samples\\nEpoch 1/20\\n[2019-04-04 14:18:28.546158: I tensorflow/core/profiler/lib/profiler_session.\\ncc:164] Profile Session started.\\n48000/48000 [==============================] - 28s 594us/sample - loss: 0.2035 \\n- accuracy: 0.9398 - val_loss: 0.0739 - val_accuracy: 0.9783\\nEpoch 2/20\\n48000/48000 [==============================] - 26s 534us/sample - loss: 0.0520 \\n- accuracy: 0.9839 - val_loss: 0.0435 - val_accuracy: 0.9868\\nEpoch 3/20\\n48000/48000 [==============================] - 27s 564us/sample - loss: 0.0343 \\n- accuracy: 0.9893 - val_loss: 0.0365 - val_accuracy: 0.9895\\nEpoch 4/20\\n48000/48000 [==============================] - 27s 562us/sample - loss: 0.0248 \\n- accuracy: 0.9921 - val_loss: 0.0452 - val_accuracy: 0.9868\\nEpoch 5/20\\n48000/48000 [==============================] - 27s 562us/sample - loss: 0.0195 \\n- accuracy: 0.9939 - val_loss: 0.0428 - val_accuracy: 0.9873\\nEpoch 6/20\\n48000/48000 [==============================] - 28s 548us/sample - loss: 0.0585 \\n- accuracy: 0.9820 - val_loss: 0.1038 - val_accuracy: 0.9685\\nEpoch 7/20\\n48000/48000 [==============================] - 26s 537us/sample - loss: 0.0134 \\n- accuracy: 0.9955 - val_loss: 0.0388 - val_accuracy: 0.9896\\nEpoch 8/20\\n48000/48000 [==============================] - 29s 589us/sample - loss: 0.0097 \\n- accuracy: 0.9966 - val_loss: 0.0347 - val_accuracy: 0.9899\\nEpoch 9/20\\n48000/48000 [==============================] - 29s 607us/sample - loss: 0.0091 \\n- accuracy: 0.9971 - val_loss: 0.0515 - val_accuracy: 0.9859\\nEpoch 10/20\\n48000/48000 [==============================] - 27s 565us/sample - loss: 0.0062 \\n- accuracy: 0.9980 - val_loss: 0.0376 - val_accuracy: 0.9904\\nEpoch 11/20\\n48000/48000 [==============================] - 30s 627us/sample - loss: 0.0068 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eb17f401-f566-43b5-a8c1-4f092c07e1cf', embedding=None, metadata={'page_label': '75', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 75\\n- accuracy: 0.9976 - val_loss: 0.0366 - val_accuracy: 0.9911\\nEpoch 12/20\\n48000/48000 [==============================] - 24s 505us/sample - loss: 0.0079 \\n- accuracy: 0.9975 - val_loss: 0.0389 - val_accuracy: 0.9910\\nEpoch 13/20\\n48000/48000 [==============================] - 28s 584us/sample - loss: 0.0057 \\n- accuracy: 0.9978 - val_loss: 0.0531 - val_accuracy: 0.9890\\nEpoch 14/20\\n48000/48000 [==============================] - 28s 580us/sample - loss: 0.0045 \\n- accuracy: 0.9984 - val_loss: 0.0409 - val_accuracy: 0.9911\\nEpoch 15/20\\n48000/48000 [==============================] - 26s 537us/sample - loss: 0.0039 \\n- accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 0.9911\\nEpoch 16/20\\n48000/48000 [==============================] - 25s 513us/sample - loss: 0.0059 \\n- accuracy: 0.9983 - val_loss: 0.0480 - val_accuracy: 0.9890\\nEpoch 17/20\\n48000/48000 [==============================] - 24s 499us/sample - loss: 0.0042 \\n- accuracy: 0.9988 - val_loss: 0.0535 - val_accuracy: 0.9888\\nEpoch 18/20\\n48000/48000 [==============================] - 24s 505us/sample - loss: 0.0042 \\n- accuracy: 0.9986 - val_loss: 0.0349 - val_accuracy: 0.9926\\nEpoch 19/20\\n48000/48000 [==============================] - 29s 599us/sample - loss: 0.0052 \\n- accuracy: 0.9984 - val_loss: 0.0377 - val_accuracy: 0.9920\\nEpoch 20/20\\n48000/48000 [==============================] - 25s 524us/sample - loss: 0.0028 \\n- accuracy: 0.9991 - val_loss: 0.0477 - val_accuracy: 0.9917\\n10000/10000 [==============================] - 2s 248us/sample - loss: 0.0383 - \\naccuracy: 0.9915\\nTest score: 0.03832608199457617\\nTest accuracy: 0.9915\\nLet’s plot the model  accuracy and the model loss, and we understand that we can train in only 10 \\niterations to achieve a similar accuracy of 99.1%:\\nTrain on 48000 samples, validate on 12000 samples\\nEpoch 1/10\\n[2019-04-04 15:57:17.848186: I tensorflow/core/profiler/lib/profiler_session.\\ncc:164] Profile Session started.\\n48000/48000 [==============================] - 26s 544us/sample - loss: 0.2134 \\n- accuracy: 0.9361 - val_loss: 0.0688 - val_accuracy: 0.9783\\nEpoch 2/10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75e0afe1-d532-4e8f-a4e4-a4c518b959ee', embedding=None, metadata={'page_label': '76', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 76\\n48000/48000 [==============================] - 30s 631us/sample - loss: 0.0550 \\n- accuracy: 0.9831 - val_loss: 0.0533 - val_accuracy: 0.9843\\nEpoch 3/10\\n48000/48000 [==============================] - 30s 621us/sample - loss: 0.0353 \\n- accuracy: 0.9884 - val_loss: 0.0410 - val_accuracy: 0.9874\\nEpoch 4/10\\n48000/48000 [==============================] - 37s 767us/sample - loss: 0.0276 \\n- accuracy: 0.9910 - val_loss: 0.0381 - val_accuracy: 0.9887\\nEpoch 5/10\\n48000/48000 [==============================] - 24s 509us/sample - loss: 0.0200 \\n- accuracy: 0.9932 - val_loss: 0.0406 - val_accuracy: 0.9881\\nEpoch 6/10\\n48000/48000 [==============================] - 31s 641us/sample - loss: 0.0161 \\n- accuracy: 0.9950 - val_loss: 0.0423 - val_accuracy: 0.9881\\nEpoch 7/10\\n48000/48000 [==============================] - 29s 613us/sample - loss: 0.0129 \\n- accuracy: 0.9955 - val_loss: 0.0396 - val_accuracy: 0.9894\\nEpoch 8/10\\n48000/48000 [==============================] - 27s 554us/sample - loss: 0.0107 \\n- accuracy: 0.9965 - val_loss: 0.0454 - val_accuracy: 0.9871\\nEpoch 9/10\\n48000/48000 [==============================] - 24s 510us/sample - loss: 0.0082 \\n- accuracy: 0.9973 - val_loss: 0.0388 - val_accuracy: 0.9902\\nEpoch 10/10\\n48000/48000 [==============================] - 26s 542us/sample - loss: 0.0083 \\n- accuracy: 0.9970 - val_loss: 0.0440 - val_accuracy: 0.99892\\n10000/10000 [==============================] - 2s 196us/sample - loss: 0.0327 - \\naccuracy: 0.9910\\nTest score: 0.03265062951518773\\nTest accuracy: 0.991\\nLet us see some of the MNIST images just to understand how good the number 99.1% is! For instance, \\nthere are many ways in which humans write a 9, one of them being in Figure 3.6. The same goes for \\n3, 7, 4, and 5, and number 1 in this figure is so difficult to recognize that even a human would likely \\nhave trouble:\\nFigure 3.6: An example of MNIST handwritten characters', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29f7fe22-0242-48ff-b53d-f03b943bb3d9', embedding=None, metadata={'page_label': '77', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 77\\nWe can summarize all the progress made so far with our different models in the following graph. Our \\nsimple net started with an accuracy of 90.71%, meaning that about 9 handwritten characters out of \\n100 are not correctly recognized. Then, we gained 8% with the deep learning architecture, reaching \\nan accuracy of 99.2%, which means that less than one handwritten character out of one hundred is \\nincorrectly recognized, as shown in Figure 3.7:\\nFigure 3.7: Accuracy for different models and optimizers\\nUnderstanding the power of deep learning\\nAnother test we can run for a better understanding of the power of deep learning and ConvNets is to \\nreduce the size of the training set and observe the resulting decay in performance. One way to do this \\nis to split the training set of 50,000 examples into two different sets:\\n• The proper training set used for training our model will progressively reduce in size: 5,900, \\n3,000, 1,800, 600, and 300 examples.\\n• The validation set used to estimate how well our model has been trained will consist of the \\nremaining examples. Our test set is always fixed, and it consists of 10,000 examples.\\nWith this setup, we compare the previously defined deep learning ConvNet against the first example \\nneural network defined in Chapter 1 , Neural Network Foundations with TF . As we can see in the following \\ngraph, our deep network always outperforms the simple network when there is more data available. \\nWith 5,900 training examples, the deep learning net had an accuracy of 97.23% against an accuracy \\nof 94% for the simple net. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='32e2be82-08f8-4694-85d3-f95331c622c2', embedding=None, metadata={'page_label': '78', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 78\\nIn general, deep networks require more training data available to fully express their power, as shown \\nin Figure 3.8:\\n \\nFigure 3.8: Accuracy for different amounts of data\\nA list of state-of-the-art results (for example, the highest performance available) for MNIST is available \\nonline (see http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_\\nresults.html ). As of March 2019, the best result has an error rate of 0.21% [2].\\nRecognizing CIFAR-10 images with deep learning\\nThe CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in three channels, divided into 10 \\nclasses. Each class contains 6,000 images. The training set contains 50,000 images, while the test set \\nprovides 10,000 images. This image taken from the CIFAR repository (see https://www.cs.toronto.\\nedu/~kriz/cifar.html ) shows a few random examples from the 10 classes:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ac79cdb-5159-4945-8356-2cb02e6f6aa3', embedding=None, metadata={'page_label': '79', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 79\\nFigure 3.9: An example of CIFAR-10 images\\nThe goal is to recognize previously unseen images and assign them to one of the ten classes. Let us \\ndefine a suitable deep net.\\nFirst of all, we import a number of useful modules and define a few constants and load the dataset \\n(the full code including the load operations is available online): \\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models, optimizers\\n# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\\nIMG_CHANNELS = 3\\nIMG_ROWS = 32\\nIMG_COLS = 32The images in this section are from Learning Multiple Layers of Features from Tiny Images, Alex \\nKrizhevsky, 2009: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.\\npdf. They are part of the CIFAR-10 dataset (toronto.edu): https://www.cs.toronto.\\nedu/~kriz/cifar.html .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6cfd78b-dae1-4404-bcfe-636ab8bd7e61', embedding=None, metadata={'page_label': '80', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 80\\n#constant\\nBATCH_SIZE = 128\\nEPOCHS = 20\\nCLASSES = 10\\nVERBOSE = 1\\nVALIDATION_SPLIT = 0.2\\nOPTIM = tf.keras.optimizers.RMSprop()\\nOur net will learn 32 convolutional filters, each of which with a 3 x 3 size. The output dimension is \\nthe same as the input shape, so it will be 32 x 32 and the activation function used is a ReLU function, \\nwhich is a simple way of introducing non-linearity. After that, we have a MaxPooling  operation with \\na pool size of 2 x 2 and dropout at 25%:\\n#define the convnet \\ndef build(input_shape, classes):\\n    model = models.Sequential() \\n    model.add(layers.Convolution2D( 32, (3, 3), activation= 'relu',\\n                        input_shape=input_shape))\\n    model.add(layers.MaxPooling2D(pool_size=( 2, 2)))\\n    model.add(layers.Dropout( 0.25)) \\nThe next stage in the deep pipeline is a dense network with 512 units and ReLU activation followed by \\ndropout at 50% and by a softmax layer with 10 classes as output, one for each category:\\n    model.add(layers.Flatten())\\n    model.add(layers.Dense( 512, activation= 'relu'))\\n    model.add(layers.Dropout( 0.5))\\n    model.add(layers.Dense(classes, activation= 'softmax' ))\\n    return  model\\nAfter defining the network, we can train the model. In this case, we split the data and compute a \\nvalidation set in addition to the training and testing sets. The training is used to build our models, \\nthe validation is used to select the best-performing approach, while the test set is used to check the \\nperformance of our best models on fresh unseen data:\\n# use TensorBoard, princess Aurora!\\ncallbacks = [\\n  # Write TensorBoard logs to './logs' directory\\n  tf.keras.callbacks.TensorBoard(log_dir= './logs' )\\n]\\n# train\\nmodel.compile(loss='categorical_crossentropy' , optimizer=OPTIM,\\n    metrics=[ 'accuracy' ])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5414dd13-19cf-45d1-8109-e57dd1236c97', embedding=None, metadata={'page_label': '81', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 81\\n \\nmodel.fit(X_train, y_train, batch_size=BATCH_SIZE,\\n    epochs=EPOCHS, validation_split=VALIDATION_SPLIT, \\n    verbose=VERBOSE, callbacks=callbacks) \\nscore = model.evaluate(X_test, y_test,\\n                     batch_size=BATCH_SIZE, verbose=VERBOSE)\\nprint(\"\\\\nTest score:\" , score[ 0])\\nprint(\\'Test accuracy:\\' , score[ 1])\\nLet’s run the code. Our network reaches a test accuracy of 66.8% with 20 iterations. We also print the \\naccuracy and loss plot and dump the network with model.summary() :\\nEpoch 17/20\\n40000/40000 [==============================] - 112s 3ms/sample - loss: 0.6282 - \\naccuracy: 0.7841 - val_loss: 1.0296 - val_accuracy: 0.6734\\nEpoch 18/20\\n40000/40000 [==============================] - 76s 2ms/sample - loss: 0.6140 - \\naccuracy: 0.7879 - val_loss: 1.0789 - val_accuracy: 0.6489\\nEpoch 19/20\\n40000/40000 [==============================] - 74s 2ms/sample - loss: 0.5931 - \\naccuracy: 0.7958 - val_loss: 1.0461 - val_accuracy: 0.6811\\nEpoch 20/20\\n40000/40000 [==============================] - 71s 2ms/sample - loss: 0.5724 - \\naccuracy: 0.8042 - val_loss: 0.1.0527 - val_accuracy: 0.6773\\n10000/10000 [==============================] - 5s 472us/sample - loss: 1.0423 - \\naccuracy: 0.6686\\nTest score: 1.0423416819572449\\nTest accuracy: 0.6686\\nFigure 3.10 shows the accuracy and loss plot:\\nFigure 3.10: Accuracy and loss for the defined network', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16d3dcb9-d635-4fd5-89b7-a0b256236fcc', embedding=None, metadata={'page_label': '82', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 82\\nWe have seen how to improve accuracy and how the loss changes for CIFAR-10 datasets. The next \\nsection is about improving the current results.\\nImproving the CIFAR-10 performance with a deeper network\\nOne way to improve the performance is to define a deeper network with multiple convolutional \\noperations. In the following example, we have a sequence of modules:\\n1st module: (CONV+CONV+MaxPool+DropOut)\\n2nd module: (CONV+CONV+MaxPool+DropOut)\\n3rd module: (CONV+CONV+MaxPool+DropOut)\\nThese are followed by a standard dense output layer. All the activation functions used are ReLU \\nfunctions. There is a new layer that we also discussed in Chapter 1, Neural Network Foundations with \\nTF, BatchNormalization() , used to introduce a form of regularization between modules:\\ndef build_model (): \\n    model = models.Sequential()\\n    \\n    #1st block\\n    model.add(layers.Conv2D( 32, (3,3), padding= 'same', \\n        input_shape=x_train.shape[ 1:], activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Conv2D( 32, (3,3), padding= 'same', activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.MaxPooling2D(pool_size=( 2,2)))\\n    model.add(layers.Dropout( 0.2))\\n    #2nd block\\n    model.add(layers.Conv2D( 64, (3,3), padding= 'same', activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Conv2D( 64, (3,3), padding= 'same', activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.MaxPooling2D(pool_size=( 2,2)))\\n    model.add(layers.Dropout( 0.3))\\n    #3d block \\n    model.add(layers.Conv2D( 128, (3,3), padding= 'same', activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Conv2D( 128, (3,3), padding= 'same', activation= 'relu'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.MaxPooling2D(pool_size=( 2,2)))\\n    model.add(layers.Dropout( 0.4))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d29330ec-ebc7-487f-bfac-8b3f77cc8249', embedding=None, metadata={'page_label': '83', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 83\\n    #dense  \\n    model.add(layers.Flatten())\\n    model.add(layers.Dense(NUM_CLASSES, activation= 'softmax' ))\\n    return model\\n    model.summary()\\nCongratulations! You have defined a deeper network. Let us run the code for 40 iterations reaching \\nan accuracy of 82%! Let’s add the remaining part of the code for the sake of completeness. The first \\npart is to load and normalize the data:\\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models, regularizers, optimizers\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nimport numpy as np\\n \\nEPOCHS=50\\nNUM_CLASSES = 10\\ndef load_data ():\\n    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\\n    x_train = x_train.astype( 'float32' )\\n    x_test = x_test.astype( 'float32' )\\n \\n    #normalize \\n    mean = np.mean(x_train,axis=( 0,1,2,3))\\n    std = np.std(x_train,axis=( 0,1,2,3))\\n    x_train = (x_train-mean)/(std+ 1e-7)\\n    x_test = (x_test-mean)/(std+ 1e-7)\\n \\n    y_train =  tf.keras.utils.to_categorical(y_train,NUM_CLASSES)\\n    y_test =  tf.keras.utils.to_categorical(y_test,NUM_CLASSES)\\n    return x_train, y_train, x_test, y_test\\nThen we need to have a part to train the network:\\n(x_train, y_train, x_test, y_test) = load_data()\\nmodel = build_model()\\nmodel.compile(loss='categorical_crossentropy' , \\n            optimizer= 'RMSprop' , \\n            metrics=[ 'accuracy' ])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74f6ab9d-4784-46bc-9b13-71ce15704795', embedding=None, metadata={'page_label': '84', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 84\\n#train\\nbatch_size = 64\\nmodel.fit(x_train, y_train, batch_size=batch_size,\\n    epochs=EPOCHS, validation_data=(x_test,y_test)) \\nscore = model.evaluate(x_test, y_test,\\n                     batch_size=batch_size)\\nprint(\"\\\\nTest score:\" , score[ 0])\\nprint(\\'Test accuracy:\\' , score[ 1])\\nSo, we have an improvement of 15.14% with respect to the previous simpler deeper network.\\nImproving the CIFAR-10 performance with data augmentation\\nAnother way to improve the performance is to generate more images for our training. The idea \\nhere is that we can take the standard CIFAR training set and augment this set with multiple types of \\ntransformation, including rotation, rescaling, horizontal or vertical flip, zooming, channel shift, and \\nmany more. Let’s see the code applied on the same network defined in the previous section:\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n#image augmentation\\ndatagen = ImageDataGenerator(\\n    rotation_range= 30,\\n    width_shift_range= 0.2,\\n    height_shift_range= 0.2,\\n    horizontal_flip= True,\\n    )\\ndatagen.fit(x_train)\\nrotation_range  is a value in degrees (0-180) for randomly rotating pictures; width_shift  and \\nheight_shift  are ranges for randomly translating pictures vertically or horizontally; zoom_range  is for \\nrandomly zooming pictures; horizontal_flip  is for randomly flipping half of the images horizontally; \\nfill_mode  is the strategy used for filling in new pixels that can appear after a rotation or a shift.\\nAfter augmentation we have generated many more training images starting from the standard CIFAR-10 \\nset, as shown in Figure 3.11:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='089f59cc-6386-44bb-80f2-0917a32a4d6d', embedding=None, metadata={'page_label': '85', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 85\\nFigure.3.11: An example of image augmentation\\nNow we can apply this intuition directly for training. Using the same ConvNet defined before, we \\nsimply generate more augmented images, and then we train. For efficiency, the generator runs in \\nparallel to the model. This allows an image augmentation on a CPU while training in parallel on a \\nGPU. Here is the code:\\n#train\\nbatch_size = 64\\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\n                    epochs=EPOCHS,\\n                    verbose= 1,validation_data=(x_test,y_test))\\n#save to disk\\nmodel_json = model.to_json()\\nwith open('model.json' , 'w') as json_file:\\n    json_file.write(model_json)\\nmodel.save_weights( 'model.h5' ) \\n#test\\nscores = model.evaluate(x_test, y_test, batch_size= 128, verbose= 1)\\nprint('\\\\nTest result: %.3f loss: %.3f'  % (scores[ 1]*100,scores[ 0])) \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4d755ec-5fe5-45b8-9f79-9b03d20609b6', embedding=None, metadata={'page_label': '86', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 86\\nEach iteration is now more expensive because we have more training data. Therefore, let’s run for 50 \\niterations only. We see that by doing this we reach an accuracy of 85.91%:\\nEpoch 46/50\\n50000/50000 [==============================] - 36s 722us/sample - loss: 0.2440 \\n- accuracy: 0.9183 - val_loss: 0.4918 - val_accuracy: 0.8546\\nEpoch 47/50\\n50000/50000 [==============================] - 34s 685us/sample - loss: 0.2338 \\n- accuracy: 0.9208 - val_loss: 0.4884 - val_accuracy: 0.8574\\nEpoch 48/50\\n50000/50000 [==============================] - 32s 643us/sample - loss: 0.2383 \\n- accuracy: 0.9189 - val_loss: 0.5106 - val_accuracy: 0.8556\\nEpoch 49/50\\n50000/50000 [==============================] - 37s 734us/sample - loss: 0.2285 \\n- accuracy: 0.9212 - val_loss: 0.5017 - val_accuracy: 0.8581\\nEpoch 49/50\\n50000/50000 [==============================] - 36s 712us/sample - loss: 0.2263 \\n- accuracy: 0.9228 - val_loss: 0.4911 - val_accuracy: 0.8591\\n10000/10000 [==============================] - 2s 160us/sample - loss: 0.4911 - \\naccuracy: 0.8591\\nTest score: 0.4911323667049408\\nTest accuracy: 0.8591\\nThe results obtained during our experiments are summarized in the following figure:\\nFigure 3.12: Accuracy on CIFAR-10 with different networks. On the x-axis, we have the increasing \\nnumber of iterations', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1bbe4f38-ab35-4599-94ff-452938486a99', embedding=None, metadata={'page_label': '87', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 87\\nA list of state-of-the-art results for CIFAR-10 is available online (see http://rodrigob.github.io/\\nare_we_there_yet/build/classification_datasets_results.html ). As of April 2019, the best result \\nhas an accuracy of 96.53% [3].\\nPredicting with CIFAR-10\\nLet’s suppose that we want to use the deep learning model we just trained for CIFAR-10 for a bulk \\nevaluation of images. Since we saved the model and the weights, we do not need to train each time:\\nimport numpy as np\\nimport scipy.misc\\nfrom tensorflow.keras.models import model_from_json\\nfrom tensorflow.keras.optimizers import SGD\\n#load model\\nmodel_architecture = 'cifar10_architecture.json'\\nmodel_weights = 'cifar10_weights.h5'\\nmodel = model_from_json( open(model_architecture).read())\\nmodel.load_weights(model_weights)\\n#load images\\nimg_names = [ 'cat-standing.jpg' , 'dog.jpg' ]\\nimgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), ( 32, \\n32)),\\n                     ( 2, 0, 1)).astype( 'float32' )\\n           for img_name in img_names]\\nimgs = np.array(imgs) / 255\\n# train\\noptim = SGD()\\nmodel.compile(loss='categorical_crossentropy' , optimizer=optim,\\n    metrics=[ 'accuracy' ])\\n# predict \\npredictions = model.predict_classes(imgs)\\nprint(predictions)\\nNote that we use SciPy’s imread  to load the images and then resize them to 32 × 32 pixels. The resulting  \\nimage tensor has dimensions of (32, 32, 3). However, we want the color dimension to be first instead \\nof last, so we take the transpose. After that, the list of image tensors is combined into a single tensor \\nand normalized to be between 0 and 1.0.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25ff72fa-84cc-4275-9e56-c14719f4778a', embedding=None, metadata={'page_label': '88', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 88\\nNow let us get the prediction for a \\n and for a \\n . We get categories 3 (cat) and 5 (dog) as \\noutput as expected. We successfully created a ConvNet to classify CIFAR-10 images. Next, we will look \\nat VGG16: a breakthrough in deep learning.\\nVery deep convolutional networks for large-scale image \\nrecognition\\nIn 2014, an interesting contribution to image recognition was presented in the paper Very Deep \\nConvolutional Networks for Large-Scale Image Recognition, K. Simonyan and A. Zisserman [4]. The paper \\nshowed that a significant improvement on the prior-art configurations can be achieved by pushing the \\ndepth to 16-19 weight layers. One model in the paper denoted as D or VGG16 had 16 deep layers. An \\nimplementation in Java Caffe (see http://caffe.berkeleyvision.org/ ) was used for training the \\nmodel on the ImageNet ILSVRC-2012 (see http://image-net.org/challenges/LSVRC/2012/ ) dataset, \\nwhich includes images of 1,000 classes, and is split into three sets: training (1.3M images), validation \\n(50K images), and testing (100K images). Each image is (224 x 224) on 3 channels. The model achieves \\n7.5% top-5 error (the error of the top 5 results) on ILSVRC-2012-val and 7.4% top-5 error on ILSVRC-\\n2012-test.\\nAccording to the ImageNet site:\\nThe weights learned by the model implemented in Caffe have been directly converted ( https://gist.\\ngithub.com/baraldilorenzo/07d7802847aaad0a35d3 ) in tf.Keras  and can be used by preloading \\nthem into the tf.Keras  model, which is implemented below, as described in the paper:\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\n# define a VGG16 network\\ndef VGG_16(weights_path= None):\\n    model = models.Sequential()\\n    model.add(layers.ZeroPadding2D(( 1,1),input_shape=( 224,224, 3)))\\n    model.add(layers.Convolution2D( 64, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 64, (3, 3), activation= 'relu'))\\n    model.add(layers.MaxPooling2D(( 2,2), strides=( 2,2)))The goal of this competition is to estimate the content of photographs for the purpose of \\nretrieval and automatic annotation using a subset of the large hand-labeled ImageNet \\ndataset (10,000,000 labeled images depicting 10,000+ object categories) as training. \\nTest images will be presented with no initial annotation -- no segmentation or labels \\n-- and algorithms will have to produce labelings specifying what objects are present in \\nthe images.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aad510f6-bdfe-4c49-a91c-e1f5e75343de', embedding=None, metadata={'page_label': '89', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 89\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 128, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 128, (3, 3), activation= 'relu'))\\n    model.add(layers.MaxPooling2D(( 2,2), strides=( 2,2)))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 256, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 256, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 256, (3, 3), activation= 'relu'))\\n    model.add(layers.MaxPooling2D(( 2,2), strides=( 2,2)))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.MaxPooling2D(( 2,2), strides=( 2,2)))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.ZeroPadding2D(( 1,1)))\\n    model.add(layers.Convolution2D( 512, (3, 3), activation= 'relu'))\\n    model.add(layers.MaxPooling2D(( 2,2), strides=( 2,2)))\\n    model.add(layers.Flatten())\\n    #top layer of the VGG net\\n    model.add(layers.Dense( 4096, activation= 'relu'))\\n    model.add(layers.Dropout( 0.5))\\n    model.add(layers.Dense( 4096, activation= 'relu'))\\n    model.add(layers.Dropout( 0.5))\\n    model.add(layers.Dense( 1000, activation= 'softmax' ))\\n    if weights_path:\\n        model.load_weights(weights_path)\\n    return model\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42ec29ed-aea2-401b-bf0d-9c5099a5f6c4', embedding=None, metadata={'page_label': '90', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 90\\nWe have implemented a VGG16 network. Note that we could also have used tf.keras.applications.\\nvgg16 . to get the model and its weights directly. Here, I wanted to show how VGG16 works internally. \\nNext, we are going to utilize it.\\nRecognizing cats with a VGG16 network\\nNow let us test the image of a \\n .\\nNote that we are going to use predefined weights:\\nimport cv2\\nim = cv2.resize(cv2.imread( 'cat.jpg' ), (224, 224)).astype(np.float32)\\n#im = im.transpose((2,0,1))\\nim = np.expand_dims(im, axis= 0)\\n# Test pretrained model\\nmodel = VGG_16( '/Users/antonio/.keras/models/vgg16_weights_tf_dim_ordering_tf_\\nkernels.h5' )\\nmodel.summary()\\nmodel.compile(optimizer= 'sgd', loss='categorical_crossentropy' )\\nout = model.predict(im)\\nprint(np.argmax(out))\\nWhen the code is executed, the class 285 is returned, which corresponds (see https://gist.github.\\ncom/yrevar/942d3a0ac09ec9e5eb3a ) to “Egyptian cat”:\\nTotal params: 138,357,544\\nTrainable params: 138,357,544\\nNon-trainable params: 0\\n---------------------------------------------------------------\\n285\\nImpressive, isn’t it? Our VGG16 network can successfully recognize images of cats! An important step \\nfor deep learning. It is only seven years since the paper in [4], but that was a game-changing moment.\\nUtilizing the tf.Keras built-in VGG16 net module\\ntf.Keras  applications are pre-built and pretrained deep learning models. The weights are downloaded \\nautomatically when instantiating a model and stored at ~/.keras/models/ . Using built-in code is \\nvery easy:\\nimport tensorflow as tf\\nfrom tensorflow.keras.applications.vgg16 import VGG16\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport cv2\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0b28d20-c4c7-4f23-a686-99d5bc04e44d', embedding=None, metadata={'page_label': '91', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 91\\n# pre built model with pre-trained weights on imagenet\\nmodel = VGG16(weights= 'imagenet' , include_top= True)\\nmodel.compile(optimizer= 'sgd', loss='categorical_crossentropy' )\\n# resize into VGG16 trained images' format\\nim = cv2.resize(cv2.imread( 'steam-locomotive.jpg' ), (224, 224))\\nim = np.expand_dims(im, axis= 0)\\n# predict\\nout = model.predict(im)\\nindex = np.argmax(out)\\nprint(index)\\nplt.plot(out.ravel())\\nplt.show()\\n#this should print 820 for steaming train\\nNow, let us consider a train, \\n . If we run the code, we get 820 as a result, which is the ImageNet \\ncode for “steam locomotive.” Equally important, all the other classes have very weak support, as \\nshown in Figure 3.13:\\nFigure 3.13: A steam train is the most likely outcome\\nTo conclude this section, note that VGG16 is only one of the modules that are pre-built in tf.Keras . A \\nfull list of pretrained models is available online (see https://www.tensorflow.org/api_docs/python/\\ntf/keras/applications ).\\nRecycling pre-built deep learning models for extracting features\\nOne very simple idea is to use VGG16, and more generally DCNN, for feature extraction. This code \\nimplements the idea by extracting features from a specific layer. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce1a68bc-df3d-4b1a-afd2-57065f5a24ab', embedding=None, metadata={'page_label': '92', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 92\\nNote that we need to switch to the functional API since the sequential model only accepts layers:\\nimport tensorflow as tf\\nfrom tensorflow.keras.applications.vgg16 import VGG16 \\nfrom tensorflow.keras import models\\nfrom tensorflow.keras.preprocessing import image\\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\\nimport numpy as np\\nimport cv2\\n# prebuild model with pre-trained weights on imagenet\\nbase_model = VGG16(weights= 'imagenet' , include_top= True)\\nprint (base_model)\\nfor i, layer in enumerate (base_model.layers):\\n    print (i, layer.name, layer.output_shape)\\n# extract features from block4_pool block\\nmodel = models.Model(inputs=base_model. input, \\n    outputs=base_model.get_layer( 'block4_pool' ).output)\\nimg_path = 'cat.jpg'\\nimg = image.load_img(img_path, target_size=( 224, 224))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis= 0)\\nx = preprocess_input(x)\\n# get the features from this block\\nfeatures = model.predict(x)\\nprint(features)\\nYou might wonder why we want to extract the features from an intermediate layer in a DCNN. The \\nreasoning is that as the network learns to classify images into categories, each layer learns to identify \\nthe features that are necessary to perform the final classification. Lower layers identify lower-order \\nfeatures such as color and edges, and higher layers compose these lower-order features into higher-\\norder features such as shapes or objects. Hence, the intermediate layer has the capability to extract \\nimportant features from an image, and these features are more likely to help in different kinds of \\nclassification.\\nThis has multiple advantages. First, we can rely on publicly available large-scale training and transfer \\nthis learning to novel domains. Second, we can save time on expensive training. Third, we can provide \\nreasonable solutions even when we don’t have a large number of training examples for our domain. \\nWe also get a good starting network shape for the task at hand, instead of guessing it.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccaed3f3-aa66-41a2-ac7a-46a985d652b7', embedding=None, metadata={'page_label': '93', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 93\\nWith this, we will conclude the overview of VGG16 CNNs, the last deep learning model defined in this \\nchapter.\\nDeep Inception V3 for transfer learning\\nTransfer learning is a very powerful deep learning technique that has applications in a number of \\ndifferent domains. The idea behind transfer learning is very simple and can be explained with an \\nanalogy. Suppose you want to learn a new language, say Spanish. Then it could be useful to start from \\nwhat you already know in a different language, say English.\\nFollowing this line of thinking, computer vision researchers now commonly use pretrained CNNs to \\ngenerate representations for novel tasks [1], where the dataset may not be large enough to train an \\nentire CNN from scratch. Another common tactic is to take the pretrained ImageNet network and \\nthen fine-tune the entire network to the novel task. For instance, we can take a network trained to \\nrecognize 10 categories of music and fine-tune it to recognize 20 categories of movies.\\nInception V3 is a very deep ConvNet developed by Google [2]. tf.Keras  implements the full network, \\nas described in Figure 3.14, and it comes pretrained on ImageNet. The default input size for this model \\nis 299x299 on three channels:\\nFigure 3.14: The Inception V3 deep learning model\\nThis skeleton example is inspired by a scheme available online (see https://keras.io/applications/ ). \\nLet’s suppose we have a training dataset D in a different domain from ImageNet. D has 1,024 features \\nin input and 200 categories in output. Let’s look at a code fragment:\\nimport tensorflow as tf\\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\\nfrom tensorflow.keras.preprocessing import image\\nfrom tensorflow.keras import layers, models\\n# create the base pre-trained model\\nbase_model = InceptionV3(weights= 'imagenet' , include_top= False)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0498a06-9aa2-4a1f-b908-7ec5457a8aef', embedding=None, metadata={'page_label': '94', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Convolutional Neural Networks 94\\nWe use a trained Inception V3 model: we do not include the fully connected layer – the dense layer \\nwith 1,024 inputs – because we want to fine-tune on D. The preceding code fragment will download \\nthe pretrained weights on our behalf: \\nDownloading data from https://github.com/fchollet/deep-learning-models/\\nreleases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\\n87916544/87910968 [===========================] – 26s 0us/step\\nSo, if you look at the last four layers (where include_top=True ), you see these shapes:\\n# layer.name, layer.input_shape, layer.output_shape\\n('mixed10' , [(None, 8, 8, 320 ), (None, 8, 8, 768 ), (None, 8, 8, 768 ), (None, 8, \\n8, 192)], (None, 8, 8, 2048 ))\\n('avg_pool' , (None, 8, 8, 2048 ), (None, 1, 1, 2048 ))\\n('flatten' , (None, 1, 1, 2048 ), (None, 2048))\\n('predictions' , (None, 2048), (None, 1000))\\nWhen include_top=False , you are removing the last three layers and exposing the mixed_10  layer. The \\nGlobalAveragePooling2D  layer converts (None, 8, 8, 2048)  to (None, 2048) , where each element \\nin the (None, 2048)  tensor is the average value for each corresponding (8,8)  subtensor in the (None, \\n8, 8, 2048)  tensor. None  means an unspecified dimension, which is useful if you define a placeholder:\\nx = base_model.output\\n# let's add a fully-connected layer as first layer\\nx = layers.Dense( 1024, activation= 'relu')(x)\\n# and a logistic layer with 200 classes as last layer\\npredictions = layers.Dense( 200, activation= 'softmax' )(x)\\n# model to train\\nmodel = models.Model(inputs=base_model. input, outputs=predictions)\\nAll the convolutional levels are pretrained, so we freeze them during the training of the full model:\\n# i.e. freeze all convolutional InceptionV3 layers\\nfor layer in base_model.layers:\\n    layer.trainable = False\\nThe model is then compiled and trained for a few epochs so that the top layers are trained. For the \\nsake of simplicity, here we are omitting the training code itself: \\n# compile the model (should be done *after* setting layers to non-trainable)\\nmodel.compile(optimizer= 'rmsprop' , loss='categorical_crossentropy' )\\n# train the model on the new data for a few epochs\\nmodel.fit_generator(...)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e4d0305-88d3-4cb2-81e5-3f0f1a7f303f', embedding=None, metadata={'page_label': '95', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 95\\nThen, we freeze the top inception layers and fine-tune the other inception layers. In this example, we \\ndecide to freeze the first 172 layers (this is a tunable hyperparameter):\\n# we chose to train the top 2 inception blocks, i.e. we will freeze\\n# the first 172 layers and unfreeze the rest:\\nfor layer in model.layers[: 172]:\\n   layer.trainable = False\\nfor layer in model.layers[ 172:]:\\n   layer.trainable = True\\nThe model is then recompiled for fine-tuning optimization:\\n# we need to recompile the model for these modifications to take effect\\n# we use SGD with a low learning rate\\nfrom tensorflow.keras.optimizers import SGD\\nmodel.compile(optimizer=SGD(lr= 0.0001, momentum= 0.9), loss= 'categorical_\\ncrossentropy' )\\n# we train our model again (this time fine-tuning the top 2 inception blocks\\n# alongside the top Dense layers\\nmodel.fit_generator(...)\\nNow we have a new deep network that reuses a standard Inception V3 network, but it is trained on a \\nnew domain D via transfer learning. Of course, there are many fine-tuning parameters for achieving \\ngood accuracy. However, we are now re-using a very large pretrained network as a starting point via \\ntransfer learning. In doing so, we can save the need for training on our machines by reusing what is \\nalready available in tf.Keras .\\nOther CNN architectures\\nIn this section, we will discuss many other different CNN architectures, including AlexNet, residual \\nnetworks, highwayNets, DenseNets, and Xception.\\nAlexNet\\nOne of the first convolutional networks was AlexNet [4], which consisted of only eight layers; the first \\nfive were convolutional ones with max-pooling layers, and the last three were fully connected. AlexNet \\n[4] is an article cited more than 35,000 times, which started the deep learning revolution (for computer \\nvision). Then, networks  started to become deeper and deeper. Recently, a new idea has been proposed.\\nResidual networks\\nResidual networks are based on the interesting idea of allowing earlier layers to be fed directly into \\ndeeper layers. These are the so-called skip connections (or fast-forward connections). The key idea is to \\nminimize the risk of vanishing or exploding gradients for deep networks (see Chapter 8, Autoencoders). \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e8146b1-97f0-4d50-bc09-e6c8f47c6c0b', embedding=None, metadata={'page_label': '96', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 96\\nThe building block of a ResNet is called a “residual block” or “identity block,” which includes both \\nforward and fast-forward connections. In this example (Figure 3.15), the output of an earlier layer is \\nadded to the output of a later layer before being sent into a ReLU activation function:\\nFigure 3.15: An example of image segmentation\\nHighwayNets and DenseNets\\nAn additional weight matrix may be used to learn the skip weights and these models are frequently \\ndenoted as HighwayNets. Instead, models with several parallel skips are known as DenseNets [5]. It \\nhas been noticed that the human brain might have similar patterns to residual networks since the \\ncortical layer VI neurons get input from layer I, skipping intermediary layers. In addition, residual \\nnetworks can be faster to train than traditional CNNs since there are fewer layers to propagate through \\nduring each iteration (deeper layers get input sooner due to the skip connection). Figure 3.16 shows \\nan example of a DenseNet (based on http://arxiv.org/abs/1608.06993 ):\\nFigure 3.16: An example of a DenseNet', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43cb8379-2003-48c5-9e01-9fac6abb805b', embedding=None, metadata={'page_label': '97', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 97\\nXception\\nXception networks use two basic blocks: a depthwise convolution and a pointwise convolution. A \\ndepthwise convolution is the channel-wise n x n spatial convolution. Suppose an image has three \\nchannels, then we have three convolutions of n x n. A pointwise convolution is a 1 x 1 convolution. \\nIn Xception, an “extreme” version of an Inception module, we first use a 1 x 1 convolution to map \\ncross-channel correlations, and then separately map the spatial correlations of every output channel \\nas shown in Figure 3.17 (from https://arxiv.org/pdf/1610.02357.pdf ):\\nFigure 3.17: An example of an extreme form of an Inception module', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76c268be-d191-4179-a5bd-32f26d931342', embedding=None, metadata={'page_label': '98', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 98\\nXception ( eXtreme Inception) is a deep convolutional neural network architecture inspired by Inception, \\nwhere Inception modules have been replaced with depthwise separable convolutions. Xception uses \\nmultiple skip-connections in a similar way to ResNet. The final architecture is rather complex as \\nillustrated in Figure 3.18 (from https://arxiv.org/pdf/1610.02357.pdf ). Data first goes through the \\nentry flow, then through the middle flow, which is repeated eight times, and finally through the exit flow:\\nFigure 3.18: The full Xception architecture\\nResidual networks, HyperNets, DenseNets, Inception, and Xceptions are all available as pretrained nets  \\nin both tf.Keras.application  and tf.Hub . The Keras website has a nice summary of the performance \\nachieved on the ImageNet dataset and the depth of each network. The summary is available at https://\\nkeras.io/applications/ :', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b5b8b4f-0a35-429a-9c74-9d14ff997eb9', embedding=None, metadata={'page_label': '99', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 99\\n \\nFigure 3.19: Different CNNs and accuracy on top-1 and top-5 results\\nThe top-1 and top-5 accuracy refers to a model’s performance on the ImageNet validation dataset.\\nIn this section, we have discussed many CNN architectures. The next section is about style transfer, a \\ndeep learning technique used for training neural networks to create art.\\nStyle transfer\\nStyle transfer is a funny neural network application that provides many insights into the power of \\nneural networks. So what exactly is it? Imagine that you observe a painting done by a famous artist. In \\nprinciple, you are observing two elements: the painting itself (say the face of a woman, or a landscape) \\nand something more intrinsic, the “style” of the artist. What is the style? That is more difficult to define, \\nbut humans know that Picasso had his own style, Matisse had his own style, and each artist has his/\\nher own style. Now, imagine taking a famous painting of Matisse, giving it to a neural network, and \\nletting the neural network repaint it in Picasso’s style. Or, imagine taking your own photo, giving it to \\na neural network, and having your photo painted in Matisse’s or Picasso’s style, or in the style of any \\nother artist that you like. That’s what style transfer does.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a59a88f-0f88-4651-8bb5-0ae360213d75', embedding=None, metadata={'page_label': '100', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 100\\nFor instance, go to https://deepart.io/  and see a cool demo as shown in the image below, where \\ndeepart has been applied by taking the “Van Gogh” style as observed in the Sunflowers painting (this is \\na public domain image: “Sonnenblumen. Arles, 1888 Öl auf Leinwand, 92,5 x 73 cm Vincent van Gogh” \\nhttps://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.\\njpg) and applying it to a picture of my daughter Aurora:\\nFigure 3.20: An example of deepart\\nNow, how can we define more formally the process of style transfer? Well, style transfer is the task of \\nproducing an artificial image x that shares the content of a source content image p and the style of a \\nsource style image a. So, intuitively we need two distance functions: one distance function measures \\nhow different the content of two images is, L content , while the other distance function measures how \\ndifferent the style of two images is, L style. Then, the transfer style can be seen as an optimization problem \\nwhere we try to minimize these two metrics. As in A Neural Algorithm of Artistic Style by Leon A. Gatys, \\nAlexander S. Ecker, and Matthias Bethge ( https://arxiv.org/abs/1508.06576 ), we use a pretrained \\nnetwork to achieve style transfer. In particular, we can feed a VGG19 (or any suitable pretrained \\nnetwork) to extract features that represent images in an efficient way. Now we are going to define two \\nfunctions used for training the network: the content distance and the style distance.\\nContent distance\\nGiven two images, p content image and x input image, we define the content distance as the distance \\nin the feature space defined by a layer l for a VGG19 network receiving the two images as an input. \\nIn other words, the two images are represented by the features extracted by a pretrained VGG19. \\nThese features project the images into a feature “content” space where the “content” distance can be \\nconveniently computed as follows:\\n𝐿𝐿𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑙𝑙(𝑝𝑝𝑝𝑝𝑝)= ∑(𝐹𝐹𝑖𝑖𝑖𝑖𝑙𝑙(𝑝𝑝)−𝑃𝑃𝑖𝑖𝑖𝑖𝑙𝑙(𝑝𝑝)𝑝2\\n𝑖𝑖𝑝𝑖𝑖 \\nTo generate nice images, we need to ensure that the content of the generated image is similar to (i.e. \\nhas a small distance from) that of the input image. The distance is therefore minimized with standard \\nbackpropagation. The code is simple:\\n#\\n#content distance\\n#', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12375fc8-aeec-4340-8e0c-08cbc10a347c', embedding=None, metadata={'page_label': '101', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 101\\ndef get_content_loss (base_content, target):\\n  return  tf.reduce_mean(tf.square(base_content - target))\\nStyle distance\\nAs discussed, the features in the higher layers of VGG19 are used as content representations. You \\ncan think about these features as filter responses. To represent the style we use in a gram matrix G \\n(defined as the matrix vTv for a vector v), we consider 𝐺𝐺𝑖𝑖𝑖𝑖𝑖𝑙𝑙  as the inner matrix for map i and map j at \\nlayer l of the VGG19. It is possible to show that the Gram matrix represents the correlation matrix \\nbetween different filter responses.\\nThe contribution of each layer to the total style loss is defined as: \\n𝐸𝐸𝑙𝑙=1\\n4𝑁𝑁𝑙𝑙2𝑀𝑀𝑙𝑙2∑(𝐺𝐺𝑖𝑖𝑖𝑖𝑙𝑙−𝐴𝐴𝑖𝑖𝑖𝑖𝑙𝑙)2\\n𝑖𝑖𝑖𝑖𝑖 \\nwhere 𝐺𝐺𝑖𝑖𝑖𝑖𝑖𝑙𝑙  is the Gram matrix for input image x and 𝐴𝐴𝑖𝑖𝑖𝑖𝑖𝑙𝑙  is the gram matrix for the style image a, and \\nNl is the number of feature maps, each of size 𝑀𝑀𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙  . The Gram matrix can project the images \\ninto a space where the style is taken into account. In addition, the feature correlations from multiple \\nVGG19 layers are used because we want to consider multi-scale information and a more robust style \\nrepresentation. The total style loss across levels is the weighted sum:\\n𝐿𝐿𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝑎𝑎𝑎𝑎𝑎)=∑𝑤𝑤 𝑠𝑠𝐸𝐸𝑠𝑠\\n𝑠𝑠𝑙𝑙𝑙 (𝑤𝑤𝑙𝑙=1\\n‖𝐿𝐿‖) \\nThe key idea is therefore to perform gradient descent on the content image to make its style similar \\nto the style image. The code is simple:\\n#style distance\\n#\\ndef gram_matrix (input_tensor):\\n  # image channels first \\n  channels = int(input_tensor.shape[- 1])\\n  a = tf.reshape(input_tensor, [- 1, channels])\\n  n = tf.shape(a)[ 0]\\n  gram = tf.matmul(a, a, transpose_a= True)\\n  return  gram / tf.cast(n, tf.float32)\\n \\ndef get_style_loss (base_style, gram_target):\\n  # height, width, num filters of each layer\\n  height, width, channels = base_style.get_shape().as_list()\\n  gram_style = gram_matrix(base_style)\\n  \\n  return  tf.reduce_mean(tf.square(gram_style - gram_target))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='682058e9-493b-49a8-85f5-abd9e05c8d28', embedding=None, metadata={'page_label': '102', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks 102\\nIn short, the concepts behind style transfer are simple: first, we use VGG19 as a feature extractor and \\nthen we define two suitable function distances, one for style and the other one for contents, which \\nare appropriately minimized. If you want to try this out for yourself, then TensorFlow tutorials are \\navailable online. A tutorial is available at https://colab.research.google.com/github/tensorflow/\\nmodels/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.\\nipynb . If you are interested in a demo of this technique, you can go to the deepart.io free site where \\nthey do style transfer.\\nSummary\\nIn this chapter, we have learned how to use deep learning ConvNets to recognize MNIST handwritten \\ncharacters with high accuracy. We used the CIFAR-10 dataset to build a deep learning classifier with 10 \\ncategories, and the ImageNet dataset to build an accurate classifier with 1,000 categories. In addition, \\nwe investigated how to use large deep learning networks such as VGG16 and very deep networks such \\nas Inception V3. We concluded with a discussion on transfer learning.\\nIn the next chapter, we’ll see how to work with word embeddings and why these techniques are \\nimportant for deep learning.\\nReferences\\n1. LeCun, Y. and Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The \\nHandbook of Brain Theory Neural Networks, vol. 3361. \\n2. Wan. L, Zeiler M., Zhang S., Cun, Y. L., and Fergus R. (2014). Regularization of neural networks \\nusing dropconnect. Proc. 30th Int. Conf. Mach. Learn., pp. 1058–1066.\\n3. Graham B. (2014). Fractional Max-Pooling. arXiv Prepr. arXiv: 1412.6071.\\n4. Simonyan K. and Zisserman A. (Sep. 2014). Very Deep Convolutional Networks for Large-Scale \\nImage Recognition. arXiv ePrints.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a8196dd-c0ae-4c19-8629-55a7d728664a', embedding=None, metadata={'page_label': '103', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4\\nWord Embeddings\\nIn the previous chapter, we talked about convolutional networks, which have been very successful \\nagainst image data. Over the next few chapters, we will switch tracks to focus on strategies and networks \\nto handle text data.\\nIn this chapter, we will first look at the idea behind word embeddings, and then cover the two earliest \\nimplementations – Word2Vec and GloVe. We will learn how to build word embeddings from scratch \\nusing the popular library Gensim on our own corpus and navigate the embedding space we create.\\nWe will also learn how to use pretrained third-party embeddings as a starting point for our own NLP \\ntasks, such as spam detection, that is, learning to automatically detect unsolicited and unwanted \\nemails. We will then learn about various ways to leverage the idea of word embeddings for unrelated \\ntasks, such as constructing an embedded space for making item recommendations.\\nWe will then look at extensions to these foundational word embedding techniques that have occurred in \\nthe last decade since Word2Vec – adding syntactic similarity with fastText, adding the effect of context \\nusing neural networks such as ELMo and Google Universal Sentence Encoder, sentence encodings such \\nas InferSent and skip-thoughts, and the introduction of language models such as ULMFiT and BERT.\\nIn this chapter, we’ll learn about the following:\\n• Word embeddings – origins and fundamentals\\n• Distributed representations\\n• Static embeddings\\n• Creating your own embedding with Gensim\\n• Exploring the embedding space with Gensim\\n• Using word embedding for spam detection\\n• Neural embedding – not just for words\\n• Character and subword embedding\\n• Dynamic embeddings\\n• Sentence and paragraph embeddings\\n• Language-based model embeddings', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6cc8be5-3fd5-4103-bb63-b40b0c7ea2d0', embedding=None, metadata={'page_label': '104', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 104\\nLet’s begin!\\nWord embedding ‒ origins and fundamentals\\nWikipedia defines word embedding as the collective name for a set of language modeling and feature \\nlearning techniques in natural language processing  (NLP ) where words or phrases from a vocabulary \\nare mapped to vectors of real numbers.\\nDeep learning models, like other machine learning models, typically don’t work directly with text; \\nthe text needs to be converted to numbers instead. The process of converting text to numbers is a \\nprocess called vectorization. An early technique for vectorizing words was one-hot encoding, which you \\nlearned about in Chapter 1, Neural Network Foundations with TF. As you will recall, a major problem with \\none-hot encoding is that it treats each word as completely independent from all the others, since the \\nsimilarity between any two words (measured by the dot product of the two word vectors) is always zero.\\nThe dot product is an algebraic operation that operates on two vectors 𝑎𝑎𝑎 𝑎𝑎𝑎𝑎𝑎 1,…,𝑎𝑎𝑁𝑁]𝑎 and 𝑏𝑏 𝑏𝑏𝑏𝑏𝑏 1,…,𝑏𝑏𝑁𝑁]𝑏\\nof equal length and returns a number. It is also known as the inner product or scalar product:\\n𝑎𝑎𝑎𝑎𝑎 𝑎𝑎𝑎𝑎𝑎 𝑖𝑖𝑎𝑎𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖𝑎𝑎𝑎𝑖𝑎𝑎𝑖+⋯+𝑎𝑎 𝑁𝑁𝑎𝑎𝑁𝑁𝑎\\nWhy is the dot product of one-hot vectors of two words always 0? Consider two words w i and w j. \\nAssuming a vocabulary size of V, their corresponding one-hot vectors are a zero vector of rank V with \\npositions i and j set to 1. When combined using the dot product operation, the 1 in a[i] is multiplied \\nby 0 in b[i], and 1 in b[j] is multiplied by 0 in a[j], and all other elements in both vectors are 0, so the \\nresulting dot product is also 0.\\nTo overcome the limitations of one-hot encoding, the NLP community has borrowed techniques from \\nInformation Retrieval (IR) to vectorize text using the document as the context. Notable techniques \\nare Term Frequency-Inverse Document Frequency ( TF-IDF) [35], Latent Semantic Analysis ( LSA ) \\n[36], and topic modeling [37]. These representations attempt to capture a document-centric idea of \\nsemantic similarity between words. Of these, one-hot and TF-IDF are relatively sparse embeddings, \\nsince vocabularies are usually quite large, and a word is unlikely to occur in more than a few documents \\nin the corpus.All the code files for this chapter can be found at https://packt.link/dltfchp4 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cfbaf78-9239-42aa-9a05-f0d2029c3411', embedding=None, metadata={'page_label': '105', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 105\\nThe development of word embedding techniques began around 2000. These techniques differ from \\nprevious IR-based techniques in that they use neighboring words as their context, leading to a more \\nnatural semantic similarity from a human understanding perspective. Today, word embedding is a \\nfoundational technique for all kinds of NLP tasks, such as text classification, document clustering, part-\\nof-speech tagging, named entity recognition, sentiment analysis, and many more. Word embeddings \\nresult in dense, low-dimensional vectors, and along with LSA and topic models can be thought of as \\na vector of latent features for the word.\\nWord embeddings are based on the distributional hypothesis, which states that words that occur in \\nsimilar contexts tend to have similar meanings. Hence the class of word embedding-based encodings \\nis also known as distributed representations, which we will talk about next.\\nDistributed representations\\nDistributed representations attempt to capture the meaning of a word by considering its relations \\nwith other words in its context. The idea behind the distributed hypothesis is captured in this quote \\nfrom J. R. Firth, a linguist, who first proposed this idea:\\nHow does this work? By way of example, consider the following pair of sentences:\\nParis is the capital of France.\\nBerlin is the capital of Germany.\\nEven assuming no knowledge of world geography, the sentence pair implies some sort of relationship \\nbetween the entities Paris, France, Berlin, and Germany that could be represented as:\\n\"Paris\" is to \"France\" as \"Berlin\" is to \"Germany.\"\\nDistributed representations are based on the idea that there exists some transformation, as follows:\\nParis : France :: Berlin : Germany\\nIn other words, a distributed embedding space is one where words that are used in similar contexts are \\nclose to one another. Therefore, the similarity between the word vectors in this space would roughly \\ncorrespond to the semantic similarity between the words.\\nFigure 4.1 shows a TensorBoard visualization of word embedding of words around the word “important” \\nin the embedding space. As you can see, the neighbors of the word tend to be closely related, or \\ninterchangeable with the original word.You shall know a word by the company it keeps.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a123b69-29e5-4611-a30d-63abc2223de3', embedding=None, metadata={'page_label': '106', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 106\\nFor example, “crucial” is virtually a synonym, and it is easy to see how the words “historical” or \\n“valuable” could be substituted in certain situations:\\nFigure 4.1: Visualization of nearest neighbors of the word “important” in a word embedding dataset, \\nfrom the TensorFlow Embedding Guide (https://www.tensorflow.org/guide/embedding)\\nIn the next section, we will look at various types of distributed representations (or word embeddings).\\nStatic embeddings\\nStatic embeddings are the oldest type of word embedding. The embeddings are generated against a \\nlarge corpus but the number of words, though large, is finite. You can think of a static embedding as \\na dictionary, with words as the keys and their corresponding vector as the value. If you have a word \\nwhose embedding needs to be looked up that was not in the original corpus, then you are out of luck. \\nIn addition, a word has the same embedding regardless of how it is used, so static embeddings cannot \\naddress the problem of polysemy, that is, words with multiple meanings. We will explore this issue \\nfurther when we cover non-static embeddings later in this chapter.\\nWord2Vec\\nThe models known as Word2Vec were first created in 2013 by a team of researchers at Google led by \\nTomas Mikolov [1, 2, 3]. The models are self-supervised, that is, they are supervised models that depend \\non the structure of natural language to provide labeled training data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f733916-6017-4c5b-a2e9-51de12d3aa6f', embedding=None, metadata={'page_label': '107', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 107\\nThe two architectures for Word2Vec are as follows:\\n• Continuous Bag of Words (CBOW )\\n• Skip-gram\\nFigure 4.2: Architecture of the CBOW and Skip-gram Word2Vec models\\nIn the CBOW architecture, the model predicts the current word given a window of surrounding words. \\nThe order of context words does not influence the prediction (that is, the bag of words assumption, \\nhence the name). In the skip-gram architecture, the model predicts the surrounding words given the \\ncontext word. According to the Word2Vec website, CBOW is faster, but skip-gram does a better job at \\npredicting infrequent words.\\nFigure 4.2 summarizes the CBOW and skip-gram architectures. To understand the inputs and outputs, \\nconsider the following example sentence:\\nThe Earth travels around the Sun once per year.\\nAssuming a window size of 5, that is, two context words to the left and right of the content word, the \\nresulting context windows are shown as follows. The word in bold is the word under consideration, \\nand the other words are the context words within the window:\\n[_, _, The, Earth, travels]\\n[_, The, Earth, travels, around]\\n[The, Earth, travels, around, the]\\n[Earth, travels, around, the, Sun]\\n[travels, around, the, Sun, once]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b710296b-9c42-4475-a525-8e66aea25544', embedding=None, metadata={'page_label': '108', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 108\\n[around, the, Sun , once, per]\\n[the, Sun, once, per, year]\\n[Sun, once, per, year, _]\\n[once, per, year, _, _]\\nFor the CBOW model, the input and label tuples for the first three context windows are as follows. In \\nthe following first example, the CBOW model would learn to predict the word “The” given the set of \\nwords (“Earth,” “travels”), and so on. More correctly, the input of sparse vectors for the words “Earth” \\nand “travels.” The model will learn to predict a dense vector whose highest value, or probability, \\ncorresponds to the word “The”:\\n([Earth, travels], The)\\n([The, travels, around], Earth)\\n([The, Earth, around, the], travels)\\nFor the skip-gram model, the first three context windows correspond to the following input and label \\ntuples. We can simplify the skip-gram model objective of predicting a context word given a target word \\nto basically predicting if a pair of words are contextually related. Contextually related means that a \\npair of words within a context window are somehow related. That is, the input to the skip-gram model \\nfor the following first example would be the sparse vectors for the context words “The” and “Earth,” \\nand the output would be the value 1:\\n([The, Earth], 1)\\n([The, travels], 1)\\n([Earth, The], 1)\\n([Earth, travels], 1)\\n([Earth, around], 1)\\n([travels, The], 1)\\n([travels, Earth], 1)\\n([travels, around], 1)\\n([travels, the], 1)\\nWe also need negative samples to train a model properly, so we generate additional negative samples \\nby pairing each input word with some random word in the vocabulary. This process is called negative \\nsampling and might result in the following additional inputs:\\n([Earth, aardvark], 0)\\n([Earth, zebra], 0)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0e411b9-8457-4e53-96d3-60e6291f579f', embedding=None, metadata={'page_label': '109', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 109\\nA model trained with all of these inputs is called a Skip-Gram with Negative Sampling (SGNS ) model.\\nIt is important to understand that we are not interested in the ability of these models to classify; rather, \\nwe are interested in the side effect of training – the learned weights. These learned weights are what \\nwe call the embedding.\\nWhile it may be instructive to implement the models on your own as an academic exercise, at this \\npoint Word2Vec is so commoditized, you are unlikely to ever need to do this. For the curious, you \\nwill find code to implement the CBOW and skip-gram models in the files tf2_cbow_model.py  and \\ntf2_cbow_skipgram.py  in the source code accompanying this chapter.\\nThe Word2Vec model was trained in a self-supervised manner by Google on roughly 100 billion words \\nfrom the Google News dataset and contains a vocabulary of 3 million words. Google then released \\nthe pretrained model for anyone to download and use. The pretrained Word2Vec model is available \\nhere (https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit ). The output vector \\ndimensionality is 300. It is available as a BIN file and can be opened using Gensim by using gensim.\\nmodels.Word2Vec.load_word2vec_format()  or using the gensim()  data downloader.\\nThe other early implementation of word embedding is GloVe, which we will talk about next.\\nGloVe\\nThe Global vectors for word representation (GloVe ) embeddings were created by Jeffrey Pennington, \\nRichard Socher, and Christopher Manning [4]. The authors describe GloVe as an unsupervised learning \\nalgorithm for obtaining vector representations for words. Training is performed on aggregated global \\nword-word co-occurrence statistics from a corpus, and the resulting representations show similar \\nclustering behavior between similar words as seen in Word2Vec.\\nGloVe differs from Word2Vec in that Word2Vec is a predictive model while GloVe is a count-based \\nmodel. The first step is to construct a large matrix of (word, context) pairs that co-occur in the training \\ncorpus. Rows correspond to words and columns correspond to contexts, usually a sequence of one \\nor more words. Each element of the matrix represents how often the word co-occurs in the context.\\nThe GloVe process factorizes this co-occurrence matrix into a pair of (word, feature) and (feature, \\ncontext) matrices. The process is known as matrix factorization and is done using Stochastic Gradient \\nDescent ( SGD ), an iterative numerical method. For example, consider that we want to factorize a \\nmatrix R into its factors P and Q:\\n𝑅𝑅 𝑅 𝑅𝑅 𝑅𝑅𝑅 𝑅 𝑅𝑅𝑅  \\nThe SGD process will start with P and Q composed of random values and attempt to reconstruct the \\nmatrix R’ by multiplying them. The difference between the matrices R  and R’  represents the loss \\nand is usually computed as the mean-squared error between the two matrices. The loss dictates how \\nmuch the values of P and Q need to change for R’ to move closer to R to minimize the reconstruction \\nloss. This process is repeated multiple times until the loss is within some acceptable threshold. At \\nthat point, the (word, feature) matrix P is the GloVe embedding.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='335b1225-c9ad-47fb-8408-b10c0f094511', embedding=None, metadata={'page_label': '110', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 110\\nThe GloVe process is much more resource-intensive than Word2Vec. This is because Word2Vec learns \\nthe embedding by training over batches of word vectors, while GloVe factorizes the entire co-occurrence \\nmatrix in one shot. In order to make the process scalable, SGD is often used in parallel mode, as \\noutlined in the HOGWILD! paper [5].\\nLevy and Goldberg have also pointed out equivalences between the Word2Vec and GloVe approaches \\nin their paper [6], showing that the Word2Vec SGNS model implicitly factorizes a word-context matrix.\\nAs with Word2Vec, you are unlikely to ever need to generate your own GloVe embedding, and far more \\nlikely to use embeddings pre-generated against large corpora and made available for download. If you \\nare curious, you will find code to implement matrix factorization in tf2_matrix_factorization.py  \\nin the source code download accompanying this chapter.\\nGloVe vectors trained on various large corpora (number of tokens ranging from 6 billion to 840 billion, \\nvocabulary size from 400 thousand to 2.2 million) and of various dimensions (50, 100, 200, 300) are \\navailable from the GloVe project download page ( https://nlp.stanford.edu/projects/glove/ ). It \\ncan be downloaded directly from the site or using Gensim or spaCy data downloaders.\\nCreating your own embeddings using Gensim\\nWe will create an embedding using Gensim and a small text corpus, called text8.\\nGensim is an open-source Python library designed to extract semantic meaning from text documents. \\nOne of its features is an excellent implementation of the Word2Vec algorithm, with an easy-to-use \\nAPI that allows you to train and query your own Word2Vec model. To learn more about Gensim, see \\nhttps://radimrehurek.com/gensim/index.html . To install Gensim, please follow the instructions \\nat https://radimrehurek.com/gensim/install.html .\\nThe text8 dataset is the first 108 bytes of the Large Text Compression Benchmark, which consists of \\nthe first 109 bytes of English Wikipedia [7]. The text8 dataset is accessible from within the Gensim API \\nas an iterable of tokens, essentially a list of tokenized sentences. To download the text8 corpus, create \\na Word2Vec model from it, and save it for later use, run the following few lines of code (available in \\ncreate_embedding_with_text8.py  in the source code for this chapter):\\nimport gensim.downloader as api\\nfrom gensim.models import Word2Vec\\ndataset = api.load( \"text8\")\\nmodel = Word2Vec(dataset)\\nmodel.save( \"data/text8-word2vec.bin\" )\\nThis will train a Word2Vec model on the text8 dataset and save it as a binary file. The Word2Vec model \\nhas many parameters, but we will just use the defaults. In this case, it trains a CBOW model ( sg=0 ) \\nwith window size 5 ( window=5 ) and will produce 100 dimensional embeddings ( size=100 ). The full \\nset of parameters is described on the Word2Vec documentation page [8]. To run this code, execute \\nthe following commands at the command line:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b524a81-ea47-4b6f-8c8b-80135452f98f', embedding=None, metadata={'page_label': '111', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 111\\n$ mkdir data\\n$ python create_embedding_with_text8.py\\nThe code should run for 5-10 minutes, after which it will write out a trained model into the data  folder. \\nWe will examine this trained model in the next section.\\nExploring the embedding space with Gensim\\nLet us reload the Word2Vec model we just built and explore it using the Gensim API. The actual word \\nvectors can be accessed as a custom Gensim class from the model’s wv attribute:\\nfrom gensim.models import KeyedVectors\\nmodel = KeyedVectors.load( \"data/text8-word2vec.bin\" )\\nword_vectors = model.wv\\nWe can take a look at the first few words in the vocabulary and check to see if specific words are \\navailable:\\nwords = word_vectors.vocab.keys()\\nprint([x for i, x in enumerate (words) if i < 10])\\nassert(\"king\" in words)\\nThe preceding snippet of code produces the following output:\\n[\\'anarchism\\', \\'originated\\', \\'as\\', \\'a\\', \\'term\\', \\'of\\', \\'abuse\\', \\'first\\', \\'used\\', \\n\\'against\\']\\nWe can look for similar words to a given word (“king”), shown as follows:\\ndef print_most_similar (word_conf_pairs, k):\\n   for i, (word, conf) in enumerate (word_conf_pairs):\\n       print(\"{:.3f} {:s}\" .format(conf, word))\\n       if i >= k- 1:\\n           break\\n   if k < len(word_conf_pairs):\\n       print(\"...\")\\nprint_most_similar(word_vectors.most_similar( \"king\"), 5)Word embeddings are central to text processing; however, at the time of writing this book, \\nthere is no comparable API within TensorFlow that allows you to work with embeddings \\nat the same level of abstraction. For this reason, we have used Gensim in this chapter to \\nwork with Word2Vec models. The online Tensorflow tutorial contains an example of how \\nto train a Word2Vec model from scratch (https://www.tensorflow.org/tutorials/\\ntext/word2vec ) but that is not our focus here.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='affb1527-8ef1-40fb-a17b-10b72b29dec2', embedding=None, metadata={'page_label': '112', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 112\\nThe most_similar()  method with a single parameter produces the following  output. Here, the floating-\\npoint score is a measure of the similarity, higher values being better than lower values. As you can \\nsee, the similar words seem to be mostly accurate:\\n0.760 prince\\n0.701 queen\\n0.700 kings\\n0.698 emperor\\n0.688 throne\\n...\\nYou can also do vector arithmetic similar to the country-capital example we described earlier. Our \\nobjective is to see if the relation Paris : France :: Berlin : Germany holds true. This is equivalent to \\nsaying that the distance in embedding space between Paris and France should be the same as that \\nbetween Berlin and Germany. In other words, France - Paris + Berlin should give us Germany. In code, \\nthen, this would translate to:\\nprint_most_similar(word_vectors.most_similar(\\n   positive=[ \"france\" , \"berlin\" ], negative=[ \"paris\"]), 1\\n)\\nThis returns the following result, as expected:\\n0.803 germany\\nThe preceding similarity value reported is cosine similarity, but a better measure of similarity was \\nproposed by Levy and Goldberg [9], which is also implemented in the Gensim API. This measure \\nessentially computes the distance on a log scale thereby amplifying the difference between shorter \\ndistances and reducing the difference between longer ones.\\nprint_most_similar(word_vectors.most_similar_cosmul(\\n   positive=[ \"france\" , \"berlin\" ], negative=[ \"paris\"]), 1\\n)\\nAnd this also yields the expected result, but with higher similarity:\\n0.984 germany\\nGensim also provides a doesnt_match()  function, which can be used to detect the odd one out of a \\nlist of words:\\nprint(word_vectors.doesnt_match([ \"hindus\" , \"parsis\" , \"singapore\" , \\n\"christians\" ]))\\nThis gives us singapore  as expected, since it is the only country among a set of words identifying \\nreligions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29b00c3d-a568-41b5-82eb-7da5231c3a75', embedding=None, metadata={'page_label': '113', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 113\\nWe can also calculate the similarity between two words. Here we demonstrate that the distance between \\nrelated words is less than that of unrelated words:\\nfor word in [\"woman\" , \"dog\", \"whale\" , \"tree\" ]:\\n   print(\"similarity({:s}, {:s}) = {:.3f}\" .format(\\n       \"man\", word,\\n       word_vectors.similarity( \"man\", word)\\n   ))\\nThis gives the following interesting result:\\nsimilarity(man, woman) = 0.759\\nsimilarity(man, dog) = 0.474\\nsimilarity(man, whale) = 0.290\\nsimilarity(man, tree) = 0.260\\nThe similar_by_word()  function is functionally equivalent to similar()  except that the latter \\nnormalizes the vector before comparing by default. There is also a related similar_by_vector()  \\nfunction, which allows you to find similar words by specifying a vector as input. Here we try to find \\nwords that are similar to “singapore”:\\nprint(print_most_similar(\\n   word_vectors.similar_by_word( \"singapore\" ), 5)\\n)\\nAnd we get the following output, which seems to be mostly correct, at least from a geographical point \\nof view:\\n0.882 malaysia\\n0.837 indonesia\\n0.826 philippines\\n0.825 uganda\\n0.822 thailand\\n...\\nWe can also compute the distance between two words in the embedding space using the distance()  \\nfunction. This is really just 1 - similarity() :\\nprint(\"distance(singapore, malaysia) = {:.3f}\" .format(\\n   word_vectors.distance( \"singapore\" , \"malaysia\" )\\n))\\nWe can also look up vectors for a vocabulary word either directly from the word_vectors  object, or \\nby using the word_vec()  wrapper, shown as follows:\\nvec_song = word_vectors[ \"song\"]\\nvec_song_2 = word_vectors.word_vec( \"song\", use_norm= True)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='995ad2da-e3c2-4251-ac67-8d697245e95b', embedding=None, metadata={'page_label': '114', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 114\\nThere are a few other functions that you may find useful depending on your use case. The documentation \\npage for KeyedVectors contains a list of all the available functions [10].\\nThe code shown here can be found in the explore_text8_embedding.py  file in the code accompanying \\nthis book.\\nUsing word embeddings for spam detection\\nBecause of the widespread availability of various robust embeddings generated from large corpora, it \\nhas become quite common to use one of these embeddings to convert text input for use with machine \\nlearning models. Text is treated as a sequence of tokens. The embedding provides a dense fixed \\ndimension vector for each token. Each token is replaced with its vector, and this converts the sequence \\nof text into a matrix of examples, each of which has a fixed number of features corresponding to the \\ndimensionality of the embedding.\\nThis matrix of examples can be used directly as input to standard (non-neural network based) machine \\nlearning programs, but since this book is about deep learning and TensorFlow, we will demonstrate \\nits use with a one-dimensional version of the Convolutional Neural Network (CNN) that you learned \\nabout in Chapter 3, Convolutional Neural Networks. Our example is a spam detector that will classify \\nShort Message Service (SMS ) or text messages as either “ham” or “spam.” The example is very similar \\nto a sentiment analysis example we’ll cover in Chapter 20, Advanced Convolutional Neural Networks, that \\nuses a one-dimensional CNN, but our focus here will be on the embedding layer.\\nSpecifically, we will see how the program learns an embedding from scratch that is customized to the \\nspam detection task. Next, we will see how to use an external third-party embedding like the ones we \\nhave learned about in this chapter, a process similar to transfer learning in computer vision. Finally, \\nwe will learn how to combine the two approaches, starting with a third-party embedding and letting \\nthe network use that as a starting point for its custom embedding, a process similar to fine-tuning in \\ncomputer vision.\\nAs usual, we will start with our imports:\\nimport argparse\\nimport gensim.downloader as api\\nimport numpy as np\\nimport os\\nimport shutil\\nimport tensorflow as tf\\nfrom sklearn.metrics import accuracy_score, confusion_matrix\\nScikit-learn is an open-source Python machine learning toolkit that contains many efficient \\nand easy-to-use tools for data mining and data analysis. In this chapter, we have used \\ntwo of its predefined metrics, accuracy_score  and confusion_matrix , to evaluate our \\nmodel after it is trained.\\nYou can learn more about scikit-learn at https://scikit-learn.org/stable/ .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='061de7cc-92da-422b-8029-118d888bc589', embedding=None, metadata={'page_label': '115', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 115\\nGetting the data\\nThe data for our model is available publicly and comes from the SMS spam collection dataset from \\nthe UCI Machine Learning Repository [11]. The following code will download the file and parse it to \\nproduce a list of SMS messages and their corresponding labels:\\ndef download_and_read (url):\\n   local_file = url.split( \\'/\\')[-1]\\n   p = tf.keras.utils.get_file(local_file, url,\\n       extract= True, cache_dir= \".\")\\n   labels, texts = [], []\\n   local_file = os.path.join( \"datasets\" , \"SMSSpamCollection\" )\\n   with open(local_file, \"r\") as fin:\\n       for line in fin:\\n           label, text = line.strip().split( \\'\\\\t\\')\\n           labels.append( 1 if label == \"spam\" else 0)\\n           texts.append(text)\\n   return texts, labels\\nDATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/\\nsmsspamcollection.zip\"\\ntexts, labels = download_and_read(DATASET_URL)\\nThe dataset contains 5,574 SMS records, 747 of which are marked as “spam” and the other 4,827 are \\nmarked as “ham” (not spam). The text of the SMS records is contained in the variable texts , and the \\ncorresponding numeric labels (0 = ham, 1 = spam) are contained in the variable labels.\\nMaking the data ready for use\\nThe next step is to process the data so it can be consumed by the network. The SMS text needs to be \\nfed into the network as a sequence of integers, where each word is represented by its corresponding \\nID in the vocabulary. We will use the Keras tokenizer to convert each SMS text into a sequence of words, \\nand then create the vocabulary using the fit_on_texts()  method on the tokenizer.\\nWe then convert the SMS messages to a sequence of integers using texts_to_sequences() . Finally, \\nsince the network can only work with fixed-length sequences of integers, we call the pad_sequences()  \\nfunction to pad the shorter SMS messages with zeros.\\nThe longest SMS message in our dataset has 189 tokens (words). In many applications where there \\nmay be a few outlier sequences that are very long, we would restrict the length to a smaller number \\nby setting the maxlen  flag. In that case, sentences longer than maxlen  tokens would be truncated, and \\nsentences shorter than maxlen  tokens would be padded:\\n# tokenize and pad text\\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\\ntokenizer.fit_on_texts(texts)\\ntext_sequences = tokenizer.texts_to_sequences(texts)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a2e1100-7a9a-49a0-b3a7-47919c4605f7', embedding=None, metadata={'page_label': '116', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 116\\ntext_sequences = tf.keras.preprocessing.sequence.pad_sequences(\\n    text_sequences)\\nnum_records = len(text_sequences)\\nmax_seqlen = len(text_sequences[ 0])\\nprint(\"{:d} sentences, max length: {:d}\" .format(\\n    num_records, max_seqlen))\\nWe will also convert our labels to categorical or one-hot encoding format, because the loss function \\nwe would like to choose (categorical cross-entropy) expects to see the labels in that format:\\n# labels\\nNUM_CLASSES = 2\\ncat_labels = tf.keras.utils.to_categorical(\\n    labels, num_classes=NUM_CLASSES)\\nThe tokenizer allows access to the vocabulary created through the word_index  attribute, which is \\nbasically a dictionary of vocabulary words to their index positions in the vocabulary. We also build \\nthe reverse index that enables us to go from index position to the word itself. In addition, we create \\nentries for the PAD character:\\n# vocabulary\\nword2idx = tokenizer.word_index\\nidx2word = {v:k for k, v in word2idx.items()}\\nword2idx[ \"PAD\"] = 0\\nidx2word[ 0] = \"PAD\"\\nvocab_size = len(word2idx)\\nprint(\"vocab size: {:d}\" .format(vocab_size))\\nFinally, we create the dataset  object that our network will work with. The dataset  object allows us \\nto set up some properties, such as the batch size, declaratively. Here, we build up a dataset from our \\npadded sequence of integers and categorical labels, shuffle the data, and split it into training, validation, \\nand test sets. Finally, we set the batch size for each of the three datasets:\\n# dataset\\ndataset = tf.data.Dataset.from_tensor_slices(\\n    (text_sequences, cat_labels))\\ndataset = dataset.shuffle( 10000)\\ntest_size = num_records // 4\\nval_size = (num_records - test_size) // 10\\ntest_dataset = dataset.take(test_size)\\nval_dataset = dataset.skip(test_size).take(val_size)\\ntrain_dataset = dataset.skip(test_size + val_size)\\nBATCH_SIZE = 128\\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder= True)\\nval_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder= True)\\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder= True)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94150b61-d936-459a-8a79-6c1eb6ad7bc6', embedding=None, metadata={'page_label': '117', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 117\\nBuilding the embedding matrix\\nThe Gensim toolkit provides access to various trained embedding models, as you can see from running \\nthe following command at the Python prompt:\\n>>> import gensim.downloader as api\\n>>> api.info( \"models\" ).keys()\\nThis will return (at the time of writing this book) the following trained word embeddings:\\n• Word2Vec: Two flavors, one trained on Google news (3 million word vectors based on 3 billion \\ntokens), and one trained on Russian corpora (word2vec-ruscorpora-300, word2vec-google-\\nnews-300).\\n• GloVe : Two flavors, one  trained on the Gigawords corpus (400,000 word vectors based on 6 \\nbillion tokens), available as 50d, 100d, 200d, and 300d vectors, and one trained on Twitter \\n(1.2 million word vectors based on 27 billion tokens), available as 25d, 50d, 100d, and 200d \\nvectors (glove-wiki-gigaword-50, glove-wiki-gigaword-100, glove-wiki-gigaword-200, glove-wiki-\\ngigaword-300, glove-twitter-25, glove-twitter-50, glove-twitter-100, glove-twitter-200). Smaller \\nembedding sizes would result in greater compression of the input and consequently a greater \\ndegree of approximation.\\n• fastText: One million word vectors trained with subword information on Wikipedia 2017, the \\nUMBC web corpus, and statmt.org news dataset (16B tokens) (fastText-wiki-news-subwords-300).\\n• ConceptNet Numberbatch: An ensemble embedding that uses the ConceptNet semantic \\nnetwork, the paraphrase database ( PPDB ), Word2Vec, and GloVe as input. Produces 600d \\nvectors [12, 13].\\nFor our example, we chose the 300d GloVe embeddings trained on the Gigaword corpus.\\nIn order to keep our model size small, we want to only consider embeddings for words that exist in \\nour vocabulary. This is done using the following code, which creates a smaller embedding matrix for \\neach word in the vocabulary. Each row in the matrix corresponds to a word, and the row itself is the \\nvector corresponding to the embedding for the word:\\ndef build_embedding_matrix (sequences, word2idx, embedding_dim,\\n       embedding_file):\\n   if os.path.exists(embedding_file):\\n       E = np.load(embedding_file)\\n   else:\\n       vocab_size = len(word2idx)\\n       E = np.zeros((vocab_size, embedding_dim))\\n       word_vectors = api.load(EMBEDDING_MODEL)\\n       for word, idx in word2idx.items():\\n           try:\\n               E[idx] = word_vectors.word_vec(word)\\n           except KeyError:   # word not in embedding\\n               pass', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df91285f-3ca6-4c5f-b557-718609e522e7', embedding=None, metadata={'page_label': '118', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 118\\n       np.save(embedding_file, E)\\n   return E\\nEMBEDDING_DIM = 300\\nDATA_DIR = \"data\"\\nEMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\\nEMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\\nE = build_embedding_matrix(text_sequences, word2idx, \\n   EMBEDDING_DIM,\\n   EMBEDDING_NUMPY_FILE)\\nprint(\"Embedding matrix:\" , E.shape)\\nThe output shape for the embedding matrix is (9010, 300), corresponding to the 9,010 tokens in the \\nvocabulary, and 300 features in the third-party GloVe embeddings.\\nDefining the spam classifier\\nWe are now ready to define our classifier. We will use a one-dimensional Convolutional Neural Network \\nor ConvNet (1D CNN), similar to the network you have seen already in Chapter 3, Convolutional Neural \\nNetworks.\\nThe input is a sequence of integers. The first layer is an embedding layer, which converts each input \\ninteger to a vector of size ( embedding_dim ). Depending on the run mode, that is, whether we will learn \\nthe embeddings from scratch, do transfer learning, or do fine-tuning, the embedding layer in the \\nnetwork would be slightly different. When the network starts with randomly initialized embedding \\nweights ( run_mode == \"scratch\" ) and learns the weights during the training, we set the trainable  \\nparameter to True . In the transfer learning case ( run_mode == \"vectorizer\" ), we set the weights \\nfrom our embedding matrix E but set the trainable  parameter to False , so it doesn’t train. In the fine-\\ntuning case ( run_mode == \"finetuning\" ), we set the embedding weights from our external matrix E, \\nas well as setting the layer to trainable.\\nThe output of the embedding is fed into a convolutional layer. Here, fixed-size 3-token-wide 1D windows \\n(kernel_size=3 ), also called time steps, are convolved against 256 random filters ( num_filters=256 ) \\nto produce vectors of size 256 for each time step. Thus, the output vector shape is ( batch_size , time_\\nsteps , num_filters ).\\nThe output of the convolutional layer is sent to a 1D spatial dropout layer. Spatial dropout will randomly \\ndrop entire feature maps output from the convolutional layer. This is a regularization technique to \\nprevent over-fitting. This is then sent through a global max pool layer, which takes the maximum \\nvalue from each time step for each filter, resulting in a vector of shape ( batch_size , num_filters ).\\nThe output of the dropout layer is fed into a pooling layer to flatten it, and then into a dense layer, which \\nconverts the vector of shape ( batch_size , num_filters ) to (batch_size , num_classes ). A softmax \\nactivation will convert the scores for each of (spam, ham) into a probability distribution, indicating \\nthe probability of the input SMS being spam or ham respectively:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12f74dbb-566c-4e05-a5f0-8b6948c8d9d1', embedding=None, metadata={'page_label': '119', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 119\\nclass SpamClassifierModel (tf.keras.Model):\\n   def __init__ (self, vocab_sz, embed_sz, input_length,\\n           num_filters, kernel_sz, output_sz,\\n           run_mode, embedding_weights,\\n           **kwargs):\\n       super(SpamClassifierModel, self).__init__(**kwargs)\\n       if run_mode == \"scratch\" :\\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\\n               embed_sz,\\n               input_length=input_length,\\n               trainable= True)\\n       elif run_mode == \"vectorizer\" :\\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\\n               embed_sz,\\n               input_length=input_length,\\n               weights=[embedding_weights],\\n               trainable= False)\\n       else:\\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\\n               embed_sz,\\n               input_length=input_length,\\n               weights=[embedding_weights],\\n               trainable= True)\\n       self.conv = tf.keras.layers.Conv1D(filters=num_filters,\\n           kernel_size=kernel_sz,\\n           activation= \"relu\")\\n       self.dropout = tf.keras.layers.SpatialDropout1D( 0.2)\\n       self.pool = tf.keras.layers.GlobalMaxPooling1D()\\n       self.dense = tf.keras.layers.Dense(output_sz,\\n           activation= \"softmax\" )\\n   def call(self, x):\\n       x = self.embedding(x)\\n       x = self.conv(x)\\n       x = self.dropout(x)\\n       x = self.pool(x)\\n       x = self.dense(x)\\n       return x\\n# model definition\\nconv_num_filters = 256\\nconv_kernel_size = 3\\nmodel = SpamClassifierModel(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d557ca12-3332-4bf4-8eaf-a93128045d9b', embedding=None, metadata={'page_label': '120', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 120\\n   vocab_size, EMBEDDING_DIM, max_seqlen,\\n   conv_num_filters, conv_kernel_size, NUM_CLASSES,\\n   run_mode, E)\\nmodel.build(input_shape=( None, max_seqlen))\\nFinally, we compile the model using the categorical cross entropy loss function and the Adam optimizer:\\n# compile\\nmodel.compile(optimizer= \"adam\", loss=\"categorical_crossentropy\" , \\nmetrics=[ \"accuracy\" ])\\nTraining and evaluating the model\\nOne thing to notice is that the dataset is somewhat imbalanced; there are only 747 instances of spam \\ncompared to 4,827 instances of ham. The network could achieve close to 87% accuracy simply by \\nalways predicting the majority class. To alleviate this problem, we set class weights to indicate that \\nan error on a spam SMS is eight times as expensive as an error on a ham SMS. This is indicated by \\nthe CLASS_WEIGHTS  variable, which is passed into the model.fit()  call as an additional parameter.\\nAfter training for 3 epochs, we evaluate the model against the test set, and report the accuracy and \\nconfusion matrix of the model against the test set. However, for imbalance data, even with the use \\nof class weights, the model may end up learning to always predict the majority class. Therefore, it \\nis generally advisable to report accuracy on a per-class basis to make sure that the model learns to \\ndistinguish each class effectively. This can be done quite easily using the confusion matrix by dividing \\nthe diagonal element for each row by the sum of elements for that row, where each row corresponds \\nto a labeled class:\\nNUM_EPOCHS = 3\\n# data distribution is 4827 ham and 747 spam (total 5574), which\\n# works out to approx 87% ham and 13% spam, so we take reciprocals\\n# and this works out to being each spam (1) item as being \\n# approximately 8 times as important as each ham (0) message.\\nCLASS_WEIGHTS = { 0: 1, 1: 8 }\\n# train model\\nmodel.fit(train_dataset, epochs=NUM_EPOCHS,\\n   validation_data=val_dataset,\\n   class_weight=CLASS_WEIGHTS)\\n# evaluate against test set\\nlabels, predictions = [], []\\nfor Xtest, Ytest in test_dataset:\\n   Ytest_ = model.predict_on_batch(Xtest)\\n   ytest = np.argmax(Ytest, axis= 1)\\n   ytest_ = np.argmax(Ytest_, axis= 1)\\n   labels.extend(ytest.tolist())\\n   predictions.extend(ytest.tolist())', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d72add44-d9bd-42dc-abc7-451d1e6d373c', embedding=None, metadata={'page_label': '121', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 121\\nprint(\"test accuracy: {:.3f}\" .format(accuracy_score(labels, predictions)))\\nprint(\"confusion matrix\" )\\nprint(confusion_matrix(labels, predictions))\\nRunning the spam detector\\nThe three scenarios we want to look at are:\\n• Letting the network learn the embedding for the task.\\n• Starting with a fixed external third-party embedding where the embedding matrix is treated \\nlike a vectorizer to transform the sequence of integers into a sequence of vectors.\\n• Starting with an external third-party embedding which is further fine-tuned to the task during \\nthe training.\\nEach scenario can be evaluated by setting the value of the mode  argument as shown in the following \\ncommand:\\n$ python spam_classifier --mode [scratch|vectorizer|finetune]\\nThe dataset is small, and the model is fairly simple. We were able to achieve very good results (validation \\nset accuracies in the high 90s, and perfect test set accuracy) with only minimal training (3 epochs). In \\nall three cases, the network achieved a perfect score, accurately predicting the 1,111 ham messages, \\nas well as the 169 spam cases.\\nThe change in validation accuracies, shown in Figure 4.3, illustrates the differences between the three \\napproaches:\\nFigure 4.3: Comparison of validation accuracy across training  \\nepochs for different embedding techniques', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83d89ae9-4e96-44b7-bf4a-318c49d9bbde', embedding=None, metadata={'page_label': '122', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 122\\nIn the learning from scratch case, at the end of the first epoch, the validation accuracy is 0.93, but over \\nthe next two epochs, it rises to 0.98. In the vectorizer case, the network gets something of a head start \\nfrom the third-party embeddings and ends up with a validation accuracy of almost 0.95 at the end of \\nthe first epoch. However, because the embedding weights are not allowed to change, it is not able to \\ncustomize the embeddings to the spam detection task, and the validation accuracy at the end of the \\nthird epoch is the lowest among the three. The fine-tuning case, like the vectorizer, also gets a head \\nstart, but can customize the embedding to the task as well, and therefore is able to learn at the most \\nrapid rate among the three cases. The fine-tuning case has the highest validation accuracy at the end \\nof the first epoch and reaches the same validation accuracy at the end of the second epoch that the \\nscratch case achieves at the end of the third.\\nIn the next section, we will see that distributional similarity is not restricted to word embeddings; it \\napplies to other scenarios as well.\\nNeural embeddings – not just for words\\nWord embedding technology has evolved in various ways since Word2Vec and GloVe. One such direction \\nis the application of word embeddings to non-word settings, also known as neural embeddings. As you \\nwill recall, word embeddings leverage the distributional hypothesis that words occurring in similar \\ncontexts tend to have similar meanings, where context is usually a fixed-size (in number of words) \\nwindow around the target word.\\nThe idea of neural embeddings is very similar; that is, entities that occur in similar contexts tend to \\nbe strongly related to each other. The way in which these contexts are constructed is usually situation-\\ndependent. We will describe two techniques here that are foundational and general enough to be \\napplied easily to a variety of use cases.\\nItem2Vec\\nThe Item2Vec embedding model was originally proposed by Barkan and Koenigstein [14] for the \\ncollaborative filtering use case, that is, recommending items to users based on purchases by other \\nusers that have similar purchase histories to this user. It uses items in a web-store as the “words” and \\nthe itemset (the sequence of items purchased by a user over time) as the “sentence” from which the \\n“word context” is derived.\\nFor example, consider the problem of recommending items to shoppers in a supermarket. Assume \\nthat our supermarket sells 5,000 items, so each item can be represented as a sparse one-hot encoded \\nvector of size 5,000. Each user is represented by their shopping cart, which is a sequence of such \\nvectors. Applying a context window similar to the one we saw in the Word2Vec section, we can train \\na skip-gram model to predict likely item pairs. The learned embedding model maps the items to a \\ndense low-dimensional space where similar items are close together, which can be used to make \\nsimilar item recommendations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d9f2c5f-9cb5-474a-b2cc-cdec36327598', embedding=None, metadata={'page_label': '123', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 123\\nnode2vec\\nThe node2vec embedding model was proposed by Grover and Leskovec [15], as a scalable way to learn \\nfeatures for nodes in a graph. It learns an embedding of the structure of the graph by executing a large \\nnumber of fixed-length random walks on the graph. The nodes are the “words” and the random walks \\nare the “sentences” from which the “word context” is derived in node2vec.\\nThe Something2Vec page [40] provides a comprehensive list of ways in which researchers have tried \\nto apply the distributional hypothesis to entities other than words. Hopefully, this list will spark ideas \\nfor your own “Something2Vec” representation.\\nTo illustrate how easy it is to create your own neural embedding, we will generate a node2vec-like \\nmodel or, more accurately, a predecessor graph-based embedding called DeepWalk, proposed by  \\nPerozzi, et al. [42] for papers presented at the NeurIPS conference from 1987-2015, by leveraging word \\nco-occurrence relationships between them.\\nThe dataset is a 11,463 × 5,812 matrix of word counts, where the rows represent words, and columns \\nrepresent conference papers. We will use this to construct a graph of papers, where an edge between \\ntwo papers represents a word that occurs in both of them. Both node2vec and DeepWalk assume that \\nthe graph is undirected and unweighted. Our graph is undirected, since a relationship between a \\npair of papers is bidirectional. However, our edges could have weights based on the number of word \\nco-occurrences between the two documents. For our example, we will consider any number of co-\\noccurrences above 0 to be a valid unweighted edge.\\nAs usual, we will start by declaring our imports:\\nimport gensim\\nimport logging\\nimport numpy as np\\nimport os\\nimport shutil\\nimport tensorflow as tf\\nfrom scipy.sparse import csr_matrix\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nlogging.basicConfig( format=\\'%(asctime)s : %(levelname)s : %(message)s\\' , \\nlevel=logging.INFO)\\nThe next step is to download the data from the UCI repository and convert it to a sparse term-document \\nmatrix, TD, then construct a document-document matrix E by multiplying the transpose of the term-\\ndocument matrix by itself. Our graph is represented as an adjacency or edge matrix by the document-\\ndocument matrix. Since each element represents a similarity between two documents, we will binarize \\nthe matrix E by setting any non-zero elements to 1:\\nDATA_DIR = \"./data\"\\nUCI_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-\\ndatabases/00371/NIPS_1987-2015.csv\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66b6bc87-997f-41d1-95f2-98b68ab07114', embedding=None, metadata={'page_label': '124', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 124\\ndef download_and_read (url):\\n   local_file = url.split( \\'/\\')[-1]\\n   p = tf.keras.utils.get_file(local_file, url, cache_dir= \".\")\\n   row_ids, col_ids, data = [], [], []\\n   rid = 0\\n   f = open(p, \"r\")\\n   for line in f:\\n       line = line.strip()\\n       if line.startswith( \"\\\\\"\\\\\",\"):\\n           # header\\n           continue\\n       # compute non-zero elements for current row\\n       counts = np.array([ int(x) for x in line.split( \\',\\')[1:]])\\n       nz_col_ids = np.nonzero(counts)[ 0]\\n       nz_data = counts[nz_col_ids]\\n       nz_row_ids = np.repeat(rid, len(nz_col_ids))\\n       rid += 1\\n       # add data to big lists\\n       row_ids.extend(nz_row_ids.tolist())\\n       col_ids.extend(nz_col_ids.tolist())\\n       data.extend(nz_data.tolist())\\n   f.close()\\n   TD = csr_matrix((\\n       np.array(data), (\\n           np.array(row_ids), np.array(col_ids)\\n           )\\n       ),\\n       shape=(rid, counts.shape[ 0]))\\n   return TD\\n# read data and convert to Term-Document matrix\\nTD = download_and_read(UCI_DATA_URL)\\n# compute undirected, unweighted edge matrix\\nE = TD.T * TD\\n# binarize\\nE[E > 0] = 1\\nOnce we have our sparse binarized adjacency matrix, E, we can then generate random walks from \\neach of the vertices. From each node, we construct 32 random walks of a maximum length of 40 nodes. \\nThe walks have a random restart probability of 0.15, which means that for any node, the particular \\nrandom walk could end with a 15% probability. The following code will construct the random walks \\nand write them out to a file given by RANDOM_WALKS_FILE . To give an idea of the input, we have provided \\na snapshot of the first 10 lines of this file, showing random walks starting from node 0:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e555be6-6994-46a4-bf40-df802e196940', embedding=None, metadata={'page_label': '125', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 125\\n0 1405 4845 754 4391 3524 4282 2357 3922 1667\\n0 1341 456 495 1647 4200 5379 473 2311\\n0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273\\n0 906 3498 2286 4755 2567 2632\\n0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 \\n2288 1615 1166\\n0 2469 1353 5596 2207 4065 3100\\n0 2236 1464 1596 2554 4021\\n0 4688 864 3684 4542 3647 2859\\n0 4884 4590 5386 621 4947 2784 1309 4958 3314\\n0 5546 200 3964 1817 845\\nNote that this is a very slow process. A copy of the output is provided along with the source code for \\nthis chapter in case you prefer to skip the random walk generation process:\\nNUM_WALKS_PER_VERTEX = 32\\nMAX_PATH_LENGTH = 40\\nRESTART_PROB = 0.15\\nRANDOM_WALKS_FILE = os.path.join(DATA_DIR, \"random-walks.txt\" )\\ndef construct_random_walks (E, n, alpha, l, ofile):\\n   if os.path.exists(ofile):\\n       print(\"random walks generated already, skipping\" )\\n       return\\n   f = open(ofile, \"w\")\\n   for i in range (E.shape[ 0]):  # for each vertex\\n       if i % 100 == 0:\\n           print(\"{:d} random walks generated from {:d} vertices\"\\n               . format(n * i, i))\\n       for j in range (n):       # construct n random walks\\n           curr = i\\n           walk = [curr]\\n           target_nodes = np.nonzero(E[curr])[ 1]\\n           for k in range (l):   # each of max length l\\n               # should we restart?\\n               if np.random.random() < alpha and len(walk) > 5:\\n                   break\\n               # choose one outgoing edge and append to walk\\n               try:\\n                   curr = np.random.choice(target_nodes)\\n                   walk.append(curr)\\n                   target_nodes = np.nonzero(E[curr])[ 1]\\n               except ValueError:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a8b46a5-2350-4747-b3e9-1f9934bb922d', embedding=None, metadata={'page_label': '126', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 126\\n                   continue\\n           f.write( \"{:s}\\\\n\" .format(\" \".join([str(x) for x in walk])))\\n   print(\"{:d} random walks generated from {:d} vertices, COMPLETE\"\\n       . format(n * i, i))\\n   f.close()\\n# construct random walks (caution: very long process!)\\nconstruct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, MAX_PATH_LENGTH, \\nRANDOM_WALKS_FILE)\\nA few lines from the RANDOM_WALKS_FILE  are shown below. You could imagine that these look like \\nsentences in a language where the vocabulary of words is all the node IDs in our graph. We have learned \\nthat word embeddings exploit the structure of language to generate a distributional representation \\nfor words. Graph embedding schemes such as DeepWalk and node2vec do the exact same thing with \\nthese “sentences” created out of random walks. Such embeddings can capture similarities between \\nnodes in a graph that go beyond immediate neighbors, as we shall see:\\n0 1405 4845 754 4391 3524 4282 2357 3922 1667\\n0 1341 456 495 1647 4200 5379 473 2311\\n0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273\\n0 906 3498 2286 4755 2567 2632\\n0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 \\n2288 1615 1166\\n0 2469 1353 5596 2207 4065 3100\\n0 2236 1464 1596 2554 4021\\n0 4688 864 3684 4542 3647 2859\\n0 4884 4590 5386 621 4947 2784 1309 4958 3314\\n0 5546 200 3964 1817 845\\nWe are now ready to create our word embedding model. The Gensim package offers a simple API that \\nallows us to declaratively create and train a Word2Vec model, using the following code. The trained \\nmodel will be serialized to the file given by W2V_MODEL_FILE . The Documents  class allows us to stream \\nlarge input files to train the Word2Vec model without running into memory issues. We will train the \\nWord2Vec model in skip-gram mode with a window size of 10, which means we train it to predict up \\nto five neighboring vertices given a central vertex. The resulting embedding for each vertex is a dense \\nvector of size 128:\\nW2V_MODEL_FILE = os.path.join(DATA_DIR, \"w2v-neurips-papers.model\" )\\nclass Documents (object):\\n   def __init__ (self, input_file):\\n       self.input_file = input_file\\n   def __iter__ (self):\\n       with open(self.input_file, \"r\") as f:\\n           for i, line in enumerate (f):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9acdea95-ce8c-4766-aefe-0f9183a61042', embedding=None, metadata={'page_label': '127', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 127\\n               if i % 1000 == 0:\\n                   logging.info( \"{:d} random walks extracted\" .format(i))\\n               yield line.strip().split()\\ndef train_word2vec_model (random_walks_file, model_file):\\n   if os.path.exists(model_file):\\n       print(\"Model file {:s} already present, skipping training\"\\n           . format(model_file))\\n       return\\n   docs = Documents(random_walks_file)\\n   model = gensim.models.Word2Vec(\\n       docs,\\n       size= 128,    # size of embedding vector\\n       window= 10,   # window size\\n       sg= 1,        # skip-gram model\\n       min_count= 2,\\n       workers= 4\\n   )\\n   model.train(\\n       docs,\\n       total_examples=model.corpus_count,\\n       epochs= 50)\\n   model.save(model_file)\\n# train model\\ntrain_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE)\\nOur resulting DeepWalk model is just a Word2Vec model, so anything you can do with Word2Vec in \\nthe context of words, you can do with this model in the context of vertices. Let us use the model to \\ndiscover similarities between documents:\\ndef evaluate_model (td_matrix, model_file, source_id):\\n   model = gensim.models.Word2Vec.load(model_file).wv\\n   most_similar = model.most_similar( str(source_id))\\n   scores = [x[ 1] for x in most_similar]\\n   target_ids = [x[ 0] for x in most_similar]\\n   # compare top 10 scores with cosine similarity \\n   # between source and each target\\n   X = np.repeat(td_matrix[source_id].todense(), 10, axis= 0)\\n   Y = td_matrix[target_ids].todense()\\n   cosims = [cosine_similarity(X[i], Y[i])[ 0, 0] for  i in range (10)]\\n   for i in range (10):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c02cdf20-f425-4cda-ae88-e5600020a529', embedding=None, metadata={'page_label': '128', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 128\\n       print(\"{:d} {:s} {:.3f} {:.3f}\" .format(\\n           source_id, target_ids[i], cosims[i], scores[i]))\\nsource_id = np.random.choice(E.shape[ 0])\\nevaluate_model(TD, W2V_MODEL_FILE, source_id)\\nThe following output is shown. The first and second columns are the source and target vertex IDs. \\nThe third column is the cosine similarity between the term vectors corresponding to the source and \\ntarget documents, and the fourth is the similarity score reported by the Word2Vec model. As you can \\nsee, cosine similarity reports a similarity only between 2 of the 10 document pairs, but the Word2Vec \\nmodel is able to detect latent similarities in the embedding space. This is similar to the behavior we \\nhave noticed between one-hot encoding and dense embeddings:\\nsrc_id dst_id cosine_sim w2v_score\\n1971   5443        0.000     0.348\\n1971   1377        0.000     0.348\\n1971   3682        0.017     0.328\\n1971   51          0.022     0.322\\n1971   857         0.000     0.318\\n1971   1161        0.000     0.313\\n1971   4971        0.000     0.313\\n1971   5168        0.000     0.312\\n1971   3099        0.000     0.311\\n1971   462         0.000     0.310\\nThe code for this embedding strategy is available in neurips_papers_node2vec.py  in the source code \\nfolder accompanying this chapter. Next, we will move on to look at character and subword embeddings.\\nCharacter and subword embeddings\\nAnother evolution of the basic word embedding strategy has been to look at character and subword \\nembeddings instead of word embeddings. Character-level embeddings were first proposed by Xiang \\nand LeCun [17] and have some key advantages over word embeddings.\\nFirst, a character vocabulary is finite and small – for example, a vocabulary for English would contain \\naround 70 characters (26 characters, 10 numbers, and the rest special characters), leading to character \\nmodels that are also small and compact. Second, unlike word embeddings, which provide vectors for a \\nlarge but finite set of words, there is no concept of out-of-vocabulary for character embeddings, since \\nany word can be represented by the vocabulary. Third, character embeddings tend to be better for rare \\nand misspelled words because there is much less imbalance for character inputs than for word inputs.\\nCharacter embeddings tend to work better for applications that require the notion of syntactic rather \\nthan semantic similarity. However, unlike word embeddings, character embeddings tend to be task-\\nspecific and are usually generated inline within a network to support the task. For this reason, third-\\nparty character embeddings are generally not available.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3f6b5d5d-51df-4a28-ab85-9205312c515f', embedding=None, metadata={'page_label': '129', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 129\\nSubword embeddings combine the idea of character and word embeddings by treating a word as \\na bag of character n-grams, that is, sequences of n consecutive words. They were first proposed by \\nBojanowski, et al. [18] based on research from Facebook AI Research  (FAIR ), which they later released \\nas fastText embeddings. fastText embeddings are available for 157 languages, including English. The \\npaper has reported state-of-the-art performance on a number of NLP tasks, especially word analogies \\nand language tasks for languages with rich morphologies.\\nfastText computes embeddings for character n-grams where n is between 3 and 6 characters (default \\nsettings can be changed), as well as for the words themselves. For example, character n-grams for n=3 \\nfor the word “green” would be “<gr”, “gre”, “ree”, “een”, and “en>”. The beginning and end of words \\nare marked with “<” and “>” characters respectively, to distinguish between short words and their \\nn-grams such as “<cat>” and “cat”.\\nDuring lookup, you can look up a vector from the fastText embedding using the word as the key if the \\nword exists in the embedding. However, unlike traditional word embeddings, you can still construct \\na fastText vector for a word that does not exist in the embedding. This is done by decomposing the \\nword into its constituent trigram subwords as shown in the preceding example, looking up the vectors \\nfor the subwords, and then taking the average of these subword vectors. The fastText Python API [19] \\nwill do this automatically, but you will need to do this manually if you use other APIs to access fastText \\nword embeddings, such as Gensim or NumPy.\\nNext up, we will look at dynamic embeddings.\\nDynamic embeddings\\nSo far, all the embeddings we have considered have been static; that is, they are deployed as a dictionary \\nof words (and subwords) mapped to fixed dimensional vectors. The vector corresponding to a word \\nin these embeddings is going to be the same regardless of whether it is being used as a noun or verb \\nin the sentence, for example, the word “ensure” (the name of a health supplement when used as a \\nnoun, and to make certain when used as a verb). It also provides the same vector for polysemous \\nwords or words with multiple meanings, such as “bank” (which can mean different things depending \\non whether it co-occurs with the word “money” or “river”). In both cases, the meaning of the word \\nchanges depending on clues available in its context, the sentence. Dynamic embeddings attempt to \\nuse these signals to provide different vectors for words based on their context.\\nDynamic embeddings are deployed as trained networks that convert your input (typically a sequence \\nof one-hot vectors) into a lower-dimensional dense fixed-size embedding by looking at the entire \\nsequence, not just individual words. You can either preprocess your input to this dense embedding \\nand then use this as input to your task-specific network, or wrap the network and treat it similar to \\nthe tf.keras.layers.Embedding  layer for static embeddings. Using a dynamic embedding network \\nin this way is usually much more expensive compared to generating it ahead of time (the first option) \\nor using traditional embeddings.\\nThe earliest dynamic embedding was proposed by McCann, et al. [20], and was called Contextualized \\nVectors (CoVe ). This involved taking the output of the encoder from the encoder-decoder pair of a \\nmachine translation network and concatenating it with word vectors for the same word. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccab4bac-8c19-4270-8161-02fef2053610', embedding=None, metadata={'page_label': '130', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 130\\nYou will learn more about seq2seq networks in the next chapter. The researchers found that this \\nstrategy improved the performance of a wide variety of NLP tasks.\\nAnother dynamic embedding proposed by Peters, et al. [21], was Embeddings from Language \\nModels (ELMo ). ELMo computes contextualized word representations using character-based word \\nrepresentation and bidirectional Long Short-Term Memory ( LSTM ). You will learn more about LSTMs \\nin the next chapter. In the meantime, a trained ELMo network is available from TensorFlow’s model \\nrepository TensorFlow Hub. You can access it and use it for generating ELMo embeddings as follows.\\nThe full set of models available on TensorFlow Hub that are TensorFlow 2.0 compatible can be found \\non the TensorFlow Hub site for TensorFlow 2.0 [16]. Here I have used an array of sentences, where \\nthe model will figure out tokens by using its default strategy of tokenizing on whitespace:\\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\n \\nelmo = hub.load( \"https://tfhub.dev/google/elmo/3\" )\\nembeddings = elmo.signatures[ \"default\" ](\\n    tf.constant([\\n      \"i like green eggs and ham\" ,\\n      \"would you eat them in a box\"\\n    ]))[ \"elmo\"]\\nprint(embeddings.shape)\\nThe output is ( 2, 7, 1024 ). The first index tells us that our input contained 2 sentences. The second \\nindex refers to the maximum number of words across all sentences, in this case, 7. The model \\nautomatically pads the output to the longest sentence. The third index gives us the size of the contextual \\nword embedding created by ELMo; each word is converted to a vector of size (1024).\\nYou can also integrate the ELMo embedding layer into your TF2 model by wrapping it in a tf.keras.\\nKerasLayer  adapter. In this simple model, the model will return the embedding for the entire string:\\nembed = hub.KerasLayer( \"https://tfhub.dev/google/elmo/3\" ,input_shape=[], \\ndtype=tf.string)\\nmodel = tf.keras.Sequential([embed])\\nembeddings = model.predict([\\n    \"i i like green eggs and ham\" ,\\n    \"would you eat them in a box\"\\n])\\nprint(embeddings.shape)\\nDynamic embeddings such as ELMo are able to provide different embeddings for the same word when \\nused in different contexts and represent an improvement over static embeddings such as Word2Vec \\nor GloVe. A logical next step is embeddings that represent larger units of text, such as sentences and \\nparagraphs. This is what we will look at in the next section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e7d9ada-cc5f-472f-8ba4-d8cf3bcc4993', embedding=None, metadata={'page_label': '131', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 131\\nSentence and paragraph embeddings\\nA simple, yet surprisingly effective solution for generating useful sentence and paragraph embeddings \\nis to average the word vectors of their constituent words. Even though we will describe some popular \\nsentence and paragraph embeddings in this section, it is generally always advisable to try averaging \\nthe word vectors as a baseline.\\nSentence (and paragraph) embeddings can also be created in a task-optimized way by treating them \\nas a sequence of words and representing each word using some standard word vector. The sequence \\nof word vectors is used as input to train a network for some specific task. Vectors extracted from one \\nof the later layers of the network just before the classification layer generally tend to produce a very \\ngood vector representation for the sequence. However, they tend to be very task-specific, and are of \\nlimited use as a general vector representation.\\nAn idea for generating general vector representations for sentences that could be used across tasks \\nwas proposed by Kiros, et al. [22]. They proposed using the continuity of text from books to construct \\nan encoder-decoder model that is trained to predict surrounding sentences given a sentence. The \\nvector representation of a sequence of words constructed by an encoder-decoder network is typically \\ncalled a “thought vector.” In addition, the proposed model works on a very similar basis to skip-gram, \\nwhere we try to predict the surrounding words given a word. For these reasons, these sentence vectors \\nwere called skip-thought vectors. The project released a Theano-based model that could be used to \\ngenerate embeddings from sentences. Later, the model was re-implemented with TensorFlow by the \\nGoogle Research team [23]. The Skip-Thoughts model emits vectors of size (2048) for each sentence. \\nUsing the model is not very straightforward, but the README.md  file on the repository [23] provides \\ninstructions if you would like to use it.\\nA more convenient source of sentence embeddings is the Google Universal Sentence Encoder, available \\non TensorFlow Hub. There are two flavors of the encoder in terms of implementation. The first flavor \\nis fast but not so accurate and is based on the Deep Averaging Network (DAN ) proposed by Iyer, et \\nal. [24], which combines embeddings for words and bigrams and sends it through a fully connected \\nnetwork. The second flavor is much more accurate but slower and is based on the encoder component \\nof the transformer network proposed by Vaswani, et al. [25]. We will cover the transformer network \\nin more detail in Chapter 6, Transformers.\\nAs with ELMo, the Google Universal Sentence Encoder can also be loaded from TensorFlow Hub into \\nyour TF2 code. Here is some code that calls it with two of our example sentences:\\nembed = hub.load( \"https://tfhub.dev/google/universal-sentence-encoder-large/4\" )\\nembeddings = embed([\\n\"i like green eggs and ham\" ,\\n\"would you eat them in a box\"\\n])[\"outputs\" ]\\nprint(embeddings.shape)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d71e205c-651d-4db0-bbcd-fdccb3f04acf', embedding=None, metadata={'page_label': '132', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 132\\nThe output is ( 2, 512); that is, each sentence is represented by a vector of size (512). It is important to \\nnote that the Google Universal Sentence Encoder can handle any length of word sequence—so you \\ncould legitimately use it to get word embeddings on one end as well as paragraph embeddings on the \\nother. However, as the sequence length gets longer, the quality of the embeddings tends to get “diluted.”\\nA much earlier related line of work in producing embeddings for long sequences such as paragraphs and \\ndocuments was proposed by Le and Mikolov [26] soon after Word2Vec was proposed. It is now known \\ninterchangeably as Doc2Vec or Paragraph2Vec. The Doc2Vec algorithm is an extension of Word2Vec \\nthat uses surrounding words to predict a word. In the case of Doc2Vec, an additional parameter, the \\nparagraph ID, is provided during training. At the end of the training, the Doc2Vec network learns an \\nembedding for every word and an embedding for every paragraph. During inference, the network is \\ngiven a paragraph with some missing words. The network uses the known part of the paragraph to \\nproduce a paragraph embedding, then uses this paragraph embedding and the word embeddings to \\ninfer the missing words in the paragraph. The Doc2Vec algorithm comes in two flavors—the Paragraph \\nVectors - Distributed Memory ( PV-DM ) and Paragraph Vectors - Distributed Bag of Words ( PV-DBOW ), \\nroughly analogous to CBOW and skip-gram in Word2Vec. We will not look at Doc2Vec further in this \\nbook, except to note that the Gensim toolkit provides prebuilt implementations that you can train \\nwith your own corpus.\\nHaving looked at the different forms of static and dynamic embeddings, we will now switch gears a \\nbit and look at language model-based embeddings.\\nLanguage model-based embeddings\\nLanguage model-based embeddings represent the next step in the evolution of word embeddings. A \\nlanguage model is a probability distribution over sequences of words. Once we have a model, we can \\nask it to predict the most likely next word given a particular sequence of words. Similar to traditional \\nword embeddings, both static and dynamic, they are trained to predict the next word (or previous \\nword as well, if the language model is bidirectional) given a partial sentence from the corpus. Training \\ndoes not involve active labeling, since it leverages the natural grammatical structure of large volumes \\nof text, so in a sense, this is a self-supervised learning process.\\nThe main difference between a language model as a word embedding and more traditional embeddings \\nis that traditional embeddings are applied as a single initial transformation on the data and are then \\nfine-tuned for specific tasks. In contrast, language models are trained on large external corpora and \\nrepresent a model of a particular language, say English. This step is called pretraining. The computing \\ncost to pretrain these language models is usually fairly high; however, the people who pretrain these \\nmodels generally make them available for use by others so we usually do not need to worry about \\nthis step. The next step is to fine-tune these general-purpose language models for your particular \\napplication domain. For example, if you are working in the travel or healthcare industry, you would \\nfine-tune the language model with text from your own domain. Fine-tuning involves retraining the \\nlast few layers with your own text. Once fine-tuned, you can reuse this model for multiple tasks within \\nyour domain. The fine-tuning step is generally much less expensive compared to the pretraining step.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21fc4b68-c07d-400e-9087-45b3f497910c', embedding=None, metadata={'page_label': '133', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 133\\nOnce you have the fine-tuned language model, you remove the last layer of the language model and \\nreplace  it with a one-to-two-layer fully connected network that converts the language model embedding \\nfor your input into the final categorical or regression output that your task needs. The idea is identical \\nto transfer learning, which you learned about in Chapter 3, Convolutional Neural Networks, the only \\ndifference here is that you are doing transfer learning on text instead of images. As with transfer \\nlearning with images, these language model-based embeddings allow us to get surprisingly good \\nresults with very little labeled data. Not surprisingly, language model embeddings have been referred \\nto as the “ImageNet moment” for natural language processing.\\nThe language model-based embedding idea has its roots in the ELMo [21] network, which you have \\nalready seen in this chapter. ELMo learns about its language by being trained on a large text corpus to \\nlearn to predict the next and previous words given a sequence of words. ELMo is based on a bidirectional \\nLSTM, which you will learn more about in Chapter 8, Autoencoders.\\nThe first viable language model embedding was proposed by Howard and Ruder [27] via their Universal \\nLanguage Model Fine-Tuning ( ULMFiT ) model, which was trained on the wikitext-103 dataset consisting \\nof 28,595 Wikipedia articles and 103 million words. ULMFiT provides the same benefits that transfer \\nlearning provides for image tasks—better results from supervised learning tasks with comparatively \\nless labeled data.\\nMeanwhile, the transformer architecture has become the preferred network for machine translation \\ntasks, replacing the LSTM network because it allows for parallel operations and better handling \\nof long-term dependencies. We will learn more about the transformer architecture in Chapter 6, \\nTransformers. The OpenAI team of Radford, et al. [29] proposed using the decoder stack from the \\nstandard transformer network instead of the LSTM network used in ULMFiT. Using this, they built a \\nlanguage model embedding called Generative Pretraining (GPT ) that achieved state of the art results \\nfor many language processing tasks. The paper proposes several configurations for supervised tasks \\ninvolving single-and multi-sentence tasks such as classification, entailment, similarity, and multiple-\\nchoice question answering.\\nThe OpenAI team later followed this up by building even larger language models called GPT-2 and \\nGPT-3 respectively. GPT-2 was initially not released because of fears of misuse of the technology by \\nmalicious operators [30].\\nOne problem with the OpenAI transformer architecture is that it is unidirectional whereas its \\npredecessors ELMo and ULMFiT were bidirectional. Bidirectional Encoder Representations for \\nTransformers ( BERT ), proposed by the Google AI team [28], uses the encoder stack of the Transformer \\narchitecture and achieves bidirectionality safely by masking up to 15% of its input, which it asks the \\nmodel to predict.\\nAs with the OpenAI paper, BERT proposes configurations for using it for several supervised learning \\ntasks such as single- and multiple-sentence classification, question answering, and tagging.\\nThe BERT model comes in two major flavors—BERT-base and BERT-large. BERT-base has 12 encoder \\nlayers, 768 hidden units, and 12 attention heads, with 110 million parameters in all. BERT-large has 24 \\nencoder layers, 1,024 hidden units, and 16 attention heads, with 340 million parameters. More details \\ncan be found in the BERT GitHub repository [33].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='804b645d-3c5a-4eae-819d-63206808bcf0', embedding=None, metadata={'page_label': '134', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 134\\nBERT pretraining is an expensive process and can currently only be achieved using Tensor Processing \\nUnits (TPUs ) or large distributed Graphics Processing Units (GPUs ) clusters. TPUs are only available \\nfrom Google via its Colab network [31] or Google Cloud Platform [32]. However, fine-tuning the BERT-\\nbase with custom datasets is usually achievable on GPU instances.\\nOnce the BERT model is fine-tuned for your domain, the embeddings from the last four hidden layers \\nusually produce good results for downstream tasks. Which embedding or combination of embeddings \\n(via summing, averaging, max-pooling, or concatenating) to use is usually based on the type of task.\\nIn the following section, we will look at how to extract embeddings from the BERT language model.\\nUsing BERT as a feature extractor\\nThe BERT project [33] provides a  set of Python scripts that can be run from the command line to \\nfine-tune BERT:\\n$ git clone https://github.com/google-research/bert.git\\n$ cd bert\\nWe then download the appropriate BERT model we want to fine-tune. As mentioned earlier, BERT \\ncomes in two sizes—BERT-base and BERT-large. In addition, each model has a cased and uncased \\nversion. The cased version differentiates between upper and lowercase words, while the uncased \\nversion does not. For our example, we will use the BERT-base-uncased pretrained model. You can find \\nthe download URL for this and the other models further down the README.md  page:\\n$ mkdir data\\n$ cd data\\n$ wget \\\\ \\nhttps://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.\\nzip\\n$ unzip -a uncased_L-12_H-768_A-12.zip\\nThis will create the following folder under the data  directory of your local BERT project. The bert_\\nconfig.json  file is the configuration file used to create the original pretrained model, and the vocab.\\ntxt is the vocabulary used for the model, consisting of 30,522 words and word pieces:\\nuncased_L-12_H-768_A-12/\\n ├── bert_config.json\\n ├── bert_model.ckpt.data-00000-of-00001\\n ├── bert_model.ckpt.index\\n ├── bert_model.ckpt.meta\\n └── vocab.txt\\nThe pretrained language model can be directly used as a text feature extractor for simple machine \\nlearning pipelines. This can be useful for situations where you want to just vectorize your text input, \\nleveraging the distributional property of embeddings to get a denser and richer representation than \\none-hot encoding.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f832894-be86-4afb-8b12-6f43087d046a', embedding=None, metadata={'page_label': '135', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 135\\nThe input in this case is just a file with one sentence per line. Let us call it sentences.txt  and put it \\ninto our ${CLASSIFIER_DATA}  folder. You can generate the embeddings from the last hidden layers \\nby identifying them as -1 (last hidden layer), -2 (hidden layer before that), and so on. The command \\nto extract BERT embeddings for your input sentences is as follows:\\n$ export  BERT_BASE_DIR=./data/uncased_L-12_H-768_A-12\\n$ export  CLASSIFIER_DATA=./data/my_data\\n$ export  TRAINED_CLASSIFIER=./data/my_classifier\\n$ python extract_features.py \\\\\\n    --input_file= ${CLASSIFIER_DATA} /sentences.txt \\\\\\n    --output_file= ${CLASSIFIER_DATA} /embeddings.jsonl \\\\\\n    --vocab_file= ${BERT_BASE_DIR} /vocab.txt \\\\\\n    --bert_config_file= ${BERT_BASE_DIR} /bert_config.json \\\\\\n    --init_checkpoint= ${BERT_BASE_DIR} /bert_model.ckpt \\\\\\n    --layers=-1,-2,-3,-4 \\\\\\n    --max_seq_length=128 \\\\\\n    --batch_size=8\\nThe command will extract the BERT embeddings from the last four hidden layers of the model and \\nwrite them out into a line-oriented JSON file called embeddings.jsonl  in the same directory as the \\ninput file. These embeddings can then be used as input to downstream models that specialize in some \\nspecific task, such as sentiment analysis. Because BERT was pretrained on large quantities of English \\ntext, it learns a lot about the nuances of the language, which turn out to be useful for these downstream \\ntasks. The downstream model does not have to be a neural network, it can be a non-neural model \\nsuch as SVM or XGBoost as well.\\nThere is much more you can do with BERT. The previous use case corresponds to transfer learning in \\ncomputer vision. As in computer vision, it is also possible to fine-tune BERT (and other transformer \\nmodels) for specific tasks, where the appropriate “head” network is attached to BERT, and the combined \\nnetwork is fine-tuned for a specific task. You will learn more about these techniques in Chapter 6, \\nTransformers.\\nSummary\\nIn this chapter, we have learned about the concepts behind distributional representations of words \\nand their various implementations, starting from static word embeddings such as Word2Vec and GloVe.\\nWe then looked at improvements to the basic idea, such as subword embeddings, sentence embeddings \\nthat capture the context of the word in the sentence, and the use of entire language models for \\ngenerating embeddings. While language model-based embeddings are achieving state-of-the-art \\nresults nowadays, there are still plenty of applications where more traditional approaches yield very \\ngood results, so it is important to know them all and understand the tradeoffs.\\nWe also looked briefly at other interesting uses of word embeddings outside the realm of natural \\nlanguage, where the distributional properties of other kinds of sequences are leveraged to make \\npredictions in domains such as information retrieval and recommendation systems.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='724be021-891c-43f9-bbc7-22d339f00717', embedding=None, metadata={'page_label': '136', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 136\\nYou are now ready to use embeddings, not only for your text-based neural networks, which we will \\nlook at in greater depth in the next chapter, but also in other areas of machine learning.\\nReferences\\n1. Mikolov, T., et al. (2013, Sep 7) Efficient Estimation of Word Representations in Vector Space. \\narXiv:1301.3781v3 [cs.CL].\\n2. Mikolov, T., et al. (2013, Sep 17). Exploiting Similarities among Languages for Machine Translation. \\narXiv:1309.4168v1 [cs.CL].\\n3. Mikolov, T., et al. (2013). Distributed Representations of Words and Phrases and their Compositionality. \\nAdvances in Neural Information Processing Systems 26 (NIPS 2013).\\n4. Pennington, J., Socher, R., Manning, C. (2014). GloVe: Global Vectors for Word Representation. D14-\\n1162, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP).\\n5. Niu, F., et al (2011, 11 Nov). HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient \\nDescent. arXiv:1106.5730v2 [math.OC].\\n6. Levy, O., Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances \\nin Neural Information Processing Systems 27 (NIPS 2014).\\n7. Mahoney, M. (2011, 1 Sep). text8 dataset: http://mattmahoney.net/dc/textdata.html\\n8. Rehurek, R. (2019, 10 Apr). gensim documentation for Word2Vec model: https://radimrehurek.\\ncom/gensim/models/word2vec.html\\n9. Levy, O., Goldberg, Y. (2014, 26-27 June). Linguistic Regularities in Sparse and Explicit Word \\nRepresentations. Proceedings of the Eighteenth Conference on Computational Language \\nLearning, pp 171-180 (ACL 2014).\\n10. Rehurek, R. (2019, 10 Apr). gensim documentation for KeyedVectors: https://radimrehurek.\\ncom/gensim/models/keyedvectors.html\\n11. Almeida, T. A., Gamez Hidalgo, J. M., and Yamakami, A. (2011). Contributions to the Study of \\nSMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium \\non Document Engineering (DOCENG): https://www.dt.fee.unicamp.br/~tiago/\\nsmsspamcollection/doceng11.pdf?ref=https://githubhelp.com\\n12. Speer, R., Chin, J. (2016, 6 Apr). An Ensemble Method to Produce High-Quality Word Embeddings. \\narXiv:1604.01692v1 [cs.CL].\\n13. Speer, R. (2016, 25 May). ConceptNet Numberbatch: a new name for the best Word Embeddings you \\ncan download: http://blog.conceptnet.io/posts/2016/conceptnet-numberbatch-a-new-\\nname-for-the-best-word-embeddings-you-can-download/\\n14. Barkan, O., Koenigstein, N. (2016, 13-16 Sep). Item2Vec: Neural Item Embedding for Collaborative \\nFiltering. IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP \\n2016).\\n15. Grover, A., Leskovec, J. (2016, 13-17 Aug). node2vec: Scalable Feature Learning for Networks. \\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery \\nand Data Mining. (KDD 2016).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c230ccd-e4d9-406d-8558-4e5dadd8f350', embedding=None, metadata={'page_label': '137', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 137\\n16. TensorFlow 2.0 Models on TensorFlow Hub: https://tfhub.dev/s?q=tf2-preview\\n17. Zhang, X., LeCun, Y. (2016, 4 Apr). Text Understanding from Scratch. arXiv 1502.01710v5 [cs.LG].\\n18. Bojanowski, P., et al. (2017, 19 Jun). Enriching Word Vectors with Subword Information. arXiv: \\n1607.04606v2 [cs.CL].\\n19. Facebook AI Research, fastText (2017). GitHub repository: https://github.com/\\nfacebookresearch/fastText\\n20. McCann, B., Bradbury, J., Xiong, C., Socher, R. (2017). Learned in Translation: Contextualized \\nWord Vectors. Neural Information Processing Systems, 2017.\\n21. Peters, M., et al. (2018, 22 Mar). Deep contextualized word representations. arXiv: 1802.05365v2 \\n[cs.CL].\\n22. Kiros, R., et al. (2015, 22 June). Skip-Thought Vectors. arXiv: 1506.06727v1 [cs.CL].\\n23. Kiros, R, et al (2017). GitHub repository: https://github.com/ryankiros/skip-thoughts\\n24. Iyer, M., Manjunatha, V ., Boyd-Graber, J., Daume, H. (2015, July 26-31). Deep Unordered \\nComposition Rivals Syntactic Methods for Text Classification. Proceedings of the 53rd Annual \\nMeeting of the Association for Computational Linguistics and the 7th International Joint \\nConference on Natural Language Processing (ACL 2015).\\n25. Vaswani, A., et al. (2017, 6 Dec). Attention Is All You Need. arXiv: 1706.03762v5 [cs.CL].\\n26. Le, Q., Mikolov, T. (2014) Distributed Representation of Sentences and Documents . arXiv: 1405.4053v2 \\n[cs.CL].\\n27. Howard, J., Ruder, S. (2018, 23 May). Universal Language Model Fine-Tuning for Text Classification. \\narXiv: 1801.06146v5 [cs.CL].\\n28. Devlin, J., Chang, M., Lee, K., Toutanova, K. (2018, 11 Oct). BERT: Pretraining of Deep Bidirectional \\nTransformers for Language Understanding. arXiv: 1810.04805v1 [cs.CL]: https://arxiv.org/\\npdf/1810.04805.pdf\\n29. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. (2018). Improving Language Understanding \\nwith Unsupervised Learning: https://openai.com/blog/language-unsupervised/\\n30. Radford, A., et al. (2019). Language Models are unsupervised Multitask Learners. OpenAI Blog \\n2019: http://www.persagen.com/files/misc/radford2019language.pdf\\n31. Google Collaboratory: https://colab.research.google.com\\n32. Google Cloud Platform. https://cloud.google.com/\\n33. Google Research, BERT (2019). GitHub repository: https://github.com/google-research/bert\\n34. Nemeth (2019). Simple BERT using Tensorflow 2.0. Towards Data Science blog: https://\\ntowardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\\n35. TF-IDF. Wikipedia. Retrieved May 2019: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\\n36. Latent Semantic Analysis. Wikipedia. Retrieved May 2019: https://en.wikipedia.org/wiki/\\nLatent_semantic_analysis\\n37. Topic Model. Wikipedia. Retrieved May 2019: https://en.wikipedia.org/wiki/Topic_model\\n38. Warstadt, A., Singh, A., and Bowman, S. (2018). Neural Network Acceptability Judgements. arXiv \\n1805:12471 [cs.CL]: https://nyu-mll.github.io/CoLA/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d11c9f32-a41c-4649-8491-3daf970438e7', embedding=None, metadata={'page_label': '138', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word Embeddings 138\\n39. Microsoft Research Paraphrase Corpus. (2018): https://www.microsoft.com/en-us/download/\\ndetails.aspx?id=52398\\n40. Nozawa, K. (2019). Something2Vec papers: https://gist.github.com/nzw0301/333afc00bd\\n508501268fa7bf40cafe4e\\n41. Perrone, V ., et al. (2016). Poisson Random Fields for Dynamic Feature Models: https://archive.\\nics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015\\n42. Perozzi, B., Al-Rfou, R., and Skiena, S. (2014). DeepWalk: Online Learning of Social Representations. \\narXiv 1403.6652v2 [cs.SI].\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='385598b1-0e3d-4f42-97be-9305e7a7c19b', embedding=None, metadata={'page_label': '139', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5\\nRecurrent Neural Networks\\nIn Chapter 3, we learned about Convolutional Neural Networks (CNNs) and saw how they exploit the \\nspatial geometry of their inputs. For example, CNNs for images apply convolutions to initially small \\npatches of the image, and progress to larger and larger areas of the image using pooling operations. \\nConvolutions and pooling operations for images are in two dimensions: the width and the height. For \\naudio and text streams, one-dimensional convolution and pooling operations are applied along the \\ntime dimension, and for video streams, these operations are applied in three dimensions: along the \\nheight, width, and time dimensions.\\nIn this chapter, we will focus on Recurrent Neural Networks  (RNNs ), a class of neural networks \\nthat is popularly used on text inputs. RNNs are very flexible and have been used to solve problems \\nsuch as speech recognition, language modeling, machine translation, sentiment analysis, and image \\ncaptioning, to name a few. RNNs exploit the sequential nature of their input. Sequential inputs could \\nbe text, speech, time series, and anything else where the occurrence of an element in a sequence is \\ndependent on the elements that came before it. In this chapter, we will see examples of various RNNs \\nand learn how to implement them with TensorFlow.\\nWe will first look at the internals of a basic RNN cell and how it deals with these sequential dependencies \\nin the input. We will also learn about some limitations of the basic RNN cell (implemented as SimpleRNN \\nin Keras) and see how two popular variants of the SimpleRNN cell – the Long Short-Term Memory \\n(LSTM ) and the Gated Recurrent Unit (GRU ) – overcome this limitation.\\nWe will then zoom out one level and consider the RNN layer itself, which is just the RNN cell applied \\nto every time step. An RNN can be thought of as a graph of RNN cells, where each cell performs the \\nsame operation on successive elements of the sequence. We will describe some simple modifications \\nto improve performance, such as making the RNN bidirectional and/or stateful.\\nFinally, we look at some standard RNN topologies and the kind of applications they can be used to solve. \\nRNNs can be adapted to different types of applications by rearranging the cells in the graph. We will see \\nsome examples of these configurations and how they are used to solve specific problems. We will also \\nconsider the sequence-to-sequence (or seq2seq) architecture, which has been used with great success \\nin machine translation and various other fields. We will then look at what an attention mechanism is, \\nand how it can be used to improve the performance of sequence-to-sequence architectures.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c99cc041-6263-4901-9b90-3f2adec737af', embedding=None, metadata={'page_label': '140', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 140\\nIn this chapter, we will cover the following topics:\\n• The basic RNN cell\\n• RNN cell variants\\n• RNN variants\\n• RNN topologies\\n• Encoder-decoder architectures – seq2seq\\n• Attention mechanism\\nIt is often said that a journey of a thousand miles starts with a single step, so in that spirit, let’s begin \\nour study of RNNs by first considering the RNN cell.\\nThe basic RNN cell\\nTraditional multilayer perceptron neural networks make the assumption that all inputs are independent \\nof each other. This assumption is not true for many types of sequence data. For example, words in a \\nsentence, musical notes in a composition, stock prices over time, or even molecules in a compound \\nare examples of sequences where an element will display a dependence on previous elements.\\nRNN cells incorporate this dependence by having a hidden state, or memory, that holds the essence of \\nwhat has been seen so far. The value of the hidden state at any point in time is a function of the value \\nof the hidden state at the previous time step, and the value of the input at the current time step, that is:\\nℎ𝑡𝑡= 𝜙𝜙𝜙ℎ 𝑡𝑡𝑡𝑡,𝑋𝑋𝑡𝑡) \\nHere, ht and h t-1 are the values of the hidden states at the time t and t-1 respectively, and x t is the value \\nof the input at time t. Notice that the equation is recursive, that is, h t-1 can be represented in terms of \\nht-2 and x t-1, and so on, until the beginning of the sequence. This is how RNNs encode and incorporate \\ninformation from arbitrarily long sequences.\\nWe can also represent the RNN cell graphically, as shown in Figure 5.1(a). At time t, the cell has an \\ninput x(t) and output y(t). Part of the output y(t) (represented by the hidden state h t) is fed back into \\nthe cell for use at a later time step t+1.\\nJust as in a traditional neural network, where the learned parameters are stored as weight matrices, \\nthe RNN’s parameters are defined by the three weight matrices U, V, and W, corresponding to the \\nweights of the input, output, and hidden states respectively:All the code files for this chapter can be found at https://packt.link/dltfchp5 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d55583bb-1e6b-460c-b3d1-b615c834312c', embedding=None, metadata={'page_label': '141', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 141\\nFigure 5.1: (a) Schematic of an RNN cell; (b) the RNN cell unrolled\\nFigure 5.1(b) shows the same RNN in an “unrolled view.” Unrolling just means that we draw the network \\nout for the complete sequence. The network shown here has three time steps, suitable for processing \\nthree element sequences. Note that the weight matrices U, V, and W, that we spoke about earlier, are \\nshared between each of the time steps. This is because we are applying the same operation to different \\ninputs at each time step. Being able to share these weights across all the time steps greatly reduces \\nthe number of parameters that the RNN needs to learn.\\nWe can also describe the RNN as a computation graph in terms of equations. The internal state of the \\nRNN at a time t is given by the value of the hidden vector h(t), which is the sum of the weight matrix \\nW and the hidden state h t-1 at time t-1, and the product of the weight matrix U and the input x t at time t, \\npassed through a tanh  activation function. The choice of tanh  over other activation functions such as \\nsigmoid has to do with it being more efficient for learning in practice and helps combat the vanishing \\ngradient problem, which we will learn about later in the chapter.\\nFor notational convenience, in all our equations describing different types of RNN architec -\\ntures in this chapter, we have omitted explicit reference to the bias terms by incorporating \\nthem within the matrix. Consider the following equation of a line in an n-dimensional \\nspace. Here, w 1 through w n refer to the coefficients of the line in each of the n dimensions, \\nand the bias b refers to the y-intercept along each of these dimensions:\\n𝑦𝑦𝑦𝑦𝑦1𝑥𝑥1+𝑦𝑦2𝑥𝑥2+⋯+𝑦𝑦𝑛𝑛𝑥𝑥𝑛𝑛+𝑏𝑏 \\nWe can rewrite the equation in matrix notation as follows:\\n𝑦𝑦𝑦𝑊𝑊𝑊𝑊+𝑏𝑏 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c234fc8-a2f3-4866-b66d-b31377c2f996', embedding=None, metadata={'page_label': '142', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 142\\nThe output vector y t at time t is the product of the weight matrix V and the hidden state h t, passed \\nthrough a  softmax activation, such that the resulting vector is a set of output probabilities:\\nℎ𝑡𝑡= tanh(𝑊𝑊ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑈𝑈𝑡𝑡) \\n𝑦𝑦𝑡𝑡= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 𝑡𝑡) \\nKeras provides the SimpleRNN recurrent layer that incorporates all the logic we have seen so far, as \\nwell as the more advanced variants such as LSTM and GRU, which we will learn about later in this \\nchapter. Strictly speaking, it is not necessary to understand how they work to start building with them.\\nHowever, an understanding of the structure and equations is helpful when you need to build your own \\nspecialized RNN cell to overcome a specific problem.\\nNow that we understand the flow of data forward through the RNN cell, that is, how it combines its \\ninput and hidden states to produce the output and the next hidden state, let us now examine the flow \\nof gradients in the reverse direction. This is a process called Backpropagation Through Time (BPTT ).\\nBackpropagation through time (BPTT)\\nJust like traditional neural networks, training RNNs also involves the backpropagation of gradients. \\nThe difference, in this case, is that since the weights are shared by all time steps, the gradient at each \\noutput depends not only on the current time step but also on the previous ones. This process is called \\nbackpropagation through time [11]. Because the weights U, V, and W, are shared across the different \\ntime steps in the case of RNNs, we need to sum up the gradients across the various time steps in the \\ncase of BPTT. This is the key difference between traditional backpropagation and BPTT.\\nConsider the RNN with five time steps shown in Figure 5.2. During the forward pass, the network \\nproduces predictions ŷ t at time t  that are compared with the label y t to compute a loss L t. During \\nbackpropagation (shown by the dotted lines), the gradients of the loss with respect to the weights U, \\nV, and W, are computed at each time step and the parameters updated with the sum of the gradients:Here, W is a matrix of shape (m, n) and b is a vector of shape (m, 1), where m is the num-\\nber of rows corresponding to the records in our dataset, and n is the number of columns \\ncorresponding to the features for each record. Equivalently, we can eliminate the vector b \\nby folding it into our matrix W by treating the b vector as a feature column corresponding \\nto the “unit” feature of W. Thus:\\n𝑦𝑦𝑦𝑦𝑦1𝑥𝑥1+𝑦𝑦2𝑥𝑥2+⋯+𝑦𝑦𝑛𝑛𝑥𝑥𝑛𝑛+𝑦𝑦0(1)𝑦𝑊𝑊𝑊𝑊𝑊  \\nHere, W’  is a matrix of shape (m, n+1), where the last column contains the values of b.\\nThe resulting notation ends up being more compact and (we believe) easier to compre -\\nhend and retain as well.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c11b3f4d-ee18-43d0-a057-84d63bab788c', embedding=None, metadata={'page_label': '143', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 143\\nFigure 5.2: Backpropagation through time\\nThe following equation shows the gradient of the loss with respect to W . We focus on this weight \\nbecause it is the cause of the phenomenon known as the vanishing and exploding gradient problem.\\nThis problem manifests as the gradients of the loss approaching either zero or infinity, making the \\nnetwork hard to train. To understand why this happens, consider the equation of the SimpleRNN \\nwe saw earlier; the hidden state h t is dependent on h t-1, which in turn is dependent on h t-2, and so on:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕=∑𝜕𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝜕\\n𝑡𝑡 \\nLet’s now see what happens to this gradient at time step t=3. By the chain rule, the gradient of the loss \\nwith respect to W can be decomposed to a product of three sub-gradients. The gradient of the hidden \\nstate h2 with respect to W can be further decomposed as the sum of the gradient of each hidden state \\nwith respect to the previous one. Finally, each gradient of the hidden state with respect to the previous \\none can be further decomposed as the product of gradients of the current hidden state against the \\nprevious hidden state:\\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕3𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=∑𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕3𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝜕3\\n𝑡𝑡𝑡𝑡=∑𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕3(∏𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝑗𝑗𝑗𝑗3\\n𝑗𝑗𝑡𝑡𝑡𝑗𝑗)𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝜕3\\n𝑡𝑡𝑡𝑡 \\nSimilar calculations are done to compute the gradient of the other losses L 0 through L 4 with respect \\nto W, and sum them up into the gradient update for W. We will not explore the math further in this \\nbook, but this WildML blog post [12] has a very good explanation of BPTT, including a more detailed \\nderivation of the math behind the process.\\nVanishing and exploding gradients\\nThe reason BPTT is particularly sensitive to the problem of vanishing and exploding gradients comes \\nfrom the product part of the expression representing the final formulation of the gradient of the loss \\nwith respect to W. Consider the case where the individual gradients of a hidden state with respect to \\nthe previous one are less than 1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8dd2d71-3670-4b0b-bf15-4afac40dcf48', embedding=None, metadata={'page_label': '144', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 144\\nAs we backpropagate across multiple time steps, the product of gradients becomes smaller and smaller, \\nultimately leading to the problem of vanishing gradients. Similarly, if the gradients are larger than 1, \\nthe products get larger and larger, and ultimately lead to the problem of exploding gradients.\\nOf the two, exploding gradients are more easily detectable. The gradients will become very large \\nand turn into Not a Number (NaN ), and the training process will crash. Exploding gradients can be \\ncontrolled by clipping them at a predefined threshold [13]. TensorFlow 2.0 allows you to clip gradients \\nusing the clipvalue  or clipnorm  parameter during optimizer construction, or by explicitly clipping \\ngradients using tf.clip_by_value .\\nThe effect of vanishing gradients is that gradients from time steps that are far away do not contribute \\nanything to the learning process, so the RNN ends up not learning any long-range dependencies. While \\nthere are a few approaches toward minimizing the problem, such as proper initialization of the W \\nmatrix, more aggressive regularization, using ReLU instead of tanh  activation, and pretraining the \\nlayers using unsupervised methods, the most popular solution is to use LSTM or GRU architectures, \\nboth of which will be explained shortly. These architectures have been designed to deal with vanishing \\ngradients and learn long-term dependencies more effectively.\\nRNN cell variants\\nIn this section, we’ll look at some cell variants of RNNs. We’ll begin by looking at a variant of the \\nSimpleRNN cell: the LSTM RNN.\\nLong short-term memory (LSTM)\\nThe LSTM is a variant of the SimpleRNN cell that is capable of learning long-term dependencies. LSTMs \\nwere first proposed by Hochreiter and SchmidHuber [14] and refined by many other researchers. They \\nwork well on a large variety of problems and are the most widely used RNN variant.\\nWe have seen how the SimpleRNN combines the hidden state from the previous time step and the \\ncurrent input through a tanh  layer to implement recurrence. LSTMs also implement recurrence in a \\nsimilar way, but instead of a single tanh  layer, there are four layers interacting in a very specific way. \\nFigure 5.3 illustrates the transformations that are applied in the hidden state at time step t.\\nThe diagram looks complicated, but let’s look at it component by component. The line across the top \\nof the diagram is the cell state c, representing the internal memory of the unit.\\nThe line across the bottom is the hidden state h, and the i, f, o, and g gates are the mechanisms by \\nwhich the LSTM works around the vanishing gradient problem. During training, the LSTM learns the \\nparameters of these gates:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d87a671-be3c-41b3-afe8-7ece0229cd7a', embedding=None, metadata={'page_label': '145', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 145\\nFigure 5.3: An LSTM cell\\nAn alternative way to think about how these gates work inside an LSTM cell is to consider the equations \\nof the cell. These equations describe how the value of the hidden state h t at time t is calculated from \\nthe value of hidden state h t-1 at the previous time step. In general, the equation-based description \\ntends to be clearer and more concise and is usually the way a new cell design is presented in academic \\npapers. Diagrams, when provided, may or may not be comparable to the ones you saw earlier. For \\nthese reasons, it usually makes sense to learn to read the equations and visualize the cell design. To \\nthat end, we will describe the other cell variants in this book using equations only.\\nThe set of equations representing an LSTM is shown as follows:\\n𝑖𝑖 𝑖 𝑖𝑖𝑖𝑖𝑖 𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑖𝑖𝑥𝑥𝑡𝑡+𝑉𝑉𝑖𝑖𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑓𝑓𝑖 𝑖𝑖𝑖𝑖𝑖 𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑓𝑓𝑥𝑥𝑡𝑡+𝑉𝑉𝑓𝑓𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑜𝑜𝑖 𝑖𝑖𝑖𝑖𝑖 𝑜𝑜ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑜𝑜𝑥𝑥𝑡𝑡+𝑉𝑉𝑜𝑜𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑔𝑔𝑖𝑔𝑔𝑔𝑔𝑖𝑖𝑔𝑔ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑔𝑔𝑥𝑥𝑡𝑡\\n𝑐𝑐𝑡𝑡𝑖𝑖𝑓𝑓𝑓𝑓𝑐𝑐𝑡𝑡𝑡𝑡)+𝑖𝑔𝑔𝑓𝑖𝑖)\\nℎ𝑡𝑡𝑖 𝑔𝑔𝑔𝑔𝑖𝑐𝑐 𝑡𝑡)𝑓𝑜𝑜 \\nHere, i, f, and o are the input, forget, and output gates. They are computed using the same equations \\nbut with different parameter matrices W i, Ui, Wf, Uf, and W o, Uo. The sigmoid function modulates the \\noutput of these gates between 0 and 1, so the output vectors produced can be multiplied element-wise \\nwith another vector to define how much of the second vector can pass through the first one.\\nThe forget gate defines how much of the previous state ht-1 you want to allow to pass through. The input \\ngate defines how much of the newly computed state for the current input x t you want to let through, \\nand the output gate defines how much of the internal state you want to expose to the next layer. The \\ninternal hidden state g is computed based on the current input x t and the previous hidden state h t-1. \\nNotice that the equation for g is identical to that of the SimpleRNN, except that in this case, we will \\nmodulate the output by the output of input vector i.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0aa2d1c3-f206-4669-9926-85a3ca6644b9', embedding=None, metadata={'page_label': '146', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 146\\nGiven i, f, o, and g , we can now calculate the cell state c t at time t  as the cell state c t-1 at time (t-1) \\nmultiplied by the value of the forget gate g , plus the state g  multiplied by the input gate i . This is basically \\na way to combine the previous memory and the new input – setting the forget gate to 0 ignores the old \\nmemory and setting the input gate to 0 ignores the newly computed state. Finally, the hidden state h t \\nat time t is computed as the memory c t at time t, with the output gate o.\\nOne thing to realize is that the LSTM is a drop-in replacement for a SimpleRNN cell; the only difference \\nis that LSTMs are resistant to the vanishing gradient problem. You can replace an RNN cell in a network \\nwith an LSTM without worrying about any side effects. You should generally see better results along \\nwith longer training times.\\nTensorFlow 2.0 also provides a ConvLSTM2D implementation based on the paper by Shi, et al. [18], \\nwhere the matrix multiplications are replaced by convolution operators.\\nIf you would like to learn more about LSTMs, please take a look at the WildML RNN tutorial [15] and \\nChristopher Olah’s blog post [16]. The first covers LSTMs in somewhat greater detail, and the second \\ntakes you step by step through the computations in a very visual way.\\nNow that we have covered LTSMs, we will cover the other popular RNN cell architecture – GRUs.\\nGated recurrent unit (GRU)\\nThe GRU is a variant of the LSTM and was introduced by Cho, et al [17]. It retains the LSTM’s resistance \\nto the vanishing gradient problem, but its internal structure is simpler, and is, therefore, faster to \\ntrain, since fewer computations are needed to make updates to its hidden state.\\nInstead of the input (i), forgot ( f), and output (o) gates in the LSTM cell, the GRU cell has two gates, an \\nupdate gate z and a reset gate r. The update gate defines how much previous memory to keep around, \\nand the reset gate defines how to combine the new input with the previous memory. There is no \\npersistent cell state distinct from the hidden state as it is in LSTM.\\nThe GRU cell defines the computation of the hidden state ht at time t from the hidden state ht-1 at the \\nprevious time step using the following set of equations:\\n𝑧𝑧 𝑧 𝑧𝑧𝑧𝑧𝑧 𝑧𝑧ℎ𝑡𝑡𝑡𝑡+𝑈𝑈 𝑧𝑧𝑥𝑥𝑡𝑡)\\n𝑟𝑟𝑧 𝑧𝑧𝑧𝑧𝑧 𝑟𝑟ℎ𝑡𝑡𝑡𝑡+𝑈𝑈 𝑟𝑟𝑥𝑥𝑡𝑡)\\n𝑐𝑐𝑧𝑐𝑐𝑐𝑐 𝑧𝑧𝑧 𝑐𝑐𝑧ℎ𝑡𝑡𝑡𝑡∗𝑟𝑟) + 𝑈𝑈 𝑐𝑐𝑥𝑥𝑡𝑡)\\nℎ𝑡𝑡𝑧𝑧𝑧𝑧∗𝑐𝑐)+𝑧𝑧1−𝑧𝑧 )∗ℎ 𝑡𝑡𝑡𝑡) \\nThe outputs of the update gate z and the reset gate r are both computed using a combination of the \\nprevious hidden state ht-1 and the current input xt. The sigmoid function modulates the output of these \\nfunctions between 0 and 1. The cell state c is computed as a function of the output of the reset gate \\nr and input x t. Finally, the hidden state h t at time t is computed as a function of the cell state c and \\nthe previous hidden state h t-1. The parameters Wz, U z, W r, U r, and W c, U c, are learned during training.\\nSimilar to LSTM, TensorFlow 2.0 ( tf.keras ) provides an implementation for the basic GRU layer as \\nwell, which is a drop-in replacement for the RNN cell.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc2fcff6-91b3-4fcd-9c66-906f50f53f85', embedding=None, metadata={'page_label': '147', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 147\\nPeephole LSTM\\nThe peephole LSTM is an LSTM variant that was first proposed by Gers and Schmidhuber [19]. It \\nadds “peepholes” to the input, forget, and output gates, so they can see the previous cell state c t-1. The \\nequations for computing the hidden state h t, at time t, from the hidden state h t-1 at the previous time \\nstep, in a peephole LSTM are shown next.\\nNotice that the only difference from the equations for the LSTM is the additional c t-1 term for computing \\noutputs of the input (i), forget (f), and output (o) gates:\\n𝑖𝑖 𝑖 𝑖𝑖𝑖𝑖𝑖 𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑖𝑖𝑥𝑥𝑡𝑡+𝑉𝑉𝑖𝑖𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑓𝑓𝑖 𝑖𝑖𝑖𝑖𝑖 𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑓𝑓𝑥𝑥𝑡𝑡+𝑉𝑉𝑓𝑓𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑜𝑜𝑖 𝑖𝑖𝑖𝑖𝑖 𝑜𝑜ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑜𝑜𝑥𝑥𝑡𝑡+𝑉𝑉𝑜𝑜𝑐𝑐𝑡𝑡𝑡𝑡)\\n𝑔𝑔𝑖𝑔𝑔𝑔𝑔𝑖𝑖𝑔𝑔ℎ𝑡𝑡𝑡𝑡+𝑈𝑈𝑔𝑔𝑥𝑥𝑡𝑡\\n𝑐𝑐𝑡𝑡𝑖𝑖𝑓𝑓𝑓𝑓𝑐𝑐𝑡𝑡𝑡𝑡)+𝑖𝑔𝑔𝑓𝑖𝑖)\\nℎ𝑡𝑡𝑖 𝑔𝑔𝑔𝑔𝑖𝑐𝑐 𝑡𝑡)𝑓𝑜𝑜 \\nTensorFlow 2.0 provides an experimental implementation of the peephole LSTM cell. To use this in \\nyour own RNN layers, you will need to wrap the cell (or list of cells) in the RNN wrapper, as shown in \\nthe following code snippet:\\nhidden_dim = 256\\npeephole_cell = tf.keras.experimental.PeepholeLSTMCell(hidden_dim)\\nrnn_layer = tf.keras.layers.RNN(peephole_cell)\\nIn the previous section, we saw some RNN cell variants that were developed to target specific \\ninadequacies of the basic RNN cell. In the next section, we will look at variations in the architecture \\nof the RNN network itself, which were built to address specific use cases.\\nRNN variants\\nIn this section, we will look at a couple of variations of the basic RNN architecture that can provide \\nperformance improvements in some specific circumstances. Note that these strategies can be applied \\nto different kinds of RNN cells, as well as for different RNN topologies, which we will learn about later.\\nBidirectional RNNs\\nWe have seen how, at any given time step t, the output of the RNN is dependent on the outputs at all \\nprevious time steps. However, it is entirely possible that the output is also dependent on the future \\noutputs as well. This is especially true for applications such as natural language processing where \\nthe attributes of the word or phrase we are trying to predict may be dependent on the context given \\nby the entire enclosing sentence, not just the words that came before it.\\nThis problem can be solved using a bidirectional LSTM (see Figure 5.4), also called biLSTM, which is \\nessentially two RNNs stacked on top of each other, one reading the input from left to right, and the \\nother reading the input from the right to the left. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a4dc244-4f04-4937-b93f-64734b29a1d2', embedding=None, metadata={'page_label': '148', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 148\\nThe output at each time step will be based on the hidden state of both RNNs. Bidirectional RNNs allow \\nthe network to place equal emphasis on the beginning and end of the sequence, and typically result \\nin performance improvements:\\nFigure 5.4: Bidirectional LSTM\\nTensorFlow 2.0 provides support for bidirectional RNNs through a bidirectional wrapper layer. To make \\nan RNN layer bidirectional, all that is needed is to wrap the layer with this wrapper layer, which is \\nshown as follows. Since the output of each pair of cells in the left and right LSTM in the biLSTM pair are \\nconcatenated (see Figure 5.4), it needs to return output from each cell. Hence, we set return_sequences  \\nto True  (the default is False  meaning that the output is only returned from the last cell in the LSTM):\\nself.lstm = tf.keras.layers.Bidirectional(\\n    tf.keras.layers.LSTM( 10, return_sequences= True, \\n        input_shape=( 5, 10))\\n)\\nThe next major RNN variation we will look at is the Stateful RNN.\\nStateful RNNs\\nRNNs can be stateful, which means that they can maintain state across batches during training. That \\nis, the hidden state computed for a batch of training data will be used as the initial hidden state for the \\nnext batch of training data. However, this needs to be explicitly set, since TensorFlow 2.0 ( tf.keras ) \\nRNNs are stateless by default, and resets the state after each batch. Setting an RNN to be stateful means \\nthat it can build state across its training sequence and even maintain that state when doing predictions.\\nThe benefits of using stateful RNNs are smaller network sizes and/or lower training times. The \\ndisadvantage is that we are now responsible for training the network with a batch size that reflects \\nthe periodicity of the data and resetting the state after each epoch. In addition, data should not be \\nshuffled while training the network since the order in which the data is presented is relevant for \\nstateful networks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df939555-d13a-47bd-b733-7341d91050b5', embedding=None, metadata={'page_label': '149', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 149\\nTo set an RNN layer as stateful, set the named variable stateful to True . In our example of a one-to-\\nmany topology for learning how to generate text, we provide an example of using a stateful RNN. Here, \\nwe train using data consisting of contiguous text slices, so setting the LSTM to stateful means that the \\nhidden state generated from the previous text chunk is reused for the current text chunk.\\nIn the next section on RNN topologies, we will look at different ways to set up the RNN network for \\ndifferent use cases.\\nRNN topologies\\nWe have seen examples of how MLP and CNN architectures can be composed to form more complex \\nnetworks. RNNs offer yet another degree of freedom, in that they allow sequence input and output. \\nThis means that RNN cells can be arranged in different ways to build networks that are adapted to \\nsolve different types of problems. Figure 5.5  shows five different configurations of inputs, hidden \\nlayers, and outputs.\\nOf these, the first one (one-to-one) is not interesting from a sequence processing point of view, as it \\ncan be implemented as a simple dense network with one input and one output.\\nThe one-to-many case has a single input and outputs a sequence. An example of such a network might \\nbe a network that can generate text tags from images [6], containing short text descriptions of different \\naspects of the image. Such a network would be trained with image input and labeled sequences of \\ntext representing the image tags:\\nFigure 5.5: Common RNN topologies', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cff80764-1fbe-4ccf-9207-7476f20d557c', embedding=None, metadata={'page_label': '150', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 150\\nThe many-to-one case is the reverse; it takes a sequence of tensors as input but outputs a single tensor. \\nExamples of such networks would be a sentiment analysis network [7], which takes as input a block \\nof text such as a movie review and outputs a single sentiment value.\\nThe many-to-many use case comes in two flavors. The first one is more popular and is better known \\nas the seq2seq model. In this model, a sequence is read in and produces a context vector representing \\nthe input sequence, which is used to generate the output sequence.\\nThe topology has been used with great success in the field of machine translation, as well as problems \\nthat can be reframed as machine translation problems. Real-life examples of the former can be found \\nin [8, 9], and an example of the latter is described in [10].\\nThe second many-to-many type has an output cell corresponding to each input cell. This kind of \\nnetwork is suited for use cases where there is a 1:1 correspondence between the input and output, \\nsuch as time series. The major difference between this model and the seq2seq model is that the input \\ndoes not have to be completely encoded before the decoding process begins.\\nIn the next three sections, we provide examples of a one-to-many network that learns to generate text, \\na many-to-one network that does sentiment analysis, and a many-to-many network of the second type, \\nwhich predicts Part-of-Speech  (POS) for words in a sentence. Because of the popularity of the seq2seq \\nnetwork, we will cover it in more detail later in this chapter.\\nExample ‒ One-to-many – Learning to generate text\\nRNNs have been used extensively by the Natural Language Processing (NLP ) community for various \\napplications. One such application is to build language models. A language model is a model that \\nallows us to predict the probability of a word in a text given previous words. Language models are \\nimportant for various higher-level tasks such as machine translation, spelling correction, and so on.\\nThe ability of a language model to predict the next word in a sequence makes it a generative model \\nthat allows us to generate text by sampling from the output probabilities of different words in the \\nvocabulary. The training data is a sequence of words, and the label is the word appearing at the next \\ntime step in the sequence.\\nFor our example, we will train a character-based RNN on the text of the children’s stories Alice in \\nWonderland and its sequel Through the Looking Glass by Lewis Carroll. We have chosen to build a \\ncharacter-based model because it has a smaller vocabulary and trains quicker. The idea is the same \\nas training and using a word-based language model, except we will use characters instead of words. \\nOnce trained, the model can be used to generate some text in the same style.\\nThe data for our example will come from the plain texts of two novels on the Project Gutenberg website \\n[36]. Input to the network are sequences of 100 characters, and the corresponding output is another \\nsequence of 100 characters, offset from the input by 1 position.\\nThat is, if the input is the sequence [c 1, c2, …, cn], the output will be [c 2, c3, …, cn+1]. We will train the \\nnetwork for 50 epochs, and at the end of every 10 epochs, we will generate a fixed-size sequence of \\ncharacters starting with a standard prefix. In the following example, we have used the prefix “Alice”, \\nthe name of the protagonist in our novels.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0b1e09d-6485-43ab-88a2-dac3ff3f2531', embedding=None, metadata={'page_label': '151', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 151\\nAs always, we will first import the necessary libraries and set up some constants. Here, DATA_DIR  \\npoints to a data folder under the location where you downloaded the source code for this chapter. \\nCHECKPOINT_DIR  is the location, a folder of checkpoints under the data folder, where we will save the \\nweights of the model at the end of every 10 epochs:\\nimport os\\nimport numpy as np\\nimport re\\nimport shutil\\nimport tensorflow as tf\\nDATA_DIR = \"./data\"\\nCHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\" )\\nNext, we download and prepare the data for our network to consume. The texts of both books are \\npublicly available from the Project Gutenberg website. The tf.keras.utils.get_file()  function \\nwill check to see whether the file has already been downloaded to your local drive, and if not, it will \\ndownload to a datasets  folder under the location of the code. We also preprocess the input a little \\nhere, removing newline and byte order mark characters from the text. This step will create the texts  \\nvariable, a flat list of characters for these two books:\\ndef download_and_read (urls):\\n    texts = []\\n    for i, url in enumerate (urls):\\n        p = tf.keras.utils.get_file( \"ex1-{:d}.txt\" .format(i), url,\\n            cache_dir= \".\")\\n        text = open(p, \"r\").read()\\n        # remove byte order mark\\n        text = text.replace( \"\\\\ufeff\" , \"\")\\n        # remove newlines\\n        text = text.replace( \\'\\\\n\\', \\' \\')\\n        text = re.sub( r\\'\\\\s+\\', \" \", text)\\n        # add it to the list\\n        texts.extend(text)\\n    return texts\\ntexts = download_and_read([\\n    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\" ,\\n    \"https://www.gutenberg.org/files/12/12-0.txt\"\\n])\\nNext, we will create our vocabulary. In our case, our vocabulary contains 90 unique characters, \\ncomposed of uppercase and lowercase alphabets, numbers, and special characters. We also create \\nsome mapping dictionaries to convert each vocabulary character into a unique integer and vice versa. \\nAs noted earlier, the input and output of the network is a sequence of characters. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ba0f940-c8ed-4e6f-819d-fa9dc5bf4a6b', embedding=None, metadata={'page_label': '152', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 152\\nHowever, the actual input and output of the network are sequences of integers, and we will use these \\nmapping dictionaries to handle this conversion:\\n# create the vocabulary\\nvocab = sorted(set(texts))\\nprint(\"vocab size: {:d}\" .format(len(vocab)))\\n# create mapping from vocab chars to ints\\nchar2idx = {c:i for i, c in enumerate (vocab)}\\nidx2char = {i:c for c, i in char2idx.items()}\\nThe next step is to use these mapping dictionaries to convert our character sequence input into \\nan integer sequence and then into a TensorFlow dataset. Each of our sequences is going to be 100 \\ncharacters long, with the output being offset from the input by 1 character position. We first batch the \\ndataset into slices of 101 characters, then apply the split_train_labels()  function to every element \\nof the dataset to create our sequences dataset, which is a dataset of tuples of two elements, with each \\nelement of the tuple being a vector of size 100 and type tf.int64 . We then shuffle these sequences \\nand create batches of 64 tuples for each input to our network. Each element of the dataset is now a \\ntuple consisting of a pair of matrices, each of size (64, 100) and type tf.int64 :\\n# numericize the texts\\ntexts_as_ints = np.array([char2idx[c] for c in texts])\\ndata = tf.data.Dataset.from_tensor_slices(texts_as_ints)\\n# number of characters to show before asking for prediction\\n# sequences: [None, 100]\\nseq_length = 100\\nsequences = data.batch(seq_length + 1, drop_remainder= True)\\ndef split_train_labels (sequence):\\n    input_seq = sequence[ 0:-1]\\n    output_seq = sequence[ 1:]\\n    return input_seq, output_seq\\nsequences = sequences. map(split_train_labels)\\n# set up for training\\n# batches: [None, 64, 100]\\nbatch_size = 64\\nsteps_per_epoch = len(texts) // seq_length // batch_size\\ndataset = sequences.shuffle( 10000).batch(\\n    batch_size, drop_remainder= True)\\nWe are now ready to define our network. As before, we define our network as a subclass of tf.keras.\\nModel , as shown next. The network is fairly simple; it takes as input a sequence of integers of size 100 \\n(num_timesteps ) and passes them through an embedding layer so that each integer in the sequence \\nis converted into a vector of size 256 ( embedding_dim ). So, assuming a batch size of 64, for our input \\nsequence of size (64, 100), the output of the embedding layer is a matrix of shape (64, 100, 256).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='98183b10-eb67-4e2a-8159-70b1874231de', embedding=None, metadata={'page_label': '153', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 153\\nThe next layer is an RNN layer with 100 time steps. The implementation of RNN chosen is a GRU. This \\nGRU layer will take, at each of its time steps, a vector of size (256,) and output a vector of shape (1024,) \\n(rnn_output_dim ). Note also that the RNN is stateful, which means that the hidden state output from \\nthe previous training epoch will be used as input to the current epoch. The return_sequences=True  \\nflag also indicates that the RNN will output at each of the time steps rather than an aggregate output \\nat the last time steps.\\nFinally, each of the time steps will emit a vector of shape (1024,) into a dense layer that outputs a vector \\nof shape (90,) ( vocab_size ). The output from this layer will be a tensor of shape (64, 100, 90). Each \\nposition in the output vector corresponds to a character in our vocabulary, and the values correspond \\nto the probability of that character occurring at that output position:\\nclass CharGenModel (tf.keras.Model):\\n    def __init__ (self, vocab_size, num_timesteps,\\n            embedding_dim, **kwargs):\\n        super(CharGenModel, self).__init__(**kwargs)\\n        self.embedding_layer = tf.keras.layers.Embedding(\\n            vocab_size,\\n            embedding_dim\\n        )\\n        self.rnn_layer = tf.keras.layers.GRU(\\n            num_timesteps,\\n            recurrent_initializer= \"glorot_uniform\" ,\\n            recurrent_activation= \"sigmoid\" ,\\n            stateful= True,\\n            return_sequences= True)\\n        self.dense_layer = tf.keras.layers.Dense(vocab_size)\\n    def call(self, x):\\n        x = self.embedding_layer(x)\\n        x = self.rnn_layer(x)\\n        x = self.dense_layer(x)\\n        return x\\nvocab_size = len(vocab)\\nembedding_dim = 256\\nmodel = CharGenModel(vocab_size, seq_length, embedding_dim)\\nmodel.build(input_shape=(batch_size, seq_length))\\nNext, we define a loss function and compile our model. We will use the sparse categorical cross-entropy \\nas our loss function because that is the standard loss function to use when our inputs and outputs are \\nsequences of integers. For the optimizer, we will choose the Adam optimizer:\\ndef loss(labels, predictions):\\n    return tf.losses.sparse_categorical_crossentropy(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb0f2bcb-c823-4b44-95a7-ee0b2698c3b7', embedding=None, metadata={'page_label': '154', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 154\\n        labels,\\n        predictions,\\n        from_logits= True\\n    )\\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=loss)\\nNormally, the character at each position of the output is found by computing the argmax of the vector \\nat that position, that is, the character corresponding to the maximum probability value. This is known \\nas greedy search. In the case of language models where the output of one time step becomes the \\ninput to the next time step, this can lead to a repetitive output. The two most common approaches to \\novercome this problem are either to sample the output randomly or to use beam search, which samples \\nfrom k the most probable values at each time step. Here, we will use the tf.random.categorical()  \\nfunction to sample the output randomly. The following function takes a string as a prefix and uses it \\nto generate a string whose length is specified by num_chars_to_generate . The temperature parameter \\nis used to control the quality of the predictions. Lower values will create a more predictable output.\\nThe logic follows a predictable pattern. We convert the sequence of characters in our prefix_string  \\ninto a sequence of integers, then expand_dims  to add a batch dimension so the input can be passed \\ninto our model. We then reset the state of the model. This is needed because our model is stateful, and \\nwe don’t want the hidden state of the first time step in our prediction run to be carried over from the \\none computed during training. We then run the input through our model and get back a prediction. \\nThis is the vector of shape (90,) representing the probabilities of each character in the vocabulary \\nappearing at the next time step. We then reshape the prediction by removing the batch dimension and \\ndividing by the temperature, and then randomly sampling from the vector. We then set our prediction \\nas the input of the next time step. We repeat this for the number of characters we need to generate, \\nconverting each prediction back into character form and accumulating them in a list, and returning \\nthe list at the end of the loop:\\ndef generate_text (model, prefix_string, char2idx, idx2char,\\n        num_chars_to_generate= 1000, temperature= 1.0):\\n    input = [char2idx[s] for s in prefix_string]\\n    input = tf.expand_dims( input, 0)\\n    text_generated = []\\n    model.reset_states()\\n    for i in range (num_chars_to_generate):\\n        preds = model( input)\\n        preds = tf.squeeze(preds, 0) / temperature\\n        # predict char returned by model\\n        pred_id = tf.random.categorical(\\n            preds, num_samples= 1)[-1, 0].numpy()\\n        text_generated.append(idx2char[pred_id])\\n        # pass the prediction as the next input to the model\\n        input = tf.expand_dims([pred_id], 0)\\n    return prefix_string + \"\".join(text_generated)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee4cf547-5cf3-4e2f-bc7a-50837e60a303', embedding=None, metadata={'page_label': '155', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 155\\nFinally, we are ready to run our training and evaluation loop. As mentioned earlier, we will train our \\nnetwork for 50 epochs, and at every 10-epoch interval, we will try to generate some text with the model \\ntrained so far. Our prefix at each stage is the string \"Alice \" . Notice that in order to accommodate a \\nsingle string prefix, we save the weights after every 10 epochs and build a separate generative model \\nwith these weights but with an input shape with a batch size of 1. Here is the code to do this:\\nnum_epochs = 50\\nfor i in range (num_epochs // 10):\\n    model.fit(\\n        dataset.repeat(),\\n        epochs= 10,\\n        steps_per_epoch=steps_per_epoch\\n        # callbacks=[checkpoint_callback, tensorboard_callback]\\n    )\\n    checkpoint_file = os.path.join(\\n        CHECKPOINT_DIR, \"model_epoch_{:d}\" .format(i+1))\\n    model.save_weights(checkpoint_file)\\n    # create generative model using the trained model so far\\n    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\\n    gen_model.load_weights(checkpoint_file)\\n    gen_model.build(input_shape=( 1, seq_length))\\n    print(\"after epoch: {:d}\" .format(i+1)*10)\\n    print(generate_text(gen_model, \"Alice \" , char2idx, idx2char))\\n    print(\"---\")\\nThe output after the very first epoch of training contains words that are completely undecipherable:\\nAlice nIPJtce otaishein r. henipt il nn tu t hen mlPde hc efa \\nhdtioDDeteeybeaewI teu\"t e9B ce nd ageiw  eai rdoCr ohrSI ey Pmtte:vh ndte \\ntaudhor0-gu s5\\'ria,tr gn inoo luwomg Omke dee sdoohdn ggtdhiAoyaphotd t- kta e \\nc t- taLurtn   hiisd tl\\'lpei od y\\' tpacoe dnlhr oG mGhod ut hlhoy .i, sseodli., \\nekngnhe idlue\\'aa\\'  ndti-rla nt d\\'eiAier adwe ai\\'otteniAidee hy-ouasq\"plhgs \\ntuutandhptiw  oohe.Rastnint:e,o odwsir\"omGoeuall1*g taetphhitoge ds wr li,raa,  \\nh$jeuorsu  h cidmdg\\'t ku..n,HnbMAsn nsaathaa,\\' ase woe  ehf re ig\"hTr ddloese \\neod,aed toe rh k. nalf bte seyr udG n,ug lei hn icuimty\"onw Qee ivtsae zdrye \\ng eut rthrer n sd,Zhqehd\\' sr caseruhel are fd yse e  kgeiiday odW-ldmkhNw \\nendeM[harlhroa h Wydrygslsh EnilDnt e \"lue \"en wHeslhglidrth\"ylds rln n iiato \\ntaue flitl nnyg ittlno re \\'el yOkao itswnadoli\\'.dnd Akib-ehn hftwinh yd ee \\ntosetf tonne.;egren t wf, ota nfsr, t&he desnre e\" oo fnrvnse aid na tesd is \\nioneetIf ·itrn tttpakihc s nih\\'bheY ilenf yoh etdrwdplloU ooaeedo,,dre snno\\'ofh \\no epst. lahehrw ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ef3fb6c-dda2-4c08-96a5-99d5548a4ee4', embedding=None, metadata={'page_label': '156', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 156\\nHowever, after about 30 epochs of training, we begin to see words that look familiar:\\nAlice Red Queen. He best I had defores it,\\' glily do flose time it makes the \\ntalking of find a hand mansed in she loweven to the rund not bright prough: the \\nand she a chill be the sand using that whever sullusn--the dear of asker as \\n\\'IS now-- Chich the hood.\" \"Oh!\"\\' \\'_I\\'m num about--again was wele after a WAG \\nLoANDE BITTER OF HSE!0 UUL EXMENN 1*.t, this wouldn\\'t teese to Dumark THEVER \\nProject Gutenberg-tmy of himid out flowal woulld: \\'Nis song, Eftrin in pully be \\nbesoniokinote. \"Com, contimemustion--of could you knowfum to hard, she can\\'t \\nthe with talking to alfoeys distrint, for spacemark!\\' \\'You gake to be would \\nprescladleding readieve other togrore what it mughturied ford of it was sen!\" \\nYou squs, _It I hap: But it was minute to the Kind she notion and teem what?\" \\nsaid Alice, make there some that in at the shills distringulf out to the Froge, \\nand very mind to it were it?\\' the King was set telm, what\\'s the old all reads \\ntalking a minuse. \"Where ream put find growned his so,\" _you \\'Fust to t\\nAfter 50 epochs of training, the model still has trouble expressing coherent thought but has learned to \\nspell reasonably well. What is amazing here is that the model is character-based and has no knowledge \\nof words, yet it learns to spell words that look like they might have come from the original text:\\nAlice Vex her,\" he prope of the very managed by this thill deceed. I will ear \\nshe a much daid. \"I sha?\\' Nets: \"Woll, I should shutpelf, and now and then, \\ncried, How them yetains, a tround her about in a shy time, I pashng round the \\nsandle, droug\" shrees went on what he seting that,\" said Alice. \"Was this \\nwill resant again. Alice stook of in a faid.\\' \\'It\\'s ale. So they wentle shall \\nkneeltie-and which herfer--the about the heald in pum little each the UKECE P@\\nTTRUST GITE Ever been my hever pertanced to becristrdphariok, and your pringing \\nthat why the King as I to the King remark, but very only all Project Grizly: \\nthentiused about doment,\\' Alice with go ould, are wayings for handsn\\'t replied \\nas mave about to LISTE!\\' (If the UULE \\'TARY-HAVE BUY DIMADEANGNE\\'G THING NOOT,\\' \\nbe this plam round an any bar here! No, you\\'re alard to be a good aftered of \\nthe sam--I canon\\'t?\" said Alice. \\'It\\'s one eye of the olleations. Which saw do \\nit just opened hardly deat, we hastowe. \\'Of coum, is tried try slowing\\nGenerating the next character or next word in the text isn’t the only thing you can do with this sort \\nof model. Similar models have been built to make stock price predictions [3] or generate classical \\nmusic [4]. Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, \\nalgebraic geometry proofs, and Linux source code in his blog post [5].\\nThe full code for this example is available in alice_text_generator.py  in the source code folder for \\nthis chapter. It can be run from the command line using the following command:\\n$ python alice_text_generator.py\\nOur next example will show an implementation of a many-to-one network for sentiment analysis.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9934a322-07e8-497b-9df4-40422f7c6bfb', embedding=None, metadata={'page_label': '157', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 157\\nExample ‒ Many-to-one – Sentiment analysis\\nIn this example, we will use a many-to-one network that takes a sentence as input and predicts its \\nsentiment as being either positive or negative. Our dataset is the Sentiment-labeled sentences dataset \\non the UCI Machine Learning Repository [20], a set of 3,000 sentences from reviews on Amazon, IMDb, \\nand Yelp, each labeled with 0 if it expresses a negative sentiment, or 1 if it expresses a positive sentiment.\\nAs usual, we will start with our imports:\\nimport numpy as np\\nimport os\\nimport shutil\\nimport tensorflow as tf\\nfrom sklearn.metrics import accuracy_score, confusion_matrix\\nThe dataset is provided as a zip file, which expands into a folder containing three files of labeled \\nsentences, one for each provider, with one sentence and label per line and with the sentence and \\nlabel separated by the tab character. We first download the zip file, then parse the files into a list of \\n(sentence, label)  pairs:\\ndef download_and_read (url):\\n    local_file = url.split( \\'/\\')[-1]\\n    local_file = local_file.replace( \"%20\", \" \")\\n    p = tf.keras.utils.get_file(local_file, url,\\n        extract= True, cache_dir= \".\")\\n    local_folder = os.path.join( \"datasets\" , local_file.split( \\'.\\')[0])\\n    labeled_sentences = []\\n    for labeled_filename in os.listdir(local_folder):\\n        if labeled_filename.endswith( \"_labelled.txt\" ):\\n            with open(os.path.join(\\n                    local_folder, labeled_filename), \"r\") as f:\\n                for line in f:\\n                    sentence, label = line.strip().split( \\'\\\\t\\')\\n                    labeled_sentences.append((sentence, label))\\n    return labeled_sentences\\nlabeled_sentences = download_and_read(      \\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"  + \\n    \"00331/sentiment%20labelled%20sentences.zip\" )\\nsentences = [s for (s, l) in labeled_sentences]\\nlabels = [ int(l) for (s, l) in labeled_sentences]\\nOur objective is to train the model so that, given a sentence as input, it learns to predict the corresponding \\nsentiment provided in the label. Each sentence is a sequence of words. However, to input it into the \\nmodel, we have to convert it into a sequence of integers. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='681dbd0d-fa4e-4149-a558-1e23120347a6', embedding=None, metadata={'page_label': '158', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 158\\nEach integer in the sequence will point to a word. The mapping of integers to words for our corpus is \\ncalled a vocabulary. Thus, we need to tokenize the sentences and produce a vocabulary. This is done \\nusing the following code:\\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\\ntokenizer.fit_on_texts(sentences)\\nvocab_size = len(tokenizer.word_counts)\\nprint(\"vocabulary size: {:d}\" .format(vocab_size))\\nword2idx = tokenizer.word_index\\nidx2word = {v:k for (k, v) in word2idx.items()}\\nOur vocabulary consists of 5,271 unique words. It is possible to make the size smaller by dropping \\nwords that occur fewer than some threshold number of times, which can be found by inspecting the \\ntokenizer.word_counts  dictionary. In such cases, we need to add 1 to the vocabulary size for the \\nUNK (unknown) entry, which will be used to replace every word that is not found in the vocabulary.\\nWe also construct lookup dictionaries to convert from the word-to-word index and back. The first \\ndictionary is useful during training to construct integer sequences to feed the network. The second \\ndictionary is used to convert from the word index back into words in our prediction code later.\\nEach sentence can have a different number of words. Our model will require us to provide sequences \\nof integers of identical length for each sentence. To support this requirement, it is common to choose a \\nmaximum sequence length that is large enough to accommodate most of the sentences in the training \\nset. Any sentences that are shorter will be padded with zeros, and any sentences that are longer will \\nbe truncated. An easy way to choose a good value for the maximum sequence length is to look at the \\nsentence length (as in the number of words) at different percentile positions:\\nseq_lengths = np.array([ len(s.split()) for s in sentences])\\nprint([(p, np.percentile(seq_lengths, p)) for p\\n    in [75, 80, 90, 95, 99, 100 ]])\\nThis gives us the following output:\\n[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\\nAs can be seen, the maximum sentence length is 71 words, but 99% of the sentences are under 36 \\nwords. If we choose a value of 64, for example, we should be able to get away with not having to \\ntruncate most of the sentences.\\nThe preceding blocks of code can be run interactively multiple times to choose good values of vocabulary \\nsize and maximum sequence length respectively. In our example, we have chosen to keep all the words \\n(so vocab_size = 5271 ), and we have set our max_seqlen  to 64.\\nOur next step is to create a dataset that our model can consume. We first use our trained tokenizer to \\nconvert each sentence from a sequence of words ( sentences ) into a sequence of integers ( sentences_\\nas_ints ), where each corresponding integer is the index of the word in the tokenizer.word_index . \\nIt is then truncated and padded with zeros. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='153c400c-5d57-4190-9134-c4e239df4973', embedding=None, metadata={'page_label': '159', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 159\\nThe labels are also converted into a NumPy array labels_as_ints , and finally, we combine the tensors \\nsentences_as_ints  and labels_as_ints  to form a TensorFlow dataset:\\nmax_seqlen = 64\\n# create dataset\\nsentences_as_ints = tokenizer.texts_to_sequences(sentences)\\nsentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\\n    sentences_as_ints, maxlen=max_seqlen)\\nlabels_as_ints = np.array(labels)\\ndataset = tf.data.Dataset.from_tensor_slices(\\n    (sentences_as_ints, labels_as_ints))\\nWe want to set aside 1/3 of the dataset for evaluation. Of the remaining data, we will use 10% as an \\ninline validation dataset, which the model will use to gauge its own progress during training, and the \\nremaining as the training dataset. Finally, we create batches of 64 sentences for each dataset:\\ndataset = dataset.shuffle( 10000)\\ntest_size = len(sentences) // 3\\nval_size = ( len(sentences) - test_size) // 10\\ntest_dataset = dataset.take(test_size)\\nval_dataset = dataset.skip(test_size).take(val_size)\\ntrain_dataset = dataset.skip(test_size + val_size)\\nbatch_size = 64\\ntrain_dataset = train_dataset.batch(batch_size)\\nval_dataset = val_dataset.batch(batch_size)\\ntest_dataset = test_dataset.batch(batch_size)\\nNext, we define our model. As you can see, the model is fairly straightforward, each input sentence \\nis a sequence of integers of size max_seqlen  (64). This is input into an embedding layer that converts \\neach word into a vector given by the size of the vocabulary + 1. The additional word is to account for \\nthe padding integer 0 that was introduced during the pad_sequences()  call above. The vector at each \\nof the 64 time steps is then fed into a bidirectional LSTM layer, which converts each word into a vector \\nof size (64,). The output of the LSTM at each time step is fed into a dense layer, which produces a vector \\nof size (64,) with ReLU activation. The output of this dense layer is then fed into another dense layer, \\nwhich outputs a vector of (1,) at each time step, modulated through a sigmoid activation.\\nThe model is compiled with the binary cross-entropy loss function and the Adam optimizer, and then \\ntrained over 10 epochs:\\nclass SentimentAnalysisModel (tf.keras.Model):\\n    def __init__ (self, vocab_size, max_seqlen, **kwargs):\\n        super(SentimentAnalysisModel, self).__init__(**kwargs)\\n        self.embedding = tf.keras.layers.Embedding(\\n            vocab_size, max_seqlen)\\n        self.bilstm = tf.keras.layers.Bidirectional(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db86e120-02d3-408e-ba69-e1e4e4a4df1a', embedding=None, metadata={'page_label': '160', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 160\\n            tf.keras.layers.LSTM(max_seqlen)\\n        )\\n        self.dense = tf.keras.layers.Dense( 64, activation= \"relu\")\\n        self.out = tf.keras.layers.Dense( 1, activation= \"sigmoid\" )\\n    def call(self, x):\\n        x = self.embedding(x)\\n        x = self.bilstm(x)\\n        x = self.dense(x)\\n        x = self.out(x)\\n        return x\\nmodel = SentimentAnalysisModel(vocab_size+ 1, max_seqlen)\\nmodel.build(input_shape=(batch_size, max_seqlen))\\nmodel.summary()\\n# compile\\nmodel.compile(\\n    loss= \"binary_crossentropy\" ,\\n    optimizer= \"adam\",\\n    metrics=[ \"accuracy\" ]\\n)\\n# train\\ndata_dir = \"./data\"\\nlogs_dir = os.path.join( \"./logs\" )\\nbest_model_file = os.path.join(data_dir, \"best_model.h5\" )\\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\\n    save_weights_only= True,\\n    save_best_only= True)\\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\\nnum_epochs = 10\\nhistory = model.fit(train_dataset, epochs=num_epochs,\\n    validation_data=val_dataset,\\n    callbacks=[checkpoint, tensorboard])\\nAs you can see from the output, the training set accuracy goes to 99.8% and the validation set accuracy \\ngoes to about 78.5%. Having a higher accuracy over the training set is expected since the model was \\ntrained on this dataset. You can also look at the following loss plot to see exactly where the model \\nstarts overfitting on the training set. Notice that the training loss keeps going down, but the validation \\nloss comes down initially and then starts going up. It is at the point where it starts going up that we \\nknow that the model overfits on the training set:\\nEpoch 1/10\\n29/29 [==============================] - 7s 239ms/step - loss: 0.6918 - \\naccuracy: 0.5148 - val_loss: 0.6940 - val_accuracy: 0.4750', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bc2ded4-1697-4b13-bec7-da1960dfd7c5', embedding=None, metadata={'page_label': '161', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 161\\nEpoch 2/10\\n29/29 [==============================] - 3s 98ms/step - loss: 0.6382 - \\naccuracy: 0.5928 - val_loss: 0.6311 - val_accuracy: 0.6000\\nEpoch 3/10\\n29/29 [==============================] - 3s 100ms/step - loss: 0.3661 - \\naccuracy: 0.8250 - val_loss: 0.4894 - val_accuracy: 0.7600\\nEpoch 4/10\\n29/29 [==============================] - 3s 99ms/step - loss: 0.1567 - \\naccuracy: 0.9564 - val_loss: 0.5469 - val_accuracy: 0.7750\\nEpoch 5/10\\n29/29 [==============================] - 3s 99ms/step - loss: 0.0768 - \\naccuracy: 0.9875 - val_loss: 0.6197 - val_accuracy: 0.7450\\nEpoch 6/10\\n29/29 [==============================] - 3s 100ms/step - loss: 0.0387 - \\naccuracy: 0.9937 - val_loss: 0.6529 - val_accuracy: 0.7500\\nEpoch 7/10\\n29/29 [==============================] - 3s 99ms/step - loss: 0.0215 - \\naccuracy: 0.9989 - val_loss: 0.7597 - val_accuracy: 0.7550\\nEpoch 8/10\\n29/29 [==============================] - 3s 100ms/step - loss: 0.0196 - \\naccuracy: 0.9987 - val_loss: 0.6745 - val_accuracy: 0.7450\\nEpoch 9/10\\n29/29 [==============================] - 3s 99ms/step - loss: 0.0136 - \\naccuracy: 0.9962 - val_loss: 0.7770 - val_accuracy: 0.7500\\nEpoch 10/10\\n29/29 [==============================] - 3s 99ms/step - loss: 0.0062 - \\naccuracy: 0.9988 - val_loss: 0.8344 - val_accuracy: 0.7450\\nFigure 5.6 shows TensorBoard plots of accuracy and loss for the training and validation datasets:\\nFigure 5.6: Accuracy and loss plots from TensorBoard for sentiment analysis network training', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e2b16dd-5d60-4fad-9531-202a936fd9ac', embedding=None, metadata={'page_label': '162', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 162\\nOur checkpoint callback has saved the best model based on the lowest value of validation loss, and \\nwe can now reload this for evaluation against our held out test set:\\nbest_model = SentimentAnalysisModel(vocab_size+ 1, max_seqlen)\\nbest_model.build(input_shape=(batch_size, max_seqlen))\\nbest_model.load_weights(best_model_file)\\nbest_model. compile(\\n    loss= \"binary_crossentropy\" ,\\n    optimizer= \"adam\",\\n    metrics=[ \"accuracy\" ]\\n)\\nThe easiest high-level way to evaluate a model against a dataset is to use the model.evaluate()  call:\\ntest_loss, test_acc = best_model.evaluate(test_dataset)\\nprint(\"test loss: {:.3f}, test accuracy: {:.3f}\" .format(\\n    test_loss, test_acc))\\nThis gives us the following output:\\ntest loss: 0.487, test accuracy: 0.782\\nWe can also use model.predict()  to retrieve our predictions and compare them individually to the \\nlabels and use external tools (from scikit-learn, for example) to compute our results:\\nlabels, predictions = [], []\\nidx2word[ 0] = \"PAD\"\\nis_first_batch = True\\nfor test_batch in test_dataset:\\n   inputs_b, labels_b = test_batch\\n   pred_batch = best_model.predict(inputs_b)\\n   predictions.extend([( 1 if p > 0.5 else 0) for p in pred_batch])\\n   labels.extend([l for l in labels_b])\\n   if is_first_batch:\\n       # print first batch of label, prediction, and sentence\\n       for rid in range (inputs_b.shape[ 0]):\\n           words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\\n           words = [w for w in words if w != \"PAD\"]\\n           sentence = \" \".join(words)\\n           print(\"{:d}\\\\t{:d}\\\\t{:s}\" .format(\\n               labels[rid], predictions[rid], sentence))\\n       is_first_batch = False\\nprint(\"accuracy score: {:.3f}\" .format(accuracy_score(labels, predictions)))\\nprint(\"confusion matrix\" )\\nprint(confusion_matrix(labels, predictions))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02e4b7bc-08c7-4ce6-b28b-8960624b1597', embedding=None, metadata={'page_label': '163', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 163\\nFor the first batch of 64 sentences in our test dataset, we reconstruct the sentence and display the \\nlabel (first column) as well as the prediction from the model (second column). Here, we show the top \\n10 sentences. As you can see, the model gets it right for most sentences on this list:\\nLBL  PRED  SENT\\n1     1    one of my favorite purchases ever\\n1     1    works great\\n1     1    our waiter was very attentive friendly and informative\\n0     0    defective crap\\n0     1    and it was way to expensive\\n0     0    don't waste your money\\n0     0    friend's pasta also bad he barely touched it\\n1     1    it's a sad movie but very good\\n0     0    we recently witnessed her poor quality of management towards other \\nguests as well\\n0     1    there is so much good food in vegas that i feel cheated for wasting \\nan eating opportunity by going to rice and company\\nWe also report the results across all sentences in the test dataset. As you can see, the test accuracy is \\nthe same as that reported by the evaluate  call. We have also generated the confusion matrix, which \\nshows that out of 1,000 test examples, our sentiment analysis network predicted correctly 782 times \\nand incorrectly 218 times:\\naccuracy score: 0.782\\nconfusion matrix\\n[[391  97]\\n [121 391]]\\nThe full code for this example is available in lstm_sentiment_analysis.py  in the source code folder \\nfor this chapter. It can be run from the command line using the following command:\\n$ python lstm_sentiment_analysis.py\\nOur next example will describe a many-to-many network trained for POS tagging English text.\\nExample ‒ Many-to-many – POS tagging\\nIn this example, we will use a GRU layer to build a network that does Part of Speech (POS) tagging. \\nA POS is a grammatical category of words that are used in the same way across multiple sentences. \\nExamples of POS are nouns, verbs, adjectives, and so on. For example, nouns are typically used to \\nidentify things, verbs are typically used to identify what they do, and adjectives are used to describe \\nattributes of these things. POS tagging used to be done manually in the past, but this is now mostly a \\nsolved problem, initially through statistical models, and more recently by using deep learning models \\nin an end-to-end manner, as described in Collobert, et al. [21].\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='988b422c-0325-45d9-81b5-606612fdfece', embedding=None, metadata={'page_label': '164', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 164\\nFor our training data, we will need sentences tagged with POS tags. The Penn Treebank [22] is one \\nsuch dataset; it is a human-annotated corpus of about 4.5 million words of American English. However, \\nit is a non-free resource. A 10% sample of the Penn Treebank is freely available as part of NLTK [23], \\nwhich we will use to train our network.\\nOur model will take a sequence of words in a sentence as input, then will output the corresponding \\nPOS tag for each word. Thus, for an input sequence consisting of the words [The, cat, sat. on, the, mat, \\n.], the output sequence should be the POS symbols [DT, NN, VB, IN, DT, NN, .] .\\nIn order to get the data, you need to install the NLTK library if it is not already installed (NLTK is \\nincluded in the Anaconda distribution), as well as the 10% treebank dataset (not installed by default). \\nTo install NLTK, follow the steps on the NLTK install page [23]. To install the treebank dataset, perform \\nthe following in the Python REPL:\\n>>> import nltk\\n>>> nltk.download(\"treebank\")\\nOnce this is done, we are ready to build our network. As usual, we will start by importing the necessary \\npackages:\\nimport numpy as np\\nimport os\\nimport shutil\\nimport tensorflow as tf\\nWe will lazily import the NLTK treebank dataset into a pair of parallel flat files, one containing the \\nsentences and the other containing a corresponding POS sequence:\\ndef download_and_read (dataset_dir, num_pairs= None):\\n    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\" )\\n    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\" )\\n    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\\n        import nltk   \\n        if not os.path.exists(dataset_dir):\\n            os.makedirs(dataset_dir)\\n        fsents = open(sent_filename, \"w\")\\n        fposs = open(poss_filename, \"w\")\\n        sentences = nltk.corpus.treebank.tagged_sents()\\n        for sent in sentences:\\n            fsents.write( \" \".join([w for w, p in sent]) + \"\\\\n\")\\n            fposs.write( \" \".join([p for w, p in sent]) + \"\\\\n\")\\n        fsents.close()\\n        fposs.close()\\n    sents, poss = [], []\\n    with open(sent_filename, \"r\") as fsent:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe5b2845-89dd-4dfb-a43c-e1c73085cf63', embedding=None, metadata={'page_label': '165', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 165\\n        for idx, line in enumerate (fsent):\\n            sents.append(line.strip())\\n            if num_pairs is not None and idx >= num_pairs:\\n                break\\n    with open(poss_filename, \"r\") as fposs:\\n        for idx, line in enumerate (fposs):\\n            poss.append(line.strip())\\n            if num_pairs is not None and idx >= num_pairs:\\n                break\\n    return sents, poss\\nsents, poss = download_and_read( \"./datasets\" )\\nassert(len(sents) == len(poss))\\nprint(\"# of records: {:d}\" .format(len(sents)))\\nThere are 3,194 sentences in our dataset. The preceding code writes the sentences and corresponding \\ntags into parallel files, that is, line 1 in treebank-sents.txt  contains the first sentence, and line 1 \\nin treebank-poss.txt  contains the corresponding POS tags for each word in the sentence. Table 5.1 \\nshows two sentences from this dataset and their corresponding POS tags:\\nSentences POS Tags\\nPierre Vinken, 61 years old, will join the board as a \\nnonexecutive director Nov. 29.NNP NNP , CD NNS JJ , MD VB DT NN IN DT \\nJJ NN NNP CD.\\nMr. Vinken is chairman of Elsevier N. V ., the Dutch \\npublishing group.NNP NNP VBZ NN IN NNP NNP , DT NNP \\nVBG NN.\\nTable 5.1: Sentences and their corresponding POS tags\\nWe will then use the TensorFlow ( tf.keras ) tokenizer to tokenize the sentences and create a list of \\nsentence tokens. We reuse the same infrastructure to tokenize the POS, although we could have simply \\nsplit on spaces. Each input record to the network is currently a sequence of text tokens, but they need \\nto be a sequence of integers. During the tokenizing process, the Tokenizer also maintains the tokens \\nin the vocabulary, from which we can build mappings from the token to the integer and back.\\nWe have two vocabularies to consider, the vocabulary of word tokens in the sentence collection and \\nthe vocabulary of POS tags in the part-of-speech collection. The following code shows how to tokenize \\nboth collections and generate the necessary mapping dictionaries:\\ndef tokenize_and_build_vocab (texts, vocab_size= None, lower= True):\\n    if vocab_size is None:\\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\\n    else:\\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(\\n            num_words=vocab_size+ 1, oov_token= \"UNK\", lower=lower)\\n    tokenizer.fit_on_texts(texts)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef9a2099-2f0a-4c3a-9379-aeaf705d064e', embedding=None, metadata={'page_label': '166', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 166\\n    if vocab_size is not None:\\n        # additional workaround, see issue 8092\\n        # https://github.com/keras-team/keras/issues/8092\\n        tokenizer.word_index = {e:i for e, i in\\n            tokenizer.word_index.items() if \\n            i <= vocab_size+ 1 }\\n    word2idx = tokenizer.word_index\\n    idx2word = {v:k for k, v in word2idx.items()}\\n    return word2idx, idx2word, tokenizer\\nword2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\\n    sents, vocab_size= 9000)\\nword2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\\n    poss, vocab_size= 38, lower= False)\\nsource_vocab_size = len(word2idx_s)\\ntarget_vocab_size = len(word2idx_t)\\nprint(\"vocab sizes (source): {:d}, (target): {:d}\" .format(\\n    source_vocab_size, target_vocab_size))\\nOur sentences are going to be of different lengths, although the number of tokens in a sentence and \\ntheir corresponding POS tag sequence are the same. The network expects the input to have the same \\nlength, so we have to decide how much to make our sentence length. The following (throwaway) code \\ncomputes various percentiles and prints sentence lengths at these percentiles to the console:\\nsequence_lengths = np.array([ len(s.split()) for s in sents])\\nprint([(p, np.percentile(sequence_lengths, p))\\n    for p in [75, 80, 90, 95, 99, 100 ]])\\n[(75, 33.0 ), (80, 35.0 ), (90, 41.0 ), (95, 47.0 ), (99, 58.0 ), (100, 271.0)]\\nWe see that we could probably get away with setting the sentence length to around 100 and have a \\nfew truncated sentences as a result. Sentences shorter than our selected length will be padded at the \\nend. Because our dataset is small, we prefer to use as much of it as possible, so we end up choosing \\nthe maximum length.\\nThe next step is to create the dataset from our inputs. First, we have to convert our sequence of tokens \\nand POS tags in our input and output sequences into sequences of integers. Second, we have to pad \\nshorter sequences to the maximum length of 271. Notice that we do an additional operation on the \\nPOS tag sequences after padding, rather than keep it as a sequence of integers; we convert it into a \\nsequence of one-hot encodings using the to_categorical()  function. TensorFlow 2.0 does provide \\nloss functions to handle outputs as a sequence of integers, but we want to keep our code as simple as \\npossible, so we opt to do the conversion ourselves. Finally, we use the from_tensor_slices()  function \\nto create our dataset, shuffle it, and split it up into training, validation, and test sets:\\nmax_seqlen = 271\\n# convert sentences to sequence of integers', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='44af45d2-45e0-4320-b9b5-3f9640683c32', embedding=None, metadata={'page_label': '167', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 167\\nsents_as_ints = tokenizer_s.texts_to_sequences(sents)\\nsents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\\n    sents_as_ints, maxlen=max_seqlen, padding= \"post\")\\n# convert POS tags to sequence of (categorical) integers\\nposs_as_ints = tokenizer_t.texts_to_sequences(poss)\\nposs_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\\n    poss_as_ints, maxlen=max_seqlen, padding= \"post\")\\nposs_as_catints = []\\nfor p in poss_as_ints:\\n    poss_as_catints.append(tf.keras.utils.to_categorical(p,\\n        num_classes=target_vocab_size+ 1, dtype= \"int32\"))\\nposs_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\\n    poss_as_catints, maxlen=max_seqlen)\\ndataset = tf.data.Dataset.from_tensor_slices(\\n    (sents_as_ints, poss_as_catints))\\nidx2word_s[ 0], idx2word_t[ 0] = \"PAD\", \"PAD\"\\n# split into training, validation, and test datasets\\ndataset = dataset.shuffle( 10000)\\ntest_size = len(sents) // 3\\nval_size = ( len(sents) - test_size) // 10\\ntest_dataset = dataset.take(test_size)\\nval_dataset = dataset.skip(test_size).take(val_size)\\ntrain_dataset = dataset.skip(test_size + val_size)\\n# create batches\\nbatch_size = 128\\ntrain_dataset = train_dataset.batch(batch_size)\\nval_dataset = val_dataset.batch(batch_size)\\ntest_dataset = test_dataset.batch(batch_size)\\nNext, we will define our model and instantiate it. Our model is a sequential model consisting of an \\nembedding layer, a dropout layer, a bidirectional GRU layer, a dense layer, and a softmax activation layer. \\nThe input is a batch of integer sequences with shape ( batch_size , max_seqlen ). When passed through \\nthe embedding layer, each integer in the sequence is converted into a vector of size ( embedding_dim ), \\nso now the shape of our tensor is ( batch_size , max_seqlen , embedding_dim ). Each of these vectors is \\npassed to corresponding time steps of a bidirectional GRU with an output dimension of 256. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2166f9a1-51ac-4cb9-b477-b51d84095dd3', embedding=None, metadata={'page_label': '168', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 168\\nBecause the GRU is bidirectional, this is equivalent to stacking one GRU on top of the other, so the tensor \\nthat comes out of the bidirectional GRU has the dimension ( batch_size , max_seqlen , 2*rnn_output_\\ndimension ). Each time step tensor of shape ( batch_size , 1, 2*rnn_output_dimension ) is fed into a \\ndense layer, which converts each time step into a vector of the same size as the target vocabulary, that \\nis, (batch_size , number_of_timesteps , output_vocab_size ). Each time step represents a probability \\ndistribution of output tokens, so the final softmax layer is applied to each time step to return a sequence \\nof output POS tokens.\\nFinally, we declare the model with some parameters, then compile it with the Adam optimizer, the \\ncategorical cross-entropy loss function, and accuracy as the metric:\\nclass POSTaggingModel (tf.keras.Model):\\n    def __init__ (self, source_vocab_size, target_vocab_size,\\n            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\\n        super(POSTaggingModel, self).__init__(**kwargs)\\n        self.embed = tf.keras.layers.Embedding(\\n            source_vocab_size, embedding_dim, input_length=max_seqlen)\\n        self.dropout = tf.keras.layers.SpatialDropout1D( 0.2)\\n        self.rnn = tf.keras.layers.Bidirectional(\\n            tf.keras.layers.GRU(rnn_output_dim, return_sequences= True))\\n        self.dense = tf.keras.layers.TimeDistributed(\\n            tf.keras.layers.Dense(target_vocab_size))\\n        self.activation = tf.keras.layers.Activation( \"softmax\" )\\n    def call(self, x):\\n        x = self.embed(x)\\n        x = self.dropout(x)\\n        x = self.rnn(x)\\n        x = self.dense(x)\\n        x = self.activation(x)\\n        return x\\nembedding_dim = 128\\nrnn_output_dim = 256\\nmodel = POSTaggingModel(source_vocab_size, target_vocab_size,\\n    embedding_dim, max_seqlen, rnn_output_dim)\\nmodel.build(input_shape=(batch_size, max_seqlen))\\nmodel.summary()\\nmodel.compile(\\n    loss= \"categorical_crossentropy\" ,\\n    optimizer= \"adam\",\\n    metrics=[ \"accuracy\" , masked_accuracy()])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6359bf43-528b-4c82-a060-a64f5f8b475d', embedding=None, metadata={'page_label': '169', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 169\\nObservant readers might have noticed an additional masked_accuracy()  metric next to the accuracy  \\nmetric in the preceding code snippet. Because of the padding, there are a lot of zeros on both the \\nlabel and the prediction, as a result of which the accuracy numbers are very optimistic. In fact, the \\nvalidation accuracy reported at the end of the very first epoch is 0.9116 . However, the quality of POS \\ntags generated is very poor.\\nPerhaps the best approach is to replace the current loss function with one that ignores any matches \\nwhere both numbers are zero; however, a simpler approach is to build a stricter metric and use that \\nto judge when to stop the training. Accordingly, we build a new accuracy function masked_accuracy()  \\nwhose code is shown as follows:\\ndef masked_accuracy ():\\n    def masked_accuracy_fn (ytrue, ypred):\\n        ytrue = tf.keras.backend.argmax(ytrue, axis=- 1)\\n        ypred = tf.keras.backend.argmax(ypred, axis=- 1)\\n        mask = tf.keras.backend.cast(\\n            tf.keras.backend.not_equal(ypred, 0), tf.int32)\\n        matches = tf.keras.backend.cast(\\n            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\\n        numer = tf.keras.backend. sum(matches)\\n        denom = tf.keras.backend.maximum(tf.keras.backend. sum(mask), 1)\\n        accuracy =  numer / denom\\n        return accuracy\\n    return masked_accuracy_fn\\nWe are now ready to train our model. As usual, we set up the model checkpoint and TensorBoard \\ncallbacks, and then call the fit()  convenience method on the model to train the model with a batch \\nsize of 128 for 50 epochs:\\nnum_epochs = 50\\nbest_model_file = os.path.join(data_dir, \"best_model.h5\" )\\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\\n    best_model_file,\\n    save_weights_only= True,\\n    save_best_only= True)\\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\\nhistory = model.fit(train_dataset,\\n    epochs=num_epochs,\\n    validation_data=val_dataset,\\n    callbacks=[checkpoint, tensorboard])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4f1087c-7b22-48cd-97c5-736eaf548e71', embedding=None, metadata={'page_label': '170', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 170\\nA truncated output of the training is shown as follows. As you can see, the masked_accuracy  and val_\\nmasked_accuracy  numbers seem more conservative than the accuracy  and val_accuracy  numbers. \\nThis is because the masked versions do not consider the sequence positions where the input is a PAD \\ncharacter:\\nEpoch 1/50\\n19/19 [==============================] - 8s 431ms/step - loss: 1.4363 - \\naccuracy: 0.7511 - masked_accuracy_fn: 0.00\\n38 - val_loss: 0.3219 - val_accuracy: 0.9116 - val_masked_accuracy_fn: 0.5833\\nEpoch 2/50\\n19/19 [==============================] - 6s 291ms/step - loss: 0.3278 - \\naccuracy: 0.9183 - masked_accuracy_fn: 0.17\\n12 - val_loss: 0.3289 - val_accuracy: 0.9209 - val_masked_accuracy_fn: 0.1357\\nEpoch 3/50\\n19/19 [==============================] - 6s 292ms/step - loss: 0.3187 - \\naccuracy: 0.9242 - masked_accuracy_fn: 0.1615 - val_loss: 0.3131 - val_\\naccuracy: 0.9186 - val_masked_accuracy_fn: 0.2236\\nEpoch 4/50\\n19/19 [==============================] - 6s 293ms/step - loss: 0.3037 - \\naccuracy: 0.9186 - masked_accuracy_fn: 0.1831 - val_loss: 0.2933 - val_\\naccuracy: 0.9129 - val_masked_accuracy_fn: 0.1062\\nEpoch 5/50\\n19/19 [==============================] - 6s 294ms/step - loss: 0.2739 - \\naccuracy: 0.9182 - masked_accuracy_fn: 0.1054 - val_loss: 0.2608 - val_\\naccuracy: 0.9230 - val_masked_accuracy_fn: 0.1407\\n...\\nEpoch 45/50\\n19/19 [==============================] - 6s 292ms/step - loss: 0.0653 - \\naccuracy: 0.9810 - masked_accuracy_fn: 0.7872 - val_loss: 0.1545 - val_\\naccuracy: 0.9611 - val_masked_accuracy_fn: 0.5407\\nEpoch 46/50\\n19/19 [==============================] - 6s 291ms/step - loss: 0.0640 - \\naccuracy: 0.9815 - masked_accuracy_fn: 0.7925 - val_loss: 0.1550 - val_\\naccuracy: 0.9616 - val_masked_accuracy_fn: 0.5441\\nEpoch 47/50\\n19/19 [==============================] - 6s 291ms/step - loss: 0.0619 - \\naccuracy: 0.9818 - masked_accuracy_fn: 0.7971 - val_loss: 0.1497 - val_\\naccuracy: 0.9614 - val_masked_accuracy_fn: 0.5535\\nEpoch 48/50\\n19/19 [==============================] - 6s 292ms/step - loss: 0.0599 - \\naccuracy: 0.9825 - masked_accuracy_fn: 0.8033 - val_loss: 0.1524 - val_\\naccuracy: 0.9616 - val_masked_accuracy_fn: 0.5579\\nEpoch 49/50', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4f076ec-64ee-4d0e-9f03-3c6acd706323', embedding=None, metadata={'page_label': '171', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 171\\n19/19 [==============================] - 6s 293ms/step - loss: 0.0585 - \\naccuracy: 0.9830 - masked_accuracy_fn: 0.8092 - val_loss: 0.1544 - val_\\naccuracy: 0.9617 - val_masked_accuracy_fn: 0.5621\\nEpoch 50/50\\n19/19 [==============================] - 6s 291ms/step - loss: 0.0575 - \\naccuracy: 0.9833 - masked_accuracy_fn: 0.8140 - val_loss: 0.1569 - val_\\naccuracy: 0.9615 - val_masked_accuracy_fn: 0.5511\\n11/11 [==============================] - 2s 170ms/step - loss: 0.1436 - \\naccuracy: 0.9637 - masked_accuracy_fn: 0.5786\\ntest loss: 0.144, test accuracy: 0.963, masked test accuracy: 0.578\\nHere are some examples of POS tags generated for some random sentences in the test set, shown \\ntogether with the POS tags in the corresponding ground truth sentences. As you can see, while the \\nmetric values are not perfect, it seems to have learned to do POS tagging fairly well:\\nlabeled  : among/IN segments/NNS that/WDT t/NONE 1/VBP continue/NONE 2/TO to/VB \\noperate/RB though/DT the/NN company/POS 's/NN steel/NN division/VBD continued/\\nNONE 3/TO to/VB suffer/IN from/JJ soft/NN demand/IN for/PRP its/JJ tubular/NNS \\ngoods/VBG serving/DT the/NN oil/NN industry/CC and/JJ other/NNS\\npredicted: among/IN segments/NNS that/WDT t/NONE 1/NONE continue/NONE 2/\\nTO to/VB operate/IN though/DT the/NN company/NN 's/NN steel/NN division/NONE \\ncontinued/NONE 3/TO to/IN suffer/IN from/IN soft/JJ demand/NN for/IN its/JJ \\ntubular/NNS goods/DT serving/DT the/NNP oil/NN industry/CC and/JJ other/NNS\\nlabeled  : as/IN a/DT result/NN ms/NNP ganes/NNP said/VBD 0/NONE t/NONE 2/PRP \\nit/VBZ is/VBN believed/IN that/JJ little/CC or/DT no/NN sugar/IN from/DT the/\\nCD 1989/NN 90/VBZ crop/VBN has/VBN been/NONE shipped/RB 1/RB yet/IN even/DT \\nthough/NN the/NN crop/VBZ year/CD is/NNS six/JJ\\npredicted: as/IN a/DT result/NN ms/IN ganes/NNP said/VBD 0/NONE t/NONE 2/PRP \\nit/VBZ is/VBN believed/NONE that/DT little/NN or/DT no/NN sugar/IN from/DT the/\\nDT 1989/CD 90/NN crop/VBZ has/VBN been/VBN shipped/VBN 1/RB yet/RB even/IN \\nthough/DT the/NN crop/NN year/NN is/JJ\\nlabeled  : in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN \\nafternoon/NN both/DT men/NNS exuded/VBD confidence/NN and/CC seemed/VBD 1/NONE \\nto/TO work/VB well/RB together/RB\\npredicted: in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN \\nafternoon/NN both/DT men/NNS exuded/NNP confidence/NN and/CC seemed/VBD 1/NONE \\nto/TO work/VB well/RB together/RB\\nlabeled  : all/DT came/VBD from/IN cray/NNP research/NNP\\npredicted: all/NNP came/VBD from/IN cray/NNP research/NNP\\nlabeled  : primerica/NNP closed/VBD at/IN 28/CD 25/NONE u/RB down/CD 50/NNS\\npredicted: primerica/NNP closed/VBD at/CD 28/CD 25/CD u/CD down/CD\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16ca7469-89fb-4c4b-b376-229c303d1e23', embedding=None, metadata={'page_label': '172', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 172\\nIf you would like to run this code yourself, you can find the code in the code folder for this chapter. \\nTo run it from the command line, enter the following command. The output is written to the console:\\n$ python gru_pos_tagger.py\\nNow that we have seen some examples of three common RNN network topologies, let’s explore the \\nmost popular of them all – the seq2seq model, which is also known as the recurrent encoder-decoder \\narchitecture.\\nEncoder-decoder architecture – seq2seq\\nThe example of a many-to-many network we just saw was mostly similar to the many-to-one network. \\nThe one important difference was that the RNN returns outputs at each time step instead of a single \\ncombined output at the end. One other noticeable feature was that the number of input time steps \\nwas equal to the number of output time steps. As you learn about the encoder-decoder architecture, \\nwhich is the “other,” and arguably more popular, style of a many-to-many network, you will notice \\nanother difference – the output is in line with the input in a many-to-many network, that is, it is not \\nnecessary for the network to wait until all of the input is consumed before generating the output.\\nThe encoder-decoder architecture is also called a seq2seq model. As the name implies, the network is \\ncomposed of an encoder and a decoder part, both RNN-based and capable of consuming and returning \\nsequences of outputs corresponding to multiple time steps. The biggest application of the seq2seq \\nnetwork has been in neural machine translation, although it is equally applicable for problems that \\ncan be roughly structured as translation problems. Some examples are sentence parsing [10] and \\nimage captioning [24]. The seq2seq model has also been used for time series analysis [25] and question \\nanswering.\\nIn the seq2seq model, the encoder consumes the source sequence, which is a batch of integer sequences. \\nThe length of the sequence is the number of input time steps, which corresponds to the maximum \\ninput sequence length (padded or truncated as necessary). Thus, the dimensions of the input tensor \\nare (batch_size , number_of_encoder_timesteps ). This is passed into an embedding layer, which \\nwill convert the integer at each time step into an embedding vector. The output of the embedding is \\na tensor of shape ( batch_size , number_of_encoder_timesteps , encoder_embedding_dim ).\\nThis tensor is fed into an RNN, which converts the vector at each time step into the size corresponding \\nto its encoding dimension. This vector is a combination of the current time step and all previous time \\nsteps. Typically, the encoder will return the output at the last time step, representing the context or \\n“thought” vector for the entire sequence. This tensor has the shape ( batch_size , encoder_rnn_dim ).\\nThe decoder network has a similar architecture as the encoder, except there is an additional dense \\nlayer at each time step to convert the output. The input to each time step on the decoder side is the \\nhidden state of the previous time step and the input vector that is the token predicted by the decoder \\nof the previous time step. For the very first time step, the hidden state is the context vector from the \\nencoder, and the input vector corresponds to the token that will initiate sequence generation on the \\ntarget side. For the translation use case, for example, it is a beginning-of-string (BOS ) pseudo-token. \\nThe shape of the hidden signal is ( batch_size , encoder_rnn_dim ) and the shape of the input signal \\nacross all time steps is ( batch_size , number_of_decoder_timesteps ). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39e18ed8-fb7d-472b-bf71-785d0650e52a', embedding=None, metadata={'page_label': '173', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 173\\nOnce it passes through the embedding layer, the output tensor shape is ( batch_size , number_of_\\ndecoder_timesteps , decoder_embedding_dim ). The next step is the decoder RNN layer, the output \\nof which is a tensor of shape ( batch_size , number_of_decoder_timesteps , decoder_rnn_dim ). The \\noutput at each time step is then sent through a dense layer, which converts the vector into the size of \\nthe target vocabulary, so the output of the dense layer is ( batch_size , number_of_decoder_timesteps , \\noutput_vocab_size ). This is basically a probability distribution over tokens at each time step, so if \\nwe compute the argmax over the last dimension, we can convert it back into a predicted sequence of \\ntokens in the target language. Figure 5.7 shows a high-level view of the seq2seq architecture:\\nFigure 5.7: Seq2seq network data flow. Image Source: Artur Suilin [25]\\nIn the next section, we will look at an example of a seq2seq network for machine translation.\\nExample ‒ seq2seq without attention for machine translation\\nTo understand the seq2seq model in greater detail, we will look at an example of one that learns how to \\ntranslate from English to French using the French-English bilingual dataset from the Tatoeba Project \\n(1997-2019) [26]. The dataset contains approximately 167,000 sentence pairs. To make our training go \\nfaster, we will only consider the first 30,000 sentence pairs for our training.\\nAs always, we will start with the imports:\\nimport nltk\\nimport numpy as np\\nimport re\\nimport shutil\\nimport tensorflow as tf\\nimport os\\nimport unicodedata\\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\\nThe data is provided as a remote zip file. The easiest way to access the file is to download it from http://\\nwww.manythings.org/anki/fra-eng.zip  and expand it locally using unzip. The zip file contains a \\ntab-separated file called fra.txt , with French and English sentence pairs separated by a tab, one pair \\nper line. The code expects the fra.txt  file in a dataset folder in the same directory as itself. We want \\nto extract three different datasets from it.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf9f305a-0883-4d9a-8952-5a8241fcf0ef', embedding=None, metadata={'page_label': '174', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 174\\nIf you recall the structure of the seq2seq network, the input to the encoder is a sequence of English \\nwords. On the decoder side, the input is a set of French words, and the output is the sequence of French \\nwords offset by one time step. The following function will download the zip file, expand it, and create \\nthe datasets described before.\\nThe input is preprocessed to asciify the characters, separate out specific punctuations from their \\nneighboring word, and remove all characters other than alphabets and these specific punctuation \\nsymbols. Finally, the sentences are converted into lowercase. Each English sentence is just converted \\ninto a single sequence of words. Each French sentence is converted into two sequences, one preceded \\nby the BOS pseudo-word and the other followed by the end-of-sentence (EOS ) pseudo-word.\\nThe first sequence starts at position 0 and stops one short of the final word in the sentence, and the \\nsecond sequence starts at position 1 and goes all the way to the end of the sentence:\\ndef preprocess_sentence (sent):\\n    sent = \"\".join([c for c in unicodedata.normalize( \"NFD\", sent)\\n        if unicodedata.category(c) != \"Mn\"])\\n    sent = re.sub( r\"([!.?])\" , r\" \\\\1\" , sent)\\n    sent = re.sub( r\"[^a-zA-Z!.?]+\" , r\" \", sent)\\n    sent = re.sub( r\"\\\\s+\", \" \", sent)\\n    sent = sent.lower()\\n    return sent\\ndef download_and_read ():\\n    en_sents, fr_sents_in, fr_sents_out = [], [], []\\n    local_file = os.path.join( \"datasets\" , \"fra.txt\" )\\n    with open(local_file, \"r\") as fin:\\n        for i, line in enumerate (fin):\\n            en_sent, fr_sent = line.strip().split( \\'\\\\t\\')\\n            en_sent = [w for w in preprocess_sentence(en_sent).split()]\\n            fr_sent = preprocess_sentence(fr_sent)\\n            fr_sent_in = [w for w in (\"BOS \"  + fr_sent).split()]\\n            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\\n            en_sents.append(en_sent)\\n            fr_sents_in.append(fr_sent_in)\\n            fr_sents_out.append(fr_sent_out)\\n            if i >= num_sent_pairs - 1:\\n                break\\n    return en_sents, fr_sents_in, fr_sents_out\\nsents_en, sents_fr_in, sents_fr_out = download_and_read()\\nOur next step is to tokenize our inputs and create the vocabulary. Since we have sequences in two \\ndifferent languages, we will create two different tokenizers and vocabularies, one for each language. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='027086cf-62b2-4872-8cb3-76deb2b7d923', embedding=None, metadata={'page_label': '175', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 175\\nThe tf.keras framework provides a very powerful and versatile tokenizer class – here, we have set \\nfilters to an empty string and lower  to False  because we have already done what was needed for \\ntokenization in our preprocess_sentence()  function. The Tokenizer creates various data structures \\nfrom which we can compute the vocabulary sizes and lookup tables that allow us to go from word to \\nword index and back.\\nNext, we handle different length sequences of words by padding with zeros at the end, using the \\npad_sequences()  function. Because our strings are fairly short, we do not do any truncation; we just \\npad to the maximum length of sentence that we have (8 words for English and 16 words for French):\\ntokenizer_en = tf.keras.preprocessing.text.Tokenizer(\\n    filters= \"\", lower= False)\\ntokenizer_en.fit_on_texts(sents_en)\\ndata_en = tokenizer_en.texts_to_sequences(sents_en)\\ndata_en = tf.keras.preprocessing.sequence.pad_sequences(\\n    data_en, padding= \"post\")\\ntokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\\n    filters= \"\", lower= False)\\ntokenizer_fr.fit_on_texts(sents_fr_in)\\ntokenizer_fr.fit_on_texts(sents_fr_out)\\ndata_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\\ndata_fr_in = tf.keras.preprocessing.sequence.pad_sequences(\\n    data_fr_in, padding= \"post\")\\ndata_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\\ndata_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\\n    data_fr_out, padding= \"post\")\\nvocab_size_en = len(tokenizer_en.word_index)\\nvocab_size_fr = len(tokenizer_fr.word_index)\\nword2idx_en = tokenizer_en.word_index\\nidx2word_en = {v:k for k, v in word2idx_en.items()}\\nword2idx_fr = tokenizer_fr.word_index\\nidx2word_fr = {v:k for k, v in word2idx_fr.items()}\\nprint(\"vocab size (en): {:d}, vocab size (fr): {:d}\" .format(\\n    vocab_size_en, vocab_size_fr))\\nmaxlen_en = data_en.shape[ 1]\\nmaxlen_fr = data_fr_out.shape[ 1]\\nprint(\"seqlen (en): {:d}, (fr): {:d}\" .format(maxlen_en, maxlen_fr))\\nFinally, we convert the data into a TensorFlow dataset, and then split it into a training and test dataset:\\nbatch_size = 64\\ndataset = tf.data.Dataset.from_tensor_slices(\\n    (data_en, data_fr_in, data_fr_out))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c02faf37-55da-48a1-9179-888f7be6cf5b', embedding=None, metadata={'page_label': '176', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 176\\ndataset = dataset.shuffle( 10000)\\ntest_size = NUM_SENT_PAIRS // 4\\ntest_dataset = dataset.take(test_size).batch(\\n    batch_size, drop_remainder= True)\\ntrain_dataset = dataset.skip(test_size).batch(\\n    batch_size, drop_remainder= True)\\nOur data is now ready to be used for training the seq2seq network, which we will define next. Our \\nencoder is an embedding layer followed by a GRU layer. The input to the encoder is a sequence \\nof integers, which is converted into a sequence of embedding vectors of size embedding_dim . This \\nsequence of vectors is sent to an RNN, which converts the input at each of the num_timesteps  time \\nsteps into a vector of size encoder_dim . Only the output at the last time step is returned, as shown by \\nreturn_sequences=False .\\nThe decoder has almost the same structure as the encoder, except that it has an additional dense \\nlayer  that converts the vector of size decoder_dim , which is output from the RNN, into a vector that \\nrepresents the probability distribution across the target vocabulary. The decoder also returns outputs \\nalong with all its time steps.\\nIn our example network, we have chosen our embedding dimension to be 128, followed by an encoder \\nand decoder RNN dimension of 1024 each. Note that we have to add 1 to the vocabulary size for both \\nthe English and French vocabularies to account for the PAD character that was added during the \\npad_sequences()  step:\\nclass Encoder (tf.keras.Model):\\n    def __init__ (self, vocab_size, num_timesteps,\\n            embedding_dim, encoder_dim, **kwargs):\\n        super(Encoder, self).__init__(**kwargs)\\n        self.encoder_dim = encoder_dim\\n        self.embedding = tf.keras.layers.Embedding(\\n            vocab_size, embedding_dim, input_length=num_timesteps)\\n        self.rnn = tf.keras.layers.GRU(\\n            encoder_dim, return_sequences= False, return_state= True)\\n    def call(self, x, state):\\n        x = self.embedding(x)\\n        x, state = self.rnn(x, initial_state=state)\\n        return x, state\\n    def init_state (self, batch_size):\\n        return tf.zeros((batch_size, self.encoder_dim))\\nclass Decoder (tf.keras.Model):\\n    def __init__ (self, vocab_size, embedding_dim, num_timesteps,\\n            decoder_dim, **kwargs):\\n        super(Decoder, self).__init__(**kwargs)\\n        self.decoder_dim = decoder_dim', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c04b7dc-722c-48e1-880e-36b020a2df36', embedding=None, metadata={'page_label': '177', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 177\\n        self.embedding = tf.keras.layers.Embedding(\\n            vocab_size, embedding_dim, input_length=num_timesteps)\\n        self.rnn = tf.keras.layers.GRU(\\n            decoder_dim, return_sequences= True, return_state= True)\\n        self.dense = tf.keras.layers.Dense(vocab_size)\\n    def call(self, x, state):\\n        x = self.embedding(x)\\n        x, state = self.rnn(x, state)\\n        x = self.dense(x)\\n        return x, state\\nembedding_dim = 256\\nencoder_dim, decoder_dim = 1024, 1024\\nencoder = Encoder(vocab_size_en+ 1, \\n    embedding_dim, maxlen_en, encoder_dim)\\ndecoder = Decoder(vocab_size_fr+ 1, \\n    embedding_dim, maxlen_fr, decoder_dim)\\nNow that we have defined our Encoder  and Decoder  classes, let’s revisit the dimensions of their inputs \\nand outputs. The following piece of (throwaway) code can be used to print out the dimensions of the \\nvarious inputs and outputs of the system. It has been left in for convenience as a commented-out block \\nin the code supplied with this chapter:\\nfor encoder_in, decoder_in, decoder_out in train_dataset:\\n    encoder_state = encoder.init_state(batch_size)\\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\\n    decoder_state = encoder_state\\n    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\\n    break\\nprint(\"encoder input          :\" , encoder_in.shape)\\nprint(\"encoder output         :\" , encoder_out.shape, \"state:\" , encoder_state.\\nshape)\\nprint(\"decoder output (logits):\" , decoder_pred.shape, \"state:\" , decoder_state.\\nshape)\\nprint(\"decoder output (labels):\" , decoder_out.shape)\\nThis produces the following output, which is in line with our expectations. The encoder input is a batch \\nof a sequence of integers, each sequence being of size 8, which is the maximum number of tokens in \\nour English sentences, so its dimension is ( batch_size , maxlen_en ).\\nThe output of the encoder is a single tensor ( return_sequences=False ) of shape ( batch_size , encoder_\\ndim) and represents a batch of context vectors representing the input sentences. The encoder state \\ntensor has the same dimensions. The decoder outputs are also a batch of sequences of integers, but \\nthe maximum size of a French sentence is 16; therefore, the dimensions are ( batch_size , maxlen_fr ). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc608205-6308-460e-a5fb-b91a9bb6fcee', embedding=None, metadata={'page_label': '178', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 178\\nThe decoder predictions are a batch of probability distributions across all time steps; hence, the \\ndimensions are ( batch_size , maxlen_fr , vocab_size_fr+1 ), and the decoder state is the same \\ndimension as the encoder state ( batch_size , decoder_dim ):\\nencoder input          : (64, 8)\\nencoder output         : (64, 1024) state: (64, 1024)\\ndecoder output (logits): (64, 16, 7658) state: (64, 1024)\\ndecoder output (labels): (64, 16)\\nNext, we define the loss function. Because we padded our sentences, we don’t want to bias our results by \\nconsidering the equality of pad words between the labels and predictions. Our loss function masks our \\npredictions with the labels, so any padded positions on the label are also removed from the predictions, \\nand we only compute our loss using the non-zero elements on both the label and predictions. This \\nis done as follows:\\ndef loss_fn (ytrue, ypred):\\n    scce = tf.keras.losses.SparseCategoricalCrossentropy(\\n        from_logits= True)\\n    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\\n    mask = tf.cast(mask, dtype=tf.int64)\\n    loss = scce(ytrue, ypred, sample_weight=mask)\\n    return loss\\nBecause the seq2seq model is not easy to package into a simple Keras model, we have to handle the \\ntraining loop manually as well. Our train_step()  function handles the flow of data and computes the \\nloss at each step, applies the gradient of the loss back to the trainable weights, and returns the loss.\\nNotice that the training code is not quite the same as what was described in our discussion of the \\nseq2seq model earlier. Here, it appears that the entire decoder_input  is fed in one go into the decoder \\nto produce the output offset by one time step, whereas in the discussion, we said that this happens \\nsequentially, where the token generated in the previous time step is used as the input for the next \\ntime step.\\nThis is a common technique used to train seq2seq networks, which is called Teacher Forcing , where the \\ninput to the decoder is the ground truth output instead of the prediction from the previous time step. \\nThis is preferred because it makes training faster but also results in some degradation in prediction \\nquality. To offset this, techniques such as Scheduled Sampling can be used, where the input is sampled \\nrandomly either from the ground truth or the prediction at the previous time step, based on some \\nthreshold (this depends on the problem, but usually varies between 0.1 and 0.4):\\n@tf.function\\ndef train_step (encoder_in, decoder_in, decoder_out, encoder_state):\\n    with tf.GradientTape() as tape:\\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\\n        decoder_state = encoder_state\\n        decoder_pred, decoder_state = decoder(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8984f8b-4079-4b71-bebb-c4bfce74502a', embedding=None, metadata={'page_label': '179', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 179\\n            decoder_in, decoder_state)\\n        loss = loss_fn(decoder_out, decoder_pred)\\n  \\n    variables = (encoder.trainable_variables + \\n        decoder.trainable_variables)\\n    gradients = tape.gradient(loss, variables)\\n    optimizer.apply_gradients( zip(gradients, variables))\\n    return loss\\nThe predict()  method is used to randomly sample a single English sentence from the dataset and use \\nthe model trained so far to predict the French sentence. For reference, the label French sentence is \\nalso displayed. The evaluate()  method computes the BiLingual Evaluation Understudy ( BLEU ) score \\n[35] between the labels and predictions across all records in the test set. BLEU scores are generally \\nused where multiple ground truth labels exist (we have only one) and compare up to 4-grams (n-grams \\nwith n=4) in both reference and candidate sentences. Both the predict()  and evaluate()  methods \\nare called at the end of every epoch:\\ndef predict (encoder, decoder, batch_size,\\n        sents_en, data_en, sents_fr_out,\\n        word2idx_fr, idx2word_fr):\\n    random_id = np.random.choice( len(sents_en))\\n    print(\"input    : \" ,  \" \".join(sents_en[random_id]))\\n    print(\"label    : \" , \" \".join(sents_fr_out[random_id]))\\n    encoder_in = tf.expand_dims(data_en[random_id], axis= 0)\\n    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis= 0)\\n    encoder_state = encoder.init_state( 1)\\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\\n    decoder_state = encoder_state\\n    decoder_in = tf.expand_dims(\\n        tf.constant([word2idx_fr[ \"BOS\"]]), axis= 0)\\n    pred_sent_fr = []\\n    while True:\\n        decoder_pred, decoder_state = decoder(\\n            decoder_in, decoder_state)\\n        decoder_pred = tf.argmax(decoder_pred, axis=- 1)\\n        pred_word = idx2word_fr[decoder_pred.numpy()[ 0][0]]\\n        pred_sent_fr.append(pred_word)\\n        if pred_word == \"EOS\":\\n            break\\n        decoder_in = decoder_pred\\n  \\n    print(\"predicted: \" , \" \".join(pred_sent_fr))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bae8e80a-0325-48c6-99ee-aa1c8fccb3c9', embedding=None, metadata={'page_label': '180', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 180\\ndef evaluate_bleu_score (encoder, decoder, test_dataset,\\n        word2idx_fr, idx2word_fr):\\n    bleu_scores = []\\n    smooth_fn = SmoothingFunction()\\n    for encoder_in, decoder_in, decoder_out in test_dataset:\\n        encoder_state = encoder.init_state(batch_size)\\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\\n        decoder_state = encoder_state\\n        decoder_pred, decoder_state = decoder(\\n            decoder_in, decoder_state)\\n        # compute argmax\\n        decoder_out = decoder_out.numpy()\\n        decoder_pred = tf.argmax(decoder_pred, axis=- 1).numpy()\\n        for i in range (decoder_out.shape[ 0]):\\n            ref_sent = [idx2word_fr[j] for j in \\n                decoder_out[i].tolist() if j > 0]\\n            hyp_sent = [idx2word_fr[j] for j in \\n                decoder_pred[i].tolist() if j > 0]\\n            # remove trailing EOS\\n            ref_sent = ref_sent[ 0:-1]\\n            hyp_sent = hyp_sent[ 0:-1]\\n            bleu_score = sentence_bleu([ref_sent], hyp_sent,\\n                smoothing_function=smooth_fn.method1)\\n            bleu_scores.append(bleu_score)\\n    return np.mean(np.array(bleu_scores))\\nThe training loop is shown as follows. We will use the Adam optimizer for our model. We also set up \\na checkpoint so that we can save our model after every 10 epochs. We then train the model for 250 \\nepochs, and print out the loss, an example sentence and its translation, and the BLEU score computed \\nover the entire test set:\\noptimizer = tf.keras.optimizers.Adam()\\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\\n                                 encoder=encoder,\\n                                 decoder=decoder)\\nnum_epochs = 250\\neval_scores = []\\nfor e in range (num_epochs):\\n    encoder_state = encoder.init_state(batch_size)\\n    for batch, data in enumerate (train_dataset):\\n        encoder_in, decoder_in, decoder_out = data', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='083ef283-0fdf-4456-be21-e0d528c94509', embedding=None, metadata={'page_label': '181', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 181\\n        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\\n        loss = train_step(\\n            encoder_in, decoder_in, decoder_out, encoder_state)\\n  \\n    print(\"Epoch: {}, Loss: {:.4f}\" .format(e + 1, loss.numpy()))\\n    if e % 10 == 0:\\n        checkpoint.save(file_prefix=checkpoint_prefix)\\n  \\n    predict(encoder, decoder, batch_size, sents_en, data_en,\\n        sents_fr_out, word2idx_fr, idx2word_fr)\\n    eval_score = evaluate_bleu_score(encoder, decoder, \\n        test_dataset, word2idx_fr, idx2word_fr)\\n    print(\"Eval Score (BLEU): {:.3e}\" .format(eval_score))\\n    # eval_scores.append(eval_score)\\ncheckpoint.save(file_prefix=checkpoint_prefix)\\nThe results from the first 5 and last 5 epochs of training are shown as follows. Notice that the loss \\nhas gone down from about 1.5 to around 0.07 in epoch 247. The BLEU scores have also gone up by \\naround 2.5 times. Most impressive, however, is the difference in translation quality between the first \\n5 and last 5 epochs:\\nEpoch-# Loss (Training)BLEU Score \\n(Test)English French (true) French (predicted)\\n1 1.4119 1.957e-02tom is \\nspecial.tom est special. elle est tres bon.\\n2 1.1067 2.244e-02he hates \\nshopping.il deteste faire les \\ncourses.il est tres mineure.\\n3 0.9154 2.700e-02did she say \\nit?l a t elle dit? n est ce pas clair?\\n4 0.7817 2.803e-02i d rather \\nwalk.je prefererais \\nmarcher.je suis alle a kyoto.\\n5 0.6632 2.943e-02 i m in the car.je suis dans la \\nvoiture.je suis toujours \\ninquiet.\\n...\\n245 0.0896 4.991e-02 she sued him.elle le poursuivit \\nen justice.elle l a poursuivi \\nen justice.\\n246 0.0853 5.011e-02 she isn t poor.elle n est pas \\npauvre.elle n est pas \\npauvre.\\n247 0.0738 5.022e-02which one is \\nmine?lequel est le \\nmien?lequel est le mien?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51bf22a4-7757-45cb-9289-c6935785381f', embedding=None, metadata={'page_label': '182', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 182\\n248 0.1208 4.931e-02i m getting \\nold.je me fais vieux. je me fais vieux.\\n249 0.0837 4.856e-02it was worth \\na try.ca valait le coup d \\nessayer.ca valait le coup d \\nessayer.\\n250 0.0967 4.869e-02don t back \\naway.ne reculez pas! ne reculez pas!\\nTable 5.2: Training results by epoch\\nThe full code for this example can be found in the source code accompanying this chapter. You will need \\na GPU-based machine to run it, although you may be able to run it on the CPU using smaller network \\ndimensions ( embedding_dim , encoder_dim , decoder_dim ), smaller hyperparameters ( batch_size , \\nnum_epochs ), and a smaller number of sentence pairs. To run the code in its entirety, run the following \\ncommand. The output will be written to the console:\\n$ python seq2seq_wo_attn.py\\nIn the next section, we will look at a mechanism to improve the performance of the seq2seq network, \\nby allowing it to focus on certain parts of the input more than on others in a data-driven way. This \\nmechanism is known as the attention mechanism.\\nAttention mechanism\\nIn the previous section, we saw how the context or thought vector from the last time step of the \\nencoder is fed into the decoder as the initial hidden state. As the context flows through the time steps \\non the decoder, the signal gets combined with the decoder output and progressively gets weaker and \\nweaker. The result is that the context does not have much effect on the later time steps in the decoder.\\nIn addition, certain sections of the decoder output may depend more heavily on certain sections \\nof the input. For example, consider an input “thank you very much,” and the corresponding output \\n“merci beaucoup” for an English-to-French translation network such as the one we looked at in the \\nprevious section. Here, the English phrases “thank you,” and “very much,” correspond to the French \\n“merci” and “beaucoup” respectively. This information is also not conveyed adequately through the \\nsingle context vector.\\nThe attention mechanism provides access to all encoder hidden states at every time step on the decoder. \\nThe decoder learns which part of the encoder states to pay more attention to. The use of attention has \\nresulted in great improvements to the quality of machine translation, as well as a variety of standard \\nnatural language processing tasks.\\nThe use of attention is not limited to seq2seq networks. For example, attention is a key component in \\nthe “Embed, Encode, Attend, Predict” formula for creating state-of-the-art deep learning models for \\nNLP [34]. Here, attention has been used to preserve as much information as possible when downsizing \\nfrom a larger to a more compact representation, for example, when reducing a sequence of word \\nvectors into a single sentence vector.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80d2c2d5-2ced-428a-872d-139803343b17', embedding=None, metadata={'page_label': '183', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 183\\nEssentially, the attention mechanism provides a way to score tokens in the target against all tokens \\nin the source and modify the input signal to the decoder accordingly. Consider an encoder-decoder \\narchitecture where the input and output time steps are denoted by indices i and j respectively, and \\nthe hidden states on the encoder and decoder at these respective time steps are denoted by hi and \\nsj. Inputs to the encoder are denoted by x i, and outputs from the decoder are denoted by y j. In an \\nencoder-decoder network without attention, the value of decoder state s j is given by the hidden state \\nsj-1 and output y j-1 at the previous time step. The attention mechanism adds a third signal c j, known as \\nthe attention context. With attention, therefore, the decoder’s hidden state s j is a function of y j-1, sj-1, \\nand c j, which is shown as follows:\\n𝑠𝑠𝑗𝑗= 𝑓𝑓𝑓𝑓𝑓𝑗𝑗𝑗𝑗,𝑠𝑠𝑗𝑗𝑗𝑗,𝑐𝑐𝑗𝑗) \\nThe attention context signal cj is computed as follows. For every decoder step j, we compute the \\nalignment between the decoder state s j-1 and every encoder state h i. This gives us a set of N similarity \\nvalues eij for each decoder state j, which we then convert into a probability distribution by computing \\ntheir corresponding softmax values b ij. Finally, the attention context c j is computed as the weighted \\nsum of the encoder states h i and their corresponding softmax weights b ij over all N encoder time steps. \\nThe set of equations shown encapsulates this transformation for each decoder step j:\\n𝑒𝑒𝑖𝑖𝑖𝑖= 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 𝑖𝑖,𝑠𝑠𝑖𝑖𝑗𝑗)∀𝑎𝑎\\n𝑏𝑏𝑖𝑖𝑖𝑖= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑎𝑎𝑠𝑠𝑠 𝑒𝑒𝑖𝑖𝑖𝑖)\\n𝑐𝑐𝑖𝑖=∑𝑎 𝑖𝑖𝑁𝑁\\n𝑖𝑖𝑖𝑖𝑏𝑏𝑖𝑖𝑖𝑖 \\nMultiple attention mechanisms have been proposed based on how the alignment is done. We will \\ndescribe a few next. For notational convenience, we will indicate the state vector h i on the encoder \\nside with h, and the state vector s j-1 on the decoder side with s.\\nThe simplest formulation of alignment is content-based attention. It was proposed by Graves, Wayne, \\nand Danihelka [27], and is just the cosine similarity between the encoder and decoder states. A \\nprecondition for using this formulation is that the hidden state vector on both the encoder and decoder \\nmust have the same dimensions:\\n𝑒𝑒 𝑒 𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒  \\nAnother formulation, known as additive or Bahdanau attention, was proposed by Bahdanau, Cho, \\nand Bengio [28]. This involves combining the state vectors using learnable weights in a small neural \\nnetwork, given by the following equation. Here, the s and h vectors are concatenated and multiplied \\nby the learned weights W, which is equivalent to using two learned weights W s and W h to multiply with \\ns and h, and adding the results:\\n𝑒𝑒𝑒𝑒𝑒𝑇𝑇tanh(𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊  \\nLuong, Pham, and Manning [29] proposed a set of three attention formulations (dot, general, and \\nconcat), of which the general formulation is also known as the multiplicative or Luong’s attention. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed5c46e9-68b8-4b12-8a2d-2b1010b0be1b', embedding=None, metadata={'page_label': '184', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 184\\nThe dot and concat  attention formulations are similar to the content-based and additive attention \\nformulations discussed earlier. The multiplicative attention formulation is given by the following \\nequation:\\n𝑒𝑒𝑒𝑒𝑇𝑇𝑊𝑊𝑊𝑊 \\nFinally, Vaswani, et al. [30] proposed a variation on content-based attention, called the scaled dot-\\nproduct attention, which is given by the following equation. Here, N is the dimension of the encoder \\nhidden state h. Scaled dot-product attention is used in transformer architecture, which we will learn \\nabout in the next chapter:\\n𝑒𝑒𝑒ℎ𝑇𝑇𝑠𝑠\\n√𝑁𝑁 \\nAttention mechanisms can also be categorized by what they attend to. Using this categorization scheme \\nattention mechanisms can be self-attention, global or soft attention, and local or hard attention.\\nSelf-attention is when the alignment is computed across different sections of the same sequence and \\nhas been found to be useful for applications such as machine reading, abstractive text summarization, \\nand image caption generation.\\nSoft or global attention is when the alignment is computed over the entire input sequence, and hard \\nor local attention is when the alignment is computed over part of the sequence. The advantage of soft \\nattention is that it is differentiable; however, it can be expensive to compute. Conversely, hard attention \\nis cheaper to compute at inference time but is non-differentiable and requires more complicated \\ntechniques during training.\\nIn the next section, we will see how to integrate the attention mechanism with a seq2seq network \\nand how it improves performance.\\nExample ‒ seq2seq with attention for machine translation\\nLet’s look at the same example of machine translation that we saw earlier in this chapter, except that \\nthe decoder will now attend to the encoder outputs using the additive attention mechanism proposed \\nby Bahdanau, et al. [28], and the multiplicative one proposed by Luong, et al [29].\\nThe first change is to the encoder. Instead of returning a single context or thought vector, it will return \\noutputs at every time point, because the attention mechanism will need this information. Here is the \\nrevised encoder class with the change highlighted:\\nclass Encoder (tf.keras.Model):\\n     def __init__ (self, vocab_size, num_timesteps,\\n           embedding_dim, encoder_dim, **kwargs):\\n        super(Encoder, self).__init__(**kwargs)\\n        self.encoder_dim = encoder_dim\\n        self.embedding = tf.keras.layers.Embedding(\\n            vocab_size, embedding_dim, input_length=num_timesteps)\\n        self.rnn = tf.keras.layers.GRU(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69fed685-9a12-4cc1-aca1-30345fd39369', embedding=None, metadata={'page_label': '185', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 185\\n            encoder_dim, return_sequences= True, return_state= True)\\n    def call(self, x, state):\\n        x = self.embedding(x)\\n        x, state = self.rnn(x, initial_state=state)\\n        return x, state\\n    def init_state (self, batch_size):\\n        return tf.zeros((batch_size, self.encoder_dim))\\nThe decoder will have bigger changes. The biggest is the declaration of attention layers, which need \\nto be defined, so let’s do that first. Let’s first consider the class definition for the additive attention \\nproposed by Bahdanau. Recall that this combines the decoder hidden state at each time step with all \\nthe encoder hidden states to produce an input to the decoder at the next time step, which is given by \\nthe following equation:\\n𝑒𝑒𝑒𝑒𝑒𝑇𝑇tanh(𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊  \\nThe W [s;h] in the equation is shorthand for two separate linear transformations (of the form y = Wx + \\nb), one on s, and the other on h. The two linear transformations are implemented as dense layers, as \\nshown in the following implementation. We subclass a tf.keras  Layer object since our end goal is \\nto use this as a layer in our network, but it is also acceptable to subclass a Model object. The call()  \\nmethod takes the query (the decoder state) and values (the encoder states), computes the score, then \\nthe alignment as the corresponding softmax, and context vector as given by the equation, and then \\nreturns them. The shape of the context vector is given by ( batch_size , num_decoder_timesteps ), and \\nthe alignments have the shape ( batch_size , num_encoder_timesteps , 1). The weights for the dense \\nlayer’s W1, W2, and V tensors are learned during training:\\nclass BahdanauAttention (tf.keras.layers.Layer):\\n    def __init__ (self, num_units):\\n        super(BahdanauAttention, self).__init__()\\n        self.W1 = tf.keras.layers.Dense(num_units)\\n        self.W2 = tf.keras.layers.Dense(num_units)\\n        self.V = tf.keras.layers.Dense( 1)\\n    def call(self, query, values):\\n        # query is the decoder state at time step j\\n        # query.shape: (batch_size, num_units)\\n        # values are encoder states at every timestep i\\n        # values.shape: (batch_size, num_timesteps, num_units)\\n        # add time axis to query: (batch_size, 1, num_units)\\n        query_with_time_axis = tf.expand_dims(query, axis= 1)\\n        # compute score:\\n        score = self.V(tf.keras.activations.tanh(\\n            self.W1(values) + self.W2(query_with_time_axis)))\\n        # compute softmax\\n        alignment = tf.nn.softmax(score, axis= 1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da23261d-0625-42c6-8162-19084b4ac873', embedding=None, metadata={'page_label': '186', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 186\\n        # compute attended output\\n        context = tf.reduce_sum(\\n            tf.linalg.matmul(\\n                tf.linalg.matrix_transpose(alignment),\\n                values\\n            ), axis= 1\\n        )\\n        context = tf.expand_dims(context, axis= 1)\\n        return context, alignment\\nThe Luong attention is multiplicative, but the general implementation is similar. Instead of declaring \\nthree linear transformations W1, W2, and V, we only have a single one W. The steps in the call()  method \\nfollow the same general steps – first, we compute the scores according to the equation for Luong’s \\nattention, as described in the last section. Then, we compute the alignments as the corresponding \\nsoftmax version of the scores and then the context vector as the dot product of the alignment and \\nthe values. Like the weights in the Bahdanau attention class, the weight matrices represented by the \\ndense layer W are learned during training:\\nclass LuongAttention (tf.keras.layers.Layer):\\n    def __init__ (self, num_units):\\n        super(LuongAttention, self).__init__()\\n        self.W = tf.keras.layers.Dense(num_units)\\n    def call(self, query, values):\\n        # add time axis to query\\n        query_with_time_axis = tf.expand_dims(query, axis= 1)\\n        # compute score\\n        score = tf.linalg.matmul(\\n            query_with_time_axis, self.W(values), transpose_b= True)\\n        # compute softmax\\n        alignment = tf.nn.softmax(score, axis= 2)\\n        # compute attended output\\n        context = tf.matmul(alignment, values)\\n        return context, alignment\\nTo verify that the two classes are drop-in replacements for each other, we run the following piece of \\nthrowaway code (commented out in the source code for this example). We just manufacture some \\nrandom inputs and send them to both attention classes:\\nbatch_size = 64\\nnum_timesteps = 100\\nnum_units = 1024\\nquery = np.random.random(size=(batch_size, num_units))\\nvalues = np.random.random(size=(batch_size, num_timesteps, num_units))\\n# check out dimensions for Bahdanau attention', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fc4c9b1-d969-4ac9-b304-b5f9ce7c304c', embedding=None, metadata={'page_label': '187', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 187\\nb_attn = BahdanauAttention(num_units)\\ncontext, alignments = b_attn(query, values)\\nprint(\"Bahdanau: context.shape:\" , context.shape, \\n    \"alignments.shape:\" , alignments.shape)\\n# check out dimensions for Luong attention\\nl_attn = LuongAttention(num_units)\\ncontext, alignments = l_attn(query, values)\\nprint(\"Luong: context.shape:\" , context.shape, \\n    \"alignments.shape:\" , alignments.shape)\\nThe preceding code produces the following output and shows, as expected, that the two classes produce \\nidentically shaped outputs when given the same input. Hence, they are drop-in replacements for each \\nother:\\nBahdanau: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\\nLuong: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\\nNow that we have our attention classes, let’s look at the decoder. The difference in the init()  method \\nis the addition of the attention class variable, which we have set to the BahdanauAttention  class. In \\naddition, we have two additional transformations, Wc and Ws, that will be applied to the output of the \\ndecoder RNN. The first one has a tanh  activation to modulate the output between -1 and +1, and the \\nnext one is a standard linear transformation. Compared to the seq2seq network without an attention \\ndecoder component, this decoder takes an additional parameter encoder_output  in its call()  method \\nand returns an additional context vector:\\nclass Decoder (tf.keras.Model):\\n    def __init__ (self, vocab_size, embedding_dim, num_timesteps,\\n            decoder_dim, **kwargs):\\n        super(Decoder, self).__init__(**kwargs)\\n        self.decoder_dim = decoder_dim\\n        self.attention = BahdanauAttention(embedding_dim)\\n        # self.attention = LuongAttention(embedding_dim)\\n        \\n        self.embedding = tf.keras.layers.Embedding(\\n            vocab_size, embedding_dim, input_length=num_timesteps)\\n        self.rnn = tf.keras.layers.GRU(\\n            decoder_dim, return_sequences= True, return_state= True)\\n        self.Wc = tf.keras.layers.Dense(decoder_dim, activation= \"tanh\")\\n        self.Ws = tf.keras.layers.Dense(vocab_size)\\n    def call(self, x, state, encoder_out):\\n        x = self.embedding(x)\\n        context, alignment = self.attention(x, encoder_out)\\n        x = tf.expand_dims(\\n                tf.concat([', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2927c179-4a2e-49fd-a375-82d86c852fa5', embedding=None, metadata={'page_label': '188', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 188\\n                    x, tf.squeeze(context, axis= 1)\\n                ], axis= 1),\\n            axis= 1)\\n        x, state = self.rnn(x, state)\\n        x = self.Wc(x)\\n        x = self.Ws(x)\\n        return x, state, alignment\\nThe training loop is also a little different. Unlike the seq2seq without attention network, where we \\nused teacher forcing to speed up training, using attention means that we now have to consume the \\ndecoder input one by one. This is because the decoder output at the previous step influences more \\nstrongly, through attention, the output at the current time step. Our new training loop is described \\nby the train_step  function below and is significantly slower than the training loop on the seq2seq \\nnetwork without attention. However, this kind of training loop may be used on the former network \\nas well, especially when we want to implement scheduled sampling strategies:\\n@tf.function\\ndef train_step (encoder_in, decoder_in, decoder_out, encoder_state):\\n    with tf.GradientTape() as tape:\\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\\n        decoder_state = encoder_state\\n        loss = 0\\n        for t in range (decoder_out.shape[ 1]):\\n            decoder_in_t = decoder_in[:, t]\\n            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\\n                decoder_state, encoder_out)\\n            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\\n    variables = (encoder.trainable_variables +\\n        decoder.trainable_variables)\\n    gradients = tape.gradient(loss, variables)\\n    optimizer.apply_gradients( zip(gradients, variables))\\n    return loss / decoder_out.shape[ 1]\\nThe predict()  and evaluate()  methods also have similar changes, since they also implement the \\nnew data flow on the decoder side that involves an extra encoder_out  parameter and an extra context  \\nreturn value.\\nWe trained two versions of the seq2seq network with attention, one with additive (Bahdanau) attention \\nand one with multiplicative (Luong) attention. Both networks were trained for 50 epochs instead of 250. \\nHowever, in both cases, translations were produced with a quality similar to that obtained from the \\nseq2seq network without attention trained for 250 epochs. The training losses at the end of training for \\nthe seq2seq networks with either attention mechanism were marginally lower, and the BLEU scores \\non the test sets were slightly higher, compared with the seq2seq network without attention:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db9fca38-a987-4f54-ba5b-a95fd1d5f0dc', embedding=None, metadata={'page_label': '189', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 189\\nNetwork DescriptionEnding Loss \\n(training set)Ending BLEU \\nscore (test set)\\nseq2seq without attention, trained for 250 epochs 0.0967 4.869e-02\\nseq2seq with additive attention, trained for 50 epochs 0.0893 5.508e-02\\nseq2seq with multiplicative attention, trained for 50 epochs 0.0706 5.563e-02\\nTable 5.3: BLEU scores for the different methods\\nHere are some examples of the translations produced by the two networks. Epoch numbers and the \\ntype of attention used are mentioned with each example. Notice that even when the translations are \\nnot 100% the same as the labels, many of them are valid translations of the original:\\nAttention Type Epoch-# English French (label) French (predicted)\\nBahdanau20 your cat is fat. ton chat est gras. ton chat est mouille.\\n25 i had to go back. il m a fallu retourner. il me faut partir.\\n30 try to find it. tentez de le trouver. tentez de le trouver.\\nLuong20 that s peculiar. c est etrange. c est deconcertant.\\n25 tom is athletic. thomas est sportif. tom est sportif.\\n30 it s dangerous. c est dangereux. c est dangereux.\\nTable 5.4: Examples of English-to-French translations\\nThe full code for the network described here is in the seq2seq_with_attn.py  file in the code folder \\nfor this chapter. To run the code from the command line, please use the following command. You can \\nswitch between Bahdanau (additive) or Luong (multiplicative) attention mechanisms by commenting \\nout one or the other in the init()  method of the Decoder  class:\\n$ python seq2seq_with_attn.py\\nSummary\\nIn this chapter, we learned about RNNs, a class of networks that is specialized for dealing with sequences \\nsuch as natural language, time series, speech, and so on. Just like CNNs exploit the geometry of images, \\nRNNs exploit the sequential structure of their inputs. We learned about the basic RNN cell, how it \\nhandles state from previous time steps, and how it suffers from vanishing and exploding gradients \\nbecause of inherent problems with BPTT. We saw how these problems lead to the development of \\nnovel RNN cell architectures such as LSTM, GRU, and peephole LSTMs. We also learned about some \\nsimple ways to make your RNN more effective, such as making it bidirectional or stateful.\\nWe then looked at different RNN topologies and how each topology is adapted to a particular set of \\nproblems. After a lot of theory, we finally saw examples of three of these topologies. We then focused \\non one of these topologies, called seq2seq, which first gained popularity in the machine translation \\ncommunity, but has since been used in situations where the use case can be adapted to look like a \\nmachine translation problem.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='065d9639-7beb-4b75-ad06-bb71ada36952', embedding=None, metadata={'page_label': '190', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 190\\nFrom here, we looked at attention, which started as a way to improve the performance of seq2seq \\nnetworks but has since been used very effectively in many situations where we want to compress the \\nrepresentation while keeping the data loss to a minimum. We looked at different kinds of attention \\nand an example of using them in a seq2seq network with attention.\\nIn the next chapter, you will learn about transformers, a state-of-the-art encoder-decoder architecture \\nwhere the recurrent layers have been replaced by attention layers.\\nReferences\\n1. Jozefowicz, R., Zaremba, R. and Sutskever, I. (2015). An Empirical Exploration of Recurrent Neural \\nNetwork Architectures. Journal of Machine Learning\\n2. Greff, K., et al. (July 2016). LSTM: A Search Space Odyssey. IEEE Transactions on Neural Networks \\nand Learning Systems\\n3. Bernal, A., Fok, S., and Pidaparthi, R. (December 2012). Financial Markets Time Series Prediction \\nwith Recurrent Neural Networks\\n4. Hadjeres, G., Pachet, F., and Nielsen, F. (August 2017). DeepBach: a Steerable Model for Bach \\nChorales Generation. Proceedings of the 34th International Conference on Machine Learning \\n(ICML)\\n5. Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. URL: http://\\nkarpathy.github.io/2015/05/21/rnn-effectiveness/\\n6. Karpathy, A., Li, F. (2015). Deep Visual-Semantic Alignments for Generating Image Descriptions. \\nConference on Pattern Recognition and Pattern Recognition (CVPR)\\n7. Socher, et al. (2013). Recursive Deep Models for Sentiment Compositionality over a Sentiment \\nTreebank. Proceedings of the 2013 Conference on Empirical Methods in Natural Language \\nProcessing (EMNLP)\\n8. Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to \\nAlign and Translate. arXiv: 1409.0473 [cs.CL]\\n9. Wu, Y., et al. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human \\nand Machine Translation. arXiv 1609.08144 [cs.CL]\\n10. Vinyals, O., et al. (2015). Grammar as a Foreign Language. Advances in Neural Information \\nProcessing Systems (NIPS)\\n11. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). Learning Internal Representations \\nby Error Propagation. Parallel Distributed Processing: Explorations in the Microstructure of \\nCognition\\n12. Britz, D. (2015). Recurrent Neural Networks Tutorial, Part 3 - Backpropagation Through Time \\nand Vanishing Gradients: http://www.wildml.com/2015/10/recurrent-neural-networks-\\ntutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\\n13. Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training Recurrent Neural \\nNetworks. Proceedings of the 30th International Conference on Machine Learning (ICML)\\n14. Hochreiter, S., and Schmidhuber, J. (1997). LSTM can solve hard long time lag problems. Advances \\nin Neural Information Processing Systems (NIPS)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7855ac4-4245-4476-8c90-1a405f5a80dc', embedding=None, metadata={'page_label': '191', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 191\\n15. Britz, D. (2015). Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with \\nPython and Theano: http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-\\npart-4-implementing-a-grulstm-rnn-with-python-and-theano/\\n16. Olah, C. (2015). Understanding LSTM Networks: https://colah.github.io/posts/2015-08-\\nUnderstanding-LSTMs/\\n17. Cho, K., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical \\nMachine Translation. arXiv: 1406.1078 [cs.CL]\\n18. Shi, X., et al. (2015). Convolutional LSTM Network: A Machine Learning Approach for Precipitation \\nNowcasting. arXiv: 1506.04214 [cs.CV]\\n19. Gers, F.A., and Schmidhuber, J. (2000). Recurrent Nets that Time and Count. Proceedings of the \\nIEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN)\\n20. Kotzias, D. (2015). Sentiment Labeled Sentences Dataset, provided as part of “From Group to \\nIndividual Labels using Deep Features” (KDD 2015): https://archive.ics.uci.edu/ml/\\ndatasets/Sentiment+Labelled+Sentences\\n21. Collobert, R., et al (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine \\nLearning Research (JMLR)\\n22. Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpus \\nof English: the Penn Treebank. Journal of Computational Linguistics\\n23. Bird, S., Loper, E., and Klein, E. (2009). Natural Language Processing with Python, O’Reilly Media \\nInc. Installation: https://www.nltk.org/install.html\\n24. Liu, C., et al. (2017). MAT: A Multimodal Attentive Translator for Image Captioning . arXiv: \\n1702.05658v3 [cs.CV]\\n25. Suilin, A. (2017). Kaggle Web Traffic Time Series Forecasting. GitHub repository: https://github.\\ncom/Arturus/kaggle-web-traffic\\n26. Tatoeba Project. (1997-2019). Tab-delimited Bilingual Sentence Pairs: http://tatoeba.org \\nand http://www.manythings.org/anki\\n27. Graves, A., Wayne, G., and Danihelka, I. (2014). Neural Turing Machines. arXiv: 1410.5401v2 \\n[cs.NE]\\n28. Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural Machine Translation by jointly learning to \\nAlign and Translate. arXiv: 1409.0473v7 [cs.CL]\\n29. Luong, M., Pham, H., and Manning, C. (2015). Effective Approaches to Attention-based Neural \\nMachine Translation. arXiv: 1508.04025v5 [cs.CL]\\n30. Vaswani, A., et al. (2017). Attention Is All You Need . 31st Conference on Neural Information \\nProcessing Systems (NeurIPS)\\n31. Zhang, A., Lipton, Z. C., Li, M., and Smola, A. J. (2019). Dive into Deep Learning: http://www.\\nd2l.ai\\n32. Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer Normalization . arXiv: 1607.06450v1 [stat.ML]\\n33. Allamar, J. (2018). The Illustrated Transformer: http://jalammar.github.io/illustrated-\\ntransformer/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73da7dfe-8d6a-4243-aa6b-34be6c1abbe9', embedding=None, metadata={'page_label': '192', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 192\\n34. Honnibal, M. (2016). Embed, encode, attend, predict: The new deep learning formula for state-of-\\nthe-art NLP models: https://explosion.ai/blog/deep-learning-formula-nlp\\n35. Papineni, K., Roukos, S., Ward, T., and Zhu, W . (2002). BLEU: A Method for Automatic Evaluation \\nof Machine Translation. Proceedings of the 40th Annual Meeting for the Association of \\nComputational Linguistics (ACL)\\n36. Project Gutenberg (2019): https://www.gutenberg.org/\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce778ad9-0aa7-400b-911b-747ce8e467dc', embedding=None, metadata={'page_label': '193', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6\\nTransformers\\nThe transformer-based architectures have become almost universal in Natural Language Processing \\n(NLP ) (and beyond) when it comes to solving a wide variety of tasks, such as:\\n• Neural machine translation\\n• Text summarization\\n• Text generation\\n• Named entity recognition\\n• Question answering\\n• Text classification\\n• Text similarity\\n• Offensive message/profanity detection\\n• Query understanding \\n• Language modeling\\n• Next-sentence prediction\\n• Reading comprehension\\n• Sentiment analysis\\n• Paraphrasing\\nand a lot more.\\nIn less than four years, when the Attention Is All You Need paper was published by Google Research \\nin 2017, transformers managed to take the NLP community by storm, breaking any record achieved \\nover the previous thirty years.\\nTransformer-based models use the so-called attention mechanisms that identify complex relationships \\nbetween words in each input sequence, such as a sentence. Attention helped resolve the challenge of \\nencoding “pairwise correlations”—something that its “predecessors,” such as LSTM RNNs and even \\nCNNS, couldn’t achieve when modeling sequential data, such as text.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccd35f0a-cf56-4b0c-acdf-17c9ec12202f', embedding=None, metadata={'page_label': '194', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 194\\nModels—such as BERT, T5, and GPT (covered in more detail later in this chapter)—now constitute \\nthe state-of-the-art fundamental building blocks for new applications in almost every field, from \\ncomputer vision to speech recognition, translation, or protein and coding sequences. Attention has \\nalso been applied in reinforcement learning for games: in DeepMind’s AlphaStar ( https://rdcu.\\nbe/bVI7G  and https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-\\nii-using-multi-agent-reinforcement-learning ), observations of player and opponent StarCraft  \\ngame units were processed with self-attention, for example. For this reason, Stanford has recently \\nintroduced the term “foundation models” to define a set of Large Language Models (LLMs ) based on \\ngiant pretrained transformers.\\nThis progress has been made thanks to a few simple ideas, which we are going to review in the next \\nfew sections.\\nYou will learn:\\n• What transformers are\\n• How they evolved over time\\n• Some optimization techniques\\n• Dos and don’ts \\n• What the future will look like\\nLet’s start turning our attention to transformers. You will be surprised to discover that attention indeed \\nis all you need!\\nArchitecture\\nEven though a typical transformer architecture is usually different from that of recurrent networks, it \\nis based on several key ideas that originated in RNNs. At the time of writing this book, the transformer \\nrepresents the next evolutionary step of deep learning architectures related to texts and any data that \\ncan be represented as sequences, and as such, it should be an essential part of your toolbox.\\nThe original transformer architecture is a variant of the encoder-decoder architecture, where the \\nrecurrent layers are replaced with (self-)attention layers. The transformer was initially proposed by \\nGoogle in the seminal paper titled Attention Is All You Need by Ashish Vaswani, Noam Shazeer, Niki \\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, 2017, \\nhttps://arxiv.org/abs/1706.03762 , to which a reference implementation was provided, which we \\nwill refer to throughout this discussion.\\nThe architecture is an instance of the encoder-decoder models that have been popular since 2014-\\n2015 (such as Sequence to Sequence Learning with Neural Networks by Sutskever et al. (2014), https://\\narxiv.org/abs/1409.3215 ). Prior to that, attention had been used together with Long-Short-Term \\nMemory (LSTM ) and other RNN (Recurrent Neural Network) models discussed in a previous chapter. \\nAttention was introduced in 2014 in Neural Machine Translation by Jointly Learning to Align and Translate  \\nby Bahdanau et al., https://arxiv.org/abs/1409.0473 , and applied to neural machine translation in \\n2015 in Effective Approaches to Attention-based Neural Machine Translation by Luong et al., https://arxiv.\\norg/abs/1508.04025 , and there have been other combinations of attention with other types of models. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e921bda9-2515-4000-9c8e-690aff5edd62', embedding=None, metadata={'page_label': '195', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 195\\nIn 2017, the first transformer demonstrated that you could remove LSTMs from Neural Machine \\nTranslation ( NMT) models and use the so-called (self-)attention blocks (hence the paper title Attention \\nIs All You Need).\\nKey intuitions\\nLet’s start by defining some concepts that will be useful later on in this chapter. The innovation \\nintroduced with the transformer in 2017 is based on four main key ideas:\\n• Positional encoding\\n• Attention\\n• Self-attention\\n• Multi-head (self-)attention\\nIn the next sections, we will discuss them in greater detail.\\nPositional encoding\\nRNNs keep the word order by processing words sequentially. The advantage of this approach is \\nsimplicity, but one of the disadvantages is that this makes parallelization hard (training on multiple \\nhardware accelerators). If we want to effectively leverage highly parallel architectures, such as GPUs \\nand TPUs, we’d need an alternative way to represent ordering.\\nThe transformer uses a simple alternative order representation called positional encoding, which \\nassociates each word with a number representing its position in the text. For instance:\\n[(\"Transformers\", 1), (\"took\", 2), (\"NLP\", 3), (\"by\", 4), (\"storm\", 5)]\\nThe key intuition is that enriching transformers with a position allows the model to learn the \\nimportance of the position of each token (a word in the text/sentence). Note that positional encoding \\nexisted before transformers (as discussed in the chapter on RNNs), but this intuition is particularly \\nimportant in the context of creating transformer-based models. After (absolute) positional encoding \\nwas introduced in the original transformer paper, there have been other variants, such as relative \\npositional encoding (Self-Attention with Relative Position Representations by Shaw et al., 2018, https://\\narxiv.org/abs/1803.02155 , and rotary positional encoding (RoFormer: Enhanced Transformer with \\nRotary Position Embedding by Su et al., 2021, https://arxiv.org/abs/2104.09864 ).\\nNow that we have defined positional encoding, let’s turn our attention to the attention mechanism.\\nAttention\\nAnother crucial ingredient of the transformer recipe is attention. This mechanism was first introduced \\nin the context of machine translation in 2014 by Bahdanou et al. in Neural Machine Translation by Jointly \\nLearning to Align and Translate by Dzmitry Bahdanau, KyungHyun Cho,  and Yoshua Bengio, https://\\narxiv.org/pdf/1409.0473.pdf . Some research papers also attribute the idea behind attention to \\nAlex Graves’ Generating Sequences with Recurrent Neural Networks, which dates back to 2013, https://\\narxiv.org/pdf/1308.0850.pdf . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef90a56e-813c-4800-ba34-0888424852e3', embedding=None, metadata={'page_label': '196', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 196\\nThis ingredient—this key idea—has since become a part of the title of the first transformer paper, \\nAttention is All You Need. To get a high-level overview, let’s consider this example from the paper that \\nintroduced attention:\\nThe agreement on the European Economic Area was signed in August 1992.\\nIn French, this can be translated as: \\nL’accord sur la zone économique européenne a été signé en août 1992.\\nThe initial attempts to perform automatic machine translation back in the early 80s were based on \\nthe sequential translation of each word. This approach was very limiting because the text structure \\ncan change from a source language to a target language in many ways. For instance, some words in \\nthe French translation can have a different order: in English, adjectives usually precede nouns, like in \\n“European Economic Area,” whereas in French, adjectives can go after nouns—”la zone économique \\neuropéenne.” Moreover, unlike in English, the French language has gendered words. So, for example, \\nthe adjectives “économique” and “européenne” must be in their feminine form as they belong to the \\nfeminine noun “la zone.”\\nThe key intuition behind the attention approach is to build a text model that “looks at” every single \\nword in the source sentence when translating words into the output language. In the original 2017 \\ntransformer paper, the authors point out that the cost of doing this is quadratic, but the gain achieved \\nin terms of more accurate translation is considerable. More recent works reduced this initial quadratic \\ncomplexity, such as the Fast Attention Via positive Orthogonal Random (FAVOR +) features from the \\nRethinking Attention with Performers paper by Choromanski et al. (2020) from Google, DeepMind, the \\nUniversity of Cambridge, and the Alan Turing Institute.\\nLet’s go over a nice example from that original attention paper by Bahdanou et al. (2014):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbbdf7d9-2cfb-4019-8990-745a5bd9c570', embedding=None, metadata={'page_label': '197', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 197\\nFigure 6.1: An example of attention for the English sentence “The agreement on the European \\nEconomic Area was signed in August 1992. ” The plot visualizes “annotation weights”—the weights \\nassociated with the annotations. Source: “Neural Machine Translation by Jointly Learning to Align \\nand Translate” by Bahdanau et al. (2014) (https://arxiv.org/abs/1409.0473)\\nUsing the attention mechanism, the neural network can learn a heatmap of each source English \\nword in relation to each target French word. Note that relationships are not only on the diagonal \\nbut might spread across the whole matrix. For instance, when the model outputs the French word \\n“européenne,” it will pay a lot of attention to the input words “European” and “Economic.” (In Figure \\n6.1, this corresponds to the diagonal and the adjacent cell.) The 2014 attention paper by Bahdanou et \\nal. demonstrated that the model (which used an RNN encoder-decoder framework with attention) can \\nlearn to align and attend to the input elements without supervision, and, as Figure 6.1 shows, translate \\nthe input English sentences into French. And, of course, the larger the training set is, the greater the \\nnumber of correlations that the attention-based model can learn.\\nIn short, the attention mechanism can access all previous words and weigh them according to a learned  \\nmeasure of relevancy. This way, attention can provide relevant information about tokens located far \\naway in the target sentence.\\nNow, we can focus on another key ingredient of the transformer—”self-attention.”', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f31e08a8-5a13-4d85-ac2d-10acd9753820', embedding=None, metadata={'page_label': '198', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 198\\nSelf-attention\\nThe third key idea popularized by the original transformer paper is the use of attention within the \\nsame sentence in the source language—self-attention. With this mechanism, neural networks can be \\ntrained to learn the relationships among all words (or other elements) in each input sequence (such \\nas a sentence) irrespective of their positions before focusing on (machine) translation. Self-attention \\ncan be attributed to the idea from the 2016 paper called Long Short-Term Memory-Networks for Machine \\nReading by Cheng et al., https://arxiv.org/pdf/1601.06733.pdf .\\nLet’s go through an example with the following two sentences:\\n“Server, can I have the check?”\\n“Looks like I just crashed the server.”\\nClearly, the word “server” has a very different meaning in either sentence and self-attention can \\nunderstand each word considering the context of the surrounding words. Just to reiterate, the attention \\nmechanism can access all previous words and weigh them according to a learned measure of relevancy. \\nSelf-attention provides relevant information about tokens located far away in the source sentence.\\nMulti-head (self-)attention\\nThe original transformer performs a (self-)attention function multiple times. A single set of the so-\\ncalled weight matrices (which are covered in detail in the How to compute Attention section) is named \\nan attention head. When you have several sets of these matrices, you have multiple attention heads. \\nThe multi-head (self-)attention layer usually has several parallel (self-)attention layers. Note that \\nthe introduction of multiple heads allows us to have many definitions of which word is “relevant” to \\neach other. Plus, all these definitions of relevance can be computed in parallel by modern hardware \\naccelerators, thus speeding up the computation. \\nNow that we have gone through the high-level definitions of the key ingredients of the transformers, \\nlet’s deep dive into how to compute the attention mechanism.\\nHow to compute attention\\nIn the original transformer, the self-attention function is computed by using the so-called scaled dot-\\nproduct units. The authors of the 2017 paper even called their attention method Scaled Dot-Product \\nAttention. You might remember from high school studies that the dot-product between two vectors \\nprovides a good sense of how “close” the vectors are.\\nEach input token sequence (for example, of a sentence) embedding that passes into the transformer \\n(encoder and/or decoder) produces attention weights (covered in detail below) that are simultaneously \\ncalculated between every sequence element (such as a word). The output results in embeddings \\nproduced for every token containing the token itself together with every other relevant token weighted \\nby its relative attention weight.\\nThe attention layer transforms the input vectors into query, key, and value matrices, which are then \\nsplit into attention heads (hence, multi-head attention):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f7ad299-f33c-4050-afa5-67ecaa8ba4d2', embedding=None, metadata={'page_label': '199', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 199\\n• The query word can be interpreted as the word for which we are calculating the attention \\nfunction.\\n• The key and value words are the words to which we are paying attention.\\nThe dot-product (explained further below) tells us the similarity between words. If the vectors for two \\nwords are more aligned, the attention score will be higher. The transformer will learn the weights in such \\na way that if two words in a sentence are relevant to each other, then their word vectors will be aligned.\\nEach attention layer learns three weight matrices:\\n• The query weights W Q\\n• The key weights W K\\n• The value weights W V\\nFor each word i, an input word embedding x i is computed producing:\\n• A query vector q i = x iWQ\\n• A key vector k i = x iWK\\n• A value vector v i = x iWV\\nGiven the query and the corresponding key vectors, the following dot-product formula produces the \\nattention weight in the original transformer paper:\\n𝑎𝑎𝑖𝑖𝑖𝑖𝑖=𝑞𝑞 𝑖𝑖 . 𝑘𝑘𝑖𝑖 \\nwhere:\\n• ai,j is the attention from word i to a word j.\\n• . is the dot-product of the query with keys, which will give a sense of how “close” the vectors are.\\nNote that the attention unit for word i is the weighted sum of the value vectors of all words, weighted \\nby a i,j, the attention from word i to a word j.\\nNow, to stabilize gradients during the training, the attention weights are divided by the square root \\nof the dimension of the key vectors √𝑑𝑑𝑘𝑘 .\\nThen, the results are passed through a softmax function to normalize the weight. Note that the attention \\nfunction from a word i to a word j is not the same as the attention from word j to a word i.\\nNote that since modern deep learning accelerators work well with matrices, we can compute attention \\nfor all words using large matrices.\\nDefine qi, ki, vi (where i is the ith row) as matrices Q, K, V, respectively. Then, we can summarize the \\nattention function as an attention matrix:\\n𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴 𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝑄𝑄𝑄𝑄𝑇𝑇\\n√𝑑𝑑𝑘𝑘𝐴𝐴 \\nIn this section, we discussed how to compute the attention function introduced in the original \\ntransformer paper. Next, let’s discuss the encoder-decoder architecture.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35787143-1da4-4d39-8266-84c22d371b52', embedding=None, metadata={'page_label': '200', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 200\\nEncoder-decoder architecture\\nSimilar to the seq2seq models, (Sequence to Sequence Learning with Neural Networks by Ilya Sutskever, \\nOriol Vinyals, Quoc V . Le (2014)) described in Chapter 5, Recurrent Neural Networks, the original  \\ntransformer model also used an encoder-decoder architecture:\\n• The encoder takes the input (source) sequence of embeddings and transforms it into a new \\nfixed-length vector of input embeddings.\\n• The decoder takes the output embeddings vector from the encoder and transforms it into a \\nsequence of output embeddings.\\n• Both the encoder and the decoder consist of several stacked layers. Each encoder and decoder \\nlayer is using the attention mechanism described earlier.\\nWe’ll learn about the transformer architecture in much more detail later in this section.\\nSince the introduction of the transformer architecture, other newer networks have used only the \\nencoder or the decoder components (or both), which are discussed in the Categories of transformers \\nsection of this chapter.\\nNext, let’s briefly go over the other components of the original transformer—the residual and \\nnormalization layers.\\nResidual and normalization layers\\nTypically, transformer-based networks reuse other existing state-of-the-art machine learning \\nmethodologies, such as attention mechanisms. You shall therefore not be surprised if both encoder \\nand decoder layers combine neural networks with residual connections (Deep Residual Learning for \\nImage Recognition by He et al., 2016, https://arxiv.org/abs/1512.03385 ) and normalization steps \\n(Layer Normalization by Ba et al., 2016, https:/arxiv.org/abs/1607.06450 ).\\nOK, we now have all the key ingredients to deep dive into transformers.\\nAn overview of the transformer architecture\\nNow that we have covered some of the key concepts behind the original transformer, let’s deep dive into \\nthe architecture introduced in the seminal 2017 paper. Note that transformer-based models are usually \\nbuilt by leveraging various attention mechanisms without using RNNs. This is also a consequence of \\nthe fact that attention mechanisms themselves can match and outperform RNN (encoder-decoder) \\nmodels with attention. That’s why the seminal paper was titled Attention is all You Need.\\nFigure 6.2  shows a seq2seq network with RNNs and attention, and compares it to the original transformer \\nnetwork.\\nThe transformer is similar to seq2seq with an attention model in the following ways:\\n• Both approaches work with source (inputs) and target (output) sequences.\\n• Both use an encoder-decoder architecture, as mentioned before.\\n• The output of the last block of the encoder is used as a context—or thought vector—for computing \\nthe attention function in the decoder.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='31036f76-924b-4dce-9eec-532952bbb020', embedding=None, metadata={'page_label': '201', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 201\\n• The target (output) sequence embeddings are fed into dense (fully connected) blocks, which \\nconvert the output embeddings to the final sequence of an integer form:\\nFigure 6.2: Flow of data in (a) seq2seq + Attention, and (b) Transformer architecture. Image \\nSource: Zhang, et al.\\nAnd the two architectures differ in the following ways:\\n• The seq2seq network uses the recurrent and attention layers in the encoder, and the recurrent \\nlayer in the decoder.\\nThe transformer replaced those layers with so-called transformer blocks (a stack of N identical \\nlayers), as Figure 6.2 demonstrates:\\n• In the encoder, the transformer block consists of a sequence of sub-layers: a multi-head \\n(self-)attention layer and a position-wise feedforward layer. Each of those two layers \\nhas a residual connection, followed by a normalization layer.\\n• In the decoder, the transformer block contains a variant of a multi-head (self-)attention \\nlayer with masking—a masked multi-head self-attention—and a feedforward layer like \\nin the encoder (with identical residual connections and normalization layers). Masking \\nhelps prevent positions from attending into the future. Additionally, the decoder \\ncontains a second multi-head (self-)attention layer which computes attention over \\nthe outputs of the encoder’s transformer blockmasking is covered in more detail later \\nin this section.)\\n• In the seq2seq with attention network, the encoder state is passed to the first recurrent time \\nstep as with the seq2seq with attention network.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b610e5bc-a109-4e62-bdd9-46d85c3d5aea', embedding=None, metadata={'page_label': '202', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 202\\nIn the transformer, the encoder state is passed to every transformer block in the decoder. This \\nallows the transformer network to work in parallel across time steps since there is no longer \\na temporal dependency as with the seq2seq networks.\\nThe last decoder is followed by a final linear transformation (a dense layer) with a softmax \\nfunction to produce the output (next-token) probabilities.\\n• Because of the parallelism referred to in the previous point, an encoding layer is added to \\nprovide positional information to distinguish the position of each element in the transformer \\nnetwork sequence (positional encoding layer). This way, the first encoder takes the positional \\ninformation and embeddings of the input sequence as inputs, rather than only encodings, thus \\nallowing taking positional information into account.\\nLet’s walk through the process of data flowing through the transformer network. Later in this chapter, \\nwe will use TensorFlow with the Keras API to create and train a transformer model from scratch:\\n1. As part of data preprocessing, the inputs and the outputs are tokenized and converted to \\nembeddings.\\n2. Next, positional encoding is applied to the input and output embeddings to have information \\nabout the relative position of tokens in the sequences. In the encoder section:\\n• As per Figure 6.2, the encoder side consists of an embedding and a positional encoding \\nlayer, followed by six identical transformer blocks (there were six “layers” in the original \\ntransformer). As we learned earlier, each transformer block in the encoder consists of \\na multi-head (self-)attention layer and a position-wise feedforward layer.\\nWe have already briefly seen that self-attention is the process of attending to parts of the same \\nsequence. When we process a sentence, we might want to know what other words are most \\naligned with the current one.\\n• The multi-head attention layer consists of multiple (eight in the reference implementation \\ncontained in the seminal paper) parallel self-attention layers. Self-attention is carried \\nout by constructing three vectors Q  (query), K  (key), and V  (value), out of the input \\nembedding. These vectors are created by multiplying the input embedding with three \\ntrainable weight matrices W Q, WK, and W V. The output vector Z is created by combining \\nK, Q, and V at each self-attention layer using the following formula. Here, d K refers to \\nthe dimension of the K , Q, and V  vectors (64 in the reference implementation contained \\nin the seminal paper):\\n𝑧𝑧 𝑧 𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑄𝑄𝑄𝑄𝑇𝑇\\n√𝑑𝑑𝑘𝑘)𝑉𝑉 \\n• The multi-head attention layer will create multiple values for Z  (based on multiple \\ntrainable weight matrices WQ, WK, and W V at each self-attention layer), and then \\nconcatenate them for inputs into the position-wise feedforward layer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a00c42b2-d64e-4f1b-b195-e65d9792d26d', embedding=None, metadata={'page_label': '203', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 203\\n• The inputs to the position-wise feedforward layer consist of embeddings for the different \\nelements in the sequence (or words in the sentence), attended via self-attention in \\nthe multi-head attention layer. Each token is represented internally by a fixed-length \\nembedding vector (512 in the reference implementation introduced in the seminal \\npaper). Each vector is run through the feedforward layer in parallel. The outputs of \\nthe FFN are the inputs to (or fed into) the multi-head attention layer in the following \\ntransformer block. In the last transformer block of the encoder, the outputs are the \\ncontext vector that is passed to the decoder.\\n• Both the multi-head attention and position-wise FFN layers send out not only the signal \\nfrom the previous layer but also a residual signal from their inputs to their outputs. The \\noutputs and residual inputs are passed through a layer-normalization step, and this is \\nshown in Figure 6.2 as the “Add & Norm” layer.\\n• Since the entire sequence is consumed in parallel on the encoder, the information \\nabout the positions of individual elements gets lost. To compensate for this, the input \\nembeddings are augmented with a positional embedding, which is implemented as a \\nsinusoidal function without learned parameters. The positional embedding is added \\nto the input embedding.\\n3. Next, let’s walk through how the data flows through the decoder:\\n• The output of the encoder results in a pair of attention vectors K and V, which are sent \\nin parallel to all the transformer blocks in the decoder. The transformer block in the \\ndecoder is similar to that in the encoder, except that it has an additional multi-head \\nattention layer to handle the attention vectors from the encoder. This additional multi-\\nhead attention layer works similarly to the one in the encoder and the one below it, \\nexcept that it combines the Q vector from the layer below it and the K and Q vectors \\nfrom the encoder state.\\n• Similar to the seq2seq network, the output sequence generates one token at a time, \\nusing the input from the previous time step. As for the input to the encoder, the input \\nto the decoder is also augmented with a positional embedding. Unlike the encoder, the \\nself-attention process in the decoder is only allowed to attend to tokens at previous \\ntime points. This is done by masking out tokens at future time points.\\n• The output of the last transformer block in the decoder is a sequence of low-dimensional \\nembeddings (512 for reference implementation in the seminal paper as noted earlier). \\nThis is passed to the dense layer, which converts it into a sequence of probability \\ndistributions across the target vocabulary, from which we generate the most probable \\nword either greedily or by a more sophisticated technique such as beam search.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd8d9338-732f-4dcd-a7db-42ab28943c91', embedding=None, metadata={'page_label': '204', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 204\\nFigure 6.3 shows the transformer architecture covering everything that’s just been described:\\nFigure 6.3: The transformer architecture based on original images from “Attention Is All You Need” \\nby Vaswani et al. (2017)\\nTraining\\nTransformers are typically trained via semi-supervised learning in two steps:\\n1. First, an unsupervised pretraining, typically on a very large corpus.\\n2. Then, a supervised fine-tuning on a smaller labeled dataset.\\nBoth pretraining and fine-tuning might require significant resources in terms of GPU/TPU, memory, \\nand time. This is especially true, considering that large language models (in short, LLMs) have an \\nincreasing number of parameters as we will see in the next section.\\nSometimes, the second phase has a very limited set of labeled data. This is the so-called few-shot \\nlearning, which considers making predictions based on a limited number of samples.\\nTransformers’ architectures\\nIn this section, we have provided a high-level overview of both the most important architectures used \\nby transformers and of the different ways used to compute attention.\\nCategories of transformers\\nIn this section, we are going to classify transformers into different categories. The next paragraph will \\nintroduce the most common transformers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36d7aa79-c0e8-4aba-9119-5e6c574652fe', embedding=None, metadata={'page_label': '205', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 205\\nDecoder or autoregressive\\nA typical example is a GPT  (Generative Pre-Trained) model, which you can learn more about in the \\nGPT-2 and GPT-3 sections later in this chapter, or refer to https://openai.com/blog/language-\\nunsupervised ). Autoregressive models use only the decoder of the original transformer model, with \\nthe attention heads that can only see what is before in the text and not after with a masking mechanism \\nused on the full sentence. Autoregressive models use pretraining to guess the next token after observing \\nall the previous ones. Typically, autoregressive models are used for Natural Language Generation  \\n(NLG ) text generation tasks. Other examples of autoregressive models include the original GPT, GPT-2, \\nTransformer-XL, Reformer, and XLNet, which are covered later in this chapter.\\nEncoder or autoencoding\\nA typical example is BERT  (Bidirectional Encoder Representations from Transformers), which is \\ncovered later in this chapter. Autoencoders correspond to the encoder in the original transformer \\nmodel having access to the full input tokens with no masks. Autoencoding models use pretraining by \\nmasking/altering the input tokens and then trying to reconstruct the original sentence. Frequently, \\nthe models build a bidirectional representation of the full sentences. Note that the only difference \\nbetween autoencoders and autoregressive is the pretraining phase, so the same architecture can be \\nused in both ways. Autoencoders can be used for NLG, as well as for classification and many other \\nNLP tasks. Other examples of autoencoding models, apart from BERT, include ALBERT, RoBERTa, and \\nELECTRA, which you can learn about later in this chapter.\\nSeq2seq\\nA typical example is T5 (Text-to-Text Transfer Transformer) and the original transformer. Sequence-\\nto-sequence models use both the encoder and the decoder of the original transformer architecture. \\nSeq2seq can be fine-tuned to many tasks such as translation, summarization, ranking, and question \\nanswering. Another example of a seq2seq model, apart from the original transformer and T5, is \\nMultitask Unified Model (MUM ).\\nMultimodal\\nA typical example is MUM. Multimodal models mix text inputs with other kinds of content (for example, \\nimages, videos, and audio).\\nRetrieval\\nA typical example is RETRO. Some models use document retrieval during (pre)training and inference. \\nThis is frequently a good strategy to reduce the size of the model and rapidly access memorized \\ninformation saving on the number of used parameters.\\nAttention\\nNow that we have understood how to classify transformers, let’s focus on attention!\\nThere is a wide variety of attention mechanisms, such as self-attention, local/hard attention, and \\nglobal/soft attention, to name a few. Below, we’ll focus on some of the examples.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f118748-4daf-4962-af05-3279199c9487', embedding=None, metadata={'page_label': '206', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 206\\nFull versus sparse\\nAs discussed, the (scaled) dot-product attention from the original 2017 transformer paper is typically \\ncomputed on a full squared matrix O(L2) where L is the length of the maximal considered sequence \\n(in some configurations L = 512). The BigBird type of transformer, proposed by Google Research in \\n2020 and discussed in more detail later in this chapter, introduced the idea of using sparse attention by \\nleveraging sparse matrices (based on the 2019 work by OpenAI’s Generating long sequences with sparse \\ntransformers by Child et al., https://arxiv.org/abs/1904.10509 ).\\nLSH attention\\nThe Reformer introduced the idea of reducing the attention mechanism complexity with hashing—the \\nmodel’s authors called it locality-sensitive hashing attention. The approach is based on the notion of \\nusing only the largest elements when softmax(QKT) is computed. In other words, for each query 𝑞𝑞𝑞𝑞𝑞   \\nonly the keys 𝑘𝑘𝑘𝑘𝑘   that are close to q are computed. For computing closeness, several hash functions \\nare computed according to local sensitive hashing techniques.\\nLocal attention\\nSome transformers adopted the idea of having only a local window of context (e.g. a few tokens on \\nthe right and a few tokens on the left). The idea is that using fewer parameters allows us to consider \\nlonger sequences but with a limited degree of attention. For this reason, local attention is less popular.\\nPretraining\\nAs you have learned earlier, the original transformer had an encoder-decoder architecture. However, \\nthe research community understood that there are situations where it is beneficial to have only the \\nencoder, or only the decoder, or both.\\nEncoder pretraining\\nAs discussed, these models are also called auto-encoding and they use only the encoder during the \\npretraining. Pretraining is carried out by masking words in the input sequence and training the model \\nto reconstruct the sequence. Typically, the encoder can access all the input words. Encoder-only \\nmodels are generally used for classification.\\nDecoder pretraining\\nDecoder models are referred to as autoregressive. During pretraining, the decoder is optimized to \\npredict the next word. In particular, the decoder can only access all the words positioned before a \\ngiven word in the sequence. Decoder-only models are generally used for text generation.\\nEncoder-decoder pretraining\\nIn this case, the model can use both the encoder and the decoder. Attention in the encoder can use all \\nthe words in the sequence, while attention in the decoder can only use the words preceding a given \\nword in the sequence. Encoder-decoder has a large range of applications including text generation, \\ntranslation, summarization, and generative question answer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3aebd4b4-6394-45fa-9803-5336634f774a', embedding=None, metadata={'page_label': '207', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 207\\nA taxonomy for pretraining tasks\\nIt can be useful to organize pretraining into a taxonomy suggested by Pre-trained Models for Natural \\nLanguage Processing: A Survey, Xipeng Qiu, 2020, https://arxiv.org/abs/2003.08271 :\\n• Language Modeling ( LM): For unidirectional LM, the task is to predict the next token. For \\nbidirectional LM, the task is to predict the previous and next tokens.\\n• Masked Language Modeling (MLM ): The key idea is to mask out some tokens from the input \\nsentences. Then, the model is trained to predict the masked tokens given the non-masked \\ntokens.\\n• Permuted Language Modeling ( PLM ): This is similar to LM, but a random permutation of \\ninput sequences is performed. Then, a subset of tokens is chosen as the target, and the model \\nis trained to predict these targets.\\n• Denoising Autoencoder (DAE ): Deliberately provide partially corrupted input. For instance, \\nrandomly sample input tokens and replace them with special [MASK] elements. Alternatively, \\nrandomly delete input tokens. Alternatively, shuffle sentences in random order. The task is to \\nrecover the original undistorted input.\\n• Contrastive Learning (CTL): The task is to learn a score function for text pairs by assuming \\nthat some observed pairs of text are more semantically similar than randomly sampled text. \\nThis class of techniques includes a number of specific techniques such as: \\n• Deep InfoMax ( DIM ): Maximize mutual information between an input image \\nrepresentation and various local regions of the same image.\\n• Replaced Token Detection (RTD ): Predict whether an input token is replaced given its \\nsurroundings.\\n• Next Sentence Prediction ( NSP ): The model is trained to distinguish whether two input \\nsentences are contiguous in the training corpus.\\n• Sentence Order Prediction ( SOP ): The same ideas as NSP with some additional signals: \\ntwo consecutive segments are positive examples, and two swapped segments are \\nnegative examples.\\nIn this section, we have briefly reviewed different pretraining techniques. The next section will review \\na selection of the most used transformers.\\nAn overview of popular and well-known models\\nAfter the seminal paper Attention is All You Need, a very large number of alternative transformer-based \\nmodels have been proposed. Let’s review some of the most popular and well-known ones.\\nBERT\\nBERT, or Bidirectional Encoder Representations from Transformers, is a language representation model \\ndeveloped by the Google AI research team in 2018. Let’s go over the main intuition behind that model:\\n1. BERT considers the context of each word from both the left and the right side using the so-\\ncalled “bidirectional self-attention.”', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65305363-f9e5-4cfd-85ab-ba912f7b37ba', embedding=None, metadata={'page_label': '208', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 208\\n2. Training happens by randomly masking the input word tokens, and avoiding cycles so that \\nwords cannot see themselves indirectly. In NLP jargon, this is called “fill in the blank.” In other \\nwords, the pretraining task involves masking a small subset of unlabeled inputs and then \\ntraining the network to recover these original inputs. (This is an example of MLM.) \\n3. The model uses classification for pretraining to predict whether a sentence sequence S is before \\na sentence T. This way, BERT can understand relationships among sentences (“Next Sentence \\nPrediction”), such as “does sentence T come after sentence S?” The idea of pretraining became \\na new standard for LLM.\\n4. BERT—namely BERT Large—became one of the first large language models with 24 transformer \\nblocks, 1024-hidden layers, 16 self-attention heads, and 340M parameters. The model is trained \\non a large 3.3 billion words corpus.\\nBERT produced state-of-the-art results for 11 NLP tasks, including:\\n• GLUE score of 80.4%, a 7.6% absolute improvement from the previous best result.\\n• 93.2% accuracy on SQuAD 1.1 and outperforming human performance by 2%.\\nWe will see GLUE and SQuAD metrics later in this chapter. If you want to know more, you can explore \\nthe following material:\\n• The original research paper: BERT: Pre-training of Deep Bidirectional Transformers for Language \\nUnderstanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, https://\\narxiv.org/abs/1810.04805 .\\n• The Google AI blog post: Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language \\nProcessing, 2018, which discusses the advancement of the (then) state-of-the-art model for 11 \\nNLP tasks ( https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.\\nhtml .)\\n• An open source TensorFlow implementation and the pretrained BERT models are available at \\nhttp://goo.gl/language/bert  and from TensorFlow Model Garden at https://github.com/\\ntensorflow/models/tree/master/official/nlp/modeling/models . \\n• A Colab notebook for BERT is available here: https://colab.research.google.com/github/\\ntensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb .\\n• BERT FineTuning with Cloud TPU: A tutorial that shows how to train the BERT model on Cloud \\nTPU for sentence and sentence-pair classification tasks: https://cloud.google.com/tpu/\\ndocs/tutorials/bert .\\n• A Google blog post about applying BERT to Google Search to improve language understanding. \\nAccording to Google, BERT “will help Search better understand one in 10 searches in the U.S. in \\nEnglish.” Moreover, the post mentions that “A powerful characteristic of these systems is that they \\ncan take learnings from one language and apply them to others. So we can take models that learn from \\nimprovements in English (a language where the vast majority of web content exists) and apply them \\nto other languages.” (from Understanding search better than ever before): https://blog.google/\\nproducts/search/search-language-understanding-bert/ .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6b424c5-c762-4bc6-88e7-b576325c6dd1', embedding=None, metadata={'page_label': '209', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 209\\nGPT-2\\nGPT-2 is a model introduced by OpenAI in Language Models Are Unsupervised Multitask Learners by Alec \\nRadford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever, https://openai.com/\\nblog/better-language-models/ , https://openai.com/blog/gpt-2-6-month-follow-up/ , https://\\nwww.openai.com/blog/gpt-2-1-5b-release/ , and https://github.com/openai/gpt-2 .)\\nLet’s review the key intuitions:\\n• The largest of four model sizes was a 1.5 billion-parameter transformer with 48 layers trained \\non a new dataset called Webtext containing text from 45 million webpages.\\n• GPT-2 used the original 2017 transformer-based architecture and a modified version of the \\noriginal GPT model (also developed by OpenAI) by Radford et al., 2018, Improving Language \\nUnderstanding by Generative Pre-Training, https://openai.com/blog/language-unsupervised/ , \\nand https://cdn.openai.com/research-covers/language-unsupervised/language_\\nunderstanding_paper.pdf ). \\n• The research demonstrated that an LLM trained on a large and diverse dataset can perform \\nwell on a wide variety of NLP tasks, such as question answering, machine translation, reading \\ncomprehension, and summarization. Previously, the tasks had been typically approached with \\nsupervised learning on task-specific datasets. GPT-2 was trained in an unsupervised manner \\nand performed well at zero-shot task transfer.\\n• Initially, OpenAI released only a smaller version of GPT-2 with 117 M parameters, “due to \\nconcerns about large language models being used to generate deceptive, biased, or abusive \\nlanguage at scale.” Then, the model was released: https://openai.com/blog/gpt-2-1-5b-\\nrelease/ .\\n• Interestingly enough, OpenAI developed an ML-based detection method to test whether an \\nactor is generating synthetic texts for propaganda. The detection rates were ~95% for detecting \\n1.5B GPT-2-generated text: https://github.com/openai/gpt-2-output-dataset .\\nSimilar to the original GPT from 2018, GPT-2 does not require the encoder part of the original \\ntransformer model – it uses a multi-layer decoder for language modeling. The decoder can only get \\ninformation from the prior words in the sentence. It takes word vectors as input and produces estimates \\nfor the probability of the next word as output, but it is autoregressive, meaning that each token in the \\nsentence relies on the context of the previous words. On the other hand, BERT is not autoregressive, \\nas it uses the entire surrounding context all at once.\\nGPT-2 was the first LLM showing commonsense reasoning, capable of performing a number of NLP \\ntasks including translation, question answering, and reading comprehension. The model achieved   \\nstate-of-the-art results on 7 out of 8 tested language modeling datasets.\\nGPT-3\\nGPT-3 is an autoregressive language model developed by OpenAI and introduced in 2019 in Language \\nModels are Few-Shot Learners by Tom B. Brown, et al., https://arxiv.org/abs/2005.14165 . Let’s look \\nat the key intuitions:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c247256-6cdb-4566-8ce4-e8955d4e7bdb', embedding=None, metadata={'page_label': '210', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 210\\n• GPT-3 uses an architecture and model similar to GPT-2 with a major difference consisting of \\nthe adoption of a sparse attention mechanism.\\n• For each task, the model evaluation has three different approaches:\\n• Few-shot learning: The model receives a few demonstrations of the task (typically, less \\nthan one hundred) at inference time. However, no weight updates are allowed.\\n• One-shot learning: The model receives only one demonstration and a natural language \\ndescription of the task.\\n• Zero-shot learning: The model receives no demonstration, but it has access only to a \\nnatural language description of the task.\\n• For all tasks, GPT-3 is applied without any gradient updates, complete with tasks and few-shot \\ndemonstrations specified purely via text interaction with the model.\\nThe number of parameters the researchers trained GPT-3 with ranges from 125 million (GPT-3 Small) \\nto 175 billion (GPT-3 175B). With no fine-tuning, the model achieves significant results on many NLP \\ntasks including translation and question answering, sometimes surpassing state-of-the-art models. In \\nparticular, GPT-3 showed impressive results in NLG, creating news articles that were hard to distinguish \\nfrom real ones. The model demonstrated it was able to solve tasks requiring on-the-fly reasoning or \\ndomain adaptation, such as unscrambling words, using a novel word in a sentence, or performing \\n3-digit arithmetic.\\nGPT-3’s underlying model is not publicly available and we can’t pretrain the model, but some datasets \\nstatistics are available at https://github.com/openai/gpt-3  and we can run data on and fine-tune \\nGPT-3 engines.\\nReformer\\nThe Reformer model was introduced in the 2020 paper Reformer: The Efficient Transformer  by UC Berkeley \\nand Google AI researchers Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya, https://arxiv.org/\\nabs/2001.04451 .\\nLet’s look at the key intuitions:\\n• The authors demonstrated you can train the Reformer model, which performs on par with \\ntransformer models in a more memory-efficient and faster way on long sequences.\\n• One limitation of transformers is the capacity of dealing with long sequences, due to the \\nquadratic time needed for computing attention.\\n• Reformer addresses the computations and memory challenges during the training of \\ntransformers by using three techniques.\\n• First, Reformer replaced the (scaled) dot-product attention with an approximation using \\nlocality-sensitive hashing attention (described briefly earlier in this chapter). The paper’s \\nauthors changed the former’s O (L2) factor in attention layers with O (LlogL), where L  is the \\nlength of the sequence (see Figure 6.4 where LSH is applied to chunks in sequence). Refer to \\nlocal sensitive hashing introduced in computer science to learn more: https://en.wikipedia.\\norg/wiki/Locality-sensitive_hashing .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c68bff8-2a6f-4480-b0f5-9638bf361bbf', embedding=None, metadata={'page_label': '211', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 211\\n• Second, the model combined the attention and feedforward layers with reversible residual \\nlayers instead of normal residual layers (based on the idea from The reversible residual network: \\nBackpropagation without storing activations by Gomez et al., 2017, https://proceedings.neurips.\\ncc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html ). Reversible \\nresidual layers allow for storage activations once instead of N times, thus reducing the cost in \\nterms of memory and time complexity.\\n• Third, Reformer used a chunking technique for certain computations, including one for the \\nfeedforward layer and for a backward pass.\\n• You can read the Google AI blog post to learn more about how the Reformer reached efficiency \\nat https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html :\\nFigure 6.4: Local Sensitive Hashing to improve the transformers’ efficiency – source: https://\\nai.googleblog.com/2020/01/reformer-efficient-transformer.html\\nBigBird\\nBigBird is another type of transformer introduced in 2020 by Google Research that uses a sparse \\nattention mechanism for tackling the quadratic complexity needed to compute full attention for long \\nsequences. For a deeper overview, see the paper Big Bird: Transformers for Longer Sequences by Manzil \\nZaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, \\nAnirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed, https://arxiv.org/pdf/2007.14062.pdf .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f01c7d8-5209-4c6c-8b84-92cbcbf4a570', embedding=None, metadata={'page_label': '212', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 212\\nLet’s look at the key intuitions:\\n• The authors demonstrated that BigBird was capable of handling longer context—much longer \\nsequences of up to 8x with BERT on similar hardware. Its performance was “drastically” better \\non certain NLP tasks, such as question answering and document summarization.\\n• BigBird runs on a sparse attention mechanism for overcoming the quadratic dependency of \\nBERT. Researchers proved that the complexity reduced from O(L2) to O(L).\\n• This way, BigBird can process sequences of length up to 8x more than what was possible with \\nBERT. In other words, BERT’s limit was 512 tokens and BigBird increased to 4,096 tokens.\\nTransformer-XL\\nTransformer-XL is a self-attention-based model introduced in 2019 by Carnegie Mellon University \\nand Google Brain researchers in the paper Transformer-XL: Attentive Language Models Beyond a Fixed-\\nLength Context by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan \\nSalakhutdinov, https://aclanthology.org/P19-1285.pdf .\\nLet’s look at the key intuitions:\\n• Unlike the original transformer and RNNs, Transformer-XL demonstrated it can model longer-\\nterm dependency beyond a fixed-length context while generating relatively coherent text.\\n• Transformer-XL introduced a new segment-level recurrence mechanism and a new type \\nof relative positional encodings (as opposed to absolute ones), allowing the model to learn \\ndependencies that are 80% longer than RNNs and 450% longer than vanilla transformers. \\nTraditionally, transformers split the entire corpus into shorter segments due to computational \\nlimits and only train the model within each segment.\\n• During training, the hidden state sequence computed for the previous segment is fixed and \\ncached to be reused as an extended context when the model processes the following new \\nsegment, as shown in Figure 6.5. Although the gradient still remains within a segment, this \\nadditional input allows the network to exploit information in history, leading to an ability of \\nmodeling longer-term dependency and avoiding context fragmentation.\\n• During evaluation, the representations from the previous segments can be reused instead of \\nbeing computed from scratch as in the vanilla model case. This way, Transformer-XL proved \\nto be up to 1,800+ times faster than the vanilla model during evaluation:\\nFigure 6.5: Transformer-XL and the input with recurrent caching of previous segments', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5e93475-de93-4883-a9e0-9b77fd72781d', embedding=None, metadata={'page_label': '213', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 213\\nXLNet\\nXLNet is an unsupervised language representation learning method developed by Carnegie Mellon \\nUniversity and Google Brain researchers in 2019. It is based on generalized permutation language \\nmodeling objectives. XLNet employs Transformer-XL as a backbone model. The reference paper here \\nis XLNet: Generalized Autoregressive Pre-training for Language Understanding by  Zhilin Yang, Zihang \\nDai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le, https://arxiv.org/\\nabs/1906.08237 . \\nLet’s see the key intuitions:\\n• Like BERT, XLNet uses a bidirectional context, looking at the words before and after a given \\ntoken to predict what it should be.\\n• XLNet maximizes the expected log-likelihood of a sequence with respect to all possible \\npermutations of the factorization order. Thanks to the permutation operation, the context for \\neach position can consist of tokens from both left and right. In other words, XLNet captures \\nbidirectional context.\\n• XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.\\n• Code and pretrained models are available here: https://github.com/zihangdai/xlnet .\\nXLNet is considered better than BERT in almost all NLP tasks, outperforming BERT on 20 tasks, often \\nby a large margin. When it was introduced, the model achieved state-of-the-art performance on 18 NLP \\ntasks, including sentiment analysis, natural language inference, question answering, and document \\nranking.\\nRoBERTa\\nRoBERTa (a Robustly Optimized BERT) is a model introduced in 2019 by researchers at the University \\nof Washington and Facebook AI (Meta) in RoBERTa: A Robustly Optimized BERT Pretraining Approach \\nby Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, \\nLuke Zettlemoyer, and Veselin Stoyanov, https://arxiv.org/abs/1907.11692 .\\nLet’s look at the key intuitions:\\n• When replicating BERT, the researchers discovered that BERT was “significantly undertrained.”\\n• RoBERTa’s authors proposed a BERT variant that modifies key hyperparameters (longer training, \\nlarger batches, more data), removing the next-sentence pretraining objective, and training \\non longer sequences. The authors also proposed dynamically changing the masking pattern \\napplied to the training data.\\n• The researchers collected a new dataset called CC-News of similar size to other privately used \\ndatasets.\\n• The code is available here: https://github.com/pytorch/fairseq .\\nRoBERTa outperformed BERT on GLUE and SQuAD tasks and matched XLNet on some of them.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1930be76-78c6-42e2-9161-2160ae38b0f9', embedding=None, metadata={'page_label': '214', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 214\\nALBERT\\nALBERT  (A Lite BERT) is a model introduced in 2019 by researchers at Google Research and Toyota \\nTechnological Institute at Chicago in the paper titled ALBERT: A Lite BERT for Self-supervised Learning \\nof Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, \\nPiyush Sharma, and Radu Soricut, https://arxiv.org/abs/1909.11942v1 .\\nLet’s see the key intuitions:\\n• Large models typically aim at increasing the model size when pretraining natural language \\nrepresentations in order to get improved performance. However, increasing the model size \\nmight become difficult due to GPU/TPU memory limitations, longer training times, and \\nunexpected model degradation.\\n• ALBERT attempts to address the memory limitation, the communication overhead, and \\nmodel degradation problems with an architecture that incorporates two parameter-reduction \\ntechniques: factorized embedding parameterization and cross-layer parameter sharing. With \\nfactorized embedding parameterization, the size of the hidden layers is separated from the \\nsize of vocabulary embeddings by decomposing the large vocabulary-embedding matrix into \\ntwo small matrices. With cross-layer parameter sharing, the model prevents the number of \\nparameters from growing along with the network depth. Both of these techniques improved \\nparameter efficiency without “seriously” affecting the performance.\\n• ALBERT has 18x fewer parameters and 1.7x faster training compared to the original BERT-Large \\nmodel and achieves only slightly worse performance.\\n• The code is available here: https://github.com/brightmart/albert_zh .\\nALBERT claimed it established new state-of-the-art results on all of the current state-of-the-art language \\nbenchmarks like GLUE, SQuAD, and RACE.\\nStructBERT\\nStructBERT is a model introduced in 2019’s paper called StructBERT: Incorporating Language Structures \\ninto Pre-training for Deep Language Understanding by Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, \\nJiangnan Xia, Liwei Peng, and Luo Si, https://arxiv.org/abs/1908.04577 .\\nLet’s see the key intuitions:\\n• The Alibaba team suggested extending BERT by leveraging word-level and sentence-level \\nordering during the pretraining procedure. BERT masking during pretraining is extended by \\nmixing up a number of tokens and then the model has to predict the right order.\\n• In addition, the model randomly shuffles the sentence order and predicts the next and the \\nprevious sentence with a specific prediction task.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a123ae05-3bfa-49fd-95f0-64ecf26c530e', embedding=None, metadata={'page_label': '215', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 215\\n• This additional wording and sentence shuffling together with the task of predicting the original \\norder allow StructBERT to learn linguistic structures during the pretraining procedure.\\nStructBERT from Alibaba claimed to have achieved state-of-the-art results on different NLP tasks, \\nsuch as sentiment classification, natural language inference, semantic textual similarity, and question \\nanswering, outperforming BERT.\\nT5 and MUM\\nIn 2019, Google researchers introduced a framework dubbed Text-to-Text Transfer Transformer (in \\nshort, T5) in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin \\nRaffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, \\nWei Li, and Peter J. Liu, https://arxiv.org/abs/1910.10683 . This paper is a fundamental one for \\ntransformers.\\nHere are some of the key ideas:\\n• T5 addresses many NLP tasks as a “text-to-text” problem. T5 is a single model (with different \\nnumbers of parameters) that can be trained on a wide number of tasks. The framework is so \\npowerful that it can be applied to summarization, sentiment analysis, question answering, \\nand machine translation.\\n• Transfer learning, where a model is first pretrained on a data-rich task before being fine-\\ntuned on a downstream task, is extensively analyzed by comparing pretraining objectives, \\narchitectures, unlabeled datasets, transfer approaches, and other factors on dozens of language \\nunderstanding tasks.\\n• Similar to the original transformer, T5: 1) uses an encoder-decoder structure; 2) maps the \\ninput sequences to learned embeddings and positional embeddings, which are passed to the \\nencoder; 3) uses self-attention blocks with self-attention and feedforward layers (each with \\nnormalization and skip connections) in both the encoder and the decoder.\\n• Training happens on a “Colossal Clean Crawled Corpus’’ (C4) dataset and the number of \\nparameters per each T5 model varies from 60 million (T5 Small) up to 11 billion.\\n• The computation costs were similar to BERT, but with twice as many parameters.\\n• The code is available here: https://github.com/google-research/text-to-text-transfer-\\ntransformer .\\n• Google also offers T5 with a free TPU in a Colab tutorial at https://colab.research.google.\\ncom/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/\\nt5-trivia.ipynb . We will discuss this in more detail later this chapter.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53be31a4-d3b5-4372-b896-1efaaec7bba4', embedding=None, metadata={'page_label': '216', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 216\\nWhen presented, the T5 model with 11 billion parameters achieved state-of-the-art performances on \\n17 out of 24 tasks considered and became de-facto one of the best LMs available:\\nFigure 6.6: T5 uses the same model, loss function, hyperparameters, etc. across our diverse set of \\ntasks —including translation, question answering, and classification\\nmT5, developed by Xue et al. at Google Research in 2020, extended T5 by using a single transformer to \\nmodel multiple languages. It was pretrained on a Common Crawl-based dataset covering 101 languages. \\nYou can read more about it in mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer, https://\\narxiv.org/pdf/2010.11934.pdf .\\nMUM  (short for Multitask Unified Model) is a model using the T5 text-to-text framework and according \\nto Google is 1,000 times more powerful than BERT. Not only does MUM understand language, but it \\nalso generates it. It is also multimodal, covering modalities like text and images (expanding to more \\nmodalities in the future). The model was trained across 75 different languages and many different tasks \\nat once. MUM is currently used to support Google Search ranking: https://blog.google/products/\\nsearch/introducing-mum/ .\\nELECTRA\\nELECTRA is a model introduced in 2020 by Stanford University and Google Brain researchers in \\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators by Kevin Clark, Minh-\\nThang Luong, Quoc V . Le, and Christopher D. Manning, https://arxiv.org/abs/2003.10555 .\\nLet’s look at the key intuitions: \\n• BERT pretraining consists of masking a small subset of unlabeled inputs and then training the \\nnetwork to recover them. Typically only a small fraction of words are used (~15%).\\n• The ELECTRA authors proposed a new pretraining task named “replaced token detection.” The \\nidea is to replace some tokens with alternatives generated by a small language model. Then, the \\npretrained discriminator is used to predict whether each token is an original or a replacement. \\nThis way, the model can learn from all the tokens instead of a subset:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b954bbca-fa1b-4739-92f8-07f951e8c877', embedding=None, metadata={'page_label': '217', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 217\\nFigure 6.7: ELECTRA replacement strategy. The discriminator’s task is to detect whether \\nthe word is an original one or a replacement – source: https://arxiv.org/pdf/2003.10555.pdf \\nELECTRA outperformed previous state-of-the-art models, requiring at the same time less pretraining \\nefforts. The code is available at https://github.com/google-research/electra .\\nDeBERTa\\nDeBERTa is a model introduced by Microsoft’s researchers in 2020 in DeBERTa: Decoding-enhanced \\nBERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen, \\nhttps://arxiv.org/abs/2006.03654 .\\nLet’s look at the most important ideas:\\n• BERT’s self-attention is focused on content-to-content and content-to-position, where the \\ncontent and position embedding are added before self-attention. DeBERTa keeps two separate \\nvectors representing content and position so that self-attention is calculated between content-\\nto-content, content-to-position, position-to-content, and position-to-position.\\n• DeBERTa keeps absolute position information along with the related position information.\\nDue to additional structural information used by the model, DeBERTa claimed to have achieved state-\\nof-the-art results with half the training data when compared with other models such as RoBERTa. The \\ncode is available at https://github.com/microsoft/DeBERTa .\\nThe Evolved Transformer and MEENA\\nThe Evolved Transformer was introduced in 2019 by Google Brain researchers in the paper The Evolved \\nTransformer by David R. So, Chen Liang, and Quoc V . Le, https://arxiv.org/abs/1901.11117 .\\nLet’s go over the main ideas:\\n• Transformers are a class of architectures that are manually drafted. The Evolved Transformers \\nresearchers applied Neural Architecture Search  (NAS ), a set of automatic optimization \\ntechniques that learn how to combine basic architectural building blocks to find models better \\nthan the ones manually designed by humans.\\n• NAS was applied to both the transformer encoder and decoder blocks resulting in a new \\narchitecture shown in Figures 6.8 and 6.9.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d86c3b18-a253-4e2d-8eb3-1562f155a10f', embedding=None, metadata={'page_label': '218', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 218\\nEvolved Transformers demonstrated consistent improvement compared to the original transformer \\narchitecture. The model is at the core of MEENA, a multi-turn open-domain chatbot trained end-to-end \\non data mined and filtered from social media conversations on public domains. MEENA uses Evolved \\nTransformers with 2.6 billion parameters with a single Evolved Transformer encoder block and 13 \\nEvolved Transformer decoder blocks. The objective function used for training focuses on minimizing \\nperplexity, the uncertainty of predicting the next token. MEENA can conduct conversations that \\nare more sensitive and specific than existing state-of-the-art chatbots. Refer to the Google blog post \\nTowards a Conversational Agent that Can Chat About…Anything, https://ai.googleblog.com/2020/01/\\ntowards-conversational-agent-that-can.html :\\nFigure 6.8: The Evolved Transformer encoder block, source: https://arxiv.org/pdf/1901.11117.pdf ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7a09869-2cd2-469c-b91a-f049f673f623', embedding=None, metadata={'page_label': '219', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 219\\nFigure 6.9: The Evolved Transformer decoder block, source: https://arxiv.org/pdf/1901.11117.pdf \\nLaMDA \\nLaMDA is a model introduced in 2022 by Google’s researchers in LaMDA: Language Models for Dialog \\nApplications by Romal Thoppilan, et al., https://arxiv.org/abs/2201.08239 . It is a family of \\ntransformer-based neural language models specialized for dialog. Let’s see the key intuitions:\\n• In the pretraining stage, LaMDA uses a dataset of 1.56 trillion words — nearly 40x more than \\nwhat was previously used for LLMs — from public dialog data and other public web documents. \\nAfter tokenizing the dataset into 2.81 trillion SentencePiece tokens, the pretraining predicts \\nevery next token in a sentence, given the previous tokens.\\n• In the fine-tuning stage, LaMDA performs a mix of generative tasks to generate natural-language \\nresponses to given contexts, and classification tasks on whether a response is safe and of high \\nquality. The combination of generation and classification provides the final answer (see Figure \\n6.10).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f5e8edd-3b75-4daa-8e03-7eb643d50f6a', embedding=None, metadata={'page_label': '220', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 220\\n• LaMDA defines a robust set of metrics for quality, safety, and groundedness:\\n• Quality: This measure is decomposed into three dimensions, Sensibleness, Specificity, \\nand Interestingness ( SSI). Sensibleness considers whether the model produces \\nresponses that make sense in the dialog context. Specificity judges whether the response \\nis specific to the preceding dialog context, and not a generic response that could apply \\nto most contexts. Interestingness measures whether the model produces responses \\nthat are also insightful, unexpected, or witty.\\n• Safety: Takes into account how to avoid any unintended result that creates risks of \\nharm for the user, and to avoid reinforcing unfair bias.\\n• Groundedness: Takes into account plausible information that is, however, contradicting \\ninformation that can be supported by authoritative external sources.\\nFigure 6.10: LaMDA generates and then scores a response candidate. Source: https://\\nai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html \\nLaMDA demonstrated results that were impressively close to the human brain ones. According to \\nGoogle (https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html ), \\nLaMDA significantly outperformed the pretrained model in every dimension and across all model \\nsizes. Quality metrics (Sensibleness, Specificity, and Interestingness) generally improved with the \\nnumber of model parameters, with or without fine-tuning. Safety did not seem to benefit from model \\nscaling alone, but it did improve with fine-tuning. Groundedness improved as model size increased, \\nperhaps because larger models have a greater capacity to memorize uncommon knowledge, but \\nfine-tuning allows the model to access external knowledge sources and to effectively shift some of \\nthe load of remembering knowledge to an external knowledge source. With fine-tuning, the quality \\ngap to human levels can be shrunk, though the model performance remains below human levels in \\nsafety and groundedness:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2517a5d7-f34c-4c7c-8d18-ee5f683e64b8', embedding=None, metadata={'page_label': '221', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 221\\nFigure 6.11: LaMDA performance – source: https://ai.googleblog.com/2022/01/lamda-towards-safe-\\ngrounded-and-high.html \\nSwitch Transformer\\nThe Switch Transformer is a model introduced in 2021 by Google’s researchers in Switch Transformers: \\nScaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, \\nand Noam Shazeer, introduced in https://arxiv.org/abs/2101.03961 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64f6d8c7-48ff-406c-9e05-7906adb6ae55', embedding=None, metadata={'page_label': '222', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 222\\nLet’s look at the key intuitions: \\n• The Switch Transformer was trained from 7 billion to 1.6 trillion parameters. As discussed, \\na typical transformer is a deep stack of multi-headed self-attention layers, and at the end of \\neach layer, there’s an FFN aggregating the outputs coming from its multiple heads. The Switch \\nTransformer replaces this single FFN with multiple FFNs and calls them “experts.” On each \\nforward pass, at each layer, for each token at the input, the model activates exactly one expert:\\nFig 6.12: The Switch Transformer with multiple routing FFN – The dense FFN layer present \\nin the transformer is replaced with a sparse Switch FFN layer (light blue). Source: https://\\narxiv.org/pdf/2101.03961.pdf  \\n• Switch-Base (7 billion parameters) and Switch-Large (26 billion parameters) outperformed T5-\\nBase (0.2 billion parameters) and T5-Large (0.7 billion parameters) on tasks such as language \\nmodeling, classification, coreference resolution, question answering, and summarization.\\nAn example implementation of Switch Transformer is available at https://keras.io/examples/nlp/\\ntext_classification_with_switch_transformer/ .\\nRETRO\\nRETRO ( Retrieval-Enhanced Transformer) is a retrieval-enhanced autoregressive language model \\nintroduced by DeepMind in 2022 in Improving language models by retrieving from trillions of tokens by \\nSebastian Borgeaud et al., https://arxiv.org/pdf/2112.04426/ . Let’s look at the key intuitions:\\n• Scaling the number of parameters in an LLM has proven to be a way to improve the quality of \\nthe results. However, this approach is not sustainable because it is computationally expensive. \\n• RETRO couples a retrieval Database ( DB) with a transformer in a hybrid architecture. The idea \\nis first to search with the Nearest Neighbors algorithm on pre-computed BERT embeddings \\nstored in a retrieval DB. Then, these embeddings are used as input to a transformer’s encoder.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e7c88ff-6562-4851-8141-4660f7c9a219', embedding=None, metadata={'page_label': '223', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 223\\n• The combination of retrieval and transformers allows RETRO (scaled from 150 million to 7 \\nbillion non-embedding parameters) to save on the number of parameters used by the LLM.\\nFor instance, consider the sample query “The 2021 Women’s US Open was won” and Figure 6.13, \\nwhere the cached BERT embeddings are passed to a transformer encoder to get the final result:\\nFigure 6.13: A high-level overview of Retrieval Enhanced Transformers (RETRO). Source: \\nhttps://deepmind.com/research/publications/2021/improving-language-models-by-\\nretrieving-from-trillions-of-tokens\\nPathways and PaLM\\nGoogle Research announced Pathways ( https://blog.google/technology/ai/introducing-\\npathways-next-generation-ai-architecture/ ), a single model that could generalize across domains \\nand tasks while being highly efficient. Then, Google introduced Pathways Language Model (PaLM ), \\na 540-billion parameter, dense decoder-only transformer model, which enabled us to efficiently \\ntrain a single model across multiple TPU v4 Pods. Google evaluated PaLM on hundreds of language \\nunderstanding and generation tasks and found that it achieves state-of-the-art performance across \\nmost tasks, by significant margins in many cases (see https://ai.googleblog.com/2022/04/pathways-\\nlanguage-model-palm-scaling-to.html?m=1 ).\\nImplementation\\nIn this section, we will go through a few tasks using transformers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01ab6103-086f-4a1c-b55a-f318c4b49f7e', embedding=None, metadata={'page_label': '224', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Transformers 224\\nTransformer reference implementation: An example of \\ntranslation\\nIn this section, we will briefly review a transformer reference implementation available at https://\\nwww.tensorflow.org/text/tutorials/transformer  and specifically, we will use the opportunity to \\nrun the code in a Google Colab.\\nNot everyone realizes the number of GPUs it takes to train a transformer. Luckily, you can play with \\nresources available for free at https://colab.research.google.com/github/tensorflow/text/blob/\\nmaster/docs/tutorials/transformer.ipynb .\\nNote that implementing transformers from scratch is probably not the best choice unless you need \\nto realize some very specific customization or you are interested in core research. If you are not \\ninterested in learning the internals, then you can skip to the next section. Our tutorial is licensed under \\nthe Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 \\nLicense. The specific task we are going to perform is translating from Portuguese to English. Let’s \\nhave a look at the code, step by step:\\n1. First, let’s install datasets and import the right libraries. Note that the Colab available online \\nis apparently missing the line import tensorflow_text , which is, however, added here:\\n!pip install tensorflow_datasets\\n!pip install -U 'tensorflow-text==2.8.*'\\nimport logging\\nimport time\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport tensorflow_text\\nimport tensorflow_datasets as tfds\\nimport tensorflow as tf\\nlogging.getLogger( 'tensorflow' ).setLevel(logging.ERROR)  # suppress \\nwarnings\\n2. Then, load the Portuguese to English dataset:\\nexamples, metadata = tfds.load( 'ted_hrlr_translate/pt_to_en' , with_\\ninfo=True,\\n                               as_supervised= True)\\ntrain_examples, val_examples = examples[ 'train'], examples[ 'validation' ]\\n3. Now, let’s convert text to sequences of token IDs, which are used as indices into an embedding:\\nmodel_name = 'ted_hrlr_translate_pt_en_converter'\\ntf.keras.utils.get_file(\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfcac667-f4e1-4835-a3c9-b8f14353ac85', embedding=None, metadata={'page_label': '225', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 225\\n    f'{model_name} .zip',\\n    f'https://storage.googleapis.com/download.tensorflow.org/models/\\n{model_name} .zip',\\n    cache_dir= '.', cache_subdir= '', extract= True\\n)\\ntokenizers = tf.saved_model.load(model_name)\\n4. Let’s see the tokenized IDs and tokenized words:\\nfor pt_examples, en_examples in train_examples.batch( 3).take( 1):\\n  print( '> Examples in Portuguese:' )\\nfor en in en_examples.numpy():\\n  print(en.decode( 'utf-8'))\\nand when you improve searchability , you actually take away the one \\nadvantage of print , which is serendipity .\\nbut what if it were active ?\\nbut they did n't test for curiosity .\\nencoded = tokenizers.en.tokenize(en_examples)\\nfor row in encoded.to_list():\\n  print(row)\\n[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, \\n74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\\n[2, 87, 90, 107, 76, 129, 1852, 30, 3]\\n[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\\nround_trip = tokenizers.en.detokenize(encoded)\\nfor line in round_trip.numpy():\\n  print(line.decode( 'utf-8'))\\nand when you improve searchability , you actually take away the one \\nadvantage of print , which is serendipity .\\nbut what if it were active ?\\nbut they did n ' t test for curiosity .\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7bf86e1-30d3-4018-ad3a-a876a9ddd3b8', embedding=None, metadata={'page_label': '226', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 226\\n5. Now let’s create an input pipeline. First, we define a function to drop the examples longer \\nthan MAX_TOKENS . Second, we define a function that tokenizes the batches of raw text. Third, \\nwe create the batches:\\nMAX_TOKENS= 128\\ndef filter_max_tokens (pt, en):\\n  num_tokens = tf.maximum(tf.shape(pt)[ 1],tf.shape(en)[ 1])\\n  return  num_tokens < MAX_TOKENS\\ndef tokenize_pairs (pt, en):\\n    pt = tokenizers.pt.tokenize(pt)\\n    # Convert from ragged to dense, padding with zeros.\\n    pt = pt.to_tensor()\\n    en = tokenizers.en.tokenize(en)\\n    # Convert from ragged to dense, padding with zeros.\\n    en = en.to_tensor()\\n    return pt, en\\nBUFFER_SIZE = 20000\\nBATCH_SIZE = 64\\ndef make_batches (ds):\\n  return  (\\n      ds\\n      .cache()\\n      .shuffle(BUFFER_SIZE)\\n      .batch(BATCH_SIZE)\\n      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\\n      .filter(filter_max_tokens)\\n      .prefetch(tf.data.AUTOTUNE))\\ntrain_batches = make_batches(train_examples)\\nval_batches = make_batches(val_examples)\\n6. Now we add positional encoding, forcing tokens to be closer to each other based on the similarity \\nof their meaning and their position in the sentence, in the d-dimensional embedding space:\\ndef get_angles (pos, i, d_model):\\n  angle_rates = 1 / np.power( 10000, (2 * (i// 2)) / np.float32(d_model))\\n  return  pos * angle_rates\\ndef positional_encoding (position, d_model):\\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='453d71a0-e550-4368-8928-41802a260fc8', embedding=None, metadata={'page_label': '227', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 227\\n                          np.arange(d_model)[np.newaxis, :],\\n                          d_model)\\n  # apply sin to even indices in the array; 2i\\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\\n  # apply cos to odd indices in the array; 2i+1\\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\\n  pos_encoding = angle_rads[np.newaxis, ...]\\n  return  tf.cast(pos_encoding, dtype=tf.float32)\\n7. Let’s now focus on the masking process. The look-ahead mask is used to mask the future tokens \\nin a sequence, with the mask indicating which entries should not be used. For instance, to \\npredict the third token, only the first and second tokens will be used, and to predict the fourth \\ntoken, only the first, second, and third tokens will be used, and so on:\\ndef create_padding_mask (seq):\\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\\n  # add extra dimensions to add the padding\\n  # to the attention logits.\\n  return  seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\\ndef create_look_ahead_mask (size):\\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), - 1, 0)\\n  return  mask  # (seq_len, seq_len)\\n8. We are getting closer and closer to the essence of transformers. Let’s define the attention \\nfunction as a scaled dot-product:\\ndef scaled_dot_product_attention (q, k, v, mask):\\n  \"\"\"Calculate the attention weights.\\n  q, k, v must have matching leading dimensions.\\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_\\nlen_v.\\n  The mask has different shapes depending on its type(padding or look \\nahead)\\n  but it must be broadcastable for addition.\\n  Args:\\n    q: query shape == (..., seq_len_q, depth)\\n    k: key shape == (..., seq_len_k, depth)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='362b6bda-6b8b-476d-ba47-e8616837a3a9', embedding=None, metadata={'page_label': '228', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 228\\n    v: value shape == (..., seq_len_v, depth_v)\\n    mask: Float tensor with shape broadcastable\\n          to (..., seq_len_q, seq_len_k). Defaults to None.\\n  Returns:\\n    output, attention_weights\\n  \"\"\"\\n  matmul_qk = tf.matmul(q, k, transpose_b= True)  # (..., seq_len_q, seq_\\nlen_k)\\n  # scale matmul_qk\\n  dk = tf.cast(tf.shape(k)[- 1], tf.float32)\\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\\n  # add the mask to the scaled tensor.\\n  if mask is not None:\\n    scaled_attention_logits += (mask * - 1e9)\\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\\n  # add up to 1.\\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=- 1)  # \\n(..., seq_len_q, seq_len_k)\\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\\n  return  output, attention_weights\\n9. Now that the attention is defined, we need to implement the multi-head mechanism. There \\nare three parts: linear layers, scaled dot-product attention, and the final linear layer (see \\nFigure 6.14):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ac54be6-8e3e-4590-9060-dfd3347999c0', embedding=None, metadata={'page_label': '229', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 229\\nFigure 6.14: Multi-head attention\\nclass MultiHeadAttention (tf.keras.layers.Layer):\\n  def __init__ (self,*, d_model, num_heads):\\n    super(MultiHeadAttention, self).__init__()\\n    self.num_heads = num_heads\\n    self.d_model = d_model\\n    assert d_model % self.num_heads == 0\\n    self.depth = d_model // self.num_heads\\n    self.wq = tf.keras.layers.Dense(d_model)\\n    self.wk = tf.keras.layers.Dense(d_model)\\n    self.wv = tf.keras.layers.Dense(d_model)\\n    self.dense = tf.keras.layers.Dense(d_model)\\n  def split_heads (self, x, batch_size):\\n    \"\"\"Split the last dimension into (num_heads, depth).\\n    Transpose the result such that the shape is (batch_size, num_heads, \\nseq_len, depth)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='245b5475-be0a-412f-b5cd-fd86bceb226d', embedding=None, metadata={'page_label': '230', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 230\\n    \"\"\"\\n    x = tf.reshape(x, (batch_size, - 1, self.num_heads, self.depth))\\n    return tf.transpose(x, perm=[ 0, 2, 1, 3])\\n  def call(self, v, k, q, mask):\\n    batch_size = tf.shape(q)[ 0]\\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_\\nlen_q, depth)\\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_\\nlen_k, depth)\\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_\\nlen_v, depth)\\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_\\nlen_k)\\n    scaled_attention, attention_weights = scaled_dot_product_attention(\\n        q, k, v, mask)\\n    scaled_attention = tf.transpose(scaled_attention, perm=[ 0, 2, 1, 3])  \\n# (batch_size, seq_len_q, num_heads, depth)\\n    concat_attention = tf.reshape(scaled_attention,\\n                                  (batch_size, - 1, self.d_model))  # \\n(batch_size, seq_len_q, d_model)\\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_\\nmodel)\\n    return output, attention_weights\\n10. Now, we can define a point-wise feedforward network that consists of two fully connected \\nlayers with a ReLU activation in between:\\ndef point_wise_feed_forward_network (d_model, dff):\\n return tf.keras.Sequential([', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62d52c70-d81d-475b-aa1b-6bca5a293d86', embedding=None, metadata={'page_label': '231', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 231\\n     tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_\\nlen, dff)\\n     tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\\n ])\\n11. We can now concentrate on defining the encoder and decoder parts as described in Figure \\n6.15. Remember that the traditional transformer takes the input sentence through N encoder \\nlayers, while the decoder uses the encoder output and its own input (self-attention) to predict \\nthe next word. Each encoder layer has sublayers made by multi-head attention (with a padding \\nmask) and then point-wise feedforward networks. Each sublayer uses a residual connection \\nto contain the problem of vanishing gradient, and a normalization layer:\\nclass EncoderLayer (tf.keras.layers.Layer):\\n  def __init__ (self,*, d_model, num_heads, dff, rate= 0.1):\\n    super(EncoderLayer, self).__init__()\\n    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\\n    self.dropout1 = tf.keras.layers.Dropout(rate)\\n    self.dropout2 = tf.keras.layers.Dropout(rate)\\n  def call(self, x, training, mask):\\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_\\nlen, d_model)\\n    attn_output = self.dropout1(attn_output, training=training)\\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_\\nlen, d_model)\\n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\\n    ffn_output = self.dropout2(ffn_output, training=training)\\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_\\nlen, d_model)\\n    return out2\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5134ccd-5494-4802-998d-9757f3b50078', embedding=None, metadata={'page_label': '232', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 232\\n12. Each decoder layer is made of sublayers. First, a masked multi-head attention (with a look-ahead \\nmask and padding mask). Then, a multi-head attention (with a padding mask), V (value), and \\nK (key) receive the encoder output as inputs. Q (query) receives the output from the masked \\nmulti-head attention sublayer and, finally, the point-wise feedforward networks:\\nclass DecoderLayer (tf.keras.layers.Layer):\\n  def __init__ (self,*, d_model, num_heads, dff, rate= 0.1):\\n    super(DecoderLayer, self).__init__()\\n    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\\n    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon= 1e-6)\\n    self.dropout1 = tf.keras.layers.Dropout(rate)\\n    self.dropout2 = tf.keras.layers.Dropout(rate)\\n    self.dropout3 = tf.keras.layers.Dropout(rate)\\n  def call(self, x, enc_output, training,\\n           look_ahead_mask, padding_mask):\\n    # enc_output.shape == (batch_size, input_seq_len, d_model)\\n    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # \\n(batch_size, target_seq_len, d_model)\\n    attn1 = self.dropout1(attn1, training=training)\\n    out1 = self.layernorm1(attn1 + x)\\n    attn2, attn_weights_block2 = self.mha2(\\n        enc_output, enc_output, out1, padding_mask)  # (batch_size, \\ntarget_seq_len, d_model)\\n    attn2 = self.dropout2(attn2, training=training)\\n    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, \\nd_model)\\n    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\\n    ffn_output = self.dropout3(ffn_output, training=training)\\n    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_\\nlen, d_model)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='56c041f9-2b77-4bf5-b8ba-0aee5c2f0d11', embedding=None, metadata={'page_label': '233', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 233\\n    return out3, attn_weights_block1, attn_weights_block2\\n13. Now that we have defined the encoder layer, we can use it to define the proper encoder. This \\nconsists of three stages: input embedding, positional encoding, and N encoder layers:\\nclass Encoder (tf.keras.layers.Layer):\\n  def __init__ (self,*, num_layers, d_model, num_heads, dff, input_vocab_\\nsize,\\n               rate= 0.1):\\n    super(Encoder, self).__init__()\\n    self.d_model = d_model\\n    self.num_layers = num_layers\\n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\\n    self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\\n    self.enc_layers = [\\n        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, \\nrate=rate)\\n        for _ in range (num_layers)]\\n    self.dropout = tf.keras.layers.Dropout(rate)\\n  def call(self, x, training, mask):\\n    seq_len = tf.shape(x)[ 1]\\n    # adding embedding and position encoding.\\n    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\\n    x += self.pos_encoding[:, :seq_len, :]\\n    x = self.dropout(x, training=training)\\n    for i in range (self.num_layers):\\n      x = self.enc_layers[i](x, training, mask)\\n    return x  # (batch_size, input_seq_len, d_model)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2cefb9c1-84e1-4450-ad2b-3cb68b1809dd', embedding=None, metadata={'page_label': '234', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Transformers 234\\n14.  We can now focus our attention on the decoder itself. It consists of output embedding, positional \\nencoding, and N decoder layers:\\nclass Decoder (tf.keras.layers.Layer):\\n  def __init__ (self,*, num_layers, d_model, num_heads, dff, target_vocab_\\nsize,\\n               rate= 0.1):\\n    super(Decoder, self).__init__()\\n    self.d_model = d_model\\n    self.num_layers = num_layers\\n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_\\nmodel)\\n    self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\\n    self.dec_layers = [\\n        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, \\nrate=rate)\\n        for _ in range (num_layers)]\\n    self.dropout = tf.keras.layers.Dropout(rate)\\n  def call(self, x, enc_output, training,\\n           look_ahead_mask, padding_mask):\\n    seq_len = tf.shape(x)[ 1]\\n    attention_weights = {}\\n    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\\n    x += self.pos_encoding[:, :seq_len, :]\\n    x = self.dropout(x, training=training)\\n    for i in range (self.num_layers):\\n      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\\n                                             look_ahead_mask, padding_\\nmask)\\n      attention_weights[ f'decoder_layer {i+1}_block1' ] = block1\\n      attention_weights[ f'decoder_layer {i+1}_block2' ] = block2\\n    # x.shape == (batch_size, target_seq_len, d_model)\\n    return x, attention_weights\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bec5f9f2-c273-4e16-9541-8195d8ba1027', embedding=None, metadata={'page_label': '235', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 235\\n15. Now that we have defined the encoder and decoder, we can now turn our attention to the \\ntransformer itself, which is composed of an encoder, a decoder, and a final linear layer (see \\nFigure 6.15):\\nclass Transformer (tf.keras.Model):\\n  def __init__ (self,*, num_layers, d_model, num_heads, dff, input_vocab_\\nsize,\\n               target_vocab_size, rate= 0.1):\\n    super().__init__()\\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\\n                           num_heads=num_heads, dff=dff,\\n                           input_vocab_size=input_vocab_size, rate=rate)\\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\\n                           num_heads=num_heads, dff=dff,\\n                           target_vocab_size=target_vocab_size, \\nrate=rate)\\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\\n  def call(self, inputs, training):\\n    # Keras models prefer if you pass all your inputs in the first \\nargument\\n    inp, tar = inputs\\n    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_\\nmasks(inp, tar)\\n    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_\\nsize, inp_seq_len, d_model)\\n    # dec_output.shape == (batch_size, tar_seq_len, d_model)\\n    dec_output, attention_weights = self.decoder(\\n        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\\n    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_\\nlen, target_vocab_size)\\n    return final_output, attention_weights\\n  def create_masks (self, inp, tar):\\n    # Encoder padding mask', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4427963f-0881-4f84-b52a-3d9817aa061a', embedding=None, metadata={'page_label': '236', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 236\\n    enc_padding_mask = create_padding_mask(inp)\\n    # Used in the 2nd attention block in the decoder.\\n    # This padding mask is used to mask the encoder outputs.\\n    dec_padding_mask = create_padding_mask(inp)\\n    # Used in the 1st attention block in the decoder.\\n    # It is used to pad and mask future tokens in the input received by\\n    # the decoder.\\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[ 1])\\n    dec_target_padding_mask = create_padding_mask(tar)\\n    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_\\nmask)\\n    return enc_padding_mask, look_ahead_mask, dec_padding_mask\\nFigure 6.15: The traditional transformer', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a65759d-ba59-46e2-8937-cc88416815bd', embedding=None, metadata={'page_label': '237', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 237\\n16. We are almost done. We just need to define hyperparameters and the optimizer, using exactly \\nthe same settings as the seminal paper, and the loss function: \\nnum_layers = 4\\nd_model = 128\\ndff = 512\\nnum_heads = 8\\ndropout_rate = 0.1\\nclass CustomSchedule (tf.keras.optimizers.schedules.LearningRateSchedule):\\n  def __init__ (self, d_model, warmup_steps= 4000):\\n    super(CustomSchedule, self).__init__()\\n    self.d_model = d_model\\n    self.d_model = tf.cast(self.d_model, tf.float32)\\n    self.warmup_steps = warmup_steps\\n  def __call__ (self, step):\\n    arg1 = tf.math.rsqrt(step)\\n    arg2 = step * (self.warmup_steps ** - 1.5)\\n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\\nlearning_rate = CustomSchedule(d_model)\\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1= 0.9, \\nbeta_2=0.98,\\n                                     epsilon= 1e-9)\\ndef loss_function (real, pred):\\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\\n  loss_ = loss_object(real, pred)\\n  mask = tf.cast(mask, dtype=loss_.dtype)\\n  loss_ *= mask\\n  return  tf.reduce_sum(loss_)/tf.reduce_sum(mask)\\ndef accuracy_function (real, pred):\\n  accuracies = tf.equal(real, tf.argmax(pred, axis= 2))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0436fe2a-3219-4078-965a-a0a6f1ab8775', embedding=None, metadata={'page_label': '238', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Transformers 238\\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\\n  accuracies = tf.math.logical_and(mask, accuracies)\\n  accuracies = tf.cast(accuracies, dtype=tf.float32)\\n  mask = tf.cast(mask, dtype=tf.float32)\\n  return  tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\\ntrain_loss = tf.keras.metrics.Mean(name= 'train_loss' )\\ntrain_accuracy = tf.keras.metrics.Mean(name= 'train_accuracy' )\\n17. Time to define the transformer. Let’s see the code:\\ntransformer = Transformer(\\n    num_layers=num_layers,\\n    d_model=d_model,\\n    num_heads=num_heads,\\n    dff=dff,\\n    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\\n    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\\n    rate=dropout_rate)\\n18. Let’s also define the checkpoints with the following code:\\ncheckpoint_path = './checkpoints/train'\\nckpt = tf.train.Checkpoint(transformer=transformer,\\n                           optimizer=optimizer)\\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_\\nkeep=5)\\n# if a checkpoint exists, restore the latest checkpoint.\\nif ckpt_manager.latest_checkpoint:\\n  ckpt.restore(ckpt_manager.latest_checkpoint)\\n  print('Latest checkpoint restored!!' )\\n19. Remember that the transformer is autoregressive. The current output is used to predict what \\nwill happen next. We use a look-ahead mask, to prevent the model from peeking at the expected \\noutput. We are now ready to define train_step : \\ntrain_step_signature = [\\n    tf.TensorSpec(shape=( None, None), dtype=tf.int64),\\n    tf.TensorSpec(shape=( None, None), dtype=tf.int64),\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9427cf5b-be25-41d8-9bec-c705eb81f049', embedding=None, metadata={'page_label': '239', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 239\\n]\\n@tf.function( input_signature=train_step_signature )\\ndef train_step (inp, tar):\\n  tar_inp = tar[:, :- 1]\\n  tar_real = tar[:, 1:]\\n  with tf.GradientTape() as tape:\\n    predictions, _ = transformer([inp, tar_inp],\\n                                 training = True)\\n    loss = loss_function(tar_real, predictions)\\n  gradients = tape.gradient(loss, transformer.trainable_variables)\\n  optimizer.apply_gradients( zip(gradients, transformer.trainable_\\nvariables))\\n  train_loss(loss)\\n  train_accuracy(accuracy_function(tar_real, predictions))\\nEPOCHS = 20\\nfor epoch in range (EPOCHS):\\n  start = time.time()\\n  train_loss.reset_states()\\n  train_accuracy.reset_states()\\n  # inp -> portuguese, tar -> english\\n  for (batch, (inp, tar)) in enumerate (train_batches):\\n    train_step(inp, tar)\\n    if batch % 50 == 0:\\n      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.\\nresult(): .4f} Accuracy {train_accuracy.result(): .4f}')\\n  if (epoch + 1) % 5 == 0:\\n    ckpt_save_path = ckpt_manager.save()\\n    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path} ')\\n  print(f'Epoch {epoch + 1} Loss {train_loss.result(): .4f} Accuracy \\n{train_accuracy.result(): .4f}')\\n  print(f'Time taken for 1 epoch: {time.time() - start: .2f} secs\\\\n' )\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ba5e9cc-6366-4ea7-9ff2-a9dcde10141f', embedding=None, metadata={'page_label': '240', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Transformers 240\\nAfter running the training step in Colab, we get the following situation:\\nEpoch 20 Loss 1.5030 Accuracy 0.6720\\nTime taken for 1 epoch: 169.01 secs\\n20. We are now ready for translation. The following steps are used to translate:\\n1. Encode the input sentence using the Portuguese tokenizer ( tokenizers.pt ). \\n2. The decoder input is initialized to the [START] token.\\n3. Calculate the padding masks and the look-ahead masks.\\n4. The decoder then outputs the predictions by looking at the encoder output and its own \\noutput (self-attention).\\n5. Concatenate the predicted token to the decoder input and pass it to the decoder:\\nclass Translator (tf.Module):\\n  def __init__ (self, tokenizers, transformer):\\n    self.tokenizers = tokenizers\\n    self.transformer = transformer\\n  def __call__ (self, sentence, max_length=MAX_TOKENS):\\n    # input sentence is portuguese, hence adding the start and end token\\n    assert isinstance (sentence, tf.Tensor)\\n    if len(sentence.shape) == 0:\\n      sentence = sentence[tf.newaxis]\\n    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\\n    encoder_input = sentence\\n    # As the output language is english, initialize the output with the\\n    # english start token.\\n    start_end = self.tokenizers.en.tokenize([ ''])[0]\\n    start = start_end[ 0][tf.newaxis]\\n    end = start_end[ 1][tf.newaxis]\\n    # 'tf.TensorArray' is required here (instead of a python list) so \\nthat the\\n    # dynamic-loop can be traced by 'tf.function'.\\n    output_array = tf.TensorArray(dtype=tf.int64, size= 0, dynamic_\\nsize=True)\\n    output_array = output_array.write( 0, start)\\n    for i in tf.range(max_length):\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5e8b25a-7d28-4416-935c-2d195cccd590', embedding=None, metadata={'page_label': '241', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 241\\n      output = tf.transpose(output_array.stack())\\n      predictions, _ = self.transformer([encoder_input, output], \\ntraining= False)\\n      # select the last token from the seq_len dimension\\n      predictions = predictions[:, - 1:, :]  # (batch_size, 1, vocab_size)\\n      predicted_id = tf.argmax(predictions, axis=- 1)\\n      # concatentate the predicted_id to the output which is given to the \\ndecoder\\n      # as its input.\\n      output_array = output_array.write(i+ 1, predicted_id[ 0])\\n      if predicted_id == end:\\n        break\\n    output = tf.transpose(output_array.stack())\\n    # output.shape (1, tokens)\\n    text = tokenizers.en.detokenize(output)[ 0]  # shape: ()\\n    tokens = tokenizers.en.lookup(output)[ 0]\\n    # \\'tf.function\\' prevents us from using the attention_weights that \\nwere\\n    # calculated on the last iteration of the loop. So recalculate them \\noutside\\n    # the loop.\\n    _, attention_weights = self.transformer([encoder_input, \\noutput[:,:- 1]], training= False)\\n    return text, tokens, attention_weights\\n21. Let’s call the translator on a sample sentence with this code snippet:\\ntranslator = Translator(tokenizers, transformer)\\ndef print_translation (sentence, tokens, ground_truth):\\n  print(f\\'{\"Input:\" :15s}: {sentence} \\')\\n  print(f\\'{\"Prediction\" :15s}: {tokens.numpy().decode( \"utf-8\")}\\')\\n  print(f\\'{\"Ground truth\" :15s}: {ground_truth} \\')\\nsentence = \\'os meus vizinhos ouviram sobre esta ideia.\\'\\nground_truth = \\'and my neighboring homes heard about this idea .\\'', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9779f30-f643-4dec-9a92-400e636ed36b', embedding=None, metadata={'page_label': '242', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 242\\ntranslated_text, translated_tokens, attention_weights = translator(\\n    tf.constant(sentence))\\nprint_translation(sentence, translated_text, ground_truth)\\nGetting as the result:\\nInput:         : os meus vizinhos ouviram sobre esta ideia.\\nPrediction     : my neighbors have heard about this idea .\\nGround truth   : and my neighboring homes heard about this idea .\\nIn this detailed analysis, we have discussed how a traditional transformer is implemented taking \\npositional encoding, multi-head attention, and masking into account. The analyzed code is at https://\\nwww.tensorflow.org/text/tutorials/transformer .\\nNext, we will discuss how to use transformers making use of higher-level libraries.\\nHugging Face\\nAs discussed, implementing transformers from scratch is probably not the best choice unless you \\nneed to realize some very specific customization, or you are interested in core research. This is useful \\nif you want to understand the internal details of a transformer architecture, or perhaps modify the \\ntransformer architecture to produce a new variant. Nowadays, there are very good libraries providing \\nhigh-quality solutions. One of them is Hugging Face, which provides some efficient tools. Hugging \\nFace is built around the idea of commercializing its open source transformers library. Let’s see why \\nthe library became so popular:\\n• Hugging Face provides a common API to handle many transformer architectures.\\n• It not only provides the base model, but models with different types of “head” to \\nhandle specific tasks (for example, for the BERT architecture it provides TFBertModel , \\nand the TFBertForSequenceClassification  for tasks like sentiment analysis, \\nTFBertForTokenClassification  for tasks like named entity recognition, and \\nTFBertForQuestionAnswering  for Q and A, among others).\\n• You can also create your own network for a specific task quite easily by using the pretrained \\nweights provided here, for example, by using TFBertForPreTraining .\\n• In addition to the pipeline()  method in the next subsection, we can also define a model in \\nthe regular way and use fit()  to train it and predict()  to make inferences against it, just \\nlike a normal TF model (PyTorch also has the Trainer interface). We will see an example later \\non in this chapter.\\nNow, let’s check some examples of how to use Hugging Face.\\nGenerating text \\nIn this section, we are going to use GPT-2 for natural language generation, a software process for \\nproducing natural language outputs. Let’s start from the beginning by installing the Hugging Face \\nlibrary:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef0488c8-13ad-42f5-bed5-8a34075f06c4', embedding=None, metadata={'page_label': '243', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 243\\n1. The first step is to create a dedicated virtual environment, where we can install the transformer \\nlibrary. In my case, I use the library for TensorFlow 2.0:\\npython -m venv .env\\nsource .env/ bin/activate\\npip install transformers[tf-cpu]\\n2. Then let’s verify that everything is working correctly by downloading a pretrained model used \\nfor sentiment analysis:\\npython -c \"from transformers import pipeline; print(pipeline(\\'sentiment-\\nanalysis\\')(\\'we love you\\'))\"\\nSince the expected sentiment should be very positive, we shall see something like the following:\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9998704791069031}]\\n3. Now let’s focus on generating text with GPT-2:\\nfrom transformers import pipeline\\ngenerator = pipeline(task= \"text-generation\" )\\nYou should see something like the following:\\nNo model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\\nDownloading: 100%|██████████████████████████████| 665/665 [00:00<00:00, \\n167kB/s]\\nDownloading: 100%|███████████████████████████| 475M/475M [03:24<00:00, \\n2.44MB/s\\n4. Let’s pass some text to the generator and see what the result is. The first sentence is extracted \\nfrom Tolkien’s work, the second from Einstein’s theories, and the third one comes from “Harry \\nPotter”:\\ngenerator( \"Three Rings for the Elven-kings under the sky, Seven for the \\nDwarf-lords in their halls of stone\" )\\nSetting \\'pad_token_id\\' to 50256 (first \\'eos_token_id\\') to generate \\nsequence\\n[{\\'generated_text\\': \\'Three Rings for the Elven-kings under the sky, Seven \\nfor the Dwarf-lords in their halls of stone and Eight for the Dwarves in \\ntheir halls of rock! Three new Rings of the Elven-kings under the sky, \\nSeven for\\'}]\\ngenerator ( \"The original theory of relativity is based upon the premise \\nthat all coordinate systems in relative uniform translatory motion to \\neach other are equally valid and equivalent \" )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4ece468-afba-46b9-8000-f8efc6ef624f', embedding=None, metadata={'page_label': '244', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 244\\nSetting \\'pad_token_id\\' to 50256 (first \\'eos_token_id\\') to generate \\nsequence\\n[{\\'generated_text\\': \\'The original theory of relativity is based upon \\nthe premise that all coordinate systems in relative uniform translatory \\nmotion to each other are equally valid and equivalent \\\\xa0to one another. \\nIn other words, they can all converge, and therefore all the laws are \\nvalid\\'}]\\ngenerator ( \"It takes a great deal of bravery to stand up to our enemies\" )\\nSetting \\'pad_token_id\\' to 50256 (first \\'eos_token_id\\') to generate \\nsequence\\n[{\\'generated_text\\': \\'It takes a great deal of bravery to stand up to \\nour enemies that day. She still has a lot to learn from it, or it could \\ntake decades to do.\\\\n\\\\nWhile some braver men struggle, many are not as \\nlucky\\'}]\\nPretty  easy, isn’t it?\\nAutoselecting a model and autotokenization\\nHugging Face does a great job of helping the developer to automate as many steps as possible. Let’s \\nsee some examples:\\n1. You can easily import a pretrained model among the several dozen available. A complete list \\nof available models is here: https://huggingface.co/docs/transformers/model_doc/auto : \\nfrom transformers import TFAutoModelForSequenceClassification\\nmodel = TFAutoModelForSequenceClassification.from_pretrained( \"distilbert-\\nbase-uncased\" )\\nDownloading: 100%|█████████████████████████████| 483/483 [00:00<00:00, \\n68.9kB/s]\\nDownloading: 100%|███████████████████████████| 347M/347M [01:05<00:00, \\n5.59MB/s]\\n…\\nYou should probably train this model on a downstream task to use it for predictions and \\ninference.\\n2. You can use AutoTokenizer  to transform words into tokens used by the models:\\nfrom transformers import AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained( \"bert-base-uncased\" )\\nsequence = \"The original theory of relativity is based upon the premise ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='703adcbf-e8f0-42e6-a892-dda67ccacacf', embedding=None, metadata={'page_label': '245', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 245\\nthat all coordinate systems\"\\nprint(tokenizer(sequence))\\n{\\'input_ids\\': [101, 1996, 2434, 3399, 1997, 20805, 2003, 2241, 2588, \\n1996, 18458, 2008, 2035, 13530, 3001, 102], \\'token_type_ids\\': [0, 0, 0, \\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \\'attention_mask\\': [1, 1, 1, 1, 1, \\n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\\nNamed entity recognition\\nNamed Entity Recognition ( NER ) is a classical NLP task. According to Wikipedia, named entity \\nrecognition – also known as (named) entity identification, entity chunking, and entity extraction – is \\na subtask of information extraction that seeks to locate and classify named entities mentioned in \\nunstructured text into predefined categories such as person names, organizations, locations, medical \\ncodes, time expressions, quantities, monetary values, and percentages, among others.\\nLet’s see how easily this task can be performed with Hugging Face:\\n1. First of all, let’s create a NER pipeline:\\nfrom transformers import pipeline\\nner_pipe = pipeline( \"ner\")\\nsequence = \"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, were \\nproud to say that they were perfectly normal, thank you very much.\"\"\"\\nfor entity in ner_pipe(sequence):\\n    print(entity) \\n2. You will be able to see something like the following, where the entities are recognized:\\n{\\'entity\\': \\'I-PER\\', \\'score\\': 0.99908304, \\'index\\': 6, \\'word\\': \\'Du\\', \\n\\'start\\': 13, \\'end\\': 15}\\n{\\'entity\\': \\'I-PER\\', \\'score\\': 0.9869529, \\'index\\': 7, \\'word\\': \\'##rs\\', \\n\\'start\\': 15, \\'end\\': 17}\\n{\\'entity\\': \\'I-PER\\', \\'score\\': 0.9784202, \\'index\\': 8, \\'word\\': \\'##ley\\', \\n\\'start\\': 17, \\'end\\': 20}\\n{\\'entity\\': \\'I-ORG\\', \\'score\\': 0.6860208, \\'index\\': 14, \\'word\\': \\'P\\', \\n\\'start\\': 38, \\'end\\': 39}\\n{\\'entity\\': \\'I-ORG\\', \\'score\\': 0.7713562, \\'index\\': 15, \\'word\\': \\'##rive\\', \\n\\'start\\': 39, \\'end\\': 43}\\n{\\'entity\\': \\'I-ORG\\', \\'score\\': 0.76567733, \\'index\\': 16, \\'word\\': \\'##t\\', \\n\\'start\\': 43, \\'end\\': 44}\\n{\\'entity\\': \\'I-ORG\\', \\'score\\': 0.8087192, \\'index\\': 17, \\'word\\': \\'Drive\\', \\n\\'start\\': 45, \\'end\\': 50}\\nNamed entity recognition can understand nine different classes:\\n• O: Outside of a named entity.\\n• B-MIS : Beginning of a miscellaneous entity right after another miscellaneous entity.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6dc7c224-7452-48dc-8163-3973f22fff88', embedding=None, metadata={'page_label': '246', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 246\\n• I-MIS : Miscellaneous entity.\\n• B-PER : Beginning of a person’s name right after another person’s name.\\n• I-PER : A person’s name.\\n• B-ORG : Beginning of an organization right after another organization.\\n• I-ORG : Organization.\\n• B-LOC : Beginning of a location right after another location.\\n• I-LOC : Location.\\nThese entities are defined in the CoNLL-2003 dataset typically used for this task and automatically \\nselected by Hugging Face.\\nSummarization\\nLet’s now turn our attention to summarization, meaning the task of expressing the most important \\nfacts or ideas about something or someone in a short and clear form. Hugging Face makes it incredibly \\neasy to use the T5 model as default for this task. Let’s see the code:\\n1. First of all, let’s create a summarization pipeline using the default T5 small model: \\nfrom transformers import pipeline\\nsummarizer = pipeline( \"summarization\" )\\nARTICLE = \"\"\"\\n Mr.\\n and Mrs.\\n Dursley, of number four, Privet Drive, were proud to say\\n that they were perfectly normal, thank you very much.\\n They were the last\\n people you\\'d expect to be involved in anything strange or mysterious,\\nbecause they just didn\\'t hold with such nonsense.\\n Mr.\\n Dursley was the director of a firm called Grunnings, which made\\n drills.\\n He was a big, beefy man with hardly any neck, although he did\\n have a very large mustache.\\n Mrs.\\n Dursley was thin and blonde and had', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='af5323ef-f208-41f8-8992-7229c8b4fa19', embedding=None, metadata={'page_label': '247', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 247\\n nearly twice the usual amount of neck, which came in very useful as she\\n spent so much of her time craning over garden fences, spying on the\\n neighbors.\\n The Dursleys had a small son called Dudley and in their\\n opinion there was no finer boy anywhere\"\"\"\\nprint(summarizer(ARTICLE, max_length= 130, min_length= 30, do_\\nsample=False))\\n2. As a result, we will see something similar to the following:\\nNo model was supplied, defaulted to t5-small (https://huggingface.co/t5-\\nsmall)\\nDownloading: 100%|██████████████████████████| 1.17k/1.17k [00:00<00:00, \\n300kB/s]\\nDownloading: 100%|███████████████████████████| 231M/231M [01:29<00:00, \\n2.71MB/s]\\n[{\\'summary_text\\': \"Mr. and Mrs. Dursley, of number four, were the last \\npeople you\\'d expect to be involved in anything strange or mysterious . \\nthe Dursleys had a small son called Dudley and in their opinion there was \\nno finer boy anywhere .\"}]\\n3. Suppose that you want to change to a different model. That’s extremely simple as you only \\nneed to change one parameter: \\nsummarizer = pipeline( \"summarization\" , model= \\'t5-base\\' )\\n4. As a result, we can see something like the following: \\nDownloading: 100%|███████████████████████████████████████████████████████\\n█████| 773k/773k [00:00<00:00, 1.28MB/s]\\nDownloading: 100%|███████████████████████████████████████████████████████\\n███| 1.32M/1.32M [00:00<00:00, 1.93MB/s]\\n[{\\'summary_text\\': \"bob greene says he and his wife were perfectly normal \\n. he says they were the last people you\\'d expect to be involved in \\nanything strange or mysterious . greene: they were a big, beefy man with \\nhardly any neck, but had a very large mustache .\"}]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fab33d57-9ff2-4117-9a24-1f6c56c9ed87', embedding=None, metadata={'page_label': '248', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 248\\nFine-tuning\\nOne common usage pattern for transformers is to use a pretrained LLM and then fine-tune the model \\nfor specific downstream tasks. Of course, the fine-tuning steps will take place on your own dataset, \\nwhile pretraining is performed on very large datasets. The advantages of this two-step strategy are \\nin terms of both saving computation costs and in reducing the carbon footprint. Plus, fine-tuning \\nallows you to use state-of-the-art models without having to train one from scratch. Let’s see how to \\nfine-tune a model with TF. This example is available at https://huggingface.co/docs/transformers/\\ntraining , where the pretrained model used is bert-base-cased , which is fine-tuned on the “Yelp \\nReviews” dataset (available at https://huggingface.co/datasets/yelp_review_full ). Let’s see the \\ncode from https://huggingface.co/docs/transformers/training\\n1. First, let’s load and tokenize the Yelp dataset:\\nfrom datasets import load_dataset\\ndataset = load_dataset( \"yelp_review_full\" )\\nfrom transformers import AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained( \"bert-base-cased\" )\\ndef tokenize_function (examples):\\n    return tokenizer(examples[ \"text\"], padding= \"max_length\" , \\ntruncation= True)\\ntokenized_datasets = dataset. map(tokenize_function, batched= True)\\nsmall_train_dataset = tokenized_datasets[ \"train\"].shuffle(seed= 42).\\nselect(range(1000))\\nsmall_eval_dataset = tokenized_datasets[ \"test\"].shuffle(seed= 42).\\nselect(range(1000))\\n2. Then let’s convert them to TF format datasets:\\nfrom transformers import DefaultDataCollator\\ndata_collator = DefaultDataCollator(return_tensors= \"tf\")\\n# convert the tokenized datasets to TensorFlow datasets\\ntf_train_dataset = small_train_dataset.to_tf_dataset(\\n    columns=[ \"attention_mask\" , \"input_ids\" , \"token_type_ids\" ],\\n    label_cols=[ \"labels\" ],\\n    shuffle= True,\\n    collate_fn=data_collator,\\n    batch_size= 8,\\n)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b51b75df-7fc5-467b-83c0-4627d04e36f1', embedding=None, metadata={'page_label': '249', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 249\\ntf_validation_dataset = small_eval_dataset.to_tf_dataset(\\n    columns=[ \"attention_mask\" , \"input_ids\" , \"token_type_ids\" ],\\n    label_cols=[ \"labels\" ],\\n    shuffle= False,\\n    collate_fn=data_collator,\\n    batch_size= 8,\\n)\\n3. Now, we can use TFAutoModelForSequenceClassification , specifically selecting bert-base-\\ncased :\\nimport tensorflow as tf\\nfrom transformers import TFAutoModelForSequenceClassification\\nmodel = TFAutoModelForSequenceClassification.from_pretrained( \"bert-base-\\ncased\", num_labels= 5)\\n4. Finally, the fine-tuning is simply using the standard way to train a model used in Keras/TF 2.0 \\nby compiling the model and then using fit on it:\\nmodel.compile(\\n    optimizer=tf.keras.optimizers.Adam(learning_rate= 5e-5),\\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True),\\n    metrics=tf.metrics.SparseCategoricalAccuracy(),\\n)\\nmodel.fit(tf_train_dataset, validation_data=tf_validation_dataset, \\nepochs=3)\\nIf you want, you can test the code on a public Colab notebook (available at https://huggingface.\\nco/docs/transformers/training ). If you run the code yourself, you should be able to see something \\nsimilar to Figure 6.16:\\nFigure 6.16: Fine-tuning BERT on a Colab notebook\\nNext, we are going to introduce TFHub.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77e7f887-0a19-4f31-984d-c0d0100e8f19', embedding=None, metadata={'page_label': '250', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 250\\nTFHub\\nIn the previous section, we discussed how to use the Hugging Face Transformer library. Now, we will \\nhave a look at another library known as TFHub available at https://tfhub.dev/ . TensorFlow Hub is \\na repository of trained machine learning models ready for fine-tuning and deployable anywhere. The \\nkey idea is to reuse trained models like BERT and Faster R-CNN with just a few lines of code.\\nUsing TFHub is as easy as writing a few lines of code. Let’s see a simple example where we load a \\npretrained model for computing embeddings. In this case, we use nnlm-en-dim128 , a token-based \\ntext embedding trained on the English Google News 200B corpus:\\n!pip install --upgrade tensorflow_hub\\nimport tensorflow_hub as hub\\nmodel = hub.KerasLayer( \"https://tfhub.dev/google/nnlm-en-dim128/2\" )\\nembeddings = model([ \"The rain in Spain.\" , \"falls\" ,\\n                    \"mainly\" , \"In the plain!\" ])\\nprint(embeddings.shape)  #(4,128)\\nNow let’s see how to use BERT. This code is adapted from https://www.tensorflow.org/hub/\\ntutorials/bert_experts , and it is also available on Hugging Face ( https://huggingface.co/docs/\\ntransformers/training ):\\n1. Let’s set up the environment and import useful modules:\\n!pip install seaborn\\n!pip install sklearn\\n!pip install tensorflow_hub\\n!pip install tensorflow_text\\nimport seaborn as sns\\nfrom sklearn.metrics import pairwise\\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\nimport tensorflow_text as text  # Imports TF ops for preprocessing.\\n2. Let’s define a few sentences used for comparing their similarities:\\nsentences = [\\n    \"Do not pity the dead, Harry. Pity the living, and, above all those \\nwho live without love.\" ,\\n    \"It is impossible to manufacture or imitate love\" ,\\n    \"Differences of habit and language are nothing at all if our aims are ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc50264d-f693-4ffb-a097-2107fc54ef3d', embedding=None, metadata={'page_label': '251', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 251\\nidentical and our hearts are open.\" ,\\n    \"What do I care how he looks? I am good-looking enough for both of \\nus, I theenk! All these scars show is zat my husband is brave!\" ,\\n    \"Love as powerful as your mother\\'s for you leaves it\\'s own mark. To \\nhave been loved so deeply, even though the person who loved us is gone, \\nwill give us some protection forever.\" ,\\n    \"Family…Whatever yeh say, blood\\'s important. . . .\" ,\\n    \"I cared more for your happiness than your knowing the truth, more \\nfor your peace of mind than my plan, more for your life than the lives \\nthat might be lost if the plan failed.\"\\n]\\n3. Then, let’s use a pretrained BERT model available on TFHub to compute embeddings on the \\ninput sentences just defined. BERT’s output is the set of embeddings itself:\\n#@title Configure the model { run: \"auto\" }\\nBERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/2\"  # @\\nparam {type: \"string\"} [\"https://tfhub.dev/google/experts/bert/wiki_\\nbooks/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/mnli/2\", \\n\"https://tfhub.dev/google/experts/bert/wiki_books/qnli/2\", \"https://\\ntfhub.dev/google/experts/bert/wiki_books/qqp/2\", \"https://tfhub.dev/\\ngoogle/experts/bert/wiki_books/squad2/2\", \"https://tfhub.dev/google/\\nexperts/bert/wiki_books/sst2/2\",  \"https://tfhub.dev/google/experts/bert/\\npubmed/2\", \"https://tfhub.dev/google/experts/bert/pubmed/squad2/2\"]\\n# Preprocessing must match the model, but all the above use the same.\\nPREPROCESS_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_\\npreprocess/3\"\\npreprocess = hub.load(PREPROCESS_MODEL)\\nbert = hub.load(BERT_MODEL)\\ninputs = preprocess(sentences)\\noutputs = bert(inputs)\\n4. Now let’s define some auxiliary functions to show the similarity among embeddings based on \\npairwise.cosine_similarity :\\ndef plot_similarity (features, labels):\\n  \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\\n  cos_sim = pairwise.cosine_similarity(features)\\n  sns.set(font_scale= 1.2)\\n  cbar_kws= dict(use_gridspec= False, location= \"left\")\\n  g = sns.heatmap(\\n      cos_sim, xticklabels=labels, yticklabels=labels,\\n      vmin= 0, vmax= 1, cmap= \"Blues\", cbar_kws=cbar_kws)\\n  g.tick_params(labelright= True, labelleft= False)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e775a02e-8b1d-461d-8e2d-7d51d5399c81', embedding=None, metadata={'page_label': '252', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 252\\n  g.set_yticklabels(labels, rotation= 0)\\n  g.set_title( \"Semantic Textual Similarity\" )\\nplot_similarity(outputs[ \"pooled_output\" ], sentences)\\nThe interested reader can access the Colab notebook online on the Hugging Face website (available at \\nhttps://huggingface.co/docs/transformers/training ) and visualize a heatmap showing similarities \\namong sentences. Overall, using LLMs with TFHub is pretty easy, isn’t it?\\nEvaluation\\nEvaluating transformers involves considering multiple classes of metrics and understanding the cost \\ntradeoffs among these classes. Let’s see the main ones.\\nQuality\\nThe quality of transformers can be measured against a number of generally available datasets. Let’s \\nsee the most commonly used ones.\\nGLUE\\nThe General Language Understanding Evaluation (GLUE ) benchmark is a collection of resources for \\ntraining, evaluating, and analyzing natural language understanding systems. GLUE is available at \\nhttps://gluebenchmark.com/ .\\nGLUE consists of:\\n• A benchmark of nine sentence or sentence-pair language understanding tasks built on \\nestablished existing datasets and selected to cover a diverse range of dataset sizes, text genres, \\nand degrees of difficulty\\n• A diagnostic dataset designed to evaluate and analyze model performance with respect to a \\nwide range of linguistic phenomena found in natural language\\n• A public leaderboard for tracking performance on the benchmark and a dashboard for \\nvisualizing the performance of models on the diagnostic set', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50d4e0a1-25c6-43fb-a22e-a8ea7f824794', embedding=None, metadata={'page_label': '253', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 253\\nFigure 6.17 shows the GLUE dashboard from March 2022:\\nFigure 6.17: GLUE dashboard\\nSuperGLUE\\nIn recent years, new models and methods for pretraining and transfer learning have driven striking \\nperformance improvements across a range of language understanding tasks. The GLUE benchmark \\noffers a single-number metric that summarizes progress on a diverse set of such tasks, but performance \\non the benchmark has recently come close to the level of non-expert humans, suggesting limited \\nheadroom for further research. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c58c2160-5e82-49f5-80cf-a6de329ddb00', embedding=None, metadata={'page_label': '254', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 254\\nSuperGLUE is a new benchmark styled after GLUE with a new set of more difficult language \\nunderstanding tasks, improved resources, and a new public leaderboard. Figure 6.18 is the SuperGLUE \\nleaderboard from March 2022:\\nFigure 6.18: SuperGLUE leaderboard\\nSQuAD\\nSQuAD is a dataset used to evaluate questions and answers, https://rajpurkar.github.io/SQuAD-\\nexplorer/ . Specifically, the Stanford Question Answering Dataset ( SQuAD) is a reading comprehension \\ndataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer \\nto every question is a segment of text, or span, from the corresponding reading passage, otherwise \\nthe question might be unanswerable.\\nSQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions \\nwritten adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, \\nsystems must not only answer questions when possible but also determine when no answer is supported \\nby the paragraph and abstain from answering.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1dc81cdf-76a0-4718-8303-8329f07611f9', embedding=None, metadata={'page_label': '255', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 255\\nRACE\\nThe ReAding Comprehension dataset from Examinations ( RACE ) dataset is a machine reading \\ncomprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, \\ntargeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from \\nmiddle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has \\n69,574. Each question is associated with four candidate answers, one of which is correct. The data \\ngeneration process of RACE differs from most machine reading comprehension datasets. Instead of \\ngenerating questions and answers by heuristics or crowdsourcing, questions in RACE are specifically \\ndesigned for testing human reading skills and are created by domain experts. RACE is available at \\nhttps://www.cs.cmu.edu/~glai1/data/race/ . Figure 6.19 shows the RACE leaderboard:\\nFigure 6.19: RACE leaderboard\\nNLP-progress\\nNLP-progress is a repository, made to track progress in NLP, including the datasets and the current state-\\nof-the-art models for the most common NLP tasks. The site aims to track the progress in NLP and gives \\nan overview of the state-of-the-art models across the most common NLP tasks and their corresponding \\ndatasets. NLP-progress aims to cover both traditional and core NLP tasks such as dependency parsing \\nand part-of-speech tagging as well as more recent ones such as reading comprehension and natural \\nlanguage inference. If you need a good starting point to find quality metrics for your task, then http://\\nnlpprogress.com/  is the place to start with.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bdefaf4-f85c-4aa4-8f6f-ec1a06ba3282', embedding=None, metadata={'page_label': '256', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 256\\nSize\\nThe previous section provided an overview of quality metrics. This section focuses on the number \\nof parameters used in various transformer architectures. As shown in Figure 6.20, there has been a \\nrace to increase transformers’ size during the last few years. Back in 2018, BERT’s size was about 340 \\nmillion parameters, then in 2021, T5 reached 11 billion, and Megatron passed 500 billion. The very \\nrecent Switch Transformer has more than one trillion parameters and there is an expectation that \\nsoon we will see the first model with 100 trillion parameters. Indeed, there is evidence that the larger \\nthe model is the merrier, which can memorize information and generalize. However, training such \\nlarge models requires massive computational resources:\\nFigure 6.20: Transformers’ size in billions of parameters\\nTrillion parameter transformers are on their way!\\nIn fact, the paper https://arxiv.org/pdf/1906.02243.pdf  warns about the sustainability impact \\nof training large models (see Figure 6.21) in terms of both cloud computing cost and CO2 emissions:\\nFigure 6.21:  Estimated cost of training a model in terms of CO2 emissions (lbs) and cloud computing \\ncost (USD) - source: https://arxiv.org/pdf/1906.02243.pdf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='be30679d-772e-4b2e-a3c0-55436b312a2c', embedding=None, metadata={'page_label': '257', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 257\\nSo, size is not the only factor that enables the quality of transformers to improve, as larger sizes can \\nin reality give only marginal gains and require significant computational resources for training.\\nLarger doesn’t always mean better\\nAt the beginning of 2022, a new trend is emerging consisting of a hybrid approach where large models \\nare used together with a more traditional retrieval mechanism. We discussed this approach earlier in the \\nchapter when we discussed RETRO. The RETRO language model implements a learning scheme based \\non the use of external memory. DeepMind claimed that RETRO (or “Retrieval Enhanced Transformer”) \\nperforms like a neural network 25 times its size. GPT-3 has 175 billion parameters and RETRO uses \\njust seven billion of them. Of course, this requires less time, energy, and computing power to train.\\nCost of serving\\nThe cost of serving a model depends on many factors and it’s difficult to estimate it without making \\nreasonable assumptions. Of course, serving is a function of the number of parameters in the model. \\nIn addition, the number of queries submitted to the model for inference is another factor. Then, \\nit’s important to consider whether or not a cloud provider manages the model or is served in your \\non-prem infrastructure. In this context, it might be useful to remember that MLOps (see https://\\nen.wikipedia.org/wiki/MLOps ) is the process of developing a machine learning model and deploying \\nit as a production system. Of course, MLOps’ best practices might be adopted to optimize the costs \\nof serving.\\nIn this section, we have seen some key factors used to evaluate transformers, namely quality, size, \\nand cost of serving. The list is clearly not inclusive, and a proper evaluation will take into account \\nwhat the optimal tradeoff is among these factors. In the next section, we will discuss optimization.\\nOptimization\\nOptimizing a transformer involves building lightweight, responsive, and energy-efficient models. Let’s \\nsee the most common ideas adopted to optimize a model.\\nQuantization\\nThe key idea behind quantization is to approximate the weights of a network with a smaller precision. \\nThe idea is very simple, but it works quite well in practice. If you are interested in knowing more, we \\nrecommend the paper A Survey of Quantization Methods for Efficient Neural Network Inference, by Amir \\nGholami et al., https://arxiv.org/pdf/2103.13630.pdf .\\nWeight pruning\\nThe key idea behind weight pruning is to remove some connections in the network. Magnitude-based \\nweight pruning tends to zero out of model weights during training to increase model sparsity. This \\nsimple technique has benefits both in terms of model size and in cost of serving, as magnitude-based \\nweight pruning gradually zeroes out of model weights during the training process to achieve model \\nsparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency \\nimprovements. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b90e7514-da61-428f-b38a-03f09424e36d', embedding=None, metadata={'page_label': '258', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 258\\nOne more time, weight pruning is about tradeoffs as it might generate some quality losses although \\nnormally, they are rather small. If you are interested to know more, please have a look at the \\nTensorFlow guide about pruning: https://www.tensorflow.org/model_optimization/guide/\\npruning/comprehensive_guide .\\nDistillation\\nThe key idea behind knowledge distillation is to have a small model trained to reproduce the behavior \\nof a larger model. This compression technique is sometimes referred to as teacher-student learning. \\nThe seminal paper you should check is Distilling the Knowledge in a Neural Network by Geoffrey Hinton, \\nOriol Vinyals, and Jeff Dean, https://arxiv.org/abs/1503.02531 . \\nDuring the last few years, we have seen a number of distilled transformers. For instance, DistilBERT \\nis a small, fast, cheap, and light transformer model based on the BERT architecture. Knowledge \\ndistillation is performed during the pretraining phase to reduce the size of a BERT model by 40%. \\nHugging Face has some ready-to-use Python scripts for distilling seq2seq T5 models available online \\nat https://github.com/huggingface/transformers/tree/master/examples/research_projects/\\nseq2seq-distillation . Using the script is quite intuitive:\\npython distillation.py --teacher t5-small --data_dir cnn_dm \\\\\\n--student_decoder_layers 3 --student_encoder_layers 6 --tokenizer_name t5-small \\n\\\\\\n--learning_rate=3e-4 --freeze_encoder --no_teacher --freeze_embeds \\\\\\n--do_train --train_batch_size 32 \\\\\\n--do_predict \\\\\\n--model_name_or_path t5-small --eval_beams 2 --eval_max_gen_length 142 \\\\\\n--val_check_interval 0.25 --n_val 1000 \\\\\\n--output_dir distilt5 --gpus 1 --logger_name wandb\\nIn this section, we have discussed a few techniques used to optimize transformers, namely quantization, \\nweight pruning, and distillation. In the next section, we will discuss common pitfalls for transformers.\\nCommon pitfalls: dos and don’ts\\nIn this section, we will give five dos and a few don’ts that are typically recommended when dealing \\nwith transformers.\\nDos\\nLet’s start with recommended best practices:\\n• Do use pretrained large models � Today, it is almost always convenient to start from an already \\navailable pretrained model such as T5, instead of training your transformer from scratch. If \\nyou use a pretrained model, you for sure stand on the giant’s shoulders; think about it!\\n• Do start with few-shot learning� When you start working with transformers, it’s always a good \\nidea to start with a pretrained model and then perform a lightweight few-shot learning step. \\nGenerally, this would improve the quality of results without high computational costs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45c2a1a7-1535-4efb-8806-d423b4eedfb1', embedding=None, metadata={'page_label': '259', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 259\\n• Do use fine-tuning on your domain data and on your customer data� After playing with \\npretraining models and few-shot learning, you might consider doing a proper fine-tuning on \\nyour own proprietary data or on publicly available data for your domain of interest\\n• Get yourself familiar with transformers’ libraries � Hugging Face or TFHub provide already \\navailable state-of-the-art implementations of almost all the known transformers. It might be \\nuseful to start from there unless you either have some very peculiar needs or are doing some \\ninnovative research work.\\n• Get yourself familiar with the most commonly used evaluation metrics � When you use \\ntransformers, it is ideal to take into account the tradeoff faced in terms of quality, size, cost of \\nserving, and many other factors.\\nDon’ts\\nNow let’s have a look at some of the pitfalls that you should avoid!\\n• Do not use very large models as a starting point� Large models come with a cost in terms of \\ntraining and serving. You will need significant resources to fine-tune, and you might pay high \\ncosts for serving each query. It might be better to start with smaller models and understand \\nwhether they are useful for your quality needs.\\n• Do not use unoptimized models � Nowadays, quantization, pruning, and distillation are standard \\ntechniques that need to be used by any transformer system that is put into production.\\nIn this section, we have seen some of the best practices for transformers. In the next section, we will \\ntalk about future solutions for these architectures.\\nThe future of transformers\\nTransformers found their initial applications in NLP tasks, while CNNs are typically used for image \\nprocessing systems. Recently, transformers have started to be successfully used for vision processing \\ntasks. Vision transformers compute relationships among pixels in various small sections of an image \\n(for example, 16 x 16 pixels). This approach has been proposed in the seminar paper An Image is Worth \\n16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy et al., https://arxiv.\\norg/abs/2010.11929 , to make the attention computation feasible.\\nVision transformers ( ViTs ) are today used for complex applications such as autonomous driving. Tesla’s \\nengineers showed that their Tesla Autopilot uses a transformer on the multi-camera system in cars. \\nOf course, ViTs are also used for more traditional computer vision tasks, including but not limited \\nto image classification, object detection, video deepfake detection, image segmentation, anomaly \\ndetection, image synthesis, and cluster analysis. The results are frequently better than CNNs.\\nAnother direction to consider is few-shot learning. Few-shot learning refers to the practice of feeding \\na machine learning model with a very small amount of training data to guide its predictions, like a few \\nexamples at inference time, as opposed to standard fine-tuning techniques that require a relatively \\nlarge amount of training data for the pretrained model to adapt to the desired task with accuracy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06949b1b-d8de-4bad-b543-3ec84043921e', embedding=None, metadata={'page_label': '260', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformers 260\\nSo, a model trained for a specific task can be reused for completely new tasks with very marginal costs. \\nFor instance, suppose we train a text model to generate text. Then, we want to perform new tasks such \\nas translation or summarization. What we do is give a few examples of translations (say with pairs of \\ntext manually translated), or a few examples of summarization (again a few pairs). That’s it, no need \\nfor retraining or fine-tuning training.\\nSince FSL is proven to work well in a number of increasing domains, don’t be surprised that the \\ntraining phase will be less and less relevant for future AI. More information can be found in this paper, \\nCode Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code by \\nPatrick Bareiß, Beatriz Souza, Marcelo d’Amorim, and Michael Pradel. The authors propose to use \\nFSL to generate programming code with CodeGen, an open source mode for program synthesis (see \\nhttps://github.com/salesforce/CodeGen ).\\nSummary\\nIn this chapter, we discussed transformers, a deep learning architecture that has revolutionized the \\ntraditional natural language processing field. We started reviewing the key intuitions behind the \\narchitecture, and various categories of transformers together with a deep dive into the most popular \\nmodels. Then, we focused on implementations both based on vanilla architecture and on popular \\nlibraries such as Hugging Face and TFHub. After that, we briefly discussed evaluation, optimization, \\nand some of the best practices commonly adopted when using transformers. The last section was \\ndevoted to reviewing how transformers can be used to perform computer vision tasks, a totally different \\ndomain from NLP. That requires a careful definition of the attention mechanism. In the end, attention \\nis all you need! And at the core of attention is nothing more than the cosine similarity between vectors.\\nThe next chapter is devoted to unsupervised learning.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a7aced6-f676-4ef4-881b-861d378fbad3', embedding=None, metadata={'page_label': '261', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7\\nUnsupervised Learning\\nThe book till now has focused on supervised learning and the models that learn via supervised learning. \\nStarting from this chapter we will explore a less explored and more challenging area of unsupervised \\nlearning, self-supervised learning, and contrastive learning. In this chapter, we will delve deeper into \\nsome popular and useful unsupervised learning models. In contrast to supervised learning, where \\nthe training dataset consists of both the input and the desired labels, unsupervised learning deals \\nwith a case where the model is provided with only the input. The model learns the inherent input \\ndistribution by itself without any desired label guiding it. Clustering and dimensionality reduction are \\nthe two most commonly used unsupervised learning techniques. In this chapter, we will learn about \\ndifferent machine learning and neural network techniques for both. We will cover techniques required \\nfor clustering and dimensionality reduction, and go into the detail about Boltzmann machines, and \\nfinally, cover the implementation of the aforementioned techniques using TensorFlow. The concepts \\ncovered will be extended to build Restricted Boltzmann Machines (RBMs ). The c hapter will include:\\n• Principal component analysis\\n• K-means clustering\\n• Self-organizing maps\\n• Boltzmann machines\\n• RBMs\\nLet us start with the most common and frequently used technique for dimensionality reduction, the \\nprincipal component analysis method.\\nPrincipal component analysis\\nPrincipal component analysis ( PCA ) is the most popular multivariate statistical technique for \\ndimensionality reduction. It analyzes the training data consisting of several dependent variables, \\nwhich are, in general, intercorrelated, and extracts important information from the training data in \\nthe form of a set of new orthogonal variables called principal components. All the code files for this chapter can be found at https://packt.link/dltfchp7 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c319e604-205e-4a79-8534-3368cccb9a32', embedding=None, metadata={'page_label': '262', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 262\\nWe can perform PCA using two methods, either eigen decomposition or singular value decomposition  \\n(SVD ).\\nPCA reduces the n -dimensional input data to r -dimensional input data, where r<n. In simple terms, PCA \\ninvolves translating the origin and performing rotation of the axis such that one of the axes (principal \\naxis) has the highest variance with data points. A reduced-dimensions dataset is obtained from the \\noriginal dataset by performing this transformation and then dropping (removing) the orthogonal axes \\nwith low variance. Here, we employ the SVD method for PCA dimensionality reduction. Consider X, \\nthe n-dimensional data with p points, that is, X is a matrix of size p × n. From linear algebra we know \\nthat any real matrix can be decomposed using singular value decomposition:\\n𝑋𝑋 𝑋 𝑋𝑋𝑋𝑋𝑋𝑇𝑇 \\nWhere U and V  are orthonormal matrices (that is, U.UT = V.VT = 1) of size p × p and n × n respectively. ∑  is \\na diagonal matrix of size p × n. The U matrix is called the left singular matrix, and V the right singular \\nmatrix, and ∑ , the diagonal matrix, contains the singular values of X as its diagonal elements. Here \\nwe assume that the X matrix is centered. The columns of the V matrix are the principal components, \\nand columns of 𝑈𝑈𝑈  are the data transformed by principal components.\\nNow to reduce the dimensions of the data from n to k (where k < n), we will select the first k columns \\nof U and the upper-left k × k  part of ∑ . The product of the two gives us our reduced-dimensions matrix:\\n𝑌𝑌𝑘𝑘=𝑈𝑈𝑈𝑘𝑘 \\nThe data Y obtained will be of reduced dimensions. Next, we implement PCA in TensorFlow 2.0.\\nPCA on the MNIST dataset\\nLet us now implement PCA in TensorFlow 2.0. We will be definitely using TensorFlow; we will also \\nneed NumPy for some elementary matrix calculation, and Matplotlib, Matplotlib toolkits, and Seaborn \\nfor plotting:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nimport seaborn as sns\\nNext we load the MNIST dataset. Since we are doing dimension reduction using PCA, we do not need a \\ntest dataset or even labels; however, we are loading labels so that after reduction we can verify the PCA \\nperformance. PCA should cluster similar data points in one cluster; hence, if we see that the clusters \\nformed using PCA are similar to our labels, it would indicate that our PCA works:\\n((x_train, y_train), (_, _)) = tf.keras.datasets.mnist.load_data()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa62f7a3-c0e3-482e-a668-dcec7b6d5fd5', embedding=None, metadata={'page_label': '263', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 263\\nBefore we do PCA, we should preprocess the data. We first normalize it so that all data has values \\nbetween 0 and 1, and then reshape the image from being a 28 × 28 matrix to a 784-dimensional vector, \\nand finally, center it by subtracting the mean:\\nx_train = x_train / 255.\\nx_train = x_train.astype(np.float32)\\nx_train = np.reshape(x_train, (x_train.shape[ 0], 784))\\nmean = x_train.mean(axis = 1)\\nx_train = x_train - mean[:, None]\\nNow that our data is in the right format, we make use of TensorFlow’s powerful linear algebra ( linalg ) \\nmodule to calculate the SVD of our training dataset. TensorFlow provides the function svd()  defined \\nin tf.linalg  to perform this task. And then use the diag  function to convert the sigma array ( s, a list \\nof singular values) to a diagonal matrix:\\ns, u, v = tf.linalg.svd(x_train)\\ns = tf.linalg.diag(s)\\nThis provides us with a diagonal matrix s of size 784 × 784; a left singular matrix u of size 60,000 × 784; \\nand a right singular matrix v of size 784 × 784. This is so because the argument full_matrices  of the \\nfunction svd()  is by default set to False . As a result it does not generate the full U matrix (in this case, \\nof size 60,000 × 60,000); instead, if input X is of size m × n, it generates U of size p = min(m,n).\\nThe reduced-dimension data can now be generated by multiplying respective slices of u and s. We \\nreduce  our data from 784 to 3 dimensions; we can choose to reduce to any dimension less than 784, \\nbut we chose 3 here so that it is easier for us to visualize later. We make use of tf.Tensor.getitem  to \\nslice our matrices in the Pythonic way:\\nk = 3\\npca = tf.matmul(u[:, 0:k], s[ 0:k,0:k])\\nA comparison of the original and reduced data shape is done in the following code:\\nprint(\\'original data shape\\' ,x_train.shape)\\nprint(\\'reduced data shape\\' , pca.shape)\\noriginal data shape (60000, 784)\\nreduced data shape (60000, 3)\\nFinally, let us plot the data points in the three-dimensional space:\\nSet = sns.color_palette( \"Set2\", 10)\\ncolor_mapping = {key:value for (key,value) in enumerate (Set)}\\ncolors = list(map(lambda x: color_mapping[x], y_train))\\nfig = plt.figure()\\nax = Axes3D(fig)\\nax.scatter(pca[:, 0], pca[:, 1],pca[:, 2], c=colors)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc58aa94-f5da-409a-a7b3-f78e6891a54a', embedding=None, metadata={'page_label': '264', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 264\\nFigure 7.1: Scatter plot of MNIST dataset after dimensionality reduction using PCA\\nYou can see that the points corresponding to the same color and, hence, the same label are clustered \\ntogether. We have therefore successfully used PCA to reduce the dimensions of MNIST images. Each \\noriginal image was of size 28 × 28. Using the PCA method we can reduce it to a smaller size. Normally \\nfor image data, dimensionality reduction is necessary. This is because images are large in size and \\ncontain a significant amount of redundant data.\\nTensorFlow Embedding API\\nTensorFlow also offers an Embedding API where one can find and visualize PCA and tSNE [1] clusters \\nusing TensorBoard. You can see the live PCA on MNIST images here: http://projector.tensorflow.\\norg. The following image is reproduced for reference:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2db9c3e5-e3eb-4886-9c91-c0f8085112e6', embedding=None, metadata={'page_label': '265', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 265\\nFigure 7.2: A visualization of a principal component analysis, applied to the MNIST dataset\\nYou can process your data using TensorBoard. It contains a tool called Embedding Projector that allows \\none to interactively visualize embedding. The Embedding Projector tool has three panels:\\n• Data Panel: It is located at the top left, and you can choose the data, labels, and so on in this \\npanel.\\n• Projections Panel: Available at the bottom left, you can choose the type of projections you \\nwant here. It offers three choices: PCA, t-SNE, and custom.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ca31a54-e732-42d2-987b-52bc1d5437a0', embedding=None, metadata={'page_label': '266', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 266\\n• Inspector Panel: On the right-hand side, here you can search for particular points and see a \\nlist of nearest neighbors.\\nFigure 7.3: Screenshot of the Embedding Projector tool\\nPCA is a useful tool for visualizing datasets and for finding linear relationships between variables. It \\ncan also be used for clustering, outlier detection, and feature selection. Next, we will learn about the \\nk-means algorithm, a method for clustering data.\\nK-means clustering\\nK-means clustering, as the name suggests, is a technique to cluster data, that is, to partition data into \\na specified number of data points. It is an unsupervised learning technique. It works by identifying \\npatterns in the given data. Remember the sorting hat of Harry Potter fame? What it is doing in the book \\nis clustering—dividing new (unlabelled) students into four different clusters: Gryffindor, Ravenclaw, \\nHufflepuff, and Slytherin.\\nHumans are very good at grouping objects together; clustering algorithms try to give a similar capability \\nto computers. There are many clustering techniques available, such as hierarchical, Bayesian, or \\npartitional. K-means clustering belongs to partitional clustering; it partitions data into k  clusters. \\nEach cluster has a center, called the centroid. The number of clusters k has to be specified by the user.\\nThe k-means algorithm works in the following manner:\\n1. Randomly choose k data points as the initial centroids (cluster centers).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='216855cd-3953-44fb-a886-615da2dbf9f6', embedding=None, metadata={'page_label': '267', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 7 267\\n2. Assign each data point to the closest centroid; there can be different measures to find closeness, \\nthe most common being the Euclidean distance.\\n3. Recompute the centroids using current cluster membership, such that the sum of squared \\ndistances decreases.\\n4. Repeat the last two steps until convergence is met.\\nIn the previous TensorFlow versions, the KMeans  class was implemented in the Contrib  module; \\nhowever, the class is no longer available in TensorFlow 2.0. Here we will instead use the advanced \\nmathematical functions provided in TensorFlow 2.0 to implement k-means clustering.\\nK-means in TensorFlow\\nTo demonstrate k-means in TensorFlow, we will use randomly generated data in the code that follows. \\nOur randomly generated data will contain 200 samples, and we will divide them into three clusters. \\nWe start by importing all the required modules, defining the variables, and determining the number \\nof sample points ( points_n ), the number of clusters to be formed ( clusters_n ), and the number of \\niterations we will be doing ( iteration_n ). We also set the seed for a random number to ensure that \\nour work is reproducible:\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport tensorflow as tf\\npoints_n = 200\\nclusters_n = 3\\niteration_n = 100\\nseed = 123\\nnp.random.seed(seed)\\ntf.random.set_seed(seed)\\nNow we randomly generate data and from the data select three centroids randomly:\\npoints = np.random.uniform( 0, 10, (points_n, 2))\\ncentroids = tf. slice(tf.random.shuffle(points), [ 0, 0], [clusters_n, - 1])\\nLet us now plot the points:\\nplt.scatter(points[:, 0], points[:, 1], s=50, alpha= 0.5)\\nplt.plot(centroids[:, 0], centroids[:, 1], 'kx', markersize= 15)\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dc12cc23-a155-4380-a2b1-b0661bb9e128', embedding=None, metadata={'page_label': '268', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Unsupervised Learning 268\\nYou can see the scatter plot of all the points and the randomly selected three centroids in the following \\ngraph:\\nFigure 7.4: Randomly generated data, from three randomly selected centroids, plotted\\nWe define the function closest_centroids()  to assign each point to the centroid it is closest to:\\ndef closest_centroids (points, centroids):\\n    distances = tf.reduce_sum(tf.square(tf.subtract(points, \\ncentroids[:, None])), 2)\\n    assignments = tf.argmin(distances, 0)\\n    return assignments\\nWe create another function move_centroids() . It recalculates the centroids such that the sum of \\nsquared distances decreases:\\ndef move_centroids (points, closest, centroids):\\n    return np.array([points[closest==k].mean(axis= 0) for k in range (centroids.\\nshape[0])])\\nNow we call these two functions iteratively for 100 iterations. We have chosen the number of iterations \\narbitrarily; you can increase and decrease it to see the effect:\\nfor step in range (iteration_n):\\n    closest = closest_centroids(points, centroids)\\n    centroids = move_centroids(points, closest, centroids)\\nLet us now visualize how the centroids have changed after 100 iterations:\\nplt.scatter(points[:, 0], points[:, 1], c=closest, s= 50, alpha= 0.5)\\nplt.plot(centroids[:, 0], centroids[:, 1], 'kx', markersize= 15)\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4947261-8e93-4b8f-87df-faf4d4bdf8ab', embedding=None, metadata={'page_label': '269', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 269\\nIn Figure 7.5, you can see the final centroids after 100 iterations. We have also colored the points based \\non which centroid they are closest to. The yellow points correspond to one cluster (nearest the cross \\nin its center), and the same is true for the purple and green cluster points:\\nFigure 7.5: Plot of the final centroids after 100 iterations\\nIn the preceding code, we decided to limit the number of clusters to three, but in most cases with \\nunlabelled data, one is never sure how many clusters exist. One can determine the optimal number \\nof clusters using the elbow method. The method is based on the principle that we should choose the \\ncluster number that reduces the sum of squared error (SSE ) distance. If k is the number of clusters, \\nthen as k  increases, the SSE decreases, with SSE = 0; when k  is equal to the number of data points, each \\npoint is its own cluster. It is clear we do not want this as our number of clusters, so when we plot the \\ngraph between SSE and the number of clusters, we should see a kink in the graph, like the elbow of \\nthe hand, which is how the method gets its name – the elbow method. The following code calculates \\nthe sum of squared errors for our data:\\ndef sse(points, centroids):\\n    sse1 = tf.reduce_sum(tf.square(tf.subtract(points, centroids[:, None])), \\n2).numpy()\\n    s = np.argmin(sse1, 0)\\n    distance = 0\\n    for i in range (len(points)):\\n      distance += sse1[s[i], i]\\n    return distance/ len(points)Please note that the plot  command works in Matplotlib 3.1.1  or higher versions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f34ec669-512b-4464-8cc2-2831140b5cea', embedding=None, metadata={'page_label': '270', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Unsupervised Learning 270\\nLet us use the elbow method now for finding the optimum number of clusters for our dataset. To \\ndo that we will start with one cluster, that is, all points belonging to a single cluster, and increase \\nthe number of clusters sequentially. In the code, we increase the clusters by one, with eleven being \\nthe maximum number of clusters. For each cluster number value, we use the code above to find the \\ncentroids (and hence the clusters) and find the SSE:\\nw_sse = []\\nfor n in range (1, 11):\\n  centroids = tf. slice(tf.random.shuffle(points), [ 0, 0], [n, - 1])\\n  for step in range (iteration_n):\\n    closest = closest_centroids(points, centroids)\\n    centroids = move_centroids(points, closest, centroids)\\n  #print(sse(points, centroids))\\n  w_sse.append(sse(points, centroids))\\nplt.plot( range(1, 11),w_sse) \\nplt.xlabel( 'Number of clusters' ) \\nFigure 7.6 shows the different cluster values for the dataset. The kink is clearly visible when the number \\nof clusters is four:\\nFigure 7.6: Plotting SSE against the number of clusters\\nK-means clustering is very popular because it is fast, simple, and robust. It also has some disadvantages, \\nthe biggest being that the user has to specify the number of clusters. Second, the algorithm does not \\nguarantee global optima; the results can change if the initial randomly chosen centroids change. Third, \\nit is very sensitive to outliers.\\nVariations in k-means\\nIn the original k-means algorithm each point belongs to a specific cluster (centroid); this is  called hard \\nclustering. However, we can have one point belong to all the clusters, with a membership function \\ndefining how much it belongs to a particular cluster (centroid). This is called fuzzy clustering or soft  \\nclustering. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2975070a-1c25-41ad-92af-9e0c67735cac', embedding=None, metadata={'page_label': '271', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 271\\nThis variation was proposed in 1973 by J. C. Dunn and later improved upon by J. C. Bezdek in 1981. \\nThough soft clustering takes longer to converge, it can be useful when a point is in multiple classes, \\nor when we want to know how similar a given point is to different clusters.\\nThe accelerated k-means algorithm was created in 2003 by Charles Elkan. He exploited the triangle \\ninequality relationship (that is, a straight line is the shortest distance between two points). Instead of \\njust doing all distance calculations at each iteration, he also kept track of the lower and upper bounds \\nfor distances between points and centroids.\\nIn 2006, David Arthur and Sergei Vassilvitskii proposed the k-means++ algorithm. The major change \\nthey proposed was in the initialization of centroids. They showed that if we choose centroids that are \\ndistant from each other, then the k-means algorithm is less likely to converge on a suboptimal solution.\\nAnother alternative can be that at each iteration we do not use the entire dataset, instead using mini-\\nbatches. This modification was proposed by David Sculey in 2010. Now, that we have covered PCA and \\nk-means, we move toward an interesting network called self-organized network or winner-take-all units.\\nSelf-organizing maps\\nBoth k-means and PCA can clu ster the input data; however, they do not maintain a topological \\nrelationship. In this section, we will consider Self-Organizing Maps ( SOMs ), sometimes known as \\nKohonen networks or Winner-Take-All Units (WTUs ). They maintain the topological relation. SOMs \\nare a very special kind of neural network, inspired by a distinctive feature of the human brain. In \\nour brain, different sensory inputs are represented in a topologically ordered manner. Unlike other \\nneural networks, neurons are not all connected to each other via weights; instead, they influence each \\nother’s learning. The most important aspect of SOM is that neurons represent the learned inputs in a \\ntopographic manner. They were proposed by Teuvo Kohonen [7] in 1982.\\nIn SOMs, neurons are usually placed on the nodes of a (1D or 2D) lattice. Higher dimensions are also \\npossible but are rarely used in practice. Each neuron in the lattice is connected to all the input units \\nvia a weight matrix. Figure 7.7 shows a SOM with 6 × 8 (48 neurons) and 5 inputs. For clarity, only the \\nweight vectors connecting all inputs to one neuron are shown. In this case, each neuron will have \\nseven elements, resulting in a combined weight matrix of size 40 × 5:\\nFigure 7.7: A self-organized map with 5 inputs and 48 neurons', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4577b1e8-adbb-4f15-b0a3-ab90e80fdb30', embedding=None, metadata={'page_label': '272', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 272\\nA SOM learns via competitive learning. It can be considered as a nonlinear generalization of PCA and, \\nthus, like PCA, can be employed for dimensionality reduction.\\nIn order to implement SOM, let’s first understand how it works. As a first step, the weights of the \\nnetwork are initialized either to some random value or by taking random samples from the input. \\nEach neuron occupying a space in the lattice will be assigned specific locations. Now as an input is \\npresented, the neuron with the least distance from the input is declared the winner (WTU). This is \\ndone by measuring the distance between the weight vectors (W) and input vectors (X) of all neurons:\\n𝑑𝑑𝑗𝑗=√∑(𝑊𝑊𝑗𝑗𝑗𝑗−𝑋𝑋𝑗𝑗)2𝑁𝑁\\n𝑗𝑗𝑖𝑖 \\nHere, dj is the distance of the weights of neuron j from input X. The neuron with the lowest d value \\nis the winner.\\nNext, the weights of the winning neuron and its neighboring neurons are adjusted in a manner to \\nensure that the same neuron is the winner if the same input is presented next time.\\nTo decide which neighboring neurons need to be modified, the network uses a neighborhood function \\n∧(𝑟𝑟𝑟  ; normally, the Gaussian Mexican hat function is chosen as a neighborhood function. The \\nneighborhood function is mathematically represented as follows:\\n∧(𝑟𝑟)=𝑒𝑒−𝑑𝑑2\\n2𝜎𝜎2 \\nHere, 𝜎𝜎  is a time-dependent radius of the influence of a neuron and d is its distance from the winning \\nneuron. Graphically, the function looks like a hat (hence its name), as you can see in Figure 7.8:\\nFigure 7.8: The “Gaussian Mexican hat” function, visualized in graph form\\nAnother important property of the neighborhood function is that its radius reduces with time. As a \\nresult, in the beginning, many neighboring neurons’ weights are modified, but as the network learns, \\neventually a few neurons’ weights (at times, only one or none) are modified in the learning process. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3ac69c01-7afc-4daf-bcda-bf456c01c2a5', embedding=None, metadata={'page_label': '273', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 273\\nThe change in weight is given by the following equation:\\n𝑑𝑑𝑑𝑑= 𝜂𝜂𝜂𝜂𝜂𝜂𝜂 𝑑𝑑𝜂 \\nThe process is repeated for all the inputs for a given number of iterations. As the iterations progress, \\nwe reduce the learning rate and the radius by a factor dependent on the iteration number.\\nSOMs are computationally expensive and thus are not really useful for very large datasets. Still, they \\nare easy to understand, and they can very nicely find the similarity between input data. Thus, they \\nhave been employed for image segmentation and to determine word similarity maps in NLP.\\nColour mapping using a SOM\\nSome of the interesting properties of the feature map of the input space generated by a SOM are:\\n• The feature map provides a good representation of the input space. This property can be used \\nto perform vector quantization so that we may have a continuous input space, and using a SOM \\nwe can represent it in a discrete output space.\\n• The feature map is topologically ordered, that is, the spatial location of a neuron in the output \\nlattice corresponds to a particular feature of the input.\\n• The feature map also reflects the statistical distribution of the input space; the domain that \\nhas the largest number of input samples gets a wider area in the feature map.\\nThese features of SOM make them the natural choice for many interesting applications. Here we use \\nSOM for clustering a range of given R, G, and B pixel values to a corresponding color map. We start \\nwith the importing of modules:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nThe main component of the code is our class WTU. The class __init__  function initializes various \\nhyperparameters of our SOM, the dimensions of our 2D lattice ( m, n ), the number of features in the \\ninput (dim), the neighborhood radius ( sigma ), the initial weights, and the topographic information:\\n# Define the Winner Take All units\\nclass WTU(object):\\n  #_learned = False\\n  def __init__ (self, m, n, dim, num_iterations, eta = 0.5, sigma = None):\\n    \"\"\"\\n    m x n : The dimension of 2D lattice in which neurons are arranged\\n    dim : Dimension of input training data\\n    num_iterations: Total number of training iterations\\n    eta : Learning rate\\n    sigma: The radius of neighbourhood function.\\n    \"\"\"\\n    self._m = m', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b1b5267e-013d-46a6-945e-bb877beea90b', embedding=None, metadata={'page_label': '274', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Unsupervised Learning 274\\n    self._n = n\\n    self._neighbourhood = []\\n    self._topography = []\\n    self._num_iterations = int(num_iterations)\\n    self._learned = False\\n    self.dim = dim\\n    self.eta = float(eta)\\n    \\n    if sigma is None:\\n      sigma = max(m,n)/2.0 # Constant radius\\n    else:\\n      sigma = float(sigma)\\n    self.sigma = sigma\\n        \\n    print('Network created with dimensions' ,m,n)\\n         \\n    # Weight Matrix and the topography of neurons\\n    self._W = tf.random.normal([m*n, dim], seed = 0)\\n    self._topography = np.array( list(self._neuron_location(m, n)))\\nThe most important function of the class is the training()  function, where we use the Kohonen \\nalgorithm as discussed before to find the winner units and then update the weights based on the \\nneighborhood function:\\ndef training (self,x, i):\\n    m = self._m\\n    n= self._n\\n    \\n    # Finding the Winner and its location\\n    d = tf.sqrt(tf.reduce_sum(tf. pow(self._W - tf.stack([x for i in \\nrange(m*n)]), 2),1))\\n    self.WTU_idx = tf.argmin(d, 0)\\n    \\n    slice_start = tf.pad(tf.reshape(self.WTU_idx, [ 1]),np.array([[ 0,1]]))\\n    self.WTU_loc = tf.reshape(tf. slice(self._topography, slice_start,[ 1,2]), \\n[2])\\n    \\n    \\n    # Change learning rate and radius as a function of iterations\\n    learning_rate = 1 - i/self._num_iterations\\n    _eta_new = self.eta * learning_rate\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fde5744-8c96-4d83-8c4b-b481eae925d9', embedding=None, metadata={'page_label': '275', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 275\\n    _sigma_new = self.sigma * learning_rate\\n    \\n    \\n    # Calculating Neighbourhood function\\n    distance_square = tf.reduce_sum(tf. pow(tf.subtract(\\n        self._topography, tf.stack([self.WTU_loc for i in range (m * n)])), 2), \\n1)\\n    neighbourhood_func = tf.exp(tf.negative(tf.math.divide(tf.cast(\\ndistance_square, \"float32\" ), tf.pow(_sigma_new, 2))))\\n    \\n    # multiply learning rate with neighbourhood func\\n    eta_into_Gamma = tf.multiply(_eta_new, neighbourhood_func)\\n    \\n    # Shape it so that it can be multiplied to calculate dW\\n    weight_multiplier = tf.stack([tf.tile(tf. slice(\\n        eta_into_Gamma, np.array([i]), np.array([ 1])), [self.dim])\\n        for i in range (m * n)])\\n    delta_W = tf.multiply(weight_multiplier,\\n        tf.subtract(tf.stack([x for i in range (m * n)]),self._W))\\n    new_W = self._W + delta_W\\n    self._W = new_W\\nThe fit()  function is a helper function that calls the training()  function and stores the centroid \\ngrid for easy retrieval:\\ndef fit(self, X):\\n    \"\"\"\\n    Function to carry out training\\n    \"\"\"\\n    for i in range (self._num_iterations):\\n        for x in X:\\n            self.training(x,i)\\n    # Store a centroid grid for easy retrieval\\n    centroid_grid = [[] for i in range (self._m)]\\n    self._Wts = list(self._W)\\n    self._locations = list(self._topography)\\n    for i, loc in enumerate (self._locations):\\n        centroid_grid[loc[ 0]].append(self._Wts[i])\\n    self._centroid_grid = centroid_grid\\n    self._learned = True', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb0f5266-9bd3-41e2-875b-ded9b3591f27', embedding=None, metadata={'page_label': '276', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 276\\nThen there are some more helper functions to find the winner and generate a 2D lattice of neurons, \\nand a function to map input vectors to the corresponding neurons in the 2D lattice:\\ndef winner(self, x):\\n    idx = self.WTU_idx,self.WTU_loc\\n    return idx\\n      \\ndef _neuron_location (self,m,n):\\n    \"\"\"\\n    Function to generate the 2D lattice of neurons\\n    \"\"\"\\n    for i in range (m):\\n       for j in range (n):\\n          yield np.array([i,j])\\ndef get_centroids (self):\\n    \"\"\"\\n    Function to return a list of \\'m\\' lists, with each inner list containing the \\n\\'n\\' corresponding centroid locations as 1-D NumPy arrays.\\n    \"\"\"\\n    if not self._learned:\\n       raise ValueError( \"SOM not trained yet\" )\\n    return self._centroid_grid\\ndef map_vects (self, X):\\n    \"\"\"\\n    Function to map each input vector to the relevant neuron in the lattice\\n    \"\"\"\\n    if not self._learned:\\n       raise ValueError( \"SOM not trained yet\" )\\n       to_return = []\\n       for vect in X:\\n          min_index = min([i for i in range (len(self._Wts))],\\n                           key= lambda x: np.linalg.norm(vect -\\n                           self._Wts[x]))\\n          to_return.append(self._locations[min_index])\\n       return to_return \\nWe will also need to normalize the input data, so we create a function to do so:\\ndef normalize (df):\\n    result = df.copy()\\n    for feature_name in df.columns:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6451b2de-82e3-4e3e-9aa6-4625e4169c2d', embedding=None, metadata={'page_label': '277', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 7 277\\n        max_value = df[feature_name]. max()\\n        min_value = df[feature_name]. min()\\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - \\nmin_value)\\n    return result.astype(np.float32)\\nLet us read the data. The data contains red, green, and blue channel values for different colors. Let \\nus normalize them:\\n## Reading input data from file\\nimport pandas as pd\\ndf = pd.read_csv( 'colors.csv' )  # The last column of data file is a label\\ndata = normalize(df[[ 'R', 'G', 'B']]).values\\nname = df[ 'Color-Name' ].values\\nn_dim = len(df.columns) - 1\\n# Data for Training\\ncolors = data\\ncolor_names = name\\nLet us create our SOM and fit it:\\nsom = WTU( 30, 30, n_dim, 400, sigma= 10.0)\\nsom.fit(colors)\\nThe fit function takes slightly longer to run, since our code is not optimized for performance but for \\nexplaining the concept. Now, let’s look at the result of the trained model. Let us run the following code:\\n# Get output grid\\nimage_grid = som.get_centroids()\\n# Map colours to their closest neurons\\nmapped = som.map_vects(colors)\\n# Plot\\nplt.imshow(image_grid)\\nplt.title( 'Color Grid SOM' )\\nfor i, m in enumerate (mapped):\\n    plt.text(m[ 1], m[0], color_names[i], ha= 'center' , va='center' ,\\n             bbox= dict(facecolor= 'white', alpha= 0.5, lw=0))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='298bf2f7-7c0d-47e3-81ca-5cd2dc3f2746', embedding=None, metadata={'page_label': '278', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 278\\nYou can see the color map in the 2D neuron lattice:\\nFigure 7.9: A plotted color map of the 2D neuron lattice\\nYou can see that neurons that win for similar colors are closely placed. Next, we move to an interesting \\narchitecture, the restricted Boltzmann machines.\\nRestricted Boltzmann machines\\nThe RBM is a two-layered neural network—the first layer is called the visible layer and the second \\nlayer is called the hidden layer. They are called shallow neural networks because they are only two \\nlayers deep. They were first proposed in 1986 by Paul Smolensky (he called them Harmony Networks \\n[1]) and later by Geoffrey Hinton who in 2006 proposed Contrastive Divergence (CD) as a method to \\ntrain them. All neurons in the visible layer are connected to all the neurons in the hidden layer, but \\nthere is a restriction—no neuron in the same layer can be connected. All neurons in the RBM are \\nbinary by nature; they will either fire or not fire.\\nRBMs can be used for dimensionality reduction, feature extraction, and collaborative filtering. The \\ntraining of RBMs can be divided into three parts: forward pass, backward pass, and then a comparison.\\nLet us delve deeper into the math. We can divide the operation of RBMs into two passes:\\nForward pass: The information at visible units (V ) is passed via weights (W ) and biases (c ) to the hidden \\nunits (h 0). The hidden unit may fire or not depending on the stochastic probability ( 𝜎𝜎  is the stochastic \\nprobability), which is basically the sigmoid function:\\n𝜌𝜌(𝑣𝑣0|ℎ0)= 𝜎𝜎(𝜎𝜎𝑇𝑇𝑊𝑊𝑊𝑊𝑊)  \\nBackward pass: The hidden unit representation (h 0) is then passed back to the visible units through \\nthe same weights, W, but a different bias, c, where the model reconstructs the input. Again, the input \\nis sampled:\\n𝜌𝜌(𝑣𝑣𝑖𝑖|ℎ0)= 𝜎𝜎(𝜎𝜎𝑇𝑇ℎ0+𝑐𝑐)  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e4ac4ab-a9fd-4d12-95e1-f6fe8b76c207', embedding=None, metadata={'page_label': '279', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 279\\nThese two passes are repeated for k  steps or until the convergence [4] is reached. According to \\nresearchers, k=1 gives good results, so we will keep k = 1.\\nThe joint configuration of the visible vector V and the hidden vector h has energy given as follows:\\n𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸 𝐸 𝐸𝐸𝐸𝑇𝑇𝑉𝑉𝐸𝑉𝑉𝑇𝑇𝐸𝐸𝑉𝑉𝑇𝑇𝑊𝑊𝐸𝐸 \\nAlso associated with each visible vector V is free energy, the energy that a single configuration would \\nneed to have in order to have the same probability as all of the configurations that contain V:\\n𝐹𝐹(𝑣𝑣)= −𝑏𝑏𝑇𝑇𝑉𝑉−𝑉 𝑉𝑉𝑉𝑉 (𝑉𝑉𝑉𝑉𝑉𝑉(𝑉𝑉𝑗𝑗𝑉𝑉𝑉𝑇𝑇𝑊𝑊))\\n𝑗𝑗𝑉𝑗𝑉𝑗𝑗𝑗𝑗𝑗𝑗𝑗𝑗𝑗𝑗𝑗 \\nUsing the contrastive divergence objective function, that is, Mean(F(V original )) - Mean(F(V reconstructed )), the \\nchange in weights is given by:\\n𝑑𝑑𝑑𝑑= 𝜂𝜂𝜂𝜂𝜂𝜂𝑇𝑇ℎ)𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖−𝜂𝜂𝜂𝑇𝑇ℎ)𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑖𝑖𝑟𝑟𝑖𝑖𝑟𝑟𝑖𝑖𝑟𝑟𝑖𝑖𝑟𝑟𝑟𝑟] \\nHere, 𝜂𝜂  is the learning rate. Similar expressions exist for the biases b and c.\\nReconstructing images using an RBM\\nLet us build an RBM in TensorFlow. The RBM will be designed to reconstruct handwritten digits. This \\nis the first generative model that you are learning; in the upcoming chapters, we will learn a few more. \\nWe import the TensorFlow, NumPy, and Matplotlib libraries:\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nWe define a class RBM. The class __init_()  function initializes the number of neurons in the visible layer \\n(input_size ) and the number of neurons in the hidden layer ( output_size ). The function initializes \\nthe weights and biases for both hidden and visible layers. In the following code, we have initialized \\nthem to zero. You can try with random initialization as well:\\n#Class that defines the behavior of the RBM\\nclass RBM(object):\\n    \\n    def __init__ (self, input_size, output_size, lr= 1.0, batchsize= 100):\\n        \"\"\"\\n        m: Number of neurons in visible layer\\n        n: number of neurons in hidden layer\\n        \"\"\"\\n        # Defining the hyperparameters\\n        self._input_size = input_size # Size of Visible\\n        self._output_size = output_size # Size of outp\\n        self.learning_rate = lr # The step used in gradient descent', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cf2e062-fd78-41b3-b6c6-ae8c47b7fd34', embedding=None, metadata={'page_label': '280', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Unsupervised Learning 280\\n        self.batchsize = batchsize         # The size of how much data will be \\nused for training per sub iteration\\n        \\n        # Initializing weights and biases as matrices full of zeroes\\n        self.w = tf.zeros([input_size, output_size], np.float32) # Creates and \\ninitializes the weights with 0\\n        self.hb = tf.zeros([output_size], np.float32) # Creates and initializes \\nthe hidden biases with 0\\n        self.vb = tf.zeros([input_size], np.float32) # Creates and initializes \\nthe visible biases with 0\\nWe define methods to provide the forward and backward passes:\\n    # Forward Pass\\n    def prob_h_given_v (self, visible, w, hb):\\n        # Sigmoid \\n        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\\n    # Backward Pass\\n    def prob_v_given_h (self, hidden, w, vb):\\n        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\\nWe create a function to generate random binary values. This is because both hidden and visible units \\nare updated using stochastic probability, depending upon the input to each unit in the case of the \\nhidden layer (and the top-down input to visible layers):\\n   # Generate the sample probability\\n    def sample_prob (self, probs):\\n        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\\nWe will need functions to reconstruct the input:\\ndef rbm_reconstruct (self,X):\\n    h = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\\n    reconstruct = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.w)) + self.vb)\\n    return reconstruct\\nTo train the RBM created we define the train()  function. The function calculates the positive and \\nnegative gradient terms of contrastive divergence and uses the weight update equation to update the \\nweights and biases:\\n# Training method for the model\\ndef train(self, X, epochs= 10):\\n    \\n    loss = []\\n    for epoch in range (epochs):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87bd4f2d-6994-4865-ae22-f9f6193fbaf8', embedding=None, metadata={'page_label': '281', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 7 281\\n        #For each step/batch\\n        for start, end in zip(range(0, len(X), self.batchsize), range(self.\\nbatchsize, len(X), self.batchsize)):\\n            batch = X[start:end]\\n            \\n            #Initialize with sample probabilities\\n            \\n            h0 = self.sample_prob(self.prob_h_given_v(batch, self.w, self.hb))\\n            v1 = self.sample_prob(self.prob_v_given_h(h0, self.w, self.vb))\\n            h1 = self.prob_h_given_v(v1, self.w, self.hb)\\n            \\n            #Create the Gradients\\n            positive_grad = tf.matmul(tf.transpose(batch), h0)\\n            negative_grad = tf.matmul(tf.transpose(v1), h1)\\n            \\n            #Update learning rates \\n            self.w = self.w + self.learning_rate *(positive_grad - negative_\\ngrad) / tf.dtypes.cast(tf.shape(batch)[ 0],tf.float32)\\n            self.vb = self.vb +  self.learning_rate * tf.reduce_mean(batch - \\nv1, 0)\\n            self.hb = self.hb +  self.learning_rate * tf.reduce_mean(h0 - h1, \\n0)\\n            \\n        #Find the error rate\\n        err = tf.reduce_mean(tf.square(batch - v1))\\n        print ('Epoch: %d'  % epoch, 'reconstruction error: %f'  % err)\\n        loss.append(err)\\n        \\n    return loss\\nNow that our class is ready, we instantiate an object of RBM and train it on the MNIST dataset:\\n(train_data, _), (test_data, _) =  tf.keras.datasets.mnist.load_data()\\ntrain_data = train_data/np.float32( 255)\\ntrain_data = np.reshape(train_data, (train_data.shape[ 0], 784))\\ntest_data = test_data/np.float32( 255)\\ntest_data = np.reshape(test_data, (test_data.shape[ 0], 784))\\n#Size of inputs is the number of inputs in the training set\\ninput_size = train_data.shape[ 1]\\nrbm = RBM(input_size, 200)\\nerr = rbm.train(train_data, 50)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='121ab041-ec11-498a-b9fd-a5d0b384ec82', embedding=None, metadata={'page_label': '282', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Unsupervised Learning 282\\nLet us plot the learning curve:\\nplt.plot(err)\\nplt.xlabel( 'epochs' )\\nplt.ylabel( 'cost')\\nIn the figure below, you can see the learning curve of our RBM:\\nFigure 7.10: Learning curve for the RBM model\\nNow, we present the code to visualize the reconstructed images:\\nout = rbm.rbm_reconstruct(test_data)\\n# Plotting original and reconstructed images\\nrow, col = 2, 8\\nidx = np.random.randint( 0, 100, row * col // 2)\\nf, axarr = plt.subplots(row, col, sharex= True, sharey= True, figsize=( 20,4))\\nfor fig, row in zip([test_data,out], axarr):\\n    for i,ax in zip(idx,row):\\n        ax.imshow(tf.reshape(fig[i],[ 28, 28]), cmap= 'Greys_r' )\\n        ax.get_xaxis().set_visible( False)\\n        ax.get_yaxis().set_visible( False)\\nAnd the reconstructed images:\\nFigure 7.11: Image reconstruction using an RBM\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0980f6dc-a771-490c-9b7f-db62a523971f', embedding=None, metadata={'page_label': '283', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 7 283\\nThe top row is the input handwritten image, and the bottom row is the reconstructed image. You \\ncan see that the images look remarkably similar to the human handwritten digits. In the upcoming \\nchapters, you will learn about models that can generate even more complex images such as artificial \\nhuman faces.\\nDeep belief networks\\nNow that we have a good understanding of RBMs and know how to train them using contrastive \\ndivergence, we can move toward the first successful deep neural network architecture, the deep belief \\nnetworks (DBNs ), proposed in 2006 by Hinton and his team in the paper A fast learning algorithm for \\ndeep belief nets. Before this model it was very difficult to train deep architectures, not just because of \\nthe limited computing resources, but also, as will be discussed in Chapter 8, Autoencoders, because \\nof the vanishing gradient problem. In DBNs it was first demonstrated how deep architectures can be \\ntrained via greedy layer-wise training.\\nIn the simplest terms, DBNs are just stacked RBMs. Each RBM is trained separately using the contrastive \\ndivergence. We start with the training of the first RBM layer. Once it is trained, we train the second \\nRBM layer. The visible units of the second RBM are now fed the output of the hidden units of the first \\nRBM, when it is fed the input data. The procedure is repeated with each RBM layer addition.\\nLet us try stacking our RBM class. To make the DBN, we will need to define one more function in the \\nRBM class; the output of the hidden unit of one RBM needs to feed into the next RBM:\\n    #Create expected output for our DBN\\n    def rbm_output (self, X):\\n        out = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\\n        return out\\nNow we can just use the RBM class to create a stacked RBM structure. In the following code we create \\nan RBM stack: the first RBM will have 500 hidden units, the second will have 200 hidden units, and \\nthe third will have 50 hidden units:\\nRBM_hidden_sizes = [ 500, 200 , 50 ] #create 2 layers of RBM with size 400 and \\n100\\n#Since we are training, set input as training data\\ninpX = train_data\\n#Create list to hold our RBMs\\nrbm_list = []\\n#Size of inputs is the number of inputs in the training set\\ninput_size = train_data.shape[ 1]\\n#For each RBM we want to generate\\nfor i, size in enumerate (RBM_hidden_sizes):\\n    print ('RBM: ' ,i,' ',input_size, '->', size)\\n    rbm_list.append(RBM(input_size, size))\\n    input_size = size\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0dd7acd2-e9fe-4004-8572-6fcdee3644a6', embedding=None, metadata={'page_label': '284', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Unsupervised Learning 284\\n---------------------------------------------------------------------\\nRBM:  0   784 -> 500\\nRBM:  1   500 -> 200\\nRBM:  2   200 -> 50\\nFor the first RBM, the MNIST data is the input. The output of the first RBM is then fed as input to the \\nsecond RBM, and so on through the consecutive RBM layers:\\n#For each RBM in our list\\nfor rbm in rbm_list:\\n    print ('Next RBM:' )\\n    #Train a new one\\n    rbm.train(tf.cast(inpX,tf.float32))\\n    #Return the output layer\\n    inpX = rbm.rbm_output(inpX)\\nOur DBN is ready. The three stacked RBMs are now trained using unsupervised learning. DBNs can \\nalso be trained using supervised training. To do so we need to fine-tune the weights of the trained \\nRBMs and add a fully connected layer at the end. In their publication Classification with Deep Belief \\nNetworks, Hebbo and Kim show how they used a DBN for MNIST classification; it is a good introduction \\nto the subject.\\nSummary\\nIn this chapter, we covered the major unsupervised learning algorithms. We went through algorithms \\nbest suited for dimension reduction, clustering, and image reconstruction. We started with the \\ndimension reduction algorithm PCA, then we performed clustering using k-means and self-organized \\nmaps. After this we studied the restricted Boltzmann machine and saw how we can use it for both \\ndimension reduction and image reconstruction. Next, we delved into stacked RBMs, that is, deep belief \\nnetworks, and we trained a DBN consisting of three RBM layers on the MNIST dataset.\\nIn the next chapter, we will explore another model using an unsupervised learning paradigm – \\nautoencoders.\\nReferences\\n1. Smith, Lindsay. (2006). A tutorial on Principal Component Analysis: http://www.cs.otago.ac.nz/\\ncosc453/student_tutorials/principal_components.pdf\\n2. Movellan, J. R. Tutorial on Principal component Analysis: http://mplab.ucsd.edu/tutorials/\\npca.pdf\\n3. TensorFlow Projector: http://projector.tensorflow.org/\\n4. Singular Value Decomposition (SVD ) tutorial. MIT: https://web.mit.edu/be.400/www/SVD/\\nSingular_Value_Decomposition.htm\\n5. Shlens, Jonathon. (2014). A tutorial on principal component analysis. arXiv preprint arXiv:1404.1100: \\nhttps://arxiv.org/abs/1404.1100\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36ba420f-100a-4a55-97ec-8ac2debbfab9', embedding=None, metadata={'page_label': '285', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 285\\n6. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press: https://www.\\ndeeplearningbook.org\\n7. Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological \\ncybernetics 43, no. 1: 59-69.\\n8. Kanungo, Tapas, et al. (2002). An Efficient k-Means Clustering Algorithm: Analysis and \\nImplementation. IEEE transactions on pattern analysis and machine intelligence 24.7: 881-892.\\n9. Ortega, Joaquín P érez, et al. Research issues on K-means Algorithm: An Experimental Trial Using \\nMatlab. CEUR Workshop Proceedings: Semantic Web and New Technologies.\\n10. Chen, K. (2009). On Coresets for k-Median and k-Means Clustering in Metric and Euclidean Spaces \\nand Their Applications. SIAM Journal on Computing 39.3: 923-947.\\n11. Determining the number of clusters in a data set: https://en.wikipedia.org/wiki/Determining_\\nthe_number_of_clusters_in_a_data_set\\n12. Lloyd, S. P. (1982). Least Squares Quantization in PCM: http://mlsp.cs.cmu.edu/courses/\\nfall2010/class14/lloyd.pdf\\n13. Dunn, J. C. (1973-01-01). A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact \\nWell-Separated Clusters. Journal of Cybernetics. 3(3): 32–57.\\n14. Bezdek, James C. (1981). Pattern Recognition with Fuzzy Objective Function Algorithms.\\n15. Peters, G., Crespo, F., Lingras, P., and Weber, R. (2013). Soft clustering–Fuzzy and rough approaches \\nand their extensions and derivatives. International Journal of Approximate Reasoning 54, no. 2: \\n307-322.\\n16. Sculley, D. (2010). Web-scale k-means clustering. In Proceedings of the 19th international \\nconference on World wide web, pp. 1177-1178. ACM.\\n17. Smolensky, P. (1986). Information Processing in Dynamical Systems: Foundations of Harmony \\nTheory. No. CU-CS-321-86. COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE.\\n18. Salakhutdinov, R., Mnih, A., and Hinton, G. (2007). Restricted Boltzmann Machines for Collaborative \\nFiltering. Proceedings of the 24th international conference on Machine learning. ACM.\\n19. Hinton, G. (2010). A Practical Guide to Training Restricted Boltzmann Machines. Momentum 9.1: \\n926.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2489ec35-da9f-41fc-94a0-e6b4ee9c6c35', embedding=None, metadata={'page_label': '286', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='235cbb1f-1aca-4e5d-a1da-d9a68c862e40', embedding=None, metadata={'page_label': '287', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8\\nAutoencoders\\nAutoencoders are neural networks that learn by unsupervised learning, also sometimes called semi-\\nsupervised learning, since the input is treated as the target too. In this chapter, you will learn about  \\nand implement different variants of autoencoders and eventually learn how to stack autoencoders. \\nWe will also see how autoencoders can be used to create MNIST digits, and finally, also cover the \\nsteps involved in building a long short-term memory autoencoder to generate sentence vectors. This \\nchapter includes the following topics:\\n• Vanilla autoencoders\\n• Sparse autoencoders\\n• Denoising autoencoders\\n• Convolutional autoencoders\\n• Stacked autoencoders\\n• Generating sentences using LSTM autoencoders\\n• Variational autoencoders for generating images\\nLet’s begin!\\nIntroduction to autoencoders\\nAutoencoders are a class of neural networks that attempt to recreate input as their target using \\nbackpropagation. An autoencoder consists of two parts: an encoder and a decoder. The encoder will \\nread the input and compress it to a compact representation, and the decoder will read the compact \\nrepresentation and recreate the input from it. In other words, the autoencoder tries to learn the identity \\nfunction by minimizing the reconstruction error. All the code files for this chapter can be found at https://packt.link/dltfchp8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d9c3b16-7cd2-4dea-86b4-fdc22c2c607f', embedding=None, metadata={'page_label': '288', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 288\\nThey have an inherent capability to learn a compact representation of data. They are at the center of \\ndeep belief networks and find applications in image reconstruction, clustering, machine translation, \\nand much more.\\nYou might think that implementing an identity function using deep neural networks is boring; however, \\nthe way in which this is done makes it interesting. The number of hidden units in the autoencoder \\nis typically fewer than the number of input (and output) units. This forces the encoder to learn a \\ncompressed representation of the input, which the decoder reconstructs. If there is a structure in \\nthe input data in the form of correlations between input features, then the autoencoder will discover \\nsome of these correlations, and end up learning a low-dimensional representation of the data similar \\nto that learned using principal component analysis (PCA ).\\nWhile PCA uses linear transformations, autoencoders on the other hand use non-linear transformations.\\nOnce the autoencoder is trained, we would typically just discard the decoder component and use the \\nencoder component to generate compact representations of the input. Alternatively, we could use the \\nencoder as a feature detector that generates a compact, semantically rich representation of our input \\nand build a classifier by attaching a softmax classifier to the hidden layer.\\nThe encoder and decoder components of an autoencoder can be implemented using either dense, \\nconvolutional, or recurrent networks, depending on the kind of data that is being modeled. For example, \\ndense networks might be a good choice for autoencoders used to build collaborative filtering (CF) \\nmodels, where we learn a compressed model of user preferences based on actual sparse user ratings. \\nSimilarly, convolutional neural networks may be appropriate for the use case described in the article \\niSee: Using Deep Learning to Remove Eyeglasses from Faces, by M. Runfeldt. Recurrent networks, on the \\nother hand, are a good choice for autoencoders working on sequential or text data, such as Deep \\nPatient (Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic \\nHealth Records, Miotto et al.) and skip-thought vectors.\\nWe can think of autoencoders as consisting of two cascaded networks. The first network is an encoder; \\nit takes the input x, and encodes it using a transformation h to an encoded signal y, that is:\\ny= h(x)\\nThe second network uses the encoded signal y as its input and performs another transformation f to \\nget a reconstructed signal r, that is:\\nr= f(y) = f(h(x))\\nWe define error, e, as the difference between the original input x and the reconstructed signal r, e= x- r. \\nThe network then learns by reducing the loss function (for example, mean squared error (MSE )), and \\nthe error is propagated backward to the hidden layers as in the case of multilayer perceptrons ( MLPs ).\\nDepending upon the actual dimensions of the encoded layer with respect to the input, the loss function, \\nand constraints, there are various types of autoencoders: variational autoencoders, sparse autoencoders, \\ndenoising autoencoders, and convolution autoencoders.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88b16b79-b84f-44e3-9b03-7472cb89eb2d', embedding=None, metadata={'page_label': '289', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 289\\nAutoencoders can also be stacked by successively stacking encoders that compress their input to smaller \\nand smaller representations, then stacking decoders in the opposite sequence. Stacked autoencoders \\nhave greater expressive power and the successive layers of representations capture a hierarchical \\ngrouping of the input, similar to the convolution and pooling operations in convolutional neural \\nnetworks.\\nStacked autoencoders used to be trained layer by layer. For example, in the network in Figure 8.1, we \\nwould first train layer X  to reconstruct layer X’ using the hidden layer H1 (ignoring H2). We would then \\ntrain layer H1 to reconstruct layer H1’ using the hidden layer H2. Finally, we would stack all the layers \\ntogether in the configuration shown and fine-tune it to reconstruct X’ from X. With better activation \\nand regularization functions nowadays, however, it is quite common to train these networks in totality:\\nFigure 8.1: Visualization of stacked autoencoders\\nIn this chapter, we will learn about these variations in autoencoders and implement them using \\nTensorFlow.\\nVanilla autoencoders\\nThe vanilla autoencoder, as proposed by Hinton in his 2006 paper Reducing the Dimensionality of Data \\nwith Neural Networks, consists of one hidden layer only. The number of neurons in the hidden layer \\nis fewer than the number of neurons in the input (or output) layer.\\nThis results in producing a bottleneck effect in the flow of information in the network. The hidden \\nlayer ( y) between the encoder input and decoder output is also called the “bottleneck layer.” Learning \\nin the autoencoder consists of developing a compact representation of the input signal at the hidden \\nlayer so that the output layer can faithfully reproduce the original input.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff508bb7-edca-4325-8c5e-7391046888f3', embedding=None, metadata={'page_label': '290', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 290\\nIn Figure 8.2, you can see the architecture of a vanilla autoencoder:\\nFigure 8.2: Architecture of the vanilla autoencoder\\nLet’s try to build a vanilla autoencoder. While in the paper Hinton used it for dimension reduction, in \\nthe code to follow, we will use autoencoders for image reconstruction. We will train the autoencoder \\non the MNIST database and will use it to reconstruct the test images. In the code, we will use the \\nTensorFlow Keras Layers  class to build our own encoder and decoder layers, so firstly let’s learn a \\nlittle about the Layers  class.\\nTensorFlow Keras layers ‒ defining custom layers\\nTensorFlow provides an easy way to define your own custom layer both from scratch or as a composition \\nof existing layers. The TensorFlow Keras layers  package defines a Layers  object. We can make our \\nown layer by simply making it a subclass of the Layers  class. It is necessary to define the dimensions \\nof the output while defining the layer. Though input dimensions are optional, if you do not define \\nthem, it will infer them automatically from the data. To build our own layer we will need to implement \\nthree methods:\\n• __init__() : Here, you define all input-independent initializations.\\n• build() : Here, we define the shapes of input tensors and can perform rest initializations if \\nrequired. In our example, since we are not explicitly defining input shapes, we need not define \\nthe build()  method.\\n• call() : This is where the forward computation is performed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cbd18598-a9e2-47c2-a3f2-f4911c09e66b', embedding=None, metadata={'page_label': '291', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 291\\nUsing the tensorflow.keras.layers.Layer  class, we now define the encoder and decoder layers. First \\nlet’s start with the encoder layer. We import tensorflow.keras  as K, and create an Encoder  class. The \\nEncoder  takes in the input and generates the hidden or the bottleneck layer as the output:\\nclass Encoder (K.layers.Layer):\\n    def __init__ (self, hidden_dim):\\n        super(Encoder, self).__init__()\\n        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.\\nnn.relu)\\n    def call(self, input_features):\\n        activation = self.hidden_layer(input_features)\\n        return activation\\nNext, we define the Decoder  class; this class takes in the output from the Encoder  and then passes it \\nthrough a fully connected neural network. The aim is to be able to reconstruct the input to the Encoder :\\nclass Decoder (K.layers.Layer):\\n    def __init__ (self, hidden_dim, original_dim):\\n        super(Decoder, self).__init__()\\n        self.output_layer = K.layers.Dense(units=original_dim, activation=tf.\\nnn.relu)\\n    def call(self, encoded):\\n        activation = self.output_layer(encoded)\\n        return activation\\nNow that we have both the encoder and decoder defined we use the tensorflow.keras.Model  object \\nto build the autoencoder model. You can see in the following code that in the __init__()  function \\nwe instantiate the encoder and decoder objects, and in the call()  method we define the signal flow. \\nAlso notice the member list self.loss  initialized in the _init__() :\\nclass Autoencoder (K.Model):\\n    def __init__ (self, hidden_dim, original_dim):\\n        super(Autoencoder, self).__init__()\\n        self.loss = []\\n        self.encoder = Encoder(hidden_dim=hidden_dim)\\n        self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_\\ndim)\\n    def call(self, input_features):\\n        encoded = self.encoder(input_features)\\n        reconstructed = self.decoder(encoded)\\n        return reconstructed\\nIn the next section, we will use the autoencoder that we defined here to reconstruct handwritten digits.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bda31268-023d-4825-8cd4-455a54e1e2b6', embedding=None, metadata={'page_label': '292', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 292\\nReconstructing handwritten digits using an autoencoder\\nNow that we have our model autoencoder with its layer encoder and decoder ready, let us try to \\nreconstruct handwritten digits. The complete code is available in the GitHub repo of the chapter \\nin the notebook VanillaAutoencoder.ipynb . The code will require the NumPy, TensorFlow, and \\nMatplotlib modules:\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as K\\nimport matplotlib.pyplot as plt\\nBefore starting with the actual implementation, let’s also define some hyperparameters. If you play \\naround with them, you will notice that even though the architecture of your model remains the same, \\nthere is a significant change in model performance. Hyperparameter tuning (refer to Chapter 1, Neural \\nNetwork Foundations with TF, for more details) is one of the important steps in deep learning. For \\nreproducibility, we set the seeds for random calculation:\\nnp.random.seed( 11)\\ntf.random.set_seed( 11)\\nbatch_size = 256\\nmax_epochs = 50\\nlearning_rate = 1e-3\\nmomentum = 8e-1\\nhidden_dim = 128\\noriginal_dim = 784\\nFor training data, we are using the MNIST dataset available in the TensorFlow datasets. We normalize \\nthe data so that pixel values lie between [0,1]; this is achieved by simply dividing each pixel element \\nby 255.\\nWe reshape the tensors from 2D to 1D. We employ the from_tensor_slices  function to generate \\na batched dataset with the training dataset sliced along its first dimension (slices of tensors). Also \\nnote that we are not using one-hot encoded labels; this is because we are not using labels to train the \\nnetwork since autoencoders learn via unsupervised learning:\\n(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\\nx_train = x_train / 255.\\nx_test = x_test / 255.\\nx_train = x_train.astype(np.float32)\\nx_test = x_test.astype(np.float32)\\nx_train = np.reshape(x_train, (x_train.shape[ 0], 784))\\nx_test = np.reshape(x_test, (x_test.shape[ 0], 784))\\ntraining_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_\\nsize)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97673fa5-2e4a-444b-b5fb-abbda0a8962e', embedding=None, metadata={'page_label': '293', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 8 293\\nNow we instantiate our autoencoder model object and define the loss and optimizers to be used for \\ntraining. Observe the formulation of the loss function carefully; it is simply the difference between \\nthe original image and the reconstructed image. You may find that the term reconstruction loss is also \\nused to describe it in many books and papers:\\nautoencoder = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)\\nopt = tf.keras.optimizers.Adam(learning_rate= 1e-2)\\ndef loss(preds, real):\\n    return tf.reduce_mean(tf.square(tf.subtract(preds, real)))\\nInstead of using the auto-training loop, for our custom autoencoder model, we will define a custom \\ntraining. We use tf.GradientTape  to record the gradients as they are calculated and implicitly apply \\nthe gradients to all the trainable variables of our model:\\ndef train(loss, model, opt, original):\\n    with tf.GradientTape() as tape:\\n        preds = model(original)\\n        reconstruction_error = loss(preds, original)\\n        gradients = tape.gradient(reconstruction_error, model.trainable_\\nvariables)\\n        gradient_variables = zip(gradients, model.trainable_variables)\\n    opt.apply_gradients(gradient_variables)\\n    return reconstruction_error\\nThe preceding train()  function will be invoked in a training loop, with the dataset fed to the model \\nin batches:\\ndef train_loop (model, opt, loss, dataset, epochs= 20):\\n    for epoch in range (epochs):\\n        epoch_loss = 0\\n        for step, batch_features in enumerate (dataset):\\n            loss_values = train(loss, model, opt, batch_features)\\n            epoch_loss += loss_values\\n        model.loss.append(epoch_loss)\\n        print('Epoch {}/{}. Loss: {}' .format(epoch + 1, epochs, epoch_loss.\\nnumpy()))\\nLet’s now train our autoencoder:\\ntrain_loop(autoencoder, opt, loss, training_dataset, epochs=max_epochs)\\nAnd plot our training graph:\\nplt.plot( range(max_epochs), autoencoder.loss)\\nplt.xlabel( 'Epochs' )\\nplt.ylabel( 'Loss')\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a25f74f6-619e-4a00-b02a-68fe1b49fb02', embedding=None, metadata={'page_label': '294', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Autoencoders 294\\nThe training graph is shown as follows. We can see that loss/cost is decreasing as the network learns \\nand after 50 epochs it is almost constant about a line. This means that further increasing the number \\nof epochs will not be useful. If we want to improve our training further, we should change the \\nhyperparameters like learning rate and batch_size :\\nFigure 8.3: Loss plot of the vanilla autoencoder\\nIn Figure 8.4, you can see the original (top) and reconstructed (bottom) images; they are slightly \\nblurred, but accurate:\\nnumber = 10  # how many digits we will display\\nplt.figure(figsize=( 20, 4))\\nfor index in range (number):\\n    # display original\\n    ax = plt.subplot( 2, number, index + 1)\\n    plt.imshow(x_test[index].reshape( 28, 28), cmap= 'gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\n    # display reconstruction\\n    ax = plt.subplot( 2, number, index + 1 + number)\\n    plt.imshow(autoencoder(x_test)[index].numpy().reshape( 28, 28), cmap= 'gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\nplt.show()\\nFigure 8.4: Original and reconstructed images using vanilla autoencoder\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5b8b83a-7af3-4bfd-b8a9-e241498130d6', embedding=None, metadata={'page_label': '295', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 295\\nIt is interesting to note that in the preceding code we reduced the dimensions of the input from 784 \\nto 128 and our network could still reconstruct the original image. This should give you an idea of the \\npower of the autoencoder for dimensionality reduction. One advantage of autoencoders over PCA  \\nfor dimensionality reduction is that while PCA can only represent linear transformations, we can use \\nnon-linear activation functions in autoencoders, thus introducing non-linearities in our encodings:\\nFigure 8.5: LHS image: The two-dimensional code for 500 digits of each class produced by taking \\nthe first two principal components of all 60,000 training samples. RHS image: The two-dimensional \\ncode found by a 784-500-2 autoencoder\\nFigure 8.5 compares the result of a PCA with that of stacked autoencoders with architecture consisting \\nof 784-500-2 (here the numbers represent the size of the encoder layers in each autoencoder; the \\nautoencoders had a symmetric decoder).\\nYou can see that the colored dots on the right are nicely separated, thus stacked autoencoders give \\nmuch better results compared to PCA. Now that you are familiar with vanilla autoencoders, let us see \\ndifferent variants of autoencoders and their implementation details.\\nSparse autoencoder\\nThe autoencoder we covered in the previous section works more like an identity network; it simply \\nreconstructs the input. The emphasis is on reconstructing the image at the pixel level, and the only \\nconstraint is the number of units in the bottleneck layer. While it is interesting, pixel-level reconstruction \\nis primarily a compression mechanism and does not necessarily ensure that the network will learn \\nabstract features from the dataset. We can ensure that a network learns abstract features from the \\ndataset by adding further constraints.\\nIn sparse autoencoders, a sparse penalty term is added to the reconstruction error. This tries to ensure \\nthat fewer units in the bottleneck layer will fire at any given time. We can include the sparse penalty \\nwithin the encoder layer itself. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30d41cb2-57ee-4b2e-a575-8ec5ae247446', embedding=None, metadata={'page_label': '296', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 296\\nIn the following code, you can see that the dense layer of Encoder  now has an additional parameter, \\nactivity_regularizer :\\nclass SparseEncoder (K.layers.Layer):\\n    def __init__ (self, hidden_dim):\\n        # encoder initializer\\n        super(SparseEncoder, self).__init__()\\n        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.\\nnn.relu, activity_regularizer=regularizers.l1( 10e-5))\\n    def call(self, input_features):\\n        # forward function\\n        activation = self.hidden_layer(input_features)\\n        return activation\\nThe activity regularizer tries to reduce the layer output (refer to Chapter 1, Neural Network Foundations \\nwith TF). It will reduce both the weights and bias of the fully connected layer to ensure that the output \\nis as small as it can be. TensorFlow supports three types of activity_regularizer :\\n• l1: Here the activity is computed as the sum of absolute values\\n• l2: The activity here is calculated as the sum of the squared values\\n• l1_l2 : This includes both L1 and L2 terms\\nKeeping the rest of the code the same, and just changing the encoder, you can get the sparse autoencoder \\nfrom the vanilla autoencoder. The complete code for the sparse autoencoder is in the Jupyter notebook \\nSparseAutoencoder.ipynb .\\nAlternatively, you can explicitly add a regularization term for sparsity in the loss function. To do so you \\nwill need to implement the regularization for the sparsity term as a function. If m is the total number \\nof input patterns, then we can define a quantity 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎  (you can check the mathematical details in \\nAndrew Ng’s lecture here: https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.\\npdf), which measures the net activity (how many times on average it fires) for each hidden layer unit. \\nThe basic idea is to put a constraint 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎 , such that it is equal to the sparsity parameter 𝜌𝜌 . This results \\nin adding a regularization term for sparsity in the loss function so that now the loss function becomes:\\nloss = Mean squared error + Regularization for sparsity parameter\\nThis regularization term will penalize the network if 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎  deviates from 𝜌𝜌 . One standard way to \\ndo this is to use Kullback-Leiber ( KL) divergence (you can learn more about KL divergence from \\nthis interesting lecture: https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf ) \\nbetween 𝜌𝜌  and 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎 .\\nLet’s explore the KL divergence, D KL, a little more. It is a non-symmetric measure of the difference \\nbetween the two distributions, in our case, 𝜌𝜌  and 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎 . When 𝜌𝜌  and 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎  are equal then the difference \\nis zero; otherwise, it increases monotonically as 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎  diverges from 𝜌𝜌 . Mathematically, it is expressed as:\\n𝐷𝐷𝐾𝐾𝐾𝐾( 𝜌𝜌 𝜌 𝜌𝜌𝜌𝑗𝑗) = 𝜌𝜌𝜌𝜌𝜌𝜌𝜌𝜌𝜌𝜌𝜌\\n𝜌𝜌𝜌𝑗𝑗+(1−𝜌𝜌)𝜌𝜌𝜌𝜌𝜌𝜌1−𝜌𝜌\\n1 − 𝜌𝜌𝜌𝑗𝑗 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fb7c1c5d-e2c6-4612-a9a2-bca046dceb22', embedding=None, metadata={'page_label': '297', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 297\\nWe add this to the loss to implicitly include the sparse term. We will need to fix a constant value for \\nthe sparsity term 𝜌𝜌  and compute 𝜌𝜌𝜌𝜌𝑎𝑎𝑎𝑎  using the encoder output.\\nThe compact representation of the inputs is stored in weights. Let us visualize the weights learned by the \\nnetwork. The following are the weights of the encoder layer for the standard and sparse autoencoders \\nrespectively.\\nWe can see that in the standard autoencoder (a) many hidden units have very large weights (brighter), \\nsuggesting that they are overworked, while all the hidden units of the sparse autoencoder (b) learn \\nthe input representation almost equally, and we see a more even color distribution:\\nFigure 8.6: Encoder weight matrix for (a) standard autoencoder and (b) sparse autoencoder\\nNow that we have learned about sparse autoencoders, we next move to a case where autoencoders \\ncan learn to remove noise from the image.\\nDenoising autoencoders\\nThe two autoencoders that we have covered in the previous sections are examples of undercomplete \\nautoencoders, because the hidden layer in them has lower dimensionality compared to the input \\n(output) layer. Denoising autoencoders belong to the class of overcomplete autoencoders because \\nthey work better when the dimensions of the hidden layer are more than the input layer.\\nA denoising autoencoder learns from a corrupted (noisy) input; it feeds its encoder network the noisy \\ninput, and then the reconstructed image from the decoder is compared with the original input. The \\nidea is that this will help the network learn how to denoise an input. It will no longer just make pixel-\\nwise comparisons, but in order to denoise, it will learn the information of neighboring pixels as well.\\nA denoising autoencoder has two main differences from other autoencoders: first, n_hidden , the \\nnumber of hidden units in the bottleneck layer is greater than the number of units in the input layer, \\nm, that is, n_hidden  > m. Second, the input to the encoder is corrupted input. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12a10dda-a46a-456c-94d7-c237c8b8a15c', embedding=None, metadata={'page_label': '298', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 298\\nTo do this, we add a noise term in both the test and training images:\\nnoise = np.random.normal(loc= 0.5, scale= 0.5, size=x_train.shape)\\nx_train_noisy = x_train + noise\\nnoise = np.random.normal(loc= 0.5, scale= 0.5, size=x_test.shape)\\nx_test_noisy = x_test + noise\\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\\nLet us see the denoising autoencoder in action next.\\nClearing images using a denoising autoencoder\\nLet us use the denoising autoencoder to clear the handwritten MNIST digits:\\n1. We start by importing the required modules:\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras as K\\nimport matplotlib.pyplot as plt\\n2. Next, we define the hyperparameters for our model:\\nnp.random.seed( 11)\\ntf.random.set_seed( 11)\\nbatch_size = 256\\nmax_epochs = 50\\nlearning_rate = 1e-3\\nmomentum = 8e-1\\nhidden_dim = 128\\noriginal_dim = 784\\n3. We read in the MNIST dataset, normalize it, and introduce noise to it:\\n(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\\nx_train = x_train / 255.\\nx_test = x_test / 255.\\nx_train = x_train.astype(np.float32)\\nx_test = x_test.astype(np.float32)\\nx_train = np.reshape(x_train, (x_train.shape[ 0], 784))\\nx_test = np.reshape(x_test, (x_test.shape[ 0], 784))\\n# Generate corrupted MNIST images by adding noise with normal dist\\n# centered at 0.5 and std=0.5\\nnoise = np.random.normal(loc= 0.5, scale= 0.5, size=x_train.shape)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54ffaccc-2a09-4761-b7a0-15c9a704a414', embedding=None, metadata={'page_label': '299', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 8 299\\nx_train_noisy = x_train + noise\\nnoise = np.random.normal(loc= 0.5, scale= 0.5, size=x_test.shape)\\nx_test_noisy = x_test + noise\\n4. We use the same encoder, decoder, and autoencoder classes as defined in the Vanilla autoencoders \\nsection:\\n# Encoder\\nclass Encoder (K.layers.Layer):\\n    def __init__ (self, hidden_dim):\\n        super(Encoder, self).__init__()\\n        self.hidden_layer = K.layers.Dense(units=hidden_dim, \\nactivation=tf.nn.relu)\\n    def call(self, input_features):\\n        activation = self.hidden_layer(input_features)\\n        return activation\\n# Decoder\\nclass Decoder (K.layers.Layer):\\n    def __init__ (self, hidden_dim, original_dim):\\n        super(Decoder, self).__init__()\\n        self.output_layer = K.layers.Dense(units=original_dim, \\nactivation=tf.nn.relu)\\n    def call(self, encoded):\\n        activation = self.output_layer(encoded)\\n        return activation\\nclass Autoencoder (K.Model):\\n    def __init__ (self, hidden_dim, original_dim):\\n        super(Autoencoder, self).__init__()\\n        self.loss = []\\n        self.encoder = Encoder(hidden_dim=hidden_dim)\\n        self.decoder = Decoder(hidden_dim=hidden_dim, original_\\ndim=original_dim)\\n    def call(self, input_features):\\n        encoded = self.encoder(input_features)\\n        reconstructed = self.decoder(encoded)\\n        return reconstructed\\n5. Next, we create the model and define the loss and optimizers to be used. Notice that this time, \\ninstead of writing the custom training loop, we are using the easier Keras inbuilt compile()  \\nand fit()  methods:\\nmodel = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)\\nmodel.compile(loss='mse', optimizer= 'adam')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22916c3d-8c00-4057-9c7b-1630fcc2f12f', embedding=None, metadata={'page_label': '300', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Autoencoders 300\\nloss = model.fit(x_train_noisy,\\n            x_train,\\n            validation_data=(x_test_noisy, x_test),\\n            epochs=max_epochs,\\n            batch_size=batch_size)\\n6. Now let’s plot the training loss:\\nplt.plot( range(max_epochs), loss.history[ 'loss'])\\nplt.xlabel( 'Epochs' )\\nplt.ylabel( 'Loss')\\nplt.show()\\nFigure 8.7 shows the loss over epochs:\\nFigure 8.7: Loss plot of a denoising autoencoder\\nAnd finally, let’s see our model in action: \\nnumber = 10  # how many digits we will display\\nplt.figure(figsize=( 20, 4))\\nfor index in range (number):\\n    # display original\\n    ax = plt.subplot( 2, number, index + 1)\\n    plt.imshow(x_test_noisy[index].reshape( 28, 28), cmap= 'gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\n    # display reconstruction\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a76059f3-7a33-4a56-b13f-d94e80fa7a54', embedding=None, metadata={'page_label': '301', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 8 301\\n    ax = plt.subplot( 2, number, index + 1 + number)\\n    plt.imshow(model(x_test_noisy)[index].numpy().reshape( 28, 28), \\ncmap='gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\nplt.show()\\nThe top row shows the input noisy image, and the bottom row shows cleaned images produced \\nfrom our trained denoising autoencoder:\\nFigure 8.8: The noisy input images and corresponding denoised reconstructed images\\nAn impressive reconstruction of images from noisy images, I’m sure you’ll agree. You can access the \\ncode in the notebook DenoisingAutoencoder.ipynb  if you want to play around with it.\\nStacked autoencoder\\nUntil now, we have restricted ourselves to autoencoders with only one hidden layer. We can build deep \\nautoencoders by stacking many layers of both encoders and decoders; such an autoencoder is called \\na stacked autoencoder. The features extracted by one encoder are passed on to the next encoder as \\ninput. The stacked autoencoder can be trained as a whole network with the aim of minimizing the \\nreconstruction error. Alternatively, each individual encoder/decoder network can first be pretrained \\nusing the unsupervised method you learned earlier, and then the complete network can be fine-tuned. \\nWhen the deep autoencoder network is a convolutional network, we call it a convolutional autoencoder. \\nLet us implement a convolutional autoencoder in TensorFlow next.\\nConvolutional autoencoder for removing noise from images\\nIn the previous section, we reconstructed handwritten digits from noisy input images. We used a fully \\nconnected network as the encoder and decoder for the work. However, we know that for images, a \\nconvolutional network can give better results, so in this section, we will use a convolution network \\nfor both the encoder and decoder. To get better results we will use multiple convolution layers in both \\nthe encoder and decoder networks; that is, we will make stacks of convolutional layers (along with \\nmax pooling or upsampling layers). We will also be training the entire autoencoder as a single entity:\\n1. We import all the required modules and the specific layers from tensorflow.keras.layers :\\nimport numpy as np\\nimport tensorflow as tf\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f2a2b38-a962-415e-aa6d-a502b62a71d7', embedding=None, metadata={'page_label': '302', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Autoencoders 302\\nimport tensorflow.keras as K\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, \\nUpSampling2D\\n2. We specify our hyperparameters. If you look carefully, the list is slightly different compared \\nto earlier autoencoder implementations; instead of learning rate and momentum, this time \\nwe are concerned with filters of the convolutional layer:\\nnp.random.seed( 11)\\ntf.random.set_seed( 11)\\nbatch_size = 128\\nmax_epochs = 50\\nfilters = [ 32,32,16]\\n3. In the next step, we read in the data and preprocess it. Again, you may observe a slight variation \\nfrom the previous code, especially in the way we are adding noise and then limiting the range \\nbetween [0-1]. We are doing so because in this case, instead of the mean squared error loss, we \\nwill be using binary cross-entropy loss and the final output of the decoder will pass through \\nsigmoid activation, restricting it between [0-1]:\\n(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\\nx_train = x_train / 255.\\nx_test = x_test / 255.\\nx_train = np.reshape(x_train, ( len(x_train), 28, 28, 1))\\nx_test = np.reshape(x_test, ( len(x_test), 28, 28, 1))\\nnoise = 0.5\\nx_train_noisy = x_train + noise * np.random.normal(loc= 0.0, scale= 1.0, \\nsize=x_train.shape)\\nx_test_noisy = x_test + noise * np.random.normal(loc= 0.0, scale= 1.0, \\nsize=x_test.shape)\\nx_train_noisy = np.clip(x_train_noisy, 0, 1)\\nx_test_noisy = np.clip(x_test_noisy, 0, 1)\\nx_train_noisy = x_train_noisy.astype( 'float32' )\\nx_test_noisy = x_test_noisy.astype( 'float32' )\\n#print(x_test_noisy[1].dtype)\\n4. Let us now define our encoder. The encoder consists of three convolutional layers, each followed \\nby a max pooling layer. Since we are using the MNIST dataset the shape of the input image is \\n28 × 28 (single channel) and the output image is of size 4 × 4 (and since the last convolutional \\nlayer has 16 filters, the image has 16 channels):\\nclass Encoder (K.layers.Layer):\\n    def __init__ (self, filters):\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d3fcc61f-b108-41d5-9c0e-562c9b8155db', embedding=None, metadata={'page_label': '303', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 303\\n        super(Encoder, self).__init__()\\n        self.conv1 = Conv2D(filters=filters[ 0], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'same\\')\\n        self.conv2 = Conv2D(filters=filters[ 1], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'same\\')\\n        self.conv3 = Conv2D(filters=filters[ 2], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'same\\')\\n        self.pool = MaxPooling2D(( 2, 2), padding= \\'same\\')\\n           \\n    \\n    def call(self, input_features):\\n        x = self.conv1(input_features)\\n        x = self.pool(x)\\n        x = self.conv2(x)\\n        x = self.pool(x)\\n        x = self.conv3(x)\\n        x = self.pool(x)\\n        return x\\n5. Next comes the decoder. It is the exact opposite of the encoder in design, and instead of max \\npooling, we are using upsampling to increase the size back. Notice the commented print  \\nstatements; you can use them to understand how the shape gets modified after each step. \\n(Alternatively, you can also use the model.summary  function to get the complete model summary.) \\nAlso notice that both the encoder and decoder are still classes based on the TensorFlow Keras \\nLayers  class, but now they have multiple layers inside them. So now you know how to build \\na complex custom layer:\\nclass Decoder (K.layers.Layer):\\n    def __init__ (self, filters):\\n        super(Decoder, self).__init__()\\n        self.conv1 = Conv2D(filters=filters[ 2], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'same\\')\\n        self.conv2 = Conv2D(filters=filters[ 1], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'same\\')\\n        self.conv3 = Conv2D(filters=filters[ 0], kernel_size= 3, strides= 1, \\nactivation= \\'relu\\', padding= \\'valid\\')\\n        self.conv4 = Conv2D( 1, 3, 1, activation= \\'sigmoid\\' , \\npadding= \\'same\\')\\n        self.upsample = UpSampling2D(( 2, 2))\\n    def call(self, encoded):\\n        x = self.conv1(encoded)\\n        #print(\"dx1\", x.shape)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='924157ca-09ce-45ac-b118-53f838d8bc26', embedding=None, metadata={'page_label': '304', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 304\\n        x = self.upsample(x)\\n        #print(\"dx2\", x.shape)\\n        x = self.conv2(x)\\n        x = self.upsample(x)\\n        x = self.conv3(x)\\n        x = self.upsample(x)\\n        return self.conv4(x)\\n6. We combine the encoder and decoder to make an autoencoder model. This remains exactly \\nthe same as before:\\nclass Autoencoder (K.Model):\\n    def __init__ (self, filters):\\n        super(Autoencoder, self).__init__()\\n        self.encoder = Encoder(filters)\\n        self.decoder = Decoder(filters)\\n    def call(self, input_features):\\n        #print(input_features.shape)\\n        encoded = self.encoder(input_features)\\n        #print(encoded.shape)\\n        reconstructed = self.decoder(encoded)\\n        #print(reconstructed.shape)\\n        return reconstructed\\n7. Now we instantiate our model, then specify the binary cross-entropy as the loss function and \\nAdam as the optimizer in the compile()  method. Then, fit the model to the training dataset:\\nmodel = Autoencoder(filters)\\nmodel.compile(loss=\\'binary_crossentropy\\' , optimizer= \\'adam\\')\\nloss = model.fit(x_train_noisy,\\n            x_train,\\n            validation_data=(x_test_noisy, x_test),\\n            epochs=max_epochs,\\n            batch_size=batch_size)\\n8. Plot the loss curve:\\nplt.plot( range(max_epochs), loss.history[ \\'loss\\'])\\nplt.xlabel( \\'Epochs\\' )\\nplt.ylabel( \\'Loss\\')\\nplt.show()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ae1d62d-d708-46a2-bfb9-9d74b6b5aa9c', embedding=None, metadata={'page_label': '305', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 8 305\\nYou can see the loss curve as the model is trained; in 50 epochs the loss was reduced to 0.0988:\\nFigure 8.9: Loss plot for the convolutional autoencoder\\n9. And finally, you can see the wonderful reconstructed images from the noisy input images:\\nnumber = 10  # how many digits we will display\\nplt.figure(figsize=( 20, 4))\\nfor index in range (number):\\n    # display original\\n    ax = plt.subplot( 2, number, index + 1)\\n    plt.imshow(x_test_noisy[index].reshape( 28, 28), cmap= 'gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\n    # display reconstruction\\n    ax = plt.subplot( 2, number, index + 1 + number)\\n    plt.imshow(tf.reshape(model(x_test_noisy)[index], ( 28, 28)), \\ncmap='gray')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\nplt.show()\\nFigure 8.10: The inputted noisy images and reconstructed denoised images\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93ac6bd9-3954-4872-bb66-a83cecfe6458', embedding=None, metadata={'page_label': '306', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Autoencoders 306\\nYou can see that the images are much clearer and sharper relative to the previous autoencoders we \\nhave covered in this chapter. The magic lies in the stacking of convolutional layers. The code for this \\nsection is available in the Jupyter notebook ConvolutionAutoencoder.ipynb .\\nA TensorFlow Keras autoencoder example ‒ sentence vectors\\nIn this example, we will build and train an LSTM-based autoencoder to generate sentence vectors \\nfor documents in the Reuters-21578 corpus ( https://archive.ics.uci.edu/ml/datasets/reuters-\\n21578+text+categorization+collection ). We have already seen in Chapter 4 , Word Embeddings , how \\nto represent a word using word embeddings to create vectors that represent the word’s meaning in the \\ncontext of other words it appears with. Here, we will see how to build similar vectors for sentences. \\nSentences are sequences of words, so a sentence vector represents the meaning of a sentence.\\nThe easiest way to build a sentence vector is to just add up the word vectors and divide them by the \\nnumber of words. However, this treats the sentence as a bag of words, and does not take the order of \\nwords into account. Thus, the sentences The dog bit the man and The man bit the dog would be treated \\nas identical in this scenario. LSTMs are designed to work with sequence input and do take the order \\nof words into consideration, thus providing a better and more natural representation of the sentence.\\nFirst, we import the necessary libraries:\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.callbacks import ModelCheckpoint\\nfrom tensorflow.keras.layers import Input\\nfrom tensorflow.keras.layers import RepeatVector\\nfrom tensorflow.keras.layers import LSTM\\nfrom tensorflow.keras.layers import Bidirectional\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.preprocessing import sequence\\nfrom scipy.stats import describe\\nimport collections\\nimport matplotlib.pyplot as plt\\nimport nltk\\nimport numpy as np\\nimport os\\nfrom time import gmtime, strftime\\nfrom tensorflow.keras.callbacks import TensorBoard\\nimport re\\n# Needed to run only once\\nnltk.download( 'punkt')\\nnltk.download( 'reuters' )\\nfrom nltk.corpus import reuters\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e25d7aea-a5e2-461f-b65b-869f0a49e91a', embedding=None, metadata={'page_label': '307', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 307\\nIn case you are using Google’s Colab to run the code, you will also need to unzip the Reuters corpus \\nby adding the following to the code:\\n%%capture\\n!unzip /root/nltk_data/corpora/reuters. zip -d /root/nltk_data/corpora\\nNext, we will be using the GloVe embeddings, so let us download them as well:\\n!wget http://nlp.stanford.edu/data/glove .6B.zip\\n!unzip glove*. zip\\nNow that all our tools are in our workspace, we will first convert each block of text (documents) into a \\nlist of sentences, one sentence per line. Also, each word in the sentence is normalized as it is added. The \\nnormalization involves removing all numbers and replacing them with the number 9, then converting \\nthe word to lowercase. Simultaneously we also calculate the word frequencies in the same code. The \\nresult is the word frequency table, word_freqs :\\ndef is_number (n):\\n    temp = re.sub( \"[.,-/]\" , \"\",n)\\n    return temp.isdigit()\\n# parsing sentences and building vocabulary\\nword_freqs = collections.Counter()\\ndocuments = reuters.fileids()\\n#ftext = open(\"text.tsv\", \"r\")\\nsents = []\\nsent_lens = []\\nnum_read = 0\\nfor i in range (len(documents)):\\n    # periodic heartbeat report\\n    if num_read % 100 == 0:\\n        print(\"building features from {:d} docs\" .format(num_read))\\n    # skip docs without specified topic\\n    title_body = reuters.raw(documents[i]).lower()\\n    if len(title_body) == 0:\\n        continue\\n    num_read += 1\\n    # convert to list of word indexes\\n    title_body = re.sub( \"\\\\n\", \"\", title_body)\\n    for sent in nltk.sent_tokenize(title_body):\\n        for word in nltk.word_tokenize(sent):\\n            if is_number(word):\\n                word = \"9\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cb56ba3-7395-426a-a322-1fd66f4bd55a', embedding=None, metadata={'page_label': '308', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 308\\n            word = word.lower()\\n            word_freqs[word] += 1\\n        sents.append(sent)\\n        sent_lens.append( len(sent))\\nLet us use the preceding generated arrays to get some information about the corpus that will help us \\nfigure out good values for the constants for our LSTM network:\\nprint(\"Total number of sentences are: {:d} \" .format(len(sents)))\\nprint (\"Sentence distribution min {:d}, max {:d} , mean {:3f}, median \\n{:3f}\".format(np.min(sent_lens), np. max(sent_lens), np.mean(sent_lens), \\nnp.median(sent_lens)))\\nprint(\"Vocab size (full) {:d}\" .format(len(word_freqs)))\\nThis gives us the following information about the corpus:\\nTotal number of sentences are: 50470 \\nSentence distribution min 1, max 3688 , mean 167.072657, median 155.000000\\nVocab size (full) 33748\\nBased on this information, we set the following constants for our LSTM model. We choose our VOCAB_\\nSIZE  as 5000 ; that is, our vocabulary covers the most frequent 5,000 words, which covers over 93% of \\nthe words used in the corpus. The remaining words are treated as out of vocabulary ( OOV ) and replaced \\nwith the token UNK. At prediction time, any word that the model hasn’t seen will also be assigned the \\ntoken UNK. SEQUENCE_LEN  is set to approximately half the median length of sentences in the training \\nset. Sentences that are shorter than SEQUENCE_LEN  will be padded by a special PAD character, and those \\nthat are longer will be truncated to fit the limit:\\nVOCAB_SIZE = 5000\\nSEQUENCE_LEN = 50\\nSince the input to our LSTM will be numeric, we need to build lookup tables that go back and forth \\nbetween words and word IDs. Since we limit our vocabulary size to 5,000 and we have to add the two \\npseudo-words PAD and UNK, our lookup table contains entries for the most frequently occurring 4,998 \\nwords plus PAD and UNK:\\nword2id = {}\\nword2id[ \"PAD\"] = 0\\nword2id[ \"UNK\"] = 1\\nfor v, (k, _) in enumerate (word_freqs.most_common(VOCAB_SIZE - 2)):\\n    word2id[k] = v + 2\\nid2word = {v:k for k, v in word2id.items()}', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a2231f7-2a32-4e8d-b83c-d358fbdaa492', embedding=None, metadata={'page_label': '309', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 309\\nThe input to our network is a sequence of words, where each word is represented by a vector. \\nSimplistically, we could just use one-hot encoding for each word, but that makes the input data very \\nlarge. So, we encode each word using its 50-dimensional GloVe embeddings.\\nThe embedding is generated into  a matrix of shape ( VOCAB_SIZE  and EMBED_SIZE ) where each \\nrow represents the GloVe embedding for a word in our vocabulary. The PAD and UNK rows (0 and 1 \\nrespectively) are populated with zeros and random uniform values respectively:\\nEMBED_SIZE = 50\\ndef lookup_word2id (word):\\n    try:\\n        return word2id[word]\\n    except KeyError:\\n        return word2id[ \"UNK\"]\\ndef load_glove_vectors (glove_file, word2id, embed_size):\\n    embedding = np.zeros(( len(word2id), embed_size))\\n    fglove = open(glove_file, \"rb\")\\n    for line in fglove:\\n        cols = line.strip().split()\\n        word = cols[ 0].decode( \\'utf-8\\')\\n        if embed_size == 0:\\n            embed_size = len(cols) - 1\\n        if word in word2id:\\n            vec = np.array([ float(v) for v in cols[ 1:]])\\n        embedding[lookup_word2id(word)] = vec\\n    embedding[word2id[ \"PAD\"]] = np.zeros((embed_size))\\n    embedding[word2id[ \"UNK\"]] = np.random.uniform(- 1, 1, embed_size)\\n    return embedding\\nNext, we use these functions to generate embeddings:\\nsent_wids = [[lookup_word2id(w) for w in s.split()] for s in sents]\\nsent_wids = sequence.pad_sequences(sent_wids, SEQUENCE_LEN)\\n# load glove vectors into weight matrix\\nembeddings = load_glove_vectors( \"glove.6B.{:d}d.txt\" .format(EMBED_SIZE), \\nword2id, EMBED_SIZE)\\nOur autoencoder model takes a sequence of GloVe word vectors and learns to produce another sequence \\nthat is similar to the input sequence. The encoder LSTM compresses the sequence into a fixed-size \\ncontext vector, which the decoder LSTM uses to reconstruct the original sequence. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='195e50e9-00cc-4b70-ab60-8a771accf7ce', embedding=None, metadata={'page_label': '310', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 310\\nA schematic of the network is shown here:\\nFigure 8.11: Visualization of the LSTM network\\nBecause the input is quite large, we will use a generator to produce each batch of input. Our generator \\nproduces batches of tensors of shape ( BATCH_SIZE , SEQUENCE_LEN , EMBED_SIZE ). Here BATCH_SIZE  is 64, \\nand since we are using 50-dimensional GloVe vectors, EMBED_SIZE  is 50. We shuffle the sentences at the \\nbeginning of each epoch and return batches of 64 sentences. Each sentence is represented as a vector \\nof GloVe word vectors. If a word in the vocabulary does not have a corresponding GloVe embedding, \\nit is represented by a zero vector. We construct two instances of the generator, one for training data \\nand one for test data, consisting of 70% and 30% of the original dataset respectively:\\nBATCH_SIZE = 64\\ndef sentence_generator (X, embeddings, batch_size):\\n    while True:\\n        # loop once per epoch\\n        num_recs = X.shape[ 0]\\n        indices = np.random.permutation(np.arange(num_recs))\\n        num_batches = num_recs // batch_size\\n        for bid in range (num_batches):\\n            sids = indices[bid * batch_size : (bid + 1) * batch_size]\\n            Xbatch = embeddings[X[sids, :]]\\n            yield Xbatch, Xbatch\\ntrain_size = 0.7\\nXtrain, Xtest = train_test_split(sent_wids, train_size=train_size)\\ntrain_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)\\ntest_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6fa9b81-b394-4e8e-a5b2-8ea712dca7e3', embedding=None, metadata={'page_label': '311', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 311\\nNow we are ready to define the autoencoder. As we have shown in the diagram, it is composed of an \\nencoder LSTM and a decoder LSTM. The encoder LSTM reads a tensor of shape ( BATCH_SIZE , SEQUENCE_\\nLEN, EMBED_SIZE ) representing a batch of sentences. Each sentence is represented as a padded fixed-\\nlength sequence of words of size SEQUENCE_LEN . Each word is represented as a 300-dimensional GloVe \\nvector. The output dimension of the encoder LSTM is a hyperparameter, LATENT_SIZE , which is the \\nsize of the sentence vector that will come from the encoder part of the trained autoencoder later. The \\nvector space of dimensionality LATENT_SIZE  represents the latent space that encodes the meaning of \\nthe sentence. The output of the LSTM is a vector of size ( LATENT_SIZE ) for each sentence, so for the \\nbatch, the shape of the output tensor is ( BATCH_SIZE , LATENT_SIZE ). This is now fed to a RepeatVector  \\nlayer, which replicates this across the entire sequence; that is, the output tensor from this layer has \\nthe shape ( BATCH_SIZE , SEQUENCE_LEN , LATENT_SIZE ). This tensor is now fed into the decoder LSTM, \\nwhose output dimension is the EMBED_SIZE , so the output tensor has shape ( BATCH_SIZE , SEQUENCE_LEN , \\nEMBED_SIZE ), that is, the same shape as the input tensor.\\nWe compile this model with the Adam optimizer and the MSE loss function. The reason we use MSE \\nis that we want to reconstruct a sentence that has a similar meaning, that is, something that is close \\nto the original sentence in the embedded space of dimension LATENT_SIZE :\\nLATENT_SIZE = 512\\nEMBED_SIZE = 50\\nBATCH_SIZE = 64\\nNUM_EPOCHS = 20\\ninputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name= \"input\")\\nencoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode= \"sum\", name=\"encoder_\\nlstm\")(inputs)\\ndecoded = RepeatVector(SEQUENCE_LEN, name= \"repeater\" )(encoded)\\ndecoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences= True), merge_\\nmode=\"sum\", name=\"decoder_lstm\" )(decoded)\\nautoencoder = Model(inputs, decoded)\\nWe define the loss function as mean squared error and choose the Adam optimizer:\\nautoencoder. compile(optimizer= \"adam\", loss=\"mse\")\\nWe train the autoencoder for 20 epochs using the following code. 20 epochs was chosen because the \\nMSE loss converges within this time:\\nnum_train_steps = len(Xtrain) // BATCH_SIZE\\nnum_test_steps = len(Xtest) // BATCH_SIZE\\nsteps_per_epoch=num_train_steps,\\nepochs=NUM_EPOCHS,\\nvalidation_data=test_gen,\\nvalidation_steps=num_test_steps,\\nhistory = autoencoder.fit_generator(train_gen,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94d9c2a7-f211-4744-9e74-d22385e9f36b', embedding=None, metadata={'page_label': '312', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 312\\n                                steps_per_epoch=num_train_steps,\\n                                epochs=NUM_EPOCHS,\\n                                validation_data=test_gen,\\n                                validation_steps=num_test_steps)\\nThe results of the training are shown as follows. The plot below shows the loss plot for both training \\nand validation data; we can see that as our model learns, the losses decrease as expected:\\nFigure 8.12: Loss plot of the LSTM autoencoder\\nSince we are feeding in a matrix of embeddings, the output will also be a matrix of word embeddings. \\nSince the embedding space is continuous and our vocabulary is discrete, not every output embedding \\nwill correspond to a word. The best we can do is to find a word that is closest to the output embedding \\nin order to reconstruct the original text. This is a bit cumbersome, so we will evaluate our autoencoder \\nin a different way.\\nSince the objective of the autoencoder is to produce a good latent representation, we compare the \\nlatent vectors produced from the encoder using the original input versus the output of the autoencoder.\\nFirst, we extract the encoder component into its own network:\\nencoder = Model(autoencoder.input, autoencoder.get_layer(\"encoder_lstm\").output)\\nThen we run the autoencoder on the test set to return the predicted embeddings. We then send both \\nthe input embedding and the predicted embedding through the encoder to produce sentence vectors \\nfrom each and compare the two vectors using cosine similarity. Cosine similarities close to “one” indicate \\nhigh similarity and those close to “zero” indicate low similarity. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f647b2e6-b904-49ff-b9bf-4a539da79393', embedding=None, metadata={'page_label': '313', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 313\\nThe following code runs against a random subset of 500 test sentences and produces some sample \\nvalues of cosine similarities, between the sentence vectors generated from the source embedding and \\nthe corresponding target embedding produced by the autoencoder:\\ndef compute_cosine_similarity (x, y):\\n    return np.dot(x, y) / (np.linalg.norm(x, 2) * np.linalg.norm(y, 2))\\nk = 500\\ncosims = np.zeros((k))\\ni= 0\\nfor bid in range (num_test_steps):\\n    xtest, ytest = next(test_gen)\\n    ytest_ = autoencoder.predict(xtest)\\n    Xvec = encoder.predict(xtest)\\n    Yvec = encoder.predict(ytest_)\\n    for rid in range (Xvec.shape[ 0]):\\n        if i >= k:\\n            break\\n        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])\\n        if i <= 10:\\n            print(cosims[i])\\n        i += 1\\n    if i >= k:\\n        break\\nThe first 10 values of cosine similarities are shown as follows. As we can see, the vectors seem to be \\nquite similar:\\n0.9765363335609436\\n0.9862152338027954\\n0.9831727743148804\\n0.977733314037323\\n0.9851642847061157\\n0.9849132895469666\\n0.9831638932228088\\n0.9843543767929077\\n0.9825796484947205\\n0.9877195954322815\\n0.9820773601531982\\nFigure 8.13 shows a histogram of the distribution of values of cosine similarities for the sentence \\nvectors from the first 500 sentences. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f40ee26f-3a9c-41dd-b9fc-10bcfd7ee232', embedding=None, metadata={'page_label': '314', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 314\\nAs previously mentioned, it confirms that the sentence vectors generated from the input and output of \\nthe autoencoder are very similar, showing that the resulting sentence vector is a good representation \\nof the sentence:\\nFigure 8.13: Cosine similarity distribution\\nTill now we have focused on autoencoders that can reconstruct data; in the next section, we will go \\nthrough  a slightly different variant of the autoencoder – the variational autoencoder, which is used \\nto generate data.\\nVariational autoencoders\\nLike DBNs (Chapter 7, Unsupervised Learning) and GANs (see Chapter 9, Generative Models, for more \\ndetails), variational autoencoders are also generative models. Variational autoencoders (VA E s ) are a \\nmix of the best neural networks and Bayesian inference. They are one of the most interesting neural \\nnetworks and have emerged as one of the most popular approaches to unsupervised learning. They are \\nautoencoders with a twist. Along with the conventional encoder and decoder network of autoencoders, \\nthey have additional stochastic layers. The stochastic layer, after the encoder network, samples the \\ndata using a Gaussian distribution, and the one after the decoder network samples the data using \\nBernoulli’s distribution. Like GANs, V AEs can be used to generate images and figures based on the \\ndistribution they have been trained on. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dff8191f-fbb7-4d13-83a3-9a214f1a3d2b', embedding=None, metadata={'page_label': '315', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 315\\nV AEs allow one to set complex priors in the latent space and thus learn powerful latent representations. \\nFigure 8.14 describes a V AE:\\n \\nFigure 8.14: Architecture of a variational autoencoder\\nThe encoder network 𝑞𝑞𝜙𝜙(𝑧𝑧𝑧𝑧𝑧𝑧   approximates the true but intractable posterior distribution 𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝  \\n, where x  is the input to the V AE and z  is the latent representation. The decoder network 𝑝𝑝𝜃𝜃(𝑥𝑥𝑥𝑥𝑥𝑥   \\ntakes the d -dimensional latent variables (also called latent space) as its input and generates new \\nimages following the same distribution as P(x). As you can see from the preceding diagram, the latent \\nrepresentation z is sampled from 𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧𝑧 𝑥𝑥𝑧𝑥𝑥𝑥∑)𝑧𝑥𝑥𝑧𝑥𝑥, and the output of the decoder network samples \\n𝑥𝑥𝑥𝑥𝑥  from 𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥 𝑥𝑥𝑥𝑥𝑥𝑥∑)𝑥𝑥𝑥𝑥𝑥𝑥. Here N represents a normal distribution with mean 𝜇𝜇  and variance Σ .\\nNow that we have the basic architecture of V AEs, the question arises of how they can be trained, since \\nthe maximum likelihood of the training data and posterior density are intractable. The network is \\ntrained by maximizing the lower bound of the log data likelihood. Thus, the loss term consists of two \\ncomponents: generation loss, which is obtained from the decoder network through sampling, and \\nthe Kullback–Leibler divergence term, also called the latent loss.\\nGeneration loss ensures that the image generated by the decoder and the image used to train the \\nnetwork are similar, and latent loss ensures that the posterior distribution 𝑞𝑞𝑞𝑞𝑞𝑞𝑞𝑞𝑞   is close to the prior \\n𝑝𝑝𝜃𝜃(𝑧𝑧𝑧 . Since the encoder uses Gaussian distribution for sampling, the latent loss measures how closely \\nthe latent variables match this distribution.\\nOnce the V AE is trained, we can use only the decoder network to generate new images. Let us try coding \\na V AE. This time we are using the Fashion-MNIST dataset; the dataset contains Zalando’s ( https://\\ngithub.com/zalandoresearch/fashion-mnist ) article images. The test-train split is exactly the same \\nas for MNIST, that is, 60,000 train images and 10,000 test images. The size of each image is also 28 × \\n28, so we can easily replace the code running on the MNIST dataset with the Fashion-MNIST dataset. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df42d867-1367-4257-bde4-6aa7564eaf02', embedding=None, metadata={'page_label': '316', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 316\\nThe code in this section has been adapted from https://github.com/dragen1860/TensorFlow-2.x-\\nTutorials . As the first step we, as usual, import all the necessary libraries:\\nimport tensorflow as tf\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nLet us fix the seeds for a random number, so that the results are reproducible. We can also add an \\nassert  statement to ensure that our code runs on TensorFlow 2.0 or above:\\nnp.random.seed( 333)\\ntf.random.set_seed( 333)\\nassert tf.__version__.startswith( \\'2.\\'), \"TensorFlow Version Below 2.0\"\\nBefore going ahead with making the V AE, let us also explore the Fashion-MNIST dataset a little. The \\ndataset is available in the TensorFlow Keras API:\\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_\\ndata()\\nx_train, x_test = x_train.astype(np.float32)/ 255., x_test.astype(np.\\nfloat32)/ 255.\\nprint(x_train.shape, y_train.shape)\\nprint(x_test.shape, y_test.shape)\\n--------------------------------------------------\\n(60000, 28, 28) (60000,)\\n(10000, 28, 28) (10000,)\\nWe see some sample images:\\nnumber = 10  # how many digits we will display\\nplt.figure(figsize=( 20, 4))\\nfor index in range (number):\\n    # display original\\n    ax = plt.subplot( 2, number, index + 1)\\n    plt.imshow(x_train[index], cmap= \\'gray\\')\\n    ax.get_xaxis().set_visible( False)\\n    ax.get_yaxis().set_visible( False)\\nplt.show()\\nFigure 8.15: Sample images from the Fashion-MNIST dataset', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9551cf2d-276e-4e91-ba46-efd69ed67691', embedding=None, metadata={'page_label': '317', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 317\\nBefore we start, let us declare some hyperparameters like learning rate, dimensions of the hidden \\nlayer and the latent space, batch size, epochs, and so on:\\nimage_size = x_train.shape[ 1]*x_train.shape[ 2]\\nhidden_dim = 512\\nlatent_dim = 10\\nnum_epochs = 80\\nbatch_size = 100\\nlearning_rate = 0.001\\nWe use the TensorFlow Keras Model API to build a V AE model. The __init__()  function defines all \\nthe layers that we will be using:\\nclass VAE(tf.keras.Model):\\n    def __init__ (self,dim,**kwargs):\\n        h_dim = dim[ 0]\\n        z_dim = dim[ 1]\\n        super(VAE, self).__init__(**kwargs)\\n        self.fc1 = tf.keras.layers.Dense(h_dim)\\n        self.fc2 = tf.keras.layers.Dense(z_dim)\\n        self.fc3 = tf.keras.layers.Dense(z_dim)\\n        self.fc4 = tf.keras.layers.Dense(h_dim)\\n        self.fc5 = tf.keras.layers.Dense(image_size)\\nWe define the functions to give us the encoder output and decoder output and reparametrize. The \\nimplementation of the encoder and decoder functions are straightforward; however, we need to delve a \\nlittle deeper into the reparametrize  function. As you know, V AEs sample from a random node z, which \\nis approximated by 𝑞𝑞𝑞𝑞𝑞𝑞𝑞𝑞𝑞𝑞   of the true posterior. Now, to get parameters we need to use backpropagation. \\nHowever, backpropagation cannot work on random nodes. Using reparameterization, we can use a new \\nparameter, eps, which allows us to reparametrize z in a way that will allow backpropagation through \\nthe deterministic random node ( https://arxiv.org/pdf/1312.6114v10.pdf ):\\ndef encode(self, x):\\n    h = tf.nn.relu(self.fc1(x))\\n    return self.fc2(h), self.fc3(h)\\ndef reparameterize (self, mu, log_var):\\n    std = tf.exp(log_var * 0.5)\\n    eps = tf.random.normal(std.shape)\\n    return mu + eps * std\\ndef decode_logits (self, z):\\n    h = tf.nn.relu(self.fc4(z))\\n    return self.fc5(h)\\ndef decode(self, z):\\n    return tf.nn.sigmoid(self.decode_logits(z))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3530b474-41f0-4213-850a-87dd3f768868', embedding=None, metadata={'page_label': '318', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 318\\nLastly, we define the call()  function, which will control how signals move through different layers \\nof the V AE:\\ndef call(self, inputs, training= None, mask=None):\\n    mu, log_var = self.encode(inputs)\\n    z = self.reparameterize(mu, log_var)\\n    x_reconstructed_logits = self.decode_logits(z)\\n    return x_reconstructed_logits, mu, log_var\\nNow, we create the V AE model and declare the optimizer for it. You can see the summary of the model:\\nmodel = VAE([hidden_dim, latent_dim])\\nmodel.build(input_shape=( 4, image_size))\\nmodel.summary()\\noptimizer = tf.keras.optimizers.Adam(learning_rate)\\nModel: \"vae\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n dense (Dense)               multiple                  401920    \\n                                                                 \\n dense_1 (Dense)             multiple                  5130      \\n                                                                 \\n dense_2 (Dense)             multiple                  5130      \\n                                                                 \\n dense_3 (Dense)             multiple                  5632      \\n                                                                 \\n dense_4 (Dense)             multiple                  402192    \\n                                                                 \\n=================================================================\\nTotal params: 820,004\\nTrainable params: 820,004\\nNon-trainable params: 0\\n_________________________________________________________________\\nNow, we train the model. We define our loss function, which is the sum of the reconstruction loss \\nand KL divergence loss:\\ndataset = tf.data.Dataset.from_tensor_slices(x_train)\\ndataset = dataset.shuffle(batch_size * 5).batch(batch_size)\\nnum_batches = x_train.shape[ 0] // batch_size\\nfor epoch in range (num_epochs):\\n    for step, x in enumerate (dataset):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='564d6f3f-fbbd-4dd2-8c23-7b4957d63c29', embedding=None, metadata={'page_label': '319', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8 319\\n        x = tf.reshape(x, [- 1, image_size])\\n        with tf.GradientTape() as tape:\\n            # Forward pass\\n            x_reconstruction_logits, mu, log_var = model(x)\\n            # Compute reconstruction loss and kl divergence\\n            # Scaled by \\'image_size\\' for each individual pixel.\\n            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_\\nlogits(labels=x, logits=x_reconstruction_logits)\\n            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_\\nsize\\n            \\n            kl_div = - 0.5 * tf.reduce_sum( 1. + log_var - tf.square(mu) - \\ntf.exp(log_var), axis=- 1)\\n            kl_div = tf.reduce_mean(kl_div)\\n            # Backprop and optimize\\n            loss = tf.reduce_mean(reconstruction_loss) + kl_div\\n        gradients = tape.gradient(loss, model.trainable_variables)\\n        for g in gradients:\\n            tf.clip_by_norm(g, 15)\\n        optimizer.apply_gradients( zip(gradients, model.trainable_variables))\\n        if (step + 1) % 50 == 0:\\n            print(\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: \\n{:.4f}\"\\n            . format(epoch + 1, num_epochs, step + 1, num_batches, \\nfloat(reconstruction_loss), float(kl_div)))\\nOnce the model is trained it should be able to generate images similar to the original Fashion-MNIST \\nimages. To do so we need to use only the decoder network, and we will pass to it a randomly generated \\nz input:\\nz = tf.random.normal((batch_size, latent_dim))\\nout = model.decode(z)  # decode with sigmoid\\nout = tf.reshape(out, [- 1, 28, 28]).numpy() * 255\\nout = out.astype(np.uint8)\\nFigure 8.16 shows the results after 80 epochs:\\nFigure 8.16: Results after 80 epochs\\nThe generated images resemble the input space. The generated images are similar to the original \\nFashion-MNIST images as desired.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe98c480-3326-4915-915e-a740668052a7', embedding=None, metadata={'page_label': '320', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Autoencoders 320\\nSummary\\nIn this chapter, we’ve had an extensive look at a new generation of deep learning models: autoencoders. \\nWe started with the vanilla autoencoder, and then moved on to its variants: sparse autoencoders, \\ndenoising autoencoders, stacked autoencoders, and convolutional autoencoders. We used the \\nautoencoders to reconstruct images, and we also demonstrated how they can be used to clean noise \\nfrom an image. Finally, the chapter demonstrated how autoencoders can be used to generate sentence \\nvectors and images. The autoencoders learned through unsupervised learning.\\nIn the next chapter, we will delve deeper into generative adversarial networks, another interesting \\ndeep learning model that learns via an unsupervised learning paradigm.\\nReferences\\n1. Rumelhart,  D. E., Hinton, G. E., and Williams, R. J. (1985). Learning Internal Representations \\nby Error Propagation. No. ICS-8506. University of California, San Diego. La Jolla Institute for \\nCognitive Science: http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf\\n2. Hinton, G. E. and Salakhutdinov, R. R. (2016). Reducing the dimensionality of data with neural \\nnetworks. science 313.5786: 504–507: https://www.cs.toronto.edu/~hinton/science.pdf  \\n3. Masci, J. et al. (2011). Stacked convolutional auto-encoders for hierarchical feature extraction . Artificial \\nNeural Networks and Machine Learning–ICANN 2011: 52–59: https://www.semanticscholar.\\norg/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/\\n46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e\\n4. Japkowicz, N., Myers, C., and Gluck, M. (1995). A novelty detection approach to classification. IJCAI. \\nVol: https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf\\n5. Sedhain, S. (2015). AutoRec: Autoencoders Meet Collaborative Filtering. Proceedings of the 24th \\nInternational Conference on World Wide Web, ACM.\\n6. Cheng, H. (2016). Wide & Deep Learning for Recommender Systems. Proceedings of the 1st \\nWorkshop on Deep Learning for Recommender Systems, ACM.\\n7. Runfeldt, M. Using Deep Learning to Remove Eyeglasses from Faces.\\n8. Miotto, R. (2016). Deep Patient: An Unsupervised Representation to Predict the Future of Patients \\nfrom the Electronic Health Records. Scientific Reports.\\n9. Kiros, R. (2015). Skip-Thought Vectors, Advances in Neural Information Processing Systems.\\n10. Kullback-Leibler divergence: http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf\\n11. Denoising autoencoders: https://cs.stanford.edu/people/karpathy/convnetjs/demo/\\nautoencoder.html\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01ff9057-4066-47b8-8101-c97e58685313', embedding=None, metadata={'page_label': '321', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9\\nGenerative Models\\nGenerative models are a type of machine learning algorithm that is used to create data. They are used \\nto generate new data that is similar to the data that was used to train the model. They can be used to \\ncreate new data for testing or to fill in missing data. Generative models are used in many applications, \\nsuch as density estimation, image synthesis, and natural language processing. The V AE discussed \\nin Chapter 8, Autoencoders, was one type of generative model; in this chapter, we will discuss a wide \\nrange of generative models, Generative Adversarial Networks (GANs ) and their variants, flow-based \\nmodels, and diffusion models.\\nGANs have been defined as the most interesting idea in the last 10 years in ML ( https://www.quora.\\ncom/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning ) by \\nYann LeCun, one of the fathers of deep learning. GANs are able to learn how to reproduce synthetic \\ndata that looks real. For instance, computers can learn how to paint and create realistic images. The \\nidea was originally proposed by Ian Goodfellow (for more information, refer to NIPS 2016 Tutorial: \\nGenerative Adversarial Networks , by I. Goodfellow, 2016); he has worked with the University of Montreal, \\nGoogle Brain, and OpenAI, and is presently working in Apple Inc. as the Director of Machine Learning.\\nIn this chapter, we will cover different types of GANs; the chapter will introduce you to flow-based \\nmodels and diffusion models, and additionally, you will see some of their implementation in TensorFlow. \\nBroadly, we will cover the following topics:\\n• What is a GAN?\\n• Deep convolutional GANs\\n• InfoGAN\\n• SRGAN\\n• CycleGAN\\n• Applications of GANs\\n• Flow-based generative models\\n• Diffusion models for data generation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c95d7bb0-8768-4842-a376-3ce043dfd94d', embedding=None, metadata={'page_label': '322', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 322\\nLet’s begin!\\nWhat is a GAN?\\nThe ability of GANs to learn high-dimensional, complex data distributions has made them very popular \\nwith researchers in recent years. Between 2016, when they were first proposed by Ian Goodfellow, to \\nMarch 2022, we have more than 100,000 research papers related to GANs, just in the space of 6 years!\\nThe applications of GANs include creating images, videos, music, and even natural languages. They \\nhave been employed in tasks like image-to-image translation, image super-resolution, drug discovery, \\nand even next-frame prediction in video. They have been especially successful in the task of synthetic \\ndata generation – both for training the deep learning models and assessing the adversarial attacks.\\nThe key idea of GAN can be easily understood by considering it analogous to “art forgery,” which is \\nthe process of creating works of art that are falsely credited to other usually more famous artists. \\nGANs train two neural nets simultaneously. The generator G(Z) is the one that makes the forgery, \\nand the discriminator D(Y) is the one that can judge how realistic the reproductions are, based on its \\nobservations of authentic pieces of art and copies. D(Y) takes an input Y (for instance, an image), and \\nexpresses a vote to judge how real the input is. In general, a value close to 1 denotes “real,” while a \\nvalue close to 0 denotes “forgery.” G(Z) takes an input from random noise Z and it trains itself to fool \\nD into thinking that whatever G(Z) produces is real.\\nThe goal of training the discriminator D(Y) is to maximize D(Y) for every image from the true data \\ndistribution and to minimize D(Y) for every image not from the true data distribution. So, G  and D  play \\nopposite games, hence the name adversarial training. Note that we train G and D in an alternating \\nmanner, where each one of their objectives is expressed as a loss function optimized via a gradient \\ndescent. The generative model continues to improve its forgery capabilities, and the discriminative \\nmodel continues to improve its forgery recognition capabilities. The discriminator network (usually \\na standard convolutional neural network) tries to classify if an input image is real or generated. The \\nimportant new idea is to backpropagate through both the discriminator and the generator to adjust \\nthe generator’s parameters in such a way that the generator can learn how to fool the discriminator \\nmore often. In the end, the generator will learn how to produce images that are indistinguishable \\nfrom the real ones:All the code files for this chapter can be found at https://packt.link/dltfchp9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a1171723-6c77-4cc4-a037-cf18ae4e4b4b', embedding=None, metadata={'page_label': '323', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 323\\nFigure 9.1: Basic architecture of a GAN\\nOf course, GANs involve working towards equilibrium in a game involving two players. Let us first \\nunderstand what we mean by equilibrium here. When we start, one of the two players is hopefully better \\nthan the other. This pushes the other to improve and this way, both the generator and discriminator \\npush each other towards improvement.\\nEventually, we reach a state where the improvement is not significant in either player. We check this \\nby plotting the loss function, to see when the two losses (gradient loss and discriminator loss) reach a \\nplateau. We don’t want the game to be skewed too heavily one way; if the forger were to immediately \\nlearn how to fool the judge on every occasion, then the forger has nothing more to learn. Practically \\ntraining GANs is really hard, and a lot of research is being done in analyzing GAN convergence; \\ncheck this site: https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-\\ntraining  for details on convergence and stability of different types of GANs. In generative applications \\nof GAN, we want the generator to learn a little better than the discriminator.\\nLet’s now delve deep into how GANs learn. Both the discriminator and generator take turns to learn. \\nThe learning can be divided into two steps:\\n1. Here the discriminator, D(x) , learns. The generator, G(z) , is used to generate fake images \\nfrom random noise z (which follows some prior distribution P(z)). The fake images from the \\ngenerator and the real images from the training dataset are both fed to the discriminator, and \\nit performs supervised learning trying to separate fake from real. If Pdata (x) is the training \\ndataset distribution, then the discriminator network tries to maximize its objective so that D(x)  \\nis close to 1 when the input data is real and close to zero when the input data is fake.\\n2. In the next step, the generator network learns. Its goal is to fool the discriminator network \\ninto thinking that generated G(z)  is real, that is, force D(G(z))  close to 1.\\nThe two steps are repeated sequentially. Once the training ends, the discriminator is no longer able to \\ndiscriminate between real and fake data and the generator becomes a pro in creating data very similar to \\nthe training data. The stability between discriminator and generator is an actively researched problem.\\nNow that you have got an idea of what GANs are, let’s look at a practical application of a GAN in which \\n“handwritten” digits are generated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='493c6c35-40df-4411-b16e-d7a76d0d879c', embedding=None, metadata={'page_label': '324', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 324\\nMNIST using GAN in TensorFlow\\nLet us build a simple GAN capable of generating handwritten digits. We will use the MNIST handwritten \\ndigits to train the network. We will need to import TensorFlow modules; to keep the code clean, we \\nexport all the classes that we will require from the TensorFlow framework:\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\\nfrom tensorflow.keras.layers import BatchNormalization, Activation, \\nZeroPadding2D\\nfrom tensorflow.keras.layers import LeakyReLU\\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D\\nfrom tensorflow.keras.models import Sequential, Model\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras import initializers\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nWe use the TensorFlow Keras dataset to access the MNIST data. The data contains 60,000 training \\nimages of handwritten digits each of size 28 × 28. The pixel value of the digits lies between 0-255; we \\nnormalize the input values such that each pixel has a value in the range [-1, 1]:\\nrandomDim = 10\\n(X_train, _), (_,  _) = mnist.load_data()\\nX_train = (X_train.astype(np.float32) - 127.5)/127.5\\nWe will use a simple multi-layered perceptron (MLP ) and we will feed it an image as a flat vector of \\nsize 784, so we reshape the training data:\\nX_train = X_train.reshape( 60000, 784)\\nNow we will need to build a generator and discriminator. The purpose of the generator is to take \\nin a noisy input and generate an image similar to the training dataset. The size of the noisy input is \\ndecided by the variable randomDim ; you can initialize it to any integral value. Conventionally, people \\nset it to 100. For our implementation, we tried a value of 10. This input is fed to a dense layer with \\n256 neurons with LeakyReLU activation. We next add another dense layer with 512 hidden neurons, \\nfollowed by the third hidden layer with 1024  neurons, and finally the output layer with 784 neurons. \\nYou can change the number of neurons in the hidden layers and see how the performance changes; \\nhowever, the number of neurons in the output unit has to match the number of pixels in the training \\nimages. The corresponding generator is then:\\ngenerator = Sequential()\\ngenerator.add(Dense( 256, input_dim=randomDim))\\ngenerator.add(LeakyReLU( 0.2))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b1a56692-3818-4c5c-a6c1-7a6f211ff9b0', embedding=None, metadata={'page_label': '325', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 9 325\\ngenerator.add(Dense( 512))\\ngenerator.add(LeakyReLU( 0.2))\\ngenerator.add(Dense( 1024))\\ngenerator.add(LeakyReLU( 0.2))\\ngenerator.add(Dense( 784, activation= 'tanh'))\\nSimilarly, we build a discriminator. Notice now (Figure 9.1) that the discriminator takes in the images, \\neither from the training set or images generated by the generator, thus its input size is 784. Additionally, \\nhere we are using a TensorFlow initializer to initialize the weights of the dense layer, we are using \\na normal distribution with a standard deviation of 0.02 and a mean of 0. As mentioned in Chapter 1, \\nNeural Network Foundations with TF, there are many initializers available in the TensorFlow framework. \\nThe output of the discriminator is a single bit, with 0 signifying a fake image (generated by generator) \\nand 1 signifying that the image is from the training dataset:\\ndiscriminator = Sequential()\\ndiscriminator.add(Dense( 1024, input_dim= 784, kernel_initializer=initializers.\\nRandomNormal(stddev= 0.02))\\n)\\ndiscriminator.add(LeakyReLU( 0.2))\\ndiscriminator.add(Dropout( 0.3))\\ndiscriminator.add(Dense( 512))\\ndiscriminator.add(LeakyReLU( 0.2))\\ndiscriminator.add(Dropout( 0.3))\\ndiscriminator.add(Dense( 256))\\ndiscriminator.add(LeakyReLU( 0.2))\\ndiscriminator.add(Dropout( 0.3))\\ndiscriminator.add(Dense( 1, activation= 'sigmoid' ))\\nNext, we combine the generator and discriminator together to form a GAN. In the GAN, we ensure \\nthat the discriminator weights are fixed by setting the trainable  argument to False :\\ndiscriminator.trainable = False\\nganInput = Input(shape=(randomDim,))\\nx = generator(ganInput)\\nganOutput = discriminator(x)\\ngan = Model(inputs=ganInput, outputs=ganOutput)\\nThe trick to training the two is that we first train the discriminator separately; we use binary cross-\\nentropy loss for the discriminator. Later, we freeze the weights of the discriminator and train the \\ncombined GAN; this results in the training of the generator. The loss this time is also binary cross-\\nentropy:\\ndiscriminator. compile(loss='binary_crossentropy' , optimizer= 'adam')\\ngan.compile(loss='binary_crossentropy' , optimizer= 'adam')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6f15230-2d09-459e-b2f8-b1ab16311873', embedding=None, metadata={'page_label': '326', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Generative Models 326\\nLet us now perform the training. For each epoch, we take a sample of random noise first, feed it to \\nthe generator, and the generator produces a fake image. We combine the generated fake images and \\nthe actual training images in a batch with their specific labels and use them to train the discriminator \\nfirst on the given batch:\\ndef train(epochs= 1, batchSize= 128):\\n    batchCount = int(X_train.shape[ 0] / batchSize)\\n    print ('Epochs:' , epochs)\\n    print ('Batch size:' , batchSize)\\n    print ('Batches per epoch:' , batchCount)\\n    for e in range (1, epochs+ 1):\\n        print ('-'*15, 'Epoch %d'  % e, '-'*15)\\n        for _ in range (batchCount):\\n            # Get a random set of input noise and images\\n            noise = np.random.normal( 0, 1, size=[batchSize,\\n            randomDim])\\n            imageBatch = X_train[np.random.randint( 0,\\n            X_train.shape[ 0], size=batchSize)]\\n            # Generate fake MNIST images\\n            generatedImages = generator.predict(noise)\\n            # print np.shape(imageBatch), np.shape(generatedImages)\\n            X = np.concatenate([imageBatch, generatedImages])\\n            # Labels for generated and real data\\n            yDis = np.zeros( 2*batchSize)\\n            # One-sided label smoothing\\n            yDis[:batchSize] = 0.9\\n            # Train discriminator\\n            discriminator.trainable = True\\n            dloss = discriminator.train_on_batch(X, yDis)\\nIf you notice, while assigning labels, instead of 0/1 we used 0/0.9 – this is called label smoothing. It \\nhas been found that keeping a soft target improves both generalization and learning speed (When does \\nlabel smoothing help?, Muller et al. NeurIPS 2019).\\nNow, in the same for loop, we will train the generator. We want the images generated by the generator \\nto be detected as real by the discriminator, so we use a random vector (noise) as input to the generator; \\nthis generates a fake image and then trains the GAN such that the discriminator perceives the image \\nas real (the output is 1):\\n            # Train generator\\n            noise = np.random.normal( 0, 1, size=[batchSize,\\n            randomDim])\\n            yGen = np.ones(batchSize)\\n            discriminator.trainable = False\\n            gloss = gan.train_on_batch(noise, yGen)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49476a6f-47a9-4692-b6b0-930ac0b51ff4', embedding=None, metadata={'page_label': '327', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 327\\nCool trick, right? If you wish to, you can save the generator and discriminator loss as well as the \\ngenerated images. Next, we are saving the losses for each epoch and generating images after every \\n20 epochs:\\n        # Store loss of most recent batch from this epoch\\n        dLosses.append(dloss)\\n        gLosses.append(gloss)\\n        if e == 1 or e % 20 == 0:\\n               saveGeneratedImages(e)\\nWe can now train the GAN by calling the train  function. In the following graph, you can see the plot \\nof both generative and discriminative loss as the GAN is learning:\\nFigure 9.2: Discriminator and generator loss plots\\nAnd handwritten digits generated by our GAN:\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57ba4cae-36ca-4caa-a962-d6c9c84d29a7', embedding=None, metadata={'page_label': '328', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Generative Models 328\\nFigure 9.3: Generated handwritten digits\\nYou can see from the preceding figures that as the epochs increase, the handwritten digits generated \\nby the GAN become more and more realistic.\\nTo plot the loss and the generated images of the handwritten digits, we define two helper functions,  \\nplotLoss()  and saveGeneratedImages() . Their code is given as follows:\\n# Plot the loss from each batch\\ndef plotLoss (epoch):\\n    plt.figure(figsize=( 10, 8))\\n    plt.plot(dLosses, label= 'Discriminitive loss' )\\n    plt.plot(gLosses, label= 'Generative loss' )\\n    plt.xlabel( 'Epoch')\\n    plt.ylabel( 'Loss')\\n    plt.legend()\\n    plt.savefig( 'images/gan_loss_epoch_%d.png'  % epoch)\\n# Create a wall of generated MNIST images\\ndef saveGeneratedImages (epoch, examples= 100, dim=(10, 10), figsize=( 10, 10)):\\n    noise = np.random.normal( 0, 1, size=[examples, randomDim])\\n    generatedImages = generator.predict(noise)\\n    generatedImages = generatedImages.reshape(examples, 28, 28)\\n    plt.figure(figsize=figsize)\\n    for i in range (generatedImages.shape[ 0]):\\n        plt.subplot(dim[ 0], dim[ 1], i+1)\\n        plt.imshow(generatedImages[i], interpolation= 'nearest' ,\\n        cmap= 'gray_r' )\\n        plt.axis( 'off')\\n    plt.tight_layout()\\n    plt.savefig( 'images/gan_generated_image_epoch_%d.png'  % epoch)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96448546-f3e7-48aa-b2a6-f32280c2fe6c', embedding=None, metadata={'page_label': '329', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 329\\nThe saveGeneratedImages  function saves images in the images  folder, so make sure you have created \\nthe folder in your current working directory. The complete code for this can be found in the notebook \\nVanillaGAN.ipynb  at the GitHub repo for this chapter. In the coming sections, we will cover some \\nrecent GAN architectures and implement them in TensorFlow.\\nDeep convolutional GAN (DCGAN)\\nProposed in 2016, DCGANs have become one of the most popular and successful GAN architectures. \\nThe main idea of the design was using convolutional layers without the use of pooling layers or the \\nend classifier layers. The convolutional strides and transposed convolutions are employed for the \\ndownsampling (the reduction of dimensions) and upsampling (the increase of dimensions. In GANs, we \\ndo this with the help of a transposed convolution layer. To know more about transposed convolution layers, \\nrefer to the paper A guide to convolution arithmetic for deep learning by Dumoulin and Visin) of images.\\nBefore going into the details of the DCGAN architecture and its capabilities, let us point out the major \\nchanges that were introduced in the paper:\\n• The network consisted of all convolutional layers. The pooling layers were replaced by strided \\nconvolutions (i.e., instead of one single stride while using the convolutional layer, we increased \\nthe number of strides to two) in the discriminator and transposed convolutions in the generator.\\n• The fully connected classifying layers after the convolutions are removed.\\n• To help with the gradient flow, batch normalization is done after every convolutional layer.\\nThe basic idea of DCGANs is the same as the vanilla GAN: we have a generator that takes in noise of \\n100 dimensions; the noise is projected and reshaped, and is then passed through convolutional layers. \\nFigure 9.4 shows the generator architecture:\\nFigure 9.4: Visualizing the architecture of a generator\\nThe discriminator network takes in the images (either generated by the generator or from the real \\ndataset), and the images undergo convolution followed by batch normalization. At each convolution \\nstep, the images get downsampled using strides. The final output of the convolutional layer is flattened \\nand feeds a one-neuron classifier layer. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='679ed45f-d088-44b1-b27f-953bba26d10d', embedding=None, metadata={'page_label': '330', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 330\\nIn Figure 9.5, you can see the discriminator:\\nFigure 9.5: Visualizing the architecture of a discriminator\\nThe generator and the discriminator are combined together to form the DCGAN. The training follows \\nin the same manner as before; that is, we first train the discriminator on a mini-batch, then freeze the \\ndiscriminator and train the generator. The process is repeated iteratively for a few thousand epochs. \\nThe authors found that we get more stable results with the Adam optimizer and a learning rate of 0.002.\\nNext, we’ll implement a DCGAN for generating handwritten digits.\\nDCGAN for MNIST digits\\nLet us now build a DCGAN for generating handwritten digits. We first see the code for the generator. \\nThe generator is built by adding the layers sequentially. The first layer is a dense layer that takes the \\nnoise of 100 dimensions as an input. The 100-dimensional input is expanded to a flat vector of size \\n128 × 7 × 7. This is done so that finally, we get an output of size 28 × 28, the standard size of MNIST \\nhandwritten digits. The vector is reshaped to a tensor of size 7 × 7 × 128. This vector is then upsampled \\nusing the TensorFlow Keras UpSampling2D  layer. Please note that this layer simply scales up the image \\nby doubling rows and columns. The layer has no weights, so it is computationally cheap.\\nThe Upsampling2D layer will now double the rows and columns of the 7 × 7 × 128 (rows × columns \\n× channels) image, yielding an output of size 14 × 14 × 128. The upsampled image is passed to a \\nconvolutional layer. This convolutional layer learns to fill in the details in the upsampled image. The \\noutput of a convolution is passed to batch normalization for better gradient flow. The batch normalized \\noutput then undergoes ReLU activation in all the intermediate layers. We repeat the structure, that is, \\nupsampling | convolution | batch normalization | ReLU. In the following generator, we have two such \\nstructures, the first with 128 filters, and the second with 64 filters in the convolution operation. The \\nfinal output is obtained from a pure convolutional layer with 3 filters and tan hyperbolic activation, \\nyielding an image of size 28 × 28 × 1:\\ndef build_generator (self):\\n    model = Sequential()\\n    model.add(Dense( 128 * 7 * 7, activation= \"relu\",', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4fb14537-6b6d-48b1-ae98-1e21b661aeac', embedding=None, metadata={'page_label': '331', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 331\\n    input_dim=self.latent_dim))\\n    model.add(Reshape(( 7, 7, 128 )))\\n    model.add(UpSampling2D())\\n    model.add(Conv2D( 128, kernel_size= 3, padding= \"same\"))\\n    model.add(BatchNormalization(momentum= 0.8))\\n    model.add(Activation( \"relu\"))\\n    model.add(UpSampling2D())\\n    model.add(Conv2D( 64, kernel_size= 3, padding= \"same\"))\\n    model.add(BatchNormalization(momentum= 0.8))\\n    model.add(Activation( \"relu\"))\\n    model.add(Conv2D(self.channels, kernel_size= 3, padding= \"same\"))\\n    model.add(Activation( \"tanh\"))\\n    model.summary()\\n    noise = Input(shape=(self.latent_dim,))\\n    img = model(noise)\\n    return Model(noise, img)\\nThe resultant generator model is as follows:\\nModel: \"sequential_1\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n conv2d_3 (Conv2D)           (None, 14, 14, 32)        320       \\n                                                                 \\n leaky_re_lu (LeakyReLU)     (None, 14, 14, 32)        0         \\n                                                                 \\n dropout (Dropout)           (None, 14, 14, 32)        0         \\n                                                                 \\n conv2d_4 (Conv2D)           (None, 7, 7, 64)          18496     \\n                                                                 \\n zero_padding2d (ZeroPadding  (None, 8, 8, 64)         0         \\n 2D)                                                             \\n                                                                 \\n batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \\n hNormalization)                                                 \\n                                                                 \\n leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 64)          0         \\n                                                                 \\n dropout_1 (Dropout)         (None, 8, 8, 64)          0         \\n                                                                 \\n conv2d_5 (Conv2D)           (None, 4, 4, 128)         73856     ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4920a614-ca6a-4c5d-b9bf-be04aac370ab', embedding=None, metadata={'page_label': '332', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 332\\n                                                                 \\n batch_normalization_3 (Batc  (None, 4, 4, 128)        512       \\n hNormalization)                                                 \\n                                                                 \\n leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \\n                                                                 \\n dropout_2 (Dropout)         (None, 4, 4, 128)         0         \\n                                                                 \\n conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \\n                                                                 \\n batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      \\n hNormalization)                                                 \\n                                                                 \\n leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 256)         0         \\n                                                                 \\n dropout_3 (Dropout)         (None, 4, 4, 256)         0         \\n                                                                 \\n flatten (Flatten)           (None, 4096)              0         \\n                                                                 \\n dense_1 (Dense)             (None, 1)                 4097      \\n                                                                 \\n=================================================================\\nTotal params: 393,729\\nTrainable params: 392,833\\nNon-trainable params: 896\\nYou can also experiment with the transposed convolution layer. This layer not only upsamples the \\ninput image but also learns how to fill in details during the training. Thus, you can replace upsampling \\nand convolution layers with a single transposed convolution layer. The transpose convolutional layer \\nperforms an inverse convolution operation. You can read about it in more detail in the paper: A guide \\nto convolution arithmetic for deep learning ( https://arxiv.org/abs/1603.07285 ).\\nNow that we have a generator, let us see the code to build the discriminator. The discriminator is similar \\nto a standard convolutional neural network but with one major change: instead of max pooling, we \\nuse convolutional layers with strides of 2. We also add dropout layers to avoid overfitting, and batch \\nnormalization for better accuracy and fast convergence. The activation layer is leaky ReLU. In the \\nfollowing network, we use three such convolutional layers, with filters of 32, 64, and 128 respectively. \\nThe output of the third convolutional layer is flattened and fed to a dense layer with a single unit.\\nThe output of this unit classifies the image as fake or real:\\ndef build_discriminator (self):\\n    model = Sequential()\\n    model.add(Conv2D( 32, kernel_size= 3, strides= 2,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b2f466f-5a04-4ff3-b121-35690172866b', embedding=None, metadata={'page_label': '333', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 333\\n    input_shape=self.img_shape, padding= \"same\"))\\n    model.add(LeakyReLU(alpha= 0.2))\\n    model.add(Dropout( 0.25))\\n    model.add(Conv2D( 64, kernel_size= 3, strides= 2, padding= \"same\"))\\n    model.add(ZeroPadding2D(padding=(( 0,1),(0,1))))\\n    model.add(BatchNormalization(momentum= 0.8))\\n    model.add(LeakyReLU(alpha= 0.2))\\n    model.add(Dropout( 0.25))\\n    model.add(Conv2D( 128, kernel_size= 3, strides= 2, padding= \"same\"))\\n    model.add(BatchNormalization(momentum= 0.8))\\n    model.add(LeakyReLU(alpha= 0.2))\\n    model.add(Dropout( 0.25))\\n    model.add(Conv2D( 256, kernel_size= 3, strides= 1, padding= \"same\"))\\n    model.add(BatchNormalization(momentum= 0.8))\\n    model.add(LeakyReLU(alpha= 0.2))\\n    model.add(Dropout( 0.25))\\n    model.add(Flatten())\\n    model.add(Dense( 1, activation= \\'sigmoid\\' ))\\n    model.summary()\\n    img = Input(shape=self.img_shape)\\n    validity = model(img)\\n    return  Model(img, validity)\\nThe resultant discriminator network is:\\nModel: \"sequential\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n dense (Dense)               (None, 6272)              633472    \\n                                                                 \\n reshape (Reshape)           (None, 7, 7, 128)         0         \\n                                                                 \\n up_sampling2d (UpSampling2D  (None, 14, 14, 128)      0         \\n )                                                               \\n                                                                 \\n conv2d (Conv2D)             (None, 14, 14, 128)       147584    \\n                                                                 \\n batch_normalization (BatchN  (None, 14, 14, 128)      512       \\n ormalization)                                                   \\n                                                                 \\n activation (Activation)     (None, 14, 14, 128)       0         ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b19bd858-5092-448b-9372-afbf38bd8a98', embedding=None, metadata={'page_label': '334', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 334\\n                                                                 \\n up_sampling2d_1 (UpSampling  (None, 28, 28, 128)      0         \\n 2D)                                                             \\n                                                                 \\n conv2d_1 (Conv2D)           (None, 28, 28, 64)        73792     \\n                                                                 \\n batch_normalization_1 (Batc  (None, 28, 28, 64)       256       \\n hNormalization)                                                 \\n                                                                 \\n activation_1 (Activation)   (None, 28, 28, 64)        0         \\n                                                                 \\n conv2d_2 (Conv2D)           (None, 28, 28, 1)         577       \\n                                                                 \\n activation_2 (Activation)   (None, 28, 28, 1)         0         \\n                                                                 \\n=================================================================\\nTotal params: 856,193\\nTrainable params: 855,809\\nNon-trainable params: 384\\n_________________________________________________________________\\nThe complete GAN is made by combining the two:\\nclass DCGAN():\\n    def __init__ (self, rows, cols, channels, z = 10):\\n        # Input shape\\n        self.img_rows = rows\\n        self.img_cols = cols\\n        self.channels = channels\\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\\n        self.latent_dim = z\\n        optimizer = Adam( 0.0002, 0.5)\\n        # Build and compile the discriminator\\n        self.discriminator = self.build_discriminator()\\n        self.discriminator. compile(loss=’binary_crossentropy’ ,\\n            optimizer=optimizer,\\n            metrics=[ ‘accuracy’ ])\\n        # Build the generator\\n        self.generator = self.build_generator()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e3ff13a-3750-4a8e-ba50-e760ec1847ce', embedding=None, metadata={'page_label': '335', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 335\\n        # The generator takes noise as input and generates imgs\\n        z = Input(shape=(self.latent_dim,))\\n        img = self.generator(z)\\n        # For the combined model we will only train the generator\\n        self.discriminator.trainable = False\\n        # The discriminator takes generated images as input and determines \\nvalidity\\n        valid = self.discriminator(img)\\n        # The combined model  (stacked generator and discriminator)\\n        # Trains the generator to fool the discriminator\\n        self.combined = Model(z, valid)\\n        self.combined. compile(loss=’binary_crossentropy’ , optimizer=optimizer)\\nAs you might have noticed, we are defining here the binary_crossentropy  loss object, which we will \\nuse later to define the generator and discriminator losses. Optimizers for both the generator and \\ndiscriminator is defined in this init  method. And finally, we define a TensorFlow checkpoint that we \\nwill use to save the two models (generator and discriminator) as the model trains.\\nThe GAN is trained in the same manner as before; at each step, first, random noise is fed to the \\ngenerator. The output of the generator is added with real images to initially train the discriminator, \\nand then the generator is trained to give an image that can fool the discriminator. \\nThe process is repeated for the next batch of images. The GAN takes between a few hundred to \\nthousands of epochs to train:\\n   def train(self, epochs, batch_size= 256, save_interval= 50):\\n        # Load the dataset\\n        (X_train, _), (_, _) = mnist.load_data()\\n        # Rescale -1 to 1\\n        X_train = X_train / 127.5 - 1.\\n        X_train = np.expand_dims(X_train, axis= 3)\\n        # Adversarial ground truths\\n        valid = np.ones((batch_size, 1))\\n        fake = np.zeros((batch_size, 1))\\n        for epoch in range (epochs):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f5e7e63-f068-48be-9eba-2ee33b87be4c', embedding=None, metadata={'page_label': '336', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 336\\n            # ---------------------\\n            #  Train Discriminator\\n            # ---------------------\\n            # Select a random half of images\\n            idx = np.random.randint( 0, X_train.shape[ 0], batch_size)\\n            imgs = X_train[idx]\\n            # Sample noise and generate a batch of new images\\n            noise = np.random.normal( 0, 1, (batch_size, self.latent_dim))\\n            gen_imgs = self.generator.predict(noise)\\n            # Train the discriminator (real classified as ones and generated as \\nzeros)\\n            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\\n            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\\n            # ---------------------\\n            #  Train Generator\\n            # ---------------------\\n            # Train the generator (wants discriminator to mistake images as \\nreal)\\n            g_loss = self.combined.train_on_batch(noise, valid)\\n            # Plot the progress\\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\"  % (epoch, d_\\nloss[0], 100*d_loss[ 1], g_loss))\\n            # If at save interval => save generated image samples\\n            if epoch % save_interval == 0:\\n                self.save_imgs(epoch)\\nLastly, we need a helper function to save images:\\n    def save_imgs (self, epoch):\\n        r, c = 5, 5\\n        noise = np.random.normal( 0, 1, (r * c, self.latent_dim))\\n        gen_imgs = self.generator.predict(noise)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ddd694ff-97d3-4dd2-bcf4-92735fd057bd', embedding=None, metadata={'page_label': '337', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 337\\n        # Rescale images 0 - 1\\n        gen_imgs = 0.5 * gen_imgs + 0.5\\n        fig, axs = plt.subplots(r, c)\\n        cnt = 0\\n        for i in range (r):\\n            for j in range (c):\\n                axs[i,j].imshow(gen_imgs[cnt, :,:, 0], cmap= \\'gray\\')\\n                axs[i,j].axis( \\'off\\')\\n                cnt += 1\\n        fig.savefig( \"images/dcgan_mnist_%d.png\"  % epoch)\\n        plt.close()\\nLet us now train our GAN:\\ndcgan = DCGAN( 28,28,1)\\ndcgan.train(epochs= 5000, batch_size= 256, save_interval= 50)\\nThe images generated by our GAN as it learned to fake handwritten digits are:\\n \\nFigure 9.6: Images generated by GAN – initial attempt\\nThe preceding images were the initial attempts by the GAN. As it learned through the following 10 \\nepochs, the quality of digits generated improved manyfold:\\n \\nFigure 9.7: Images generated by GAN after 6, 8, and 10 epochs\\nThe complete code is available in DCGAN.ipynb  in the GitHub repo. We can take the concepts discussed \\nhere and apply them to images in other domains. One of the interesting works on images was reported \\nin the paper, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, \\nAlec Radford, Luke Metz, Soumith Chintala, 2015. Quoting the abstract:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7657f18d-6e09-40a9-abc6-ac7a8c33f101', embedding=None, metadata={'page_label': '338', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 338\\nFollowing are some of the interesting results of applying DCGANs to a celebrity image dataset:\\nIn recent years, supervised learning with convolutional networks (CNNs) has seen huge \\nadoption in computer vision applications. Comparatively, unsupervised learning with \\nCNNs has received less attention. In this work we hope to help bridge the gap between \\nthe success of CNNs for supervised learning and unsupervised learning. We introduce \\na class of CNNs called deep convolutional generative adversarial networks (DCGANs), \\nthat have certain architectural constraints, and demonstrate that they are a strong \\ncandidate for unsupervised learning. Training on various image datasets, we show \\nconvincing evidence that our deep convolutional adversarial pair learns a hierarchy \\nof representations from object parts to scenes in both the generator and discriminator. \\nAdditionally, we use the learned features for novel tasks - demonstrating their applica-\\nbility as general image representations.\\n—Radford et al., 2015', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9e61ca2-8cd8-4f92-9769-49e780c1d99e', embedding=None, metadata={'page_label': '339', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 339\\nFigure 9.8: Generated celebrity images using DCGAN\\nAnother interesting paper is Semantic Image Inpainting with Perceptual and Contextual Losses, by Raymond \\nA. Yeh et al. in 2016. Just as content-aware fill is a tool used by photographers to fill in unwanted or \\nmissing parts of images, in this paper they used a DCGAN for image completion.\\nAs mentioned earlier, a lot of research is happening around GANs. In the next section, we will explore \\nsome of the interesting GAN architectures proposed in recent years.\\nSome interesting GAN architectures\\nSince their inception, a lot of interest has been generated in GANs, and as a result, we are seeing a \\nlot of modifications and experimentation with GAN training, architecture, and applications. In this \\nsection, we will explore some interesting GANs proposed in recent years.\\nSRGAN\\nRemember seeing a crime thriller where our hero asks the computer guy to magnify the faded image \\nof the crime scene? With the zoom, we can see the criminal’s face in detail, including the weapon \\nused and anything engraved upon it! Well, Super Resolution GANs ( SRGANs ) can perform similar \\nmagic. Magic in the sense that because GANs show that it is possible to get high-resolution images, \\nthe final results depend on the camera resolution used. Here, a GAN is trained in such a way that it \\ncan generate a photorealistic high-resolution image when given a low-resolution image. The SRGAN \\narchitecture consists of three neural networks: a very deep generator network (which uses Residual \\nmodules; see ResNets in Chapter 20, Advanced Convolutional Neural Networks), a discriminator network, \\nand a pretrained VGG-16 network.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1df87040-6dae-4731-899e-220609b9b258', embedding=None, metadata={'page_label': '340', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 340\\nSRGANs use the perceptual loss function (developed by Johnson et al; you can find the link to the \\npaper in the References section). In SRGAN, the authors first downsampled a high-resolution image and \\nused the generator to get its “high-resolution” version. The discriminator was trained to differentiate \\nbetween the real high-resolution image and the generated high-resolution image. The difference in \\nthe feature map activations in high layers of a VGG network between the network output and the \\nhigh-resolution parts comprises the perceptual loss function. Besides perceptual loss, the authors \\nfurther added content loss and an adversarial loss so that images generated look more natural and \\nthe finer details more artistic. The perceptual loss is defined as the weighted sum of the content loss \\nand adversarial loss:\\n𝑙𝑙𝑆𝑆𝑆𝑆=𝑙𝑙𝑋𝑋𝑆𝑆𝑆𝑆+10−3×𝑙𝑙𝐺𝐺𝐺𝐺𝐺𝐺𝑆𝑆𝑆𝑆 \\nThe first term on the right-hand side is the content loss, obtained using the feature maps generated \\nby pretrained VGG 19. Mathematically, it is the Euclidean distance between the feature map of the \\nreconstructed image (that is, the one generated by the generator) and the original high-resolution \\nreference image. The second term on the RHS is the adversarial loss. It is the standard generative \\nloss term, designed to ensure that images generated by the generator can fool the discriminator. You \\ncan see in the following figure that the image generated by the SRGAN is much closer to the original \\nhigh-resolution image with a PSNR value of 37.61:\\nFigure 9.9: An example following the paper Photo-Realistic Single Image Super-Resolution Using \\na Generative Adversarial Network, Ledig et al.\\nAnother noteworthy  architecture is CycleGAN; proposed in 2017, it can perform the task of image \\ntranslation. Once trained you can translate an image from one domain to another domain. For example, \\nwhen trained on a horse and zebra dataset, if you give it an image with horses in the foreground, the \\nCycleGAN can convert the horses to zebras with the same background. We will explore it next.\\nCycleGAN\\nHave you ever imagined how some scenery would look if Van Gogh or Manet had painted it? We \\nhave many scenes and landscapes painted by Van Gogh/Manet, but we do not have any collection of \\ninput-output pairs. A CycleGAN performs the image translation, that is, transfers an image given in \\none domain (scenery, for example) to another domain (a Van Gogh painting of the same scene, for \\ninstance) in the absence of training examples. The CycleGAN’s ability to perform image translation \\nin the absence of training pairs is what makes it unique.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c8b0c10-6e7d-495e-85f2-98f8dccfc1d3', embedding=None, metadata={'page_label': '341', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 341\\nTo achieve image translation, the authors used a very simple yet effective procedure. They made use of \\ntwo GANs, the generator of each GAN performing the image translation from one domain to another.\\nTo elaborate, let us say the input is X , then the generator of the first GAN performs a mapping  \\n𝐺𝐺𝐺𝐺𝐺 𝐺𝐺𝐺  ; thus, its output would be Y = G(X). The generator of the second GAN performs an inverse \\nmapping 𝐹𝐹𝐹𝐹𝐹 𝐹𝐹𝐹  , resulting in X = F(Y). Each discriminator is trained to distinguish between real \\nimages and synthesized images. The idea is shown as follows:\\nFigure 9.10: Cycle-consistency loss \\nTo train the combined GANs, the authors added, besides the conventional GAN adversarial loss, a \\nforward cycle-consistency loss (left figure) and a backward cycle-consistency loss (right figure). This \\nensures that if an image X is given as input, then after the two translations F(G(X)) ~ X the obtained \\nimage is the same, X (similarly the backward cycle-consistency loss ensures that G(F(Y)) ~ Y).\\nFollowing are some of the successful image translations by CycleGANs [7]:\\nFigure 9.11: Examples of some successful CycleGAN image translations', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a275fe9-62fa-45a0-b45f-c1aadb2b1c75', embedding=None, metadata={'page_label': '342', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 342\\nFollowing are a few more examples; you can see the translation of seasons (summer →  winter), photo \\n→  painting and vice versa, and horses →  zebras and vice versa [7]:\\nFigure 9.12: Further examples of CycleGAN translations\\nLater in the chapter, we will also explore a TensorFlow implementation of CycleGANs. Next, we talk \\nabout the InfoGAN, a conditional GAN where the GAN not only generates an image, but you also have \\na control variable to control the images generated.\\nInfoGAN\\nThe GAN architectures that we have considered up to now provide us with little or no control over \\nthe generated images. The InfoGAN changes this; it provides control over various attributes of the \\nimages generated. The InfoGAN uses the concepts from information theory such that the noise term \\nis transformed into latent code that provides predictable and systematic control over the output.\\nThe generator in an InfoGAN takes two inputs: the latent space Z and a latent code c, thus the output \\nof the generator is G(Z,c). The GAN is trained such that it maximizes the mutual information between \\nthe latent code c and the generated image G(Z,c). The following figure shows the architecture of the \\nInfoGAN:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fa85fdf-9836-42a7-a7c4-a4183892440d', embedding=None, metadata={'page_label': '343', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 343\\nFigure 9.13: The architecture of the InfoGAN, visualized\\nThe concatenated vector (Z,c) is fed to the generator. Q(c|X) is also a neural network. Combined with \\nthe generator, it works to form a mapping between random noise Z and its latent code c_hat. It aims \\nto estimate c given X. This is achieved by adding a regularization term to the objective function of the \\nconventional GAN:\\n𝑚𝑚𝑚𝑚𝑚𝑚 𝐷𝐷𝑚𝑚𝑚𝑚𝑚𝑚 𝐺𝐺𝑉𝑉1(𝐷𝐷𝐷 𝐷𝐷 )= 𝑉𝑉 𝐺𝐺(𝐷𝐷𝐷 𝐷𝐷 )− 𝜆𝜆𝜆𝜆(𝑐𝑐𝑐𝐷𝐷(𝑍𝑍𝐷 𝑐𝑐)) \\nThe term V G(D,G)  is the loss function of the conventional GAN, and the second term is the regularization \\nterm, where 𝜆𝜆  is a constant. Its value was set to 1 in the paper, and I(c;G(Z,c)) is the mutual information \\nbetween the latent code c and the generator-generated image G(Z,c).\\nFollowing are the exciting results of the InfoGAN on the MNIST dataset:\\nFigure 9.14: Results of using the InfoGAN on the MNIST dataset. Here, different rows correspond to \\ndifferent random samples of fixed latent codes and noise ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8391487-c596-4bca-8dda-cdfce6977bba', embedding=None, metadata={'page_label': '344', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 344\\nNow, that we have seen some exciting GAN architectures, let us explore some cool applications of GAN.\\nCool applications of GANs\\nWe have seen that the generator can learn how to forge data. This means that it learns how to create \\nnew synthetic data that is created by the network that appears to be authentic and human-made. \\nBefore going into the details of some GAN code, we would like to share the results of the paper [6] \\n(code is available online at https://github.com/hanzhanggit/StackGAN ) where a GAN has been \\nused to synthesize forged images starting from a text description. The results are impressive: the \\nfirst column is the real image in the test set and all the rest of the columns are the images generated \\nfrom the same text description by Stage-I and Stage-II of StackGAN. More examples are available on \\nYouTube ( https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be ):\\nFigure 9.15: Image generation of birds, using GANs\\nFigure 9.16: Image generation of flowers, using GANs\\nNow let us see how a GAN can learn to “forge” the MNIST dataset. In this case, it is a combination of \\nGAN and CNNs used for the generator and discriminator networks. In the beginning, the generator \\ncreates nothing understandable, but after a few iterations, synthetic forged numbers are progressively \\nclearer and clearer. In this image, the panels are ordered by increasing training epochs and you can \\nsee the quality improving among the panels:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9577769-7f95-47e9-8645-9105ffa2e2d0', embedding=None, metadata={'page_label': '345', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 345\\nFigure 9.17: Illegible initial outputs of the GAN\\nAs the training progresses, you can see in Figure 9.17 that the digits start taking a more recognizable \\nform:\\nFigure 9.18: Improved outputs of the GAN, following further iterations\\nFigure 9.19: Final outputs of the GAN, showing significant improvement from previous iterations\\nAfter 10,000 epochs, you can see that the handwritten digits are even more realistic. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ba9afce-960e-4cb2-9675-fb4801a227ef', embedding=None, metadata={'page_label': '346', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 346\\nOne of the coolest uses of GANs is doing arithmetic on faces in the generator’s vector Z . In other words, \\nif we stay in the space of synthetic forged images, it is possible to see things like this: [smiling woman] \\n- [neutral woman] + [neutral man] = [smiling man], or like this: [man with glasses] - [man without glasses] + \\n[woman without glasses] = [woman with glasses]. This was shown in the paper Unsupervised Representation \\nLearning with Deep Convolutional Generative Adversarial Networks by Alec Radford and his colleagues in \\n2015. All images in this work are generated by a version of GAN. They are NOT REAL. The full paper \\nis available here: http://arxiv.org/abs/1511.06434 . Following are some examples from the paper. \\nThe authors also share their code in this GitHub repo: https://github.com/Newmu/dcgan_code :\\nFigure 9.20: Image arithmetic using GANs', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45d03c4b-5d26-4065-904e-d696f7bbbb4a', embedding=None, metadata={'page_label': '347', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 347\\nBedrooms: Generated bedrooms after five epochs of training:\\nFigure 9.21: Generated bedrooms using GAN after 5 epochs of training\\nAlbum covers: These images are generated by the GAN, but look like authentic album covers:\\nFigure 9.22: Album covers generated using DCGAN', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24afbd59-7a98-4a33-aca0-ae11824ffe68', embedding=None, metadata={'page_label': '348', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 348\\nAnother cool application of GANs is the generation of artificial faces. NVIDIA introduced a model in \\n2018, which it named StyleGAN (the second version, StyleGAN2, was released in February 2020, and \\nthe third version in 2021), which it showed can be used to generate realistic-looking images of people. \\nBelow you can see some of the realistic-looking fake people’s faces generated by StyleGAN obtained \\nafter training of 1,000 epochs; for better results, you will need to train more:\\nFigure 9.23: Fake faces generated by StyleGAN\\nNot only does it generate fake images but like InfoGAN, you can control the features from coarse to \\ngrain. This is the official video released by NVIDIA showing how features affect the results: https://\\nwww.youtube.com/watch?v=kSLJriaOumA . They were able to do this by adding a non-linear mapping \\nnetwork after the Latent variable Z . The mapping network transformed the latent variable to a mapping \\nof the same size; the output of the mapping vector is fed to different layers of the generator network, \\nand this allows the StyleGAN to control different visual features. To know more about StyleGAN, you \\nshould read the paper A style-based generator architecture for Generative Adversarial Networks from NVIDIA \\nLabs [10].\\nCycleGAN in TensorFlow\\nIn this section, we will implement a CycleGAN in TensorFlow. The CycleGAN requires a special dataset, \\na paired dataset, from one domain of images to another domain. So, besides the necessary modules, we \\nwill use tensorflow_datasets  as well. Also, we will make use of the library tensorflow_examples , we \\nwill directly use the generator and the discriminator from the pix2pix  model defined in tensorflow_\\nexamples . The code here is adapted from the code here https://github.com/tensorflow/docs/blob/\\nmaster/site/en/tutorials/generative/cyclegan.ipynb :\\nimport tensorflow_datasets as tfds\\nfrom tensorflow_examples.models.pix2pix import pix2pix\\nimport os\\nimport time\\nimport matplotlib.pyplot as plt\\nfrom IPython.display import clear_output\\nimport tensorflow as tf\\nTensorFlow’s Dataset  API contains a list of datasets. It has many paired datasets for CycleGANs, such \\nas horse to zebra, apples to oranges, and so on. You can access the complete list here: https://\\nwww.tensorflow.org/datasets/catalog/cycle_gan . For our code, we will be using summer2winter_\\nyosemite , which contains images of Yosemite (USA) in summer (Dataset A) and winter (Dataset B). We \\nwill train the CycleGAN to convert an input image of summer to winter and vice versa. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8b4278a-6ff9-40d8-8db2-091f05b651aa', embedding=None, metadata={'page_label': '349', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 9 349\\nLet us load the data and get train and test images:\\ndataset, metadata = tfds.load( 'cycle_gan/summer2winter_yosemite' ,\\n                              with_info= True, as_supervised= True)\\ntrain_summer, train_winter = dataset[ 'trainA' ], dataset[ 'trainB' ]\\ntest_summer, test_winter = dataset[ 'testA'], dataset[ 'testB']\\nWe need to set some hyperparameters:\\nBUFFER_SIZE = 1000\\nBATCH_SIZE = 1\\nIMG_WIDTH = 256\\nIMG_HEIGHT = 256\\nEPOCHS = 100\\nLAMBDA = 10\\nAUTOTUNE = tf.data.AUTOTUNE\\nThe images need to be normalized before we train the network. For better performance, we also add \\nrandom jittering to the train images; the images are first resized to size 286x286, then we randomly \\ncrop them back to the size 256x256, and finally apply the random jitter:\\ndef normalize (input_image, label):\\n    input_image = tf.cast(input_image, tf.float32)\\n    input_image = (input_image / 127.5) - 1\\n    return input_image\\ndef random_crop (image):\\n    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT,\\n    IMG_WIDTH, 3])\\n    return cropped_image\\ndef random_jitter (image):\\n    # resizing to 286 x 286 x 3\\n    image = tf.image.resize(image, [ 286, 286],\\n    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\\n    # randomly cropping to 256 x 256 x 3\\n    image = random_crop(image)\\n    # random mirroring\\n    image = tf.image.random_flip_left_right(image)\\n    return image\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='258949d1-155b-44ee-ac4b-e7a1d6d9cc8b', embedding=None, metadata={'page_label': '350', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 350\\nThe augmentation (random crop and jitter) is done only to the train images; therefore, we will need \\nto separate functions for preprocessing the images, one for train data, and the other for test data:\\ndef preprocess_image_train (image, label):\\n    image = random_jitter(image)\\n    image = normalize(image)\\n    return image\\ndef preprocess_image_test (image, label):\\n    image = normalize(image)\\n    return image\\nThe preceding functions, when applied to images, will normalize them in the range [-1,1] and apply \\naugmentation to train images. Let us apply this to our train and test datasets and create a data generator \\nthat will provide images for training in batches:\\ntrain_summer = train_summer.cache(). map(\\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\\n    BUFFER_SIZE).batch(BATCH_SIZE)\\ntrain_winter = train_winter.cache(). map(\\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\\n    BUFFER_SIZE).batch(BATCH_SIZE)\\ntest_summer = test_summer. map(\\n    preprocess_image_test,\\n    num_parallel_calls=AUTOTUNE).cache().shuffle(\\n    BUFFER_SIZE).batch(BATCH_SIZE)\\ntest_winter = test_winter. map(\\n    preprocess_image_test,\\n    num_parallel_calls=AUTOTUNE).cache().shuffle(\\n    BUFFER_SIZE).batch(BATCH_SIZE)\\nIn the preceding code, the argument num_parallel_calls  allows one to take benefit from multiple \\nCPU cores in the system; one should set its value to the number of CPU cores in your system. If you \\nare not sure, use the AUTOTUNE = tf.data.AUTOTUNE  value so that TensorFlow dynamically determines \\nthe right number for you.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='98ad4f38-223a-4dde-812e-955e4babcc9e', embedding=None, metadata={'page_label': '351', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 9 351\\nAs mentioned in the beginning, we use a generator and discriminator from the pix2pix  model defined \\nin the tensorflow_examples  module. We will have two generators and two discriminators:\\nOUTPUT_CHANNELS = 3\\ngenerator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type= 'instancenorm' )\\ngenerator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type= 'instancenorm' )\\ndiscriminator_x = pix2pix.discriminator(norm_type= 'instancenorm' , target= False)\\ndiscriminator_y = pix2pix.discriminator(norm_type= 'instancenorm' , target= False)\\nBefore moving ahead with the model definition, let us see the images. Each image is processed before \\nplotting so that its intensity is normal:\\nto_winter = generator_g(sample_summer)\\nto_summer = generator_f(sample_winter)\\nplt.figure(figsize=( 8, 8))\\ncontrast = 8\\nimgs = [sample_summer, to_winter, sample_winter, to_summer]\\ntitle = [ 'Summer' , 'To Winter' , 'Winter' , 'To Summer' ]\\nfor i in range (len(imgs)):\\n  plt.subplot( 2, 2, i+ 1)\\n  plt.title(title[i])\\n  if i % 2 == 0:\\n    plt.imshow(imgs[i][ 0] * 0.5 + 0.5)\\n  else:\\n    plt.imshow(imgs[i][ 0] * 0.5 * contrast + 0.5)\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11491c83-9b9b-4801-add7-c61825060c5d', embedding=None, metadata={'page_label': '352', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 352\\nFigure 9.24: The input of GAN 1 and output of GAN 2 in CycleGAN architecture before training\\nWe next define the loss and  optimizers. We retain the same loss functions for generator and discriminator \\nas we did in DCGAN:\\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits= True)\\ndef discriminator_loss (real, generated):\\n    real_loss = loss_obj(tf.ones_like(real), real)\\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\\n    total_disc_loss = real_loss + generated_loss\\n    return total_disc_loss * 0.5\\ndef generator_loss (generated):\\n    return loss_obj(tf.ones_like(generated), generated)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='624bca1a-8bc2-4d75-a161-6154d6b96848', embedding=None, metadata={'page_label': '353', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 353\\nSince there are now four models, two generators and two discriminators, we need to define four \\noptimizers:\\ngenerator_g_optimizer = tf.keras.optimizers.Adam( 2e-4, beta_1= 0.5)\\ngenerator_f_optimizer = tf.keras.optimizers.Adam( 2e-4, beta_1= 0.5)\\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam( 2e-4, beta_1= 0.5)\\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam( 2e-4, beta_1= 0.5)\\nAdditionally, in the CycleGAN, we require to define two more loss functions, first the cycle-consistency \\nloss; we can use the same function for forward and backward cycle-consistency loss calculation. The \\ncycle-consistency loss ensures that the result is close to the original input:\\ndef calc_cycle_loss (real_image, cycled_image):\\n    loss1 = tf.reduce_mean(tf. abs(real_image - cycled_image))\\n    return LAMBDA * loss1\\nWe also need to define an identity loss, which ensures that if an image Y is fed to the generator, it \\nwould yield the real image Y or an image similar to Y. Thus, if we give our summer image generator \\nan image of summer as input, it should not change it much:\\ndef identity_loss (real_image, same_image):\\n    loss = tf.reduce_mean(tf. abs(real_image - same_image))\\n    return LAMBDA * 0.5 * loss\\nNow we define the function that trains the generator and discriminator in a batch, a pair of images \\nat a time. The two discriminators and the two generators are trained via this function with the help \\nof the tape gradient. The training step can be divided into four parts:\\n1. Get the output images from the two generators.\\n2. Calculate the losses.\\n3. Calculate the gradients.\\n4. And finally, apply the gradients:\\n@tf.function\\ndef train_step (real_x, real_y):\\n    # persistent is set to True because the tape is used\\n    # more than  once to calculate the gradients.\\n  with tf.GradientTape(persistent= True) as tape:\\n    # Generator G translates X -> Y\\n    # Generator F translates Y -> X.\\n    \\n    fake_y = generator_g(real_x, training= True)\\n    cycled_x = generator_f(fake_y, training= True)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d4f606e-3724-40df-a19c-98473213dc7a', embedding=None, metadata={'page_label': '354', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 354\\n    fake_x = generator_f(real_y, training= True)\\n    cycled_y = generator_g(fake_x, training= True)\\n    # same_x and same_y are used for identity loss.\\n    same_x = generator_f(real_x, training= True)\\n    same_y = generator_g(real_y, training= True)\\n    disc_real_x = discriminator_x(real_x, training= True)\\n    disc_real_y = discriminator_y(real_y, training= True)\\n    disc_fake_x = discriminator_x(fake_x, training= True)\\n    disc_fake_y = discriminator_y(fake_y, training= True)\\n    # calculate the loss\\n    gen_g_loss = generator_loss(disc_fake_y)\\n    gen_f_loss = generator_loss(disc_fake_x)\\n    \\n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + \\\\\\n    calc_cycle_loss(real_y, cycled_y)\\n    \\n    # Total generator loss = adversarial loss + cycle loss\\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + \\\\\\n    identity_loss(real_y, same_y)\\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + \\\\\\n    identity_loss(real_x, same_x)\\n    disc_x_loss = discriminator_loss(disc_real_x,\\n    disc_fake_x)\\n    disc_y_loss = discriminator_loss(disc_real_y,\\n    disc_fake_y)\\n    # Calculate the gradients for generator and discriminator\\n    generator_g_gradients = tape.gradient(total_gen_g_loss,\\n    generator_g.trainable_variables)\\n    generator_f_gradients = tape.gradient(total_gen_f_loss,\\n    generator_f.trainable_variables)\\n    discriminator_x_gradients = tape.gradient(disc_x_loss,\\n    discriminator_x.trainable_variables)\\n    discriminator_y_gradients = tape.gradient(disc_y_loss,\\n    discriminator_y.trainable_variables)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac215f63-12e2-4e7a-8dda-c3057c2b0a3d', embedding=None, metadata={'page_label': '355', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 355\\n    # Apply the gradients to the optimizer\\n    generator_g_optimizer.apply_gradients( zip(generator_g_gradients, \\ngenerator_g.trainable_variables))\\n    generator_f_optimizer.apply_gradients( zip(generator_f_gradients, \\ngenerator_f.trainable_variables))\\n    discriminator_x_optimizer.apply_gradients( zip(discriminator_x_\\ngradients, discriminator_x.trainable_variables))\\n    discriminator_y_optimizer.apply_gradients( zip(discriminator_y_\\ngradients, discriminator_y.trainable_variables))\\nWe define checkpoints to save the model weights. Since it can take a while to train a sufficiently \\ngood CycleGAN, we save the checkpoints, and if we start next, we can start with loading the existing \\ncheckpoints – this will ensure that model starts learning from where it left:\\ncheckpoint_path = \"./checkpoints/train\"\\nckpt = tf.train.Checkpoint(generator_g=generator_g,\\n                           generator_f=generator_f,\\n                           discriminator_x=discriminator_x,\\n                           discriminator_y=discriminator_y,\\n                           generator_g_optimizer=generator_g_optimizer,\\ngenerator_f_optimizer=generator_f_optimizer,\\ndiscriminator_x_optimizer=discriminator_x_optimizer,\\ndiscriminator_y_optimizer=discriminator_y_optimizer)\\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep= 5)\\n# if a checkpoint exists, restore the latest checkpoint.\\nif ckpt_manager.latest_checkpoint:\\n    ckpt.restore(ckpt_manager.latest_checkpoint)\\n    print (\\'Latest checkpoint restored!!\\' )\\nLet us now combine it all and train the network for 100 epochs. Please remember that in the paper, \\nthe test network was trained for 200 epochs, so our results will not be that good:\\nfor epoch in range (EPOCHS):\\n    start = time.time()\\n    n = 0\\n    for image_x, image_y in tf.data.Dataset. zip((train_summer, train_winter)):\\n        train_step(image_x, image_y)\\n        if n % 10 == 0:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f46003cc-9842-4753-a773-2c37115facee', embedding=None, metadata={'page_label': '356', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Generative Models 356\\n            print ('.', end='')\\n        n += 1\\n    clear_output(wait= True)\\n    # Using a consistent image (sample_summer) so that the progress of\\n    # the model  is clearly visible.\\n    generate_images(generator_g, sample_summer)\\n    if (epoch + 1) % 5 == 0:\\n        ckpt_save_path = ckpt_manager.save()\\n        print ('Saving checkpoint for epoch {} at {}' .format(epoch+1,\\n                                                             ckpt_save_path))\\n    print ('Time taken for epoch {} is {} sec\\\\n' .format(epoch + 1,\\n                                                        time.time()-start))\\nYou can see some of the images generated by our CycleGAN. Generator A takes in summer photos \\nand converts them to winter, while generator B takes in winter photos and converts them to summer:\\nFigure 9.25: Images using CycleGAN after training\\nWe suggest you experiment with other datasets in the TensorFlow CycleGAN datasets. Some will be \\neasy like apples and oranges, but some will require much more training. The authors also maintain a \\nGitHub repository where they have shared their own implementation in PyTorch along with the links to \\nimplementations in other frameworks including TensorFlow: https://github.com/junyanz/CycleGAN .\\nFlow-based models for data generation\\nWhile both V AEs (Chapter 8, Autoencoders) and GANs do a good job of data generation, they do not \\nexplicitly learn the probability density function of the input data. GANs learn by converting the \\nunsupervised problem to a supervised learning problem. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='867096bd-f636-42e1-8bd9-1dcd4cb216b5', embedding=None, metadata={'page_label': '357', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 357\\nV AEs try to learn by optimizing the maximum log-likelihood of the data by maximizing the Evidence \\nLower Bound ( ELBO ). Flow-based models differ from the two in that they explicitly learn data \\ndistribution 𝑝𝑝𝑝𝑝𝑝𝑝 . This offers an advantage over V AEs and GANs, because this makes it possible to use \\nflow-based models for tasks like filling incomplete data, sampling data, and even identifying bias in \\ndata distributions. Flow-based models accomplish this by maximizing the log-likelihood estimation. \\nTo understand how, let us delve a little into its math.\\nLet 𝑝𝑝𝐷𝐷(𝑥𝑥𝑥  be the probability density of data D, and let 𝑝𝑝𝑀𝑀(𝑥𝑥𝑥  be the probability density approximated \\nby our model M. The goal of a flow-based model is to find the model parameters 𝜃𝜃∗  such that the \\ndistance between two is minimum, i.e.:\\n𝜃𝜃∗= 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎\\n𝜃𝜃𝜃𝜃𝜃𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑎𝑎𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑 𝐷𝐷𝑑𝑥𝑥),𝑑𝑑𝜃𝜃𝑑𝑥𝑥)) \\nIf we use the KL divergence as our distance metrics, the expression above reduces to:\\n𝜃𝜃∗= argmin\\n𝜃𝜃𝜃𝜃𝜃\\u2008𝐸𝐸𝑥𝑥𝑥𝑥𝑥𝐷𝐷[log𝑝𝑝𝜃𝜃(𝑥𝑥)] \\nThis equation represents minimizing the Negative Log-Likelihood (NLL ) (equivalent to maximizing \\nlog-likelihood estimation.)\\nThe basic architecture of flow-based models consists of a series of invertible functions, as shown in \\nthe figure below. The challenge is to find the function f(x), such that its inverse f-1(x) generates x’, the \\nreconstructed version of the input x:\\nFigure 9.26: Architecture of flow-based model\\nThere are mainly two ways flow-based models are implemented:\\n• Normalized Flow: Here, the basic idea is to use a series of simple invertible functions to \\ntransform the complex input. As we flow through the sequence of transformations, we \\nrepeatedly substitute the variable with a new one, as per the change of variables theorem \\n(https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm ), and finally, we obtain a \\nprobability distribution of the target variable. The path that the variables z i traverse is the flow \\nand the complete chain formed by the successive distributions is called the normalizing flow. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ca0d4a8-9fd0-4f7a-9db4-591ba79fbd1d', embedding=None, metadata={'page_label': '358', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 358\\nThe RealNVP ( Real-valued Non-Volume Preserving) model proposed by Dinh et al., 2017, NICE  \\n(Non-linear Independent Components Estimation) by Dinh et al., 2015, and Glow by Knigma \\nand Dhariwal, 2018, use the normalized flow trick:\\nFigure 9.27: Normalizing flow model: https://lilianweng.github.io/posts/2018-10-13-flow-\\nmodels/\\n• Autoregressive Flow: Models like MADE  (Masked Autoencoder for Distribution Estimation), \\nPixelRNN, and wavenet are based on autoregressive models. Here, each dimension in a vector \\nvariable is dependent on the previous dimensions. Thus, the probability of observing 𝑧𝑧𝑖𝑖  depends \\nonly on 𝑧𝑧1,𝑧𝑧2,…,𝑧𝑧𝑖𝑖𝑖1 , and therefore, the product of these conditional  probabilities gives us the \\nprobability of the entire sequence.\\nLilian Weng’s blog ( https://lilianweng.github.io/posts/2018-10-13-flow-models/ ) provides a \\nvery good description of flow-based models.\\nDiffusion models for data generation\\nThe 2021 paper Diffusion Models Beat GANs on Image synthesis by two OpenAI research scientists Prafulla \\nDhariwal and Alex Nichol garnered a lot of interest in diffusion models for data generation. \\nUsing the Frechet Inception Distance (FID ) as the metrics for evaluation of generated images, they \\nwere able to achieve an FID score of 3.85 on a diffusion model trained on ImageNet data:\\nFigure 9.28: Selected samples of images generated from ImageNet (FID 3.85). Image Source: Dhariwal, \\nPrafulla, and Alexander Nichol. “Diffusion models beat GANs on image synthesis. ” Advances in Neural \\nInformation Processing Systems 34 (2021)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='699956fb-8445-49cd-a9b8-71daa8848cae', embedding=None, metadata={'page_label': '359', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9 359\\nThe idea behind diffusion models is very simple. We take our input image 𝑥𝑥0 , and at each time step \\n(forward step), we add a Gaussian noise to it (diffusion of noise) such that after 𝑇𝑇  time steps, the \\noriginal image is no longer decipherable. And then find a model that can, starting from a noisy input, \\nperform the reverse diffusion and generate a clear image:\\nFigure 9.29: Graphical model as a Markov chain for the forward and reverse diffusion process\\nThe only problem is that while the conditional probabilities 𝑝𝑝𝑝𝑝𝑝𝑡𝑡𝑡𝑡|𝑝𝑝𝑡𝑡)  can be obtained using the \\nreparameterization trick, the reverse conditional probability 𝑞𝑞𝑞𝑞𝑞𝑡𝑡|𝑞𝑞𝑡𝑡𝑡𝑡)  is unknown. We train a neural \\nnetwork model 𝑝𝑝𝜃𝜃  to approximate these conditional probabilities. Below is the training and the sampling \\nalgorithm used by Ho et al., 2020, in their Denoising Diffusion Probabilistic Models paper:\\nAlgorithm 1 Training Algorithm 2 Sampling\\n1. repeat\\n2. x0 ~ 𝑞𝑞𝑞 x0) \\n3. 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 {1,...,𝑇𝑇}) \\n4. 𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖 \\n5. Take gradient descent step on  \\n     ∇𝜃𝜃‖ 𝝐𝝐 𝝐 𝝐𝝐 𝜃𝜃(√𝛼𝛼𝛼𝑡𝑡x0+√1𝝐𝛼𝛼𝛼𝑡𝑡𝝐𝝐𝝐 𝝐𝝐𝝐𝝐2 \\n6. until converged1. x𝑇𝑇 ~ 𝒩𝒩𝒩𝒩𝒩 𝒩𝒩𝒩  \\n2. for  t = T, ..., 1 do\\n3. 𝑧𝑧𝑧𝑧𝑧𝑧𝑧(0,𝐈𝐈)𝑧if𝑧𝑡𝑡 𝑡 𝑡,𝑡𝑡𝑡𝑡𝑧𝑡𝑡 𝑡 𝑡𝑡  \\n4. x𝑡𝑡𝑡𝑡=𝑡\\n√𝛼𝛼𝑡𝑡(x𝑡𝑡−𝑡𝑡𝛼𝛼𝑡𝑡\\n√𝑡𝑡𝛼𝛼𝑡𝑡𝜖𝜖𝜃𝜃(x𝑡𝑡,𝑡𝑡))+𝜎𝜎𝑡𝑡𝐳𝐳 \\n5. end for\\n6. return x 0 \\nTable 9.1: Training and sampling steps used by Ho et al., 2020\\nDiffusion models offer both tractability and flexibility – two conflicting objectives in generative models. \\nHowever, they rely on a long Markov chain of diffusion steps and thus are computationally expensive. \\nThere is a lot of traction in diffusion models, and we hope that in the near future there will be algorithms \\nthat can give as fast sampling as GANs.\\nSummary\\nThis chapter explored one of the most exciting deep neural networks of our times: GANs. Unlike \\ndiscriminative networks, GANs have the ability to generate images based on the probability distribution \\nof the input space. We started with the first GAN model proposed by Ian Goodfellow and used it to \\ngenerate handwritten digits. We next moved to DCGANs where convolutional neural networks were \\nused to generate images and we saw the remarkable pictures of celebrities, bedrooms, and even album \\nartwork generated by DCGANs. Finally, the chapter delved into some awesome GAN architectures: \\nthe SRGAN, CycleGAN, InfoGAN, and StyleGAN. The chapter also included an implementation of the \\nCycleGAN in TensorFlow 2.0.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bd19ae6-0c35-42b5-ad61-828a03a84539', embedding=None, metadata={'page_label': '360', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Models 360\\nIn this chapter and the ones before it, we have been continuing with different unsupervised learning \\nmodels, with both autoencoders and GANs examples of self-supervised learning; the next chapter will \\nfurther detail the difference between self-supervised, joint, and contrastive learning.\\nReferences\\n1. Goodfellow, Ian J. (2014). On Distinguishability Criteria for Estimating Generative Models. arXiv \\npreprint arXiv:1412.6515: https://arxiv.org/pdf/1412.6515.pdf\\n2. Dumoulin, Vincent, and Visin, Francesco. (2016). A guide to convolution arithmetic for deep \\nlearning. arXiv preprint arXiv:1603.07285: https://arxiv.org/abs/1603.07285\\n3. Salimans, Tim, et al. (2016). Improved Techniques for Training GANs. Advances in neural \\ninformation processing systems: http://papers.nips.cc/paper/6125-improved-techniques-\\nfor-training-gans.pdf\\n4. Johnson, Justin, Alahi, Alexandre, and Fei-Fei, Li. (2016). Perceptual Losses for Real-Time Style \\nTransfer and Super-Resolution . European conference on computer vision. Springer, Cham: \\nhttps://arxiv.org/abs/1603.08155\\n5. Radford, Alec, Metz, Luke., and Chintala, Soumith. (2015). Unsupervised Representation Learning \\nwith Deep Convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434: \\nhttps://arxiv.org/abs/1511.06434\\n6. Ledig, Christian, et al. (2017). Photo-Realistic Single Image Super-Resolution Using a Generative \\nAdversarial Network. Proceedings of the IEEE conference on computer vision and pattern \\nrecognition: http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-\\nRealistic_Single_Image_CVPR_2017_paper.pdf\\n7. Zhu, Jun-Yan, et al. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial \\nNetworks. Proceedings of the IEEE international conference on computer vision: http://\\nopenaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_\\nTranslation_ICCV_2017_paper.pdf\\n8. Karras, Tero, Laine, Samuli, and Aila, Timo. (2019). A style-based generator architecture for \\ngenerative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision \\nand pattern recognition, pp. 4401-4410.\\n9. Chen, Xi, et al. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing \\nGenerative Adversarial Nets. Advances in neural information processing systems: https://\\narxiv.org/abs/1606.03657\\n10. TensorFlow implementation of the Sty leGAN: https://github.com/NVlabs/stylegan\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e67a3896-c488-429d-a1f9-d4a337b855fc', embedding=None, metadata={'page_label': '361', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10\\nSelf-Supervised Learning\\nImagine that you are in the middle of the ocean, and you are thirsty. There is water all around you, \\nbut you cannot drink any of it. But what if you had the resources to boil the salt out of the water and \\nthereby make it drinkable? Of course, the energy costs associated with the process can be quite high, \\nso you will likely use the process in moderation. However, if your energy costs effectively became \\nfree, for example, if you were harnessing the power of the sun, the process might be more attractive \\nfor you to do on a larger scale.\\nIn our somewhat simplistic situation described above, the first scenario is roughly analogous to \\nsupervised learning, and the second to the class of unsupervised / semi-supervised learning techniques \\nwe will cover in this chapter. The biggest problem with supervised learning techniques is the time \\nand expense associated with the collection of labeled training data. As a result, labeled datasets are \\noften relatively small.\\nDeep learning trades off computation against manual feature engineering, and while this can be very \\neffective, deep learning models typically need more data to train than traditional (non-deep learning) \\nmodels. Deep learning models tend to be more complex and have more learnable parameters, which \\nresults in them performing better at various tasks. However, more complex models also require more \\ndata to train. Because the creation of training data is expensive, this effectively limits us from scaling \\nup Deep learning models using supervised learning.\\nUnfortunately, completely unsupervised learning techniques that do not need labeled data have had \\nlimited success so far. Self-supervised techniques that leverage the structure of data in the wild to \\ncreate labeled data to feed supervised learning models offer a middle ground. In this chapter, we will \\nlearn about various self-supervised techniques and some of their applications in the areas of natural \\nlanguage processing, computer vision, and audio signal processing.\\nThe chapter covers the following topics:\\n• Previous work\\n• Self-supervised learning\\n• Self-prediction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='697731a3-be46-4b1c-b7a8-eedb551d9272', embedding=None, metadata={'page_label': '362', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 362\\n• Contrastive learning\\n• Pretext tasks\\nSelf-supervised learning is the process of imaginatively reusing labels that already exist implicitly in \\nyour data. In this chapter, we will learn about some common strategies for self-supervised learning \\nand examples of their use to solve real-life problems. Let’s begin.\\nPrevious work\\nSelf-supervised learning is not a new concept. However, the term became popular with the advent \\nof transformer-based models such as BERT and GPT-2, which were trained in a semi-supervised \\nmanner on large quantities of unlabeled text. In the past, self-supervised learning was often labeled \\nas unsupervised learning. However, there were many earlier models that attempted to leverage \\nregularities in the input data to produce results comparable to that using supervised learning. You \\nhave encountered some of them in previous chapters already, but we will briefly cover them again \\nin this section.\\nThe Restricted Boltzmann Machine (RBM ) is a generative neural model that can learn a probability \\ndistribution over its inputs. It was invented in 1986 and subsequently improved in the mid-2000s. It \\ncan be trained in either supervised or unsupervised mode and can be applied to many downstream \\ntasks, such as dimensionality reduction, classification, etc.\\nAutoencoders ( AEs) are unsupervised learning models that attempt to learn an efficient latent \\nrepresentation of input data by learning to reconstruct its input. The latent representation can be used \\nto encode the input for downstream tasks. There are several variants of the model. Sparse, denoising, \\nand contrastive AEs are effective in learning representations for downstream classification tasks, \\nwhereas variational AEs are more useful as generative models.\\nThe Word2Vec model is another great example of what we would now call self-supervised learning. \\nThe CBOW and skip-gram models used to build the latent representation of words in a corpus, attempt \\nto learn mappings of neighbors to words and words to neighbors respectively. However, the latent \\nrepresentation can now be used as word embeddings for a variety of downstream tasks. Similarly, the \\nGloVe model is also a self-supervised model, which uses word co-occurrences and matrix factorization \\nto generate word embeddings useful for downstream tasks.\\nAutoregressive (AR ) models predict future behavior based on past behavior. We cover them in this \\nchapter in the Self-prediction section. However, AR models have their roots in time series analysis  \\nin statistics, hidden Markov models in pre-neural natural language processing, Recurrent Neural \\nNetworks (RNNs ) in neural (but pre-transformer) NLP.\\nContrastive Learning (CL) models try to learn representations whereby similar pairs of items cluster \\ntogether and dissimilar pairs are pushed far apart. All the code files for this chapter can be found at https://packt.link/dltfchp10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a36f594-7def-465c-a3d3-7c76b5fd9228', embedding=None, metadata={'page_label': '363', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 363\\nCL models are also covered in this chapter in the Contrastive learning section. However, Self Organizing \\nMaps  (SOMs ) and Siamese networks use very similar ideas and may have been a precursor of current \\nCL models.\\nSelf-supervised learning\\nIn self-supervised learning, the network is trained using supervised learning, but the labels are obtained \\nin an automated manner by leveraging some property of the data and without human labeling effort. \\nUsually, this automation is achieved by leveraging how parts of the data sample interact with each \\nother and learning to predict that. In other words, the data itself provides the supervision for the \\nlearning process.\\nOne class of techniques involves leveraging co-occurrences within parts of the same data sample \\nor co-occurrences between the same data sample at different points in time. These techniques are \\ndiscussed in more detail in the Self-prediction section.\\nAnother class of techniques involves leveraging co-occurring modality for a given data sample, for \\nexample, between a piece of text and its associated audio stream, or an image and its caption. Examples \\nof this technique are discussed in the sections on joint learning.\\nYet another class of self-supervised learning techniques involves exploiting relationships between \\npairs of data samples. These pairs are selected from the dataset based on some domain-level heuristic. \\nExamples of these techniques are covered in the Contrastive learning section.\\nThese techniques can either be used to train a model to learn to solve a business task (such as sentiment \\nanalysis, classification, etc.) directly or to learn a latent (embedding) representation of the data that \\ncan then be used to generate features to learn to solve a downstream business task. The latter class \\nof tasks that are used to indirectly learn the latent representation of the data are called pretext tasks. \\nThe Pretext tasks section will cover this subject, with examples, in more detail.\\nThe advantages of self-supervised learning are twofold. First, as noted already, supervised learning \\ninvolves the manual labeling of data, which is very expensive to create, and therefore it is difficult to \\nget high-quality labeled data. Second, self-supervised tasks may not address a business task directly \\nbut can be used to learn a good representation of the data, which can then be applied to transfer this \\ninformation to actual business tasks downstream.\\nSelf-prediction\\nThe idea behind self-prediction is to predict one part of a data sample given another part. For the \\npurposes of prediction, we pretend that the part to be predicted is hidden or missing and learn to \\npredict it. Obviously, both parts are known, and the part to be predicted serves as the data label. The \\nmodel is trained in a supervised manner, using the non-hidden part as the input and the hidden part \\nas the label, learning to predict the hidden part accurately. Essentially, it is to pretend that there is a \\npart of the input that you don’t know and predict that.\\nThe idea can also be extended to reversing the pipeline, for example, deliberately adding noise to an \\nimage and using the original image as the label and the corrupted image as the input.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6877d58-a798-4d96-92d8-2cd1e294f3e3', embedding=None, metadata={'page_label': '364', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 364\\nAutoregressive generation\\nAutoregressive ( AR) models attempt to predict a future event, behavior, or property based on past events, \\nbehavior, or properties. Any data that comes with some innate sequential order can be modeled using \\nAR generation. Unlike latent variable models such as V AEs or GANs, AR models make no assumptions \\nof independence.\\nPixelRNN\\nThe PixelRNN [1] AR model uses two-dimensional Recurrent Neural Networks ( RNNs ) to model images  \\non a large scale. The idea is to learn to generate a pixel by conditioning on all pixels to the left and \\nabove it. A convolution operation is used to compute all the states along each dimension at once. The \\nLSTM layers used in PixelRNN are one of two types – the Row LSTM and the Diagonal BiLSTM. In the \\nrow LSTM, the convolution is applied along each row, and in the Diagonal BiLSTM, the convolutions \\nare applied along the diagonals of the image:\\n𝑝𝑝(𝑥𝑥)=∏𝑝𝑝(𝑥𝑥𝑖𝑖|𝑥𝑥1,...,𝑥𝑥𝑖𝑖𝑖1)𝑛𝑛2\\n𝑖𝑖𝑖1 \\nFigure 10.1: PixelRNN tries to predict a pixel by conditioning on all pixels to the left and above it. From \\nthe paper Pixel Recurrent Neural Networks [1]\\nImage GPT (IPT)\\nImage GPT (IPT) [14] is similar to PixelRNN except it works on patches, and each patch is treated as a \\nword. The Image GPT is based on the Transformer model and is trained on images from the ImageNet \\ndataset. The images are corrupted in multiple different ways (super-resolution, bicubic interpolation, \\nadding noise, etc.) and pretrained to predict the original image. The core of the IPT model consisted \\nof a transformer encoder decoder pair but had multiple heads and tails to extract features from the \\ncorrupted input image and format the decoder output into the output image respectively. The multiple \\nheads and tails were specialized for each of the different tasks IPT is trained to do (denoising, de-\\nraining, x2 and x4 super-resolution, etc.):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa74908a-0210-4f08-b000-0da222a4ba6f', embedding=None, metadata={'page_label': '365', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 365\\nFigure 10.2: Architecture of the Image GPT (IPT) AR model. From the paper Pre-trained Image \\nProcessing Transformer [14]\\nGPT-3\\nThe GPT-3, or Generative Pre-trained Transformer [9] model from OpenAI is an AR language model \\nthat can generate human-like text. It generates sequences of words, code, and other data, starting \\nfrom a human-provided prompt. The first version of GPT used 110 million learning parameters, GPT-2 \\nused 1.5 billion, and GPT-3 used 175 billion parameters. The model is trained on unlabeled text such \\nas Wikipedia that is readily available on the internet, initially in English but later in other languages as \\nwell. The GPT-3 model has a wide variety of use cases, including summarization, translation, grammar \\ncorrection, question answering, chatbots, and email composition.\\nThe popularity of GPT-3 has given rise to a new profession called prompt engineering [39], which is \\nbasically to create the most effective prompts to start GPT-3 on various tasks. A partial list of possible \\napplications for GPT-3 can be found on the OpenAI GPT-3 examples page ( https://beta.openai.\\ncom/examples/ ).\\nXLNet\\nXLNet [38] is similar to GPT-3 in that it is a generalized  AR model. However, it leverages  both AR \\nlanguage modeling and AutoEncoding while avoiding their limitations. Instead of using only tokens \\nfrom the left or right context to predict the next token, it uses all possible permutations of the tokens \\nfrom the left and right contexts, thus using tokens from both the left and right contexts for prediction. \\nSecondly, unlike AE approaches such as BERT, it does not depend on input corruption (as in masked \\nlanguage modeling) since it is a generalized AR language model. Empirically, under comparable \\nexperimental settings, XLNet consistently outperforms BERT on a wide spectrum of tasks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc2f3893-9063-4e13-a500-d26c5094178f', embedding=None, metadata={'page_label': '366', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 366\\nWaveNet\\nWaveNet [3] is an AR generative model based on PixelCNN’s architecture but operates on the raw \\naudio waveform. As with PixelCNN, an audio sample at a particular point in time is conditioned on \\nthe samples at all previous timesteps. The conditional probability distribution is modeled as a stack \\nof convolutional layers. The main ingredient of the WaveNet is causal convolutions. The predictions \\nemitted by the model at a time step cannot depend on any future timesteps. When applied to text to \\nspeech, WaveNet yields state-of-the-art performance, with human listeners rating it as significantly \\nmore natural sounding for English and Mandarin than comparable text-to-speech models.\\nWaveRNN\\nWaveRNN [28] is an AR generative model that learns the joint probability of the data by factorizing the \\ndistribution into a product of conditional probabilities over each sample. The convolutional layers of \\nthe WaveNet architecture are replaced with a single-layer RNN. It also uses more efficient sampling \\ntechniques that, overall, reduce the number of operations to perform and result in approximately 4x \\nspeedup over WaveNet.\\nMasked generation\\nMasked generation models mask some random portion of themselves and pretend it is missing, and \\nthe models learn to predict the masked information using the unmasked information available to \\nthem. Unlike autoregressive models, in the case of masked generation models, there is no need for \\nthe masked information to be located before or after the unmasked information; it can be anywhere \\nin the input.\\nBERT\\nBERT  [16], or Bidirectional Encoder Representation from Transformers, is a transformer-based \\nlanguage  model that was trained using text from the internet by a team from Google. It uses two \\nobjectives during the pretraining phase – Masked Language Modeling ( MLM ) and Next Sentence \\nPrediction ( NSP ). During training, 15% of the input tokens are masked and the model learns to predict \\nthe masked token. Since the model is transformer based, it can use context from anywhere in the \\nsentence to help with predicting the masked tokens. BERT models, once pretrained, can be fine-tuned \\nwith smaller supervised datasets for a variety of downstream tasks such as classification, sentiment \\nanalysis, textual entailment, etc. BERT is covered in more depth in Chapter 6, Transformers.\\nYou can see BERT’s masked generation in action using a pretrained BERT model from the Hugging Face \\nTransformers library and the code snippet shown below. Here, we ask a pretrained BERT transformer \\nmodel to predict the masked token [MASK]  in the sentence \"The capital of France is [MASK].\" :\\nfrom transformers import BertTokenizer, TFBertForMaskedLM\\nimport tensorflow as tf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1bc0d6ac-6fd0-4994-ad43-8c04abafd300', embedding=None, metadata={'page_label': '367', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 367\\ntokenizer = BertTokenizer.from_pretrained( \"bert-base-cased\" )\\nmodel = TFBertForMaskedLM.from_pretrained( \"bert-base-cased\" )\\ninputs = tokenizer( \"The capital of France is [MASK].\" , return_tensors= \"tf\")\\nlogits = model(**inputs).logits\\nmask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[ 0][1]\\npredicted_token_id = tf.math.argmax(logits[:, mask_token_index], axis=- 1)\\nprint(tokenizer.convert_ids_to_tokens(predicted_token_id)[ 0])\\nSomewhat predictably, the output of this code block is \"Paris\" .\\nStacked denoising autoencoder\\nA stacked denoising autoencoder (AE) [29] adds random noise to images and uses them as input to \\na denoising AE to predict the original image. Multiple layers of denoising AEs are each individually \\ntrained and stacked. This results in the composition of several levels of non-linearity and is key \\nto achieving better generalization performance on difficult image recognition tasks. Higher-level \\nrepresentations learned in this purely unsupervised manner can be used as image features to boost \\nthe performance of downstream SVM based image classifiers. Each layer functions like a regular AE, \\ni.e., it takes an image as input and tries to reconstruct it after it passes through a “bottleneck” layer. \\nThe bottleneck layer learns a compact feature representation of the input image. Unfortunately, AEs \\nusually end up only learning how to compress the image without learning a semantically meaningful \\nrepresentation. Denoising AEs address this issue by corrupting the input and requiring the network \\nto undo the corruption and hence learn a better semantic representation of the input image.\\nContext autoencoder\\nThe context autoencoder [12] masks out a region of the image and uses it to train a convolutional neural \\nnetwork (the context AE) to regress the missing pixel values to predict the original image. The task of \\na context AE is even harder than that of a denoising AE since it has to fill in larger missing areas and \\ncannot use information from immediately neighboring pixels. This requires a much deeper semantic \\nunderstanding of the image, and the ability to generate high-level features over large spatial areas. In \\na sense, the context AE is a more powerful generative model since it needs to fill in the missing region \\nwhile maintaining coherence with the supplied context. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d84fd2a3-0317-4d06-959e-6d5c5444f548', embedding=None, metadata={'page_label': '368', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 368\\nFor that reason, the context AE is trained to reconstruct a combination of reconstruction loss and \\nadversarial loss. This results in sharper predictions than training on reconstruction (L2) loss alone:\\nFigure 10.3: Qualitative illustration of the context encoder task (from Context Encoders: Feature \\nLearning by Inpainting [10]) \\nContext does not have to be image features, it could also be color, as we will see in the next section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8683d316-f1ea-49f5-96e9-b79d7d6f4394', embedding=None, metadata={'page_label': '369', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 369\\nColorization\\nThe paper Colorization as a Proxy Task for Visual Understanding [12] uses colorization as a way to learn \\nimage  representations. Color images are converted to their grayscale equivalent, which is then used \\nas input to predict the original color image. The model can be used to automatically colorize grayscale \\nimages, as well as learn a representation that can help in downstream tasks such as image classification \\nand segmentation. In functional terms, the model predicts the a and b (color information) channels \\nin their Lab encoding given their L (grayscale) channel. Experiments on the ImageNet dataset by the \\nauthors of this paper have resulted in models that produce state-of-the-art results against datasets for \\nsemantic segmentation and image classification for models that don’t use ImageNet labels, and even \\nsurpass some earlier models that have been trained on ImageNet using supervised learning.\\nInnate relationship prediction\\nModels using this technique attempt to learn visual common-sense tasks by leveraging innate \\nrelationships between parts of an input image. Weights from these learned models could be used to \\ngenerate semantic representations of images for other downstream tasks.\\nRelative position\\nThe paper Unsupervised Visual Representation Learning by Context Prediction [8] predicts the relative \\nposition of one patch in an image with respect to another. Effectively, this approach uses spatial \\ncontext as a source of self-supervision for training visual representations. Given a large unlabeled \\nimage collection, random pairs of patches are extracted from each image as shown in Figure 10.4. \\nEach pair is labeled depending on the orientation of the second patch with respect to the central one. \\nA convolutional network is trained to predict the position of the second patch relative to the first. The \\nfeature representation learned is found to capture the notion of visual similarity across images. Using \\nthis representation, it has been shown to aid in visual data mining, i.e., discovering image fragments \\nthat depict the same semantic object, against the Pascal VOC 2007 dataset:\\nFigure 10.4: Illustration of relative position prediction. The model must predict the configuration of the \\nsecond patch relative to the (central) first patch. From the paper Unsupervised Visual Representation \\nLearning by Context Prediction [8]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='758384c6-a352-424b-a5a0-c6a8d25824e6', embedding=None, metadata={'page_label': '370', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 370\\nSolving jigsaw puzzles\\nThe paper Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles [26] describes an \\napproach somewhat similar to the previous approach of predicting relative position. This method \\nattempts to learn the visual representation of images by solving jigsaw puzzles of natural images. \\nPatches are extracted from the input image and shuffled to form a jigsaw puzzle. The network learns \\nto reconstruct the original image from the jigsaw puzzle, i.e., to solve the jigsaw puzzle. The network \\nused is a Context Free Network ( CFN ), an n-way Siamese network. Each patch corresponds to a column \\nin the n-way CFN. The shared layers in each column are implemented exactly as in AlexNet. The \\nclassification head predicts the original index of the patch (before shuffling). On the Pascal VOC dataset, \\nit outperforms all previous self-supervised models in image classification and object detection tasks:\\nFigure 10.5: The image is split up into patches and shuffled, and the model learns to put the shuffled \\npatches back in the correct order. From the paper Unsupervised Learning of Visual Representations \\n[26]\\nRotation\\nThe RotNet model [34] learns an image representation by using rotation as a self-supervision signal. \\nInput images are rotated by 0, 90, 180, and 270 degrees, and a convolutional network (RotNet) is \\ntrained to learn to predict the rotation angle as one of 4 target classes. It turns out that this apparently \\nsimple task provides a very powerful supervisory signal for semantic feature learning. RotNet features \\nwere used as input for image classification against the CIFAR-10 dataset and resulted in classification \\naccuracy of only 1.6% less than the state-of-the-art result obtained using supervised learning. It also \\nobtained state-of-the-art results at the time for some classification tasks against ImageNet, and some \\nclassification and object detection tasks against Pascal VOC.\\nHybrid self-prediction\\nWith hybrid self-prediction models, self-prediction is achieved using not one but multiple self-prediction \\nstrategies. For example, our first two examples, Jukebox and DALL-E, achieve self-prediction by first \\nreducing the input data to a more manageable format using one self-supervision technique (VQ-V AE \\nor Vector Quantized Variational AutoEncoder [35]) and then use another (AR) on the reduced image \\nto produce the final prediction. In our third example, the predictions from the VQ-V AE component \\nare further refined using a discriminator trained in an adversarial manner.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95aac391-d8cb-4a1e-a7b4-642daa7136a3', embedding=None, metadata={'page_label': '371', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 371\\nVQ-VAE\\nSince the VQ-V AE is common to all our hybrid self-prediction models, let us try to understand what it \\ndoes at a high level. You have already read about autoencoders and variational autoencoders in Chapter \\n8, Autoencoders. Autoencoders try to learn to reconstruct their input by first encoding the input onto a \\nsmaller dimension and then decoding the output of the smaller dimension. However, autoencoders \\ntypically just end up compressing the input and do not learn a good semantic representation. \\nVariational Autoencoders ( VA E s ) can do better in this respect by enforcing a probabilistic prior, \\ngenerally in the form of a standard Gaussian distribution, and by minimizing not only the reconstruction \\nloss but also the KL divergence between the prior distribution and posterior distribution (the actual \\ndistribution in the latent space).\\nWhile the V AE learns a continuous latent distribution, the VQ-V AE learns a discrete latent distribution. \\nThis is useful because transformers are designed to take discrete data as input. VQ-V AE extends V AE \\nby adding a discrete codebook component to the network, which is used to quantize the latent vectors \\noutput by the encoder by choosing the vector in the codebook that is closest to each latent vector \\nby Euclidean distance. The VQ-V AE decoder is then tasked with reconstructing the input from the \\ndiscretized latent vector.\\nJukebox\\nOur first example is the Jukebox paper [32], which is a generative model for music, similar to how \\nGPT-3 is a generative model for text and Image-GPT is a generative model for images. That is, given a \\nmusical (voice and music) prompt, Jukebox can create the music that might follow this prompt. Early \\nattempts at generative models for audio attempted symbolic music generation in the form of a piano \\nroll, since the problem with generating raw audio directly is the extremely large amount of information \\nit contains and consequently, the extreme long-range dependencies that need to be modeled. The \\nVQ-V AE addresses this problem by learning a lower-dimensional encoding of the audio with the goal \\nof losing the least important information but retaining most of the useful information. \\nJukebox uses hierarchical VQ-V AEs to discretize the input signal into different temporal resolutions, \\nthen generates a new sequence at each resolution, and finally combines the generated sequence at \\neach level into the final prediction.\\nDALL-E\\nOur second example of hybrid prediction models is the DALL-E model [5] from OpenAI. DALL-E can \\nalso be classified as a joint learning (multimodal) model, since it attempts to learn to create images \\nfrom text captions, using pairs of text and image as training input. However, we classify it here as a \\nhybrid prediction model because, like Jukebox, it attempts to address the high dimensionality of image \\ninformation (compared with the dimensionality of the associated text) using a VQ-V AE.\\nDALL-E receives text and images as a single stream of data. DALL-E uses a two-stage training regime. In \\nthe first stage, a VQ-V AE is trained to compress each input RGB image of size (256, 256, 3) into a grid of \\nimage tokens of size (32, 32), each element of which can assume one of 8,192 possible discrete values. \\nThis reduces the size of the image input by a factor of 192 without a corresponding loss in image quality. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='675bc25c-64f9-4584-82ee-f48274f8dc70', embedding=None, metadata={'page_label': '372', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 372\\nIn the second stage, the text is BPE-encoded and truncated to 256 tokens. Byte Pair Encoding (BPE ) is \\na hybrid character/word encoding that can represent large corpora using a relatively small vocabulary \\nby encoding common byte pairs. This encoding is then concatenated with the flattened sequence of \\n1,024 (32 x 32) image tokens. This combined sequence is used to train an autoregressive transformer to \\nmodel the joint distribution over the text and image tokens. The first stage learns the visual codebook \\nin the VQ-V AE and the second stage learns the prior of the discrete latent distribution over the text and \\nimage tokens. The trained DALL-E model can then be used to generate images given a text prompt.\\nText-to-image generation is getting quite popular. A newer version of DALL-E, called DALL-E 2, was \\nrecently released by OpenAI. It has 35 billion parameters compared to DALL-E’s 12 billion. Even \\nthough they are named similarly, DALL-E is a version of GPT-3 trained to generate images from text \\ndescriptions, and DALL-E 2 is an encoder-decoder pipeline that uses CLIP to encode the text description \\ninto a CLIP embedding, and then decode the embedding back to an image using a diffusion model \\nthat you learned about in Chapter 9, Generative Models. As expected, DALL-E 2 generates more realistic \\nand accurate images than DALL-E.\\nEven more recently, Google Research has released Imagen, another model in this space that competes \\nwith DALL-E 2. Like DALL-E 2, Imagen uses a T5-XXL encoder to map input text into embeddings and \\na diffusion model to decode the embedding into an image.\\nVQ-GAN\\nThe VQ-GAN [30] uses an encoder-decoder framework where the encoder uses a VQ-V AE style  \\nencoder that learns a discrete latent representation, but the decoder is a discriminator component \\nof a Generative Adversarial Network (GAN ). Instead of the L2 loss used in the VQ-V AE, the VQ-GAN \\nuses a combination of perceptual loss and discriminator loss, which helps in keeping good perceptual \\nquality at increased compression rates. The use of a GAN architecture rather than a traditional V AE \\ndecoder helps with training efficiency.\\nLike VQ-V AE, the VQ-GAN learns a codebook of context-rich visual components, which are used \\nto compose sequences for training the autoregressive component. The VQ-GAN has been found to \\noutperform the VQ-V AE-2 model on images from ImageNet using the Fréchet Inception Distance ( FID), \\nwhich measures the distance between feature vectors of real versus fake images) metric, even though \\nit uses approximately 10x fewer parameters:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbd33f78-f254-48c1-836c-4b859a5c3eba', embedding=None, metadata={'page_label': '373', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 373\\nFigure 10.6: Architecture of the VQ-GAN. From the paper: Taming Transformers for High Resolution \\nImage Synthesis [30]\\nNext, we will look at another popular self-supervised technique called contrastive learning.\\nContrastive learning\\nContrastive Learning (CL) tries to predict the relationship between a pair of input samples. The goal \\nof CL is to learn an embedding space where pairs of similar samples are pulled close together and \\ndissimilar samples are pushed far apart. Inputs to train CL models are in the form of pairs of data \\npoints. CL can be used in both supervised and unsupervised settings.\\nWhen used in an unsupervised setting, it can be a very powerful self-supervised learning approach. \\nSimilar pairs are found from existing data in a self-supervised manner, and dissimilar pairs are found \\nfrom pairs of similar pairs of data. The model learns to predict if a pair of data points are similar or \\ndifferent.\\nA taxonomy of CL can be derived by considering the techniques used to generate contrastive examples. \\nBefore we do that, we will take a brief detour to explore the various training objectives that are popular \\nin CL.\\nTraining objectives\\nEarly CL models used data points consisting of a single positive and a single negative example to learn \\nfrom. However, the trend in more recent CL models is to learn from multiple positive and negative \\nsamples in a single batch. In this section, we will cover some training objectives (also called loss \\nfunctions) that are commonly used for training CL models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27d0855b-336c-45af-8e56-924fa6d747bb', embedding=None, metadata={'page_label': '374', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 374\\nContrastive loss\\nContrastive loss [35] is one of the earliest training  objectives to be used for learning  using CL techniques. \\nIt tries to encode data into an embedding space such that examples from the same class have similar \\nembeddings and examples from different classes have dissimilar embeddings. Thus, given two data \\npairs, (x i, yi) and (x j, yj), the contrastive loss objective is described using the following formula:\\nℒ(𝑥𝑥𝑖𝑖,𝑥𝑥𝑗𝑗,𝜃𝜃𝜃 𝜃 𝜃𝜃𝜃𝜃𝜃 𝑖𝑖𝜃𝜃𝜃𝑗𝑗]‖𝑓𝑓𝜃𝜃(𝑥𝑥𝑖𝑖)−𝑓𝑓𝜃𝜃(𝑥𝑥𝑗𝑗)‖22+𝜃𝜃𝜃𝜃𝜃𝑖𝑖≠𝜃𝜃𝑗𝑗]𝑚𝑚𝑚𝑚𝑥𝑥𝑚𝑚,𝑚𝑚−‖𝑓𝑓𝜃𝜃(𝑥𝑥𝑖𝑖)−𝑓𝑓𝜃𝜃(𝑥𝑥𝑗𝑗)‖2)2\\n \\nThe first term is activated when the pairs i and j are similar, and the second term is activated when \\nthe pair is dissimilar. The objective is designed to maximize the square of the differences in the first \\nterm and minimize the square of differences in the second term (thus maximizing the second term \\nin the case of dissimilar pairs). The 𝜀𝜀  is a hyperparameter and represents a margin of the minimum \\nallowable distance between samples of different classes.\\nTriplet loss\\nTriplet loss [11] is an enhancement of contrastive loss in that it uses three data points instead of two – \\nthe anchor point, the positive point, and the negative point. Thus, given an anchor point x, we select \\na positive sample x+  and one negative sample x− , where x and x+  belong to the same class and x and \\nx−  belong to different classes. Triplet loss learns to minimize the distance between the anchor x and \\npositive sample x+  and maximize the distance between x and negative sample x− . This is illustrated \\nin Figure 10.7:\\nFigure 10.7: Illustration of triplet loss. Based on the paper: FaceNet: A Unified Embedding for Face \\nRecognition and Clustering [11]\\nThe equation for triplet loss is shown below. As with contrastive loss, the 𝜀𝜀  is a hyperparameter  \\nrepresenting the minimum allowed difference between distances between similar and dissimilar \\npairs. Triplet loss-based models typically need challenging values for x− , the so-called hard negatives, \\nto provide good representations: \\nℒ(𝑥𝑥𝑥𝑥𝑥+𝑥𝑥𝑥−)= ∑𝑚𝑚𝑚𝑚𝑥𝑥 (0𝑥‖𝑓𝑓(𝑥𝑥)−𝑓𝑓(𝑥𝑥+)‖22−‖𝑓𝑓(𝑥𝑥)−𝑓𝑓(𝑥𝑥−)‖22+𝜖𝜖)\\n𝑥𝑥𝑥𝑥𝑥 \\nN-pair loss\\nN-pair loss [21] generalizes triplet loss to incorporate comparison with multiple negative samples  \\ninstead of just one. Thus, given an (N+1) tuple of training samples, {x, x+, x1-, x2-, …, x N+1-}, where there \\nis one positive sample and N-1 negative ones, the N-pair loss is defined using the following equation:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a5d1973-4c98-4fa3-855b-570c93566cde', embedding=None, metadata={'page_label': '375', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 375\\nℒ(𝑥𝑥𝑥𝑥𝑥+𝑥{𝑥𝑥𝑖𝑖−}𝑖𝑖𝑖𝑖𝑁𝑁−𝑖)=−log(𝑒𝑒𝑥𝑥𝑒𝑒(𝑓𝑓(𝑥𝑥)𝑇𝑇𝑓𝑓(𝑥𝑥+))\\n𝑒𝑒𝑥𝑥𝑒𝑒(𝑓𝑓(𝑥𝑥)𝑇𝑇𝑓𝑓(𝑥𝑥+))+∑𝑒𝑒𝑥𝑥𝑒𝑒(𝑓𝑓(𝑥𝑥𝑇𝑇)𝑓𝑓(𝑥𝑥𝑖𝑖−))𝑁𝑁−𝑖\\n𝑖𝑖𝑖𝑖) \\nLifted structural loss\\nLifted structured loss [15] is another generalization of triplet loss where it uses all pairwise edges \\nwithin a training batch. This leads to better training performance. Figure 10.8 illustrates the idea \\nbehind lifted structural loss, and how it evolved from contrastive and triplet loss. Red edges connect \\nsimilar pairs and blue edges connect dissimilar pairs:\\nFigure 10.8: Illustration of the idea of Lifted Structured Loss. Based on the paper: Deep Metric Learning \\nvia Lifted Structured Feature Embedding [15]\\nNCE loss\\nNoise Contrastive Estimation ( NCE ) loss [27] uses logistic regression to distinguish positive and negative \\n(noise) examples. The NCE loss attempts to maximize the log odds (logits) of positive examples x and \\nminimize the log odds of negative examples 𝑥𝑥  . The equation for NCE loss is shown below:\\nℒ=−1\\n𝑁𝑁∑[log(𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (𝑙𝑙𝑠𝑠𝑠𝑠𝑠𝑠𝑙𝑙(𝑙𝑙𝑖𝑖)))+log(1−𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (𝑙𝑙𝑠𝑠𝑠𝑠𝑠𝑠𝑙𝑙(𝑙𝑙𝑖𝑖̅)))]𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nInfoNCE loss\\nInfoNCE loss [2] was inspired by NCE loss (described in the previous section) and uses categorical cross-\\nentropy loss to identify the positive sample from the set of unrelated noise samples. Given some context \\nvector c, the positive sample should be drawn from the conditional probability distribution p(x|c), while \\nthe N-1 negative examples can be drawn from the distribution p(x) independent of the context c . The \\nInfoNCE loss optimizes the negative log probability of classifying the positive sample correctly. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3ef0c3c-be07-4b1f-9146-21cca5b0cc4d', embedding=None, metadata={'page_label': '376', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 376\\nThe InfoNCE loss is given by the following equation, where f(x, c) estimates the density ratio p(x|c) / p(x):\\nℒ = −𝐸𝐸𝐸𝐸𝐸𝐸𝑓𝑓(𝑥𝑥𝑥𝑥𝑥)\\n∑𝑓𝑓(𝑥𝑥′𝑥𝑥𝑥)𝑥𝑥′∈𝑋𝑋] \\nSoft nearest neighbors loss\\nSoft nearest neighbors loss [33] further extends the idea of contrastive loss to include multiple positive \\nsamples given known labels. Given a batch of samples, {(𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖)}𝑖𝑖𝑖𝑖𝐵𝐵  where y i is the class label of xi, and \\na similarity function f that measures similarity between two inputs, the soft nearest neighbor loss is \\ngiven by the equation:\\nℒ=−1\\n𝐵𝐵∑log(∑ 𝑒𝑒𝑒𝑒𝑒𝑒𝑒−𝑒𝑒𝑒𝑒𝑒𝑖𝑖,𝑒𝑒𝑗𝑗)𝜏𝜏⁄) 𝑖𝑖𝑖𝑗𝑗,𝑖𝑖𝑖𝑖=𝑖𝑖𝑗𝑗,𝑗𝑗=𝑗,𝑗,𝑗𝑗\\n∑ 𝑒𝑒𝑒𝑒𝑒𝑒(𝑒𝑒(𝑒𝑒𝑖𝑖,𝑒𝑒𝑘𝑘)𝜏𝜏⁄)𝑖𝑖𝑖𝑘𝑘,𝑘𝑘=𝑗,𝑗,𝑗𝑗)𝑗𝑗\\n𝑖𝑖=𝑗 \\nThe temperature τ  is a hyperparameter and is used for tuning how concentrated the features are \\nin the representation space. Thus, at low temperatures, the contribution of faraway points in the \\nrepresentation space to the soft nearest neighbors loss is also low.\\nInstance transformation\\nCL models that use instance transformation generally rely on data augmentation techniques to generate \\npositive pairs and negative mining to generate negative pairs from pairs of positive pairs. Many such \\nmodels rely on generating in-batch negative and innovative techniques for mining hard negatives.\\nData augmentation techniques are used to create pairs of the original data point and its noisy version. \\nThis introduces non-essential variation into the examples without modifying semantic meaning, which \\nthe model then learns during training.\\nIn-batch negative sampling is a technique for generating negative samples by combining information \\nfrom examples within a single batch. For each positive pair (x i, yi) in the batch, all pairs (x i, yj) and (x j, \\nyi) for all 𝑖𝑖𝑖𝑖𝑖   can be considered as negative pairs. In effect, negative pairs are created by combining \\nelements from two random positive pairs in the same batch. This technique is practical and can be \\nimplemented efficiently on GPUs and is therefore widely used.\\nSome models require hard negative samples to learn how to perform their tasks well. Hard negatives \\nare pairs that have different labels, but whose embedding features are very close to each other. You can \\nvisualize them as points that lie very close to each other in the embedding space but on opposite sides \\nof the decision boundary. Identifying hard negatives for a given task is relatively easy for supervised \\nlearning. For unsupervised learning, one approach is to increase the batch size, which will introduce \\nmore hard negative samples. Another technique [19] is to increase the sampling probability of the \\ncandidate negative sample by its similarity with the anchor sample.\\nSimCLR\\nThe SimCLR model [36] presents a simple framework for contrastive learning of visual representations. \\nEach input image (x) is augmented in two different ways (x i and x j) using the same family of image \\naugmentation strategies, resulting in 2N positive samples. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='713a1ed8-69fd-4603-a4ce-ece6089173fe', embedding=None, metadata={'page_label': '377', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 377\\nIn-batch negative sampling is used, so for each positive example, we have (2N-1) negative samples. \\nA base encoder (f ) is applied to the pair of data points in each example, and a projection head (g ) \\nattempts to maximize the agreement for positive pairs and minimize it for negative pairs. For good \\nperformance, SimCLR needs to use large batch sizes so as to incorporate enough negative examples in \\nthe training regime. SimCLR achieved state-of-the-art results for self-supervised and semi-supervised \\nmodels on ImageNet and matches the performance of a supervised ResNet-50. Figure 10.9 shows the \\narchitecture of the SimCLR model:\\nFigure 10.9: Architecture of the SimCLR model. From the paper: A Simple Framework for Contrastive \\nLearning of Visual Representations [36]\\nBarlow Twins\\nThe idea behind the Barlow Twins [20] model has its roots in neuroscience, i.e., the goal of sensory \\nprocessing is to re-code highly redundant sensory inputs into a factorial code, or a code with statistically \\nindependent components. In this model, an image is distorted into two versions of itself. The distorted \\nversions are fed into the same network to extract features and learn to make the cross-correlation matrix \\nbetween these two features as close to the identity matrix as possible. In line with the neuroscience \\nidea, the goal of this model is to reduce the redundancy between the two distorted versions of the \\nsample by reducing the redundancy between these vectors. This is reflected in its somewhat unique \\nloss function – in the first equation, the first term represents the difference between the identity \\nmatrix and the cross-correlation matrix, and the second term represents the redundancy reduction \\nterm. The second equation defines each element of the cross-correlation matrix C:\\nℒ=∑(1−𝐶𝐶𝑖𝑖𝑖𝑖)2+𝜆𝜆∑∑𝐶𝐶𝑖𝑖𝑖𝑖2\\n𝑖𝑖𝑗𝑖𝑖𝑖𝑖𝑖𝑖 \\n𝐶𝐶𝑖𝑖𝑖𝑖=∑𝑧𝑧𝑏𝑏𝑏𝑖𝑖𝐴𝐴𝑧𝑧𝑏𝑏𝑏𝑖𝑖𝐵𝐵\\n𝑏𝑏\\n√∑(𝑧𝑧𝑏𝑏𝑏𝑖𝑖𝐴𝐴)2\\n𝑏𝑏√∑(𝑧𝑧𝑏𝑏𝑏𝑖𝑖𝐵𝐵)2\\n𝑏𝑏 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='657c5abc-774d-4365-9d7e-0365d9362aa2', embedding=None, metadata={'page_label': '378', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 378\\nSome notable differences between the Barlow Twins model and other models in this genre are that \\nthe Barlow Twins model doesn’t require a large number of negative samples and can thus operate on \\nsmaller batches, and that it benefits from high-dimensional embeddings. The Barlow Twins model \\noutperforms some previous semi-supervised models trained on ImageNet and is on par with some \\nsupervised ImageNet models.\\nBYOL\\nThe Bootstrap Your Own Latent (BYOL ) model [17] is unique in that it does not use negative samples \\nat all. It relies on two neural networks, the online and target networks, that interact and learn from \\neach other. The goal of BYOL is to learn a representation 𝑦𝑦𝜃𝜃 that can be used for downstream tasks. \\nThe online network is parameterized by a set of weights 𝜃𝜃  and comprises three stages – an encoder \\n𝑓𝑓𝜃𝜃 , a projector 𝑔𝑔𝜃𝜃 , and a predictor 𝑞𝑞𝜃𝜃  The target network has the same architecture as the online network \\nbut uses a different set of weights 𝜉𝜉 . The target network provides the regression targets to train the \\nonline network, and its parameters 𝜉𝜉  are an exponential moving average of the online parameters 𝜃𝜃  \\nAfter every training step, the following update is performed:\\n𝜉𝜉𝜉𝜏𝜏𝜉𝜉+(1−𝜏𝜏)𝜃𝜃 \\nBYOL produces two augmented views of each image. From the first augmented view, the online network \\noutputs a representation 𝑦𝑦𝜃𝜃  and a projection 𝑧𝑧𝜃𝜃 . Similarly, the target network outputs a representation \\n𝑦𝑦𝜉𝜉  and a projection 𝑧𝑧𝜉𝜉  BYOL attempts to minimize the error between the L2 normalized online and \\ntarget projections 𝑧𝑧𝜃𝜃  and 𝑧𝑧𝜉𝜉 . At the end of the training, we only retain the online network (the encoder).\\nBYOL achieves competitive results against semi-supervised or transfer learning models on ImageNet. \\nIt is also less sensitive to changes in batch size and the type of image augmentations used compared to \\nother models in this genre. However, later work [4] indicates that the batch normalization component \\nin BYOL may implicitly cause a form of contrastive learning by implicitly creating negative samples \\nas a result of data redistribution it causes.\\nFeature clustering\\nFeature clustering involves finding similar data samples by clustering them. This can be useful when \\ndata augmentation techniques are not feasible. The idea here is to use clustering algorithms to assign \\npseudo-labels to samples such that we can run intra-sample CL. Although similar, feature clustering \\ndiffers from CL in that it relaxes the instance discrimination problem – rather than learn to distinguish \\nbetween a pair of transformations on a single input image, feature clustering learns to discriminate \\nbetween groups of images with similar features.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29ad39c6-4379-4fee-9324-517675e79c80', embedding=None, metadata={'page_label': '379', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 379\\nDeepCluster\\nThe DeepCluster [24] paper is predicated on the fact that datasets for supervised learning such as \\nImageNet are “too small” to account for general-purpose features that go beyond image classification. \\nFor learning general-purpose features, it is necessary to train on billions of images at internet scales. \\nHowever, labeling such large datasets is not feasible, so DeepCluster presents a clustering method \\nthat jointly learns the parameters of the neural network and the cluster assignments of the resulting \\nfeatures. DeepCluster iteratively groups these features using the K-Means clustering algorithm and \\nuses the cluster assignments as pseudo labels to learn the parameters of the ConvNet. The end product \\nof the training is the weights of the ConvNet. These weights have been shown to be useful general-\\npurpose visual features and have outperformed the best published numbers on many downstream \\ntasks regardless of the dataset.\\nSwAV\\nIn the SwAV  (SWapping Assignments between multiple Views) [25] model, features are learned by \\npredicting the cluster assignment (pseudo-label) for a view from the representation of another view. \\nSwA V uses a variant of the architecture used in CL models. The images x 1 and x 2 are transformations \\nof the same input image x, which are sent through an encoder 𝑓𝑓𝜃𝜃  to produce a representation z 1 and \\nz2. In the case of SwA V , z 1 and z 2 are used to compute q 1 and q 2 by matching their features to a set of \\nK prototype vectors {c 1, …, c K}, which are then used to predict the cluster assignment for x 2 and x 1 \\nrespectively.\\nUnlike DeepCluster, SwA V does online clustering (clustering of data that arrives continuously in a \\nstreaming manner and is not known before the clustering process begins) and can therefore scale to \\npotentially unlimited amounts of data. SwA V also works well with both large and small batch sizes. The \\nSwA V paper also proposes a new multi-crop strategy to increase the number of views of an image with \\nno computational or memory overhead. It achieves 75% top-1 accuracy on ImageNet with ResNet50 \\n(a supervised learning method) as well as surpassing results of supervised pretraining in all the \\nconsidered transfer tasks.\\nInterCLR\\nInterCLR [18] is a hybrid model that jointly learns a visual representation by leveraging intra-image as \\nwell as inter-image invariance. It has two invariance learning branches in its pipeline, one for intra-\\nimage, and the other for inter-image. The intra-image branch constructs contrastive pairs by standard \\nCL methods such as generating a pair of transformations from an input image. The inter-image branch \\nconstructs contrastive pairs using pseudo-labels obtained from clustering – two items within the same \\ncluster constitute a positive pair, and two items from different clusters form a negative pair. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab067596-b863-496d-9478-baaf0809dbf7', embedding=None, metadata={'page_label': '380', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 380\\nA variant of the InfoNCE loss function is used to compute the contrastive loss and the network is \\ntrained through back-propagation:\\nFigure 10.10: Architecture of the InterCLR model. From the paper: Delving into Inter-Image Invariance \\nfor Unsupervised Visual Representation [18]\\nThe InterCLR paper also addresses some special considerations around pseudo label maintenance, \\nsampling strategy, and decision boundary design for the inter-image branch, which we will skip here \\nin the interests of space. The InterCLR model shows many improvements over state-of-the-art intra-\\nimage invariance learning methods on multiple standard benchmarks.\\nMultiview coding\\nMultiview coding has become a mainstream CL method in recent years and involves constructing \\npositive contrastive examples using two or more views of the same object. The objective is to maximize \\nthe mutual information between the representations of the multiple views of the data for positive \\nexamples and minimize it for negative examples. This requires the model to learn higher-level features \\nwhose influence spans multiple views.\\nAMDIM\\nAugmented Multiscale Deep InfoMax (AMDIM) [31] is a model for self-supervised representational \\nlearning based on an earlier local Deep InfoMax method, which attempts to maximize the mutual \\ninformation between a global summary feature that depends on the entire input, and a collection of \\nlocal features that are extracted from intermediate layers in the encoder. AMDIM extends DIM by \\npredicting features across independently augmented features of each input and simultaneously across \\nmultiple scales, as well as using a more powerful encoder. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bfb3650f-6ee5-446f-93f8-7b639797f5e9', embedding=None, metadata={'page_label': '381', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 381\\nThe paper also considers other ways of producing contrastive pairs, such as instance transformation \\nand multimodal (discussed in the next section), but it is described here because it also considers \\nconstructing contrastive pairs using multiview coding. The model beats several benchmarks for self-\\nsupervised learning objectives.\\nCMC\\nThe Contrastive Multiview Coding ( CMC ) [37] model is based on the idea that when an object is \\nrepresented by multiple views, each of these views is noisy and incomplete, but important factors such \\nas the physics, geometry, and semantics of the object are usually shared across all the views. The goal \\nof CMC is to learn a compact representation of the object that captures these important factors. CMC \\nachieves this by using CL to learn a representation such that views of the same scene map to nearby \\npoints, whereas views of different scenes map to distant points.\\nMultimodal models\\nThe class of models covered in this section includes models that use paired inputs from two or more \\nmodalities of the same data. The input to such a model could be an image and a caption, a video and text, \\nan audio clip and its transcript, etc. These models learn a joint embedding across multiple modalities. \\nIn this class of models, we will cover the CLIP [6] and CodeSearchNet [13] models as examples.\\nAnother class of multimodal models is frameworks that can be used to do self-supervised learning \\nacross multiple modalities. The Data2Vec [7] model is an example of such a model.\\nCLIP\\nThe CLIP model [6] learns image representations by learning to predict which image goes with which \\ncaption. It is pretrained with 400 million image-text pairs from the internet. After pretraining, the \\nmodel can use natural language queries to refer to learned visual concepts. CLIP can be used in zero-\\nshot mode for downstream tasks such as image classification, text-to-image, and image-to-image image \\nsearch. The model is competitive for natural images with a fully supervised baseline without the need \\nfor any additional fine-tuning. For example, CLIP can match the accuracy of the original ResNet50 \\non ImageNet in zero-shot mode, i.e., without additional fine-tuning. CLIP can also be fine-tuned with \\nspecialized image datasets for specific downstream tasks, such as learning visual representations for \\nsatellite imagery or tumor detection.\\nFigure 10.11 shows the architecture of the CLIP model for training and inference. Both image and \\ntext encoders are transformer-based encoders. The objective of pretraining is to solve the task of \\npredicting which text as a whole is paired with which image. Thus, given a batch of N image-text pairs, \\nCLIP learns to predict which of the N x N possible image-text pairs across the batch actually occurred. \\nCLIP learns a multi-modal joint embedding space by maximizing the cosine similarity of the image \\nand text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the \\nrest of the N2 - N incorrect pairs. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97281f63-ac09-4760-805f-485657f6d638', embedding=None, metadata={'page_label': '382', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 382\\nDuring inference, the input of one modality can be used to predict the output of the other, i.e., given \\nan image, it can predict the image class as text:\\nFigure 10.11: Architecture of the CLIP model. From the paper: Learning Transferable Visual Models \\nfrom Natural Language Supervision [34x]\\nThe code snippet below demonstrates the CLIP model’s ability to compare images and text. Here, we \\ntake an image of two cats side by side and compare it to two text strings: \"a photo of a cat\"  and \\n\"a photo of a dog\" . CLIP can compare the image with the two text strings and correctly determine \\nthat the probability that the image is similar to the string \"a photo of a cat\"  is 0.995 as opposed to \\na probability of 0.005 for the image being similar to the string \"a photo of a dog\" :\\nimport tensorflow as tf\\nfrom PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, TFCLIPModel\\nmodel = TFCLIPModel.from_pretrained( \"openai/clip-vit-base-patch32\" )\\nprocessor = CLIPProcessor.from_pretrained( \"openai/clip-vit-base-patch32\" )\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image. open(requests.get(url, stream= True).raw)\\ntexts = [ \"a photo of a cat\" , \"a photo of a dog\" ]\\ninputs = processor(text=texts, images=image, return_tensors= \"tf\", padding= True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = tf.nn.softmax(logits_per_image, axis= 1)\\nprint(probs.numpy())', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a2e54c83-26dc-42bc-bcbe-01c55b61f537', embedding=None, metadata={'page_label': '383', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 383\\nThe CLIP model does this by projecting both text and image to a single embedding space. Using this \\ncommon embedding approach, CLIP is also able to compute the similarity between two images and \\na text. It also offers the ability to extract encodings of text and images.\\nCodeSearchNet\\nThe CodeSearchNet model [13] uses code snippets  representing functions or methods  in multiple \\nprogramming languages (Go, Java, JavaScript, Python, PHP, and Ruby), and pairs them with (manually \\naugmented) natural language comments describing the code to create positive examples. The corpus \\nconsists of approximately 2 million code-documentation pairs across all the different languages. As \\nwith CLIP, the goal of the CodeSearchNet model is to learn a joint embedding space of code and \\ndocumentation, which can then be queried to return the appropriate code snippet (functions or \\nmethods) that satisfy some natural language query. The code and the natural language query are \\nencoded using two separate encoders, and the model tries to learn a joint embedding that maximizes \\nthe inner product of the code and query encodings for positive pairs and minimizes it for negative pairs.\\nData2Vec\\nData2Vec [7] is a little different in that it proposes a common framework to do self-supervised learning  \\nacross multiple modalities. It uses masked prediction to apply the same learning method for either \\nspeech, language, or computer vision. The core idea is to predict latent representations of the full \\ninput based on a masked view of the input. Instead of predicting modality-specific targets such as \\nwords, visual tokens, etc., it predicts contextualized latent representations that contain information \\nfor the entire input. It uses a teacher-student architecture – first, a representation of the full input \\ndata is built, which serves as the target for the learning task (teacher mode). Then a masked version \\nof the input sample is encoded, with which the full data representation is predicted (student mode). \\nThe teacher’s parameters are updated using exponentially decaying average weights of the student. \\nAt the end of the training, the teacher’s weights are used as the learned embedding.\\nExperiments using this framework against major benchmarks in speech recognition, image \\nclassification, and natural language understanding show either state-of-the-art performance or \\ncompetitive performance to popular approaches:\\nFigure 10.12: Architecture of the Data2Vec model. From the paper: data2vec: A General Framework \\nfor Self-supervised Learning in Speech, Vision and Language [7]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72c5e3a1-4bc3-4917-8053-a86ddc62f36e', embedding=None, metadata={'page_label': '384', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 384\\nPretext tasks\\nPretext tasks are tasks that self-supervised learning models attempt to solve by leveraging some pattern \\ninherent in the unlabeled data they train on. Such tasks are not necessarily useful in and of themselves, \\nbut they help the system learn a useful latent representation, or embeddings, that can then be used, \\neither as-is or after fine-tuning, on some other downstream tasks. Training to solve pretext tasks \\nusually happens as a precursor to building the actual model, and for that reason, it is also referred \\nto as pretraining.\\nAlmost all the techniques we have discussed in this chapter have been pretext tasks. While some tasks \\nmay end up being useful in and of themselves, such as colorization or super-resolution, they also result \\nin embeddings that end up learning the semantics of the data distribution of the unlabeled data that it \\nwas trained on, in the form of learned weights. These weights can then be applied to downstream tasks.\\nThis is not a new concept – for example, the Word2Vec algorithm, which is widely used for finding \\n“synonyms,” is based on an embedding space where words used in similar contexts cluster together. \\nIt is trained using either the skip-gram or CBOW algorithm, which attempt to predict a context word \\ngiven a word, or vice versa. Neither of these objectives are useful in and of themselves, but in the \\nprocess, the network ends up learning a good latent representation of the words in the input data. \\nThis representation can then be directly used to find “synonyms” for words or do word analogies, as \\nwell as being used to produce useful vector representations of words and sequences of words (such \\nas sentences and documents) for downstream tasks, such as text classification or sentiment analysis.\\nThe biggest advantage of pretext tasks is that the training of models for downstream tasks can be \\ndone with relatively smaller amounts of labeled data. The model learns a lot about the domain (the \\nbroad strokes) based on solving the pretext task using large quantities of readily available unlabeled \\ndata. It requires relatively smaller amounts of labeled data to learn to solve more specific downstream \\ntasks based on what it already knows about the domain. Because labeled data is hard to come by and \\nexpensive to create, this two-step approach can often make some machine learning models possible, \\nif not more practical.\\nSummary\\nIn this chapter, we saw various self-supervised strategies for leveraging data to learn the data distribution \\nin the form of specialized embedding spaces, which in turn can be used for solving downstream tasks. \\nWe have looked at self-prediction, contrastive learning, and pretext tasks as specific approaches for \\nself-supervision.\\nIn the next chapter, we will look at reinforcement learning, an approach that uses rewards as a feedback \\nmechanism to train models for specific tasks.\\nReferences\\n1. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavucuoglu (2016). Pixel Recurrent Neural \\nNetworks Proceedings MLR Press: http://proceedings.mlr.press/v48/oord16.pdf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='903d90f3-2472-476d-a228-967d2e8e45f1', embedding=None, metadata={'page_label': '385', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 385\\n2. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive \\nPredictive Coding. Arxiv Preprint, arXiv 1807.03748 [cs.LG]: https://arxiv.org/pdf/1807.03748.\\npdf\\n3. Aaron van den Oord, et al. (2016). WaveNet: A Generative Model for Raw Audio. Arxiv Preprint, \\narXiv:1609.03499v2 [cs.SD]: https://arxiv.org/pdf/1609.03499.pdf\\n4. Abe Fetterman and Josh Albrecht. (2020). Understanding Self-Supervised and Contrastive Learning  \\nwith “Bootstrap your Own Latent” (BYOL). Blog post: https://generallyintelligent.ai/\\nblog/2020-08-24-understanding-self-supervised-contrastive-learning/  \\n5. Aditya Ramesh, et al. Zero Shot Text to Image generation. Arxiv Preprint, arXiv 2102.12092v2 [cs.\\nCV]: https://arxiv.org/pdf/2102.12092.pdf  \\n6. Alec Radford, et al. (2021). Learning Transferable Visual Models from Natural Language Supervision. \\nProceedings of Machine Learning Research (PMLR): http://proceedings.mlr.press/v139/\\nradford21a/radford21a.pdf\\n7. Alexei Baevsky, et al. (2022). data2vec: A General Framework for Self-Supervised Learning in \\nSpeech, Vision and Language. Arxiv Preprint, arXiv 2202.03555v1 [cs.LG]: https://arxiv.org/\\npdf/2202.03555.pdf  \\n8. Carl Doersch, Abhinav Gupta and Alexei Efros. (2015). Unsupervised Visual Representation by \\nContext Prediction. International Conference on Computer Vision (ICCV): https://www.cv-\\nfoundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_\\nRepresentation_ICCV_2015_paper.pdf   \\n9. Chuan Li. (2020). OpenAI’s GPT-3 Language Model – a Technical Overview. LambdaLabs Blog post: \\nhttps://lambdalabs.com/blog/demystifying-gpt-3/  \\n10. Deepak Pathak, et al. (2016). Context Encoders: Feature Learning by Inpainting: https://\\nopenaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_\\nCVPR_2016_paper.pdf  \\n11. Florian Schroff, Dmitry Kalenichenko and James Philbin. (2025). FaceNet: A Unified Embedding \\nfor Face Recognition and Clustering. ArXiv Preprint, arXiv 1503.03832 [cs.CV]: https://arxiv.\\norg/pdf/1503.03832.pdf  \\n12. Gustav Larsson, Michael Maire and Gregory Shakhnarovich. (2017). Colorization as a Proxy \\nTask for Visual Understanding: https://openaccess.thecvf.com/content_cvpr_2017/papers/\\nLarsson_Colorization_as_a_CVPR_2017_paper.pdf  \\n13. Hamel Husain, et al. (2020). CodeSearchNet Challenge: Evaluating the State of Semantic Code Search . \\nArxiv Preprint, arXiv: 1909.09436 [cs.LG]: https://arxiv.org/pdf/1909.09436.pdf  \\n14. Hanting Chen, et al. (2021). Pre-trained Image Processing Transformer. Conference on Computer \\nVision and Pattern Recognition (CVPR): https://openaccess.thecvf.com/content/CVPR2021/\\npapers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf  \\n15. Hyun Oh Song, Yu Xiang, Stefanie Jegelka and Silvio Savarese. (2015). Deep Metric Learning via \\nLifted Structured Feature Embedding. Arxiv Preprint, arXiv 1511.06452 [cs.CV]: https://arxiv.\\norg/pdf/1511.06452.pdf  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36a994ce-9477-4daa-9a8c-dd3be89accf6', embedding=None, metadata={'page_label': '386', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Supervised Learning 386\\n16. Jacob Devlin, et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language \\nUnderstanding. Arxiv Preprint, arXiv: 1810.04805v2 [cs.CL]: https://arxiv.org/pdf/1810.04805.\\npdf \\n17. Jean-Bastien Grill, et al. (2020). Bootstrap your own latent: A new approach to self-supervised \\nlearning. Arxiv Preprint, arXiv 2006.07733 [cs.LG]: https://arxiv.org/pdf/2006.07733.pdf  \\n18. Jiahao Xie, et al. (2021). Delving into Inter-Image Invariance for Unsupervised Visual Representations. \\nArxiv Preprint, arXiv: 2008.11702 [cs.CV]: https://arxiv.org/pdf/2008.11702.pdf  \\n19. Joshua Robinson, Ching-Yao Chuang, Suvrit Sra and Stefanie Jegelka. (2021). Contrastive Learning \\nwith Hard Negative Samples . Arxiv Preprint, arXiv 2010.04592 [cs.LG]: https://arxiv.org/\\npdf/2010.04592.pdf  \\n20. Jure Zobontar, et al. (2021). Barlow Twins: Self-Supervised Learning via Redundancy Reduction. \\nArxiv Preprint, arXiv 2103.03230 [cs.CV]: https://arxiv.org/pdf/2103.03230.pdf  \\n21. Kihyuk Sohn. (2016). Improved Deep Metric Learning with Multi-class N-pair Loss Objective. \\nAdvances in Neural Information Processing Systems: https://proceedings.neurips.cc/\\npaper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf  \\n22. Lilian Weng and Jong Wook Kim. (2021). Self-supervised Learning: Self Prediction and Contrastive \\nLearning. NeurIPS Tutorial: https://neurips.cc/media/neurips-2021/Slides/21895.pdf\\n23. Lilian Weng. (Blog post 2021). Contrastive Representation Learning: https://lilianweng.\\ngithub.io/posts/2021-05-31-contrastive/\\n24. Mathilde Caron, Piotr Bojanowsky, Armand Joulin and Matthijs Douze. (2019). Deep Clustering \\nfor Unsupervised Learning of Visual Features. Arxiv Preprint, arXiv: 1807.05520 [cs.CV]: https://\\narxiv.org/pdf/1807.05520.pdf  \\n25. Mathilde Caron, et al. (2020). Unsupervised Learning of Visual Features by Contrasting Cluster \\nAssignments. Arxiv Preprint, arXiv: 2006.099882 [cs.CV]: https://arxiv.org/pdf/2006.09882.\\npdf \\n26. Mehdi Noroozi and Paolo Favaro. (2016). Unsupervised Learning of Visual Representations by \\nsolving Jigsaw Puzzles . European Conference on Computer Vision: https://link.springer.\\ncom/chapter/10.1007/978-3-319-46466-4_5\\n27. Michael Gutmann, Aapo Hyvarinen. (2010). Noise-contrastive estimation: A new estimation \\nprinciple for unnormalized statistical models. Proceedings of Machine Learning Research (PMLR): \\nhttp://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf  \\n28. Nal Kalchbrenner, et al. (2018). Efficient Neural Audio Synthesis. Proceedings MLR Press: http://\\nproceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf\\n29. Pascal Vincent, et al. (2010). Stacked Denoising Autoencoders: Learning Useful Representations in a \\nDeep Network with a Local Denoising Criterion. Journal of Machine Learning Research (JMLR): \\nhttps://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://\\ngithubhelp.com  \\n30. Patrick Esser, Robin Rombach and Bjorn Ommer. (2021). Taming Transformers for High-\\nResolution Image Synthesis. Computer Vision and Pattern Recognition (CVPR): https://\\nopenaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-\\nResolution_Image_Synthesis_CVPR_2021_paper.pdf  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93a7934c-4c34-4884-8456-f4117c47cb9e', embedding=None, metadata={'page_label': '387', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10 387\\n31. Philip Bachman, R Devon Hjelm and William Buchwalter. (2019). Learning Representations \\nby Maximizing Mutual Information across Views. Advances in Neural Information \\nProcessing Systems (NeurIPS): https://proceedings.neurips.cc/paper/2019/file/\\nddf354219aac374f1d40b7e760ee5bb7-Paper.pdf  \\n32. Prafulla Dhariwal, et al. (2020). Jukebox: A Generative Model for Music. Arxiv Preprint, arXiv \\n2005.00341v1 [eess.AS]: https://arxiv.org/pdf/2005.00341.pdf  \\n33. Ruslan Salakhutdinov and Geoff Hinton. (2007). Learning a Nonlinear Embedding by Preserving \\nClass Neighborhood Structure.  Proceedings of Machine Learning Research (PMLR): http://\\nproceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf  \\n34. Spyros Gidaris, Praveer Singh and Nicos Komodakis. (2018). Unsupervised Representation Learning \\nby Predicting Image Rotations. Arxiv Preprint, arXiv 1803.07728v1 [cs.CV]: https://arxiv.org/\\npdf/1803.07728.pdf  \\n35. Sumit Chopra, et al. (2005). Learning a Similarity Metric Discriminatively, with application to \\nFace Verification. IEEE Computer Society: http://www.cs.utoronto.ca/~hinton/csc2535_06/\\nreadings/chopra-05.pdf  \\n36. Ting Chen, Simon Kornblith, Mohammed Norouzi and Geoffrey Hinton. (2020). A Simple \\nFramework for Contrastive Learning. Arxiv Preprint, arXiv 2002.05709 [cs.LG]: https://arxiv.\\norg/pdf/2002.05709.pdf  \\n37. Yonglong Tian, Dilip Krishnan and Philip Isola. (2020). Contrastive Multiview Coding. Arxiv \\nPreprint, arXiv: 1906.05849 [cs.CV]: https://arxiv.org/pdf/1906.05849.pdf?ref=https://\\ngithubhelp.com  \\n38. Zhilin Yang, et al. (2019). XLNet: Generalized Autoregressive Pre-training for Language Understanding: \\nhttps://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-\\nPaper.pdf  \\n39. Prompt Engineering. (7th July 2022). Wikipedia, Wikimedia Foundation: https://en.wikipedia.\\norg/wiki/Prompt_engineering\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97533511-dcfc-4ff9-9bd8-e03041d12c88', embedding=None, metadata={'page_label': '388', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='870f3f8e-79a8-49fa-9876-ab46252cffce', embedding=None, metadata={'page_label': '389', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11\\nReinforcement Learning\\nThis chapter introduces Reinforcement Learning (RL)—the least explored and yet most promising \\nlearning paradigm. Reinforcement learning is very different from the supervised and unsupervised \\nlearning models we covered in earlier chapters. Starting from a clean slate (that is, having no prior \\ninformation), the RL agent can go through multiple stages of trial and error, and learn to achieve a goal, \\nall the while the only input being the feedback from the environment. The research in RL by OpenAI \\nseems to suggest that continuous competition can be a cause for the evolution of intelligence. Many \\ndeep learning practitioners believe that RL will play an important role in the big AI dream: Artificial \\nGeneral Intelligence (AGI ). This chapter will delve into different RL algorithms. The following topics \\nwill be covered:\\n• What RL is and its lingo\\n• Learn how to use the OpenAI Gym interface\\n• Applications of RL\\n• Deep Q-Networks\\n• Policy gradients\\nAn introduction to RL\\nWhat is common between a baby learning to walk, birds learning to fly, and an RL agent learning to \\nplay an Atari game? Well, all three involve:\\n• Trial and error: The child (or the bird) tries various ways, fails many times, and succeeds in \\nsome ways before it can really walk (or fly). The RL agent plays many games, winning some \\nand losing many, before it can become reliably successful.\\n• Goal: The child has the goal to walk, the bird to fly, and the RL agent to win the game.\\n• Interaction with the environment: The only feedback they have is from their environment.All the code files for this chapter can be found at https://packt.link/dltfchp11 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='369ad541-b765-4b40-b553-a502e49a52d5', embedding=None, metadata={'page_label': '390', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 390\\nSo, the first questions that arise are what is RL, and how is it different from supervised and unsupervised \\nlearning? Anyone who owns a pet knows that the best strategy to train a pet is rewarding it for desirable \\nbehavior and disciplining it for bad behavior. RL, also called learning with a critic, is a learning \\nparadigm where the agent learns in the same manner. The agent here corresponds to our network \\n(program); it can perform a set of actions ( a), which brings about a change in the state ( s) of the \\nenvironment, and, in turn, the agent receives a reward or punishment from the environment.\\nFor example, consider the case of training a dog to fetch a ball: here, the dog is our agent, the voluntary \\nmuscle movements that the dog makes are the actions, and the ground (as well as the person and ball) \\nis the environment; the dog perceives our reaction to its action in terms of giving it a treat as a reward. \\nRL can be defined as a computational approach to goal-directed learning and decision making, from \\ninteraction with the environment, under some idealized conditions. The agent can sense the state of \\nthe environment, and the agent can perform specific well-defined actions on the environment. This \\ncauses two things: first, a change in the state of the environment, and second, a reward is generated \\n(under ideal conditions). This cycle continues, and in theory the agent learns how to more frequently \\ngenerate a reward over time:\\nFigure 11.1: Reinforcement learning: interaction between agent and environment\\nUnlike supervised learning, the agent is not presented with any training examples; it does not know \\nwhat the correct action is.\\nAnd unlike unsupervised learning, the agent’s goal is not to find some inherent structure in the input \\n(the learning may find some structure, but that isn’t the goal); instead, its only goal is to maximize the \\nrewards (in the long run) and reduce the punishments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab4af290-11f0-4e39-bbbc-bf22532acf86', embedding=None, metadata={'page_label': '391', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 391\\nRL lingo\\nBefore learning various RL algorithms, it is important we understand a few important terms. We \\nwill illustrate the terms with the help of two examples, first a robot in a maze, and second an agent \\ncontrolling the wheels of a Self-Driving Car (SDC ). The two RL agents are shown as follows:\\nFigure 11.2: State for a robot trying to find a path in a maze (LHS). State for an agent trying to control \\nthe steering wheel of a self-driving car (RHS)\\nFigure 11.2 shows the two examples we will be considering. Let us start with the terms:\\n• State, S: State is the set of tokens (or representations) that can define all of the possible states \\nthe environment can be in. It can be continuous or discrete. In the case of the robot finding its \\npath through a maze, the state can be represented by a 4×4 matrix, with elements indicating \\nwhether that block is empty, occupied, or blocked. A block with a value of 1 means it is occupied \\nby the robot, 0 means it is empty, and X represents that the block is impassable. Each element \\nin this array, S, can have one of these three discrete values, so the state is discrete in nature. \\nNext, consider the agent controlling the steering wheel of a self-driving car. The agent takes \\nas input the front-view image. The image contains continuous valued pixels, so here the state \\nis continuous.\\n• Action, A(S) : Actions are the set of all possible things that the agent can do in a particular state. \\nThe set of possible actions, A, depends on the present state, S. Actions may or may not result \\nin a change of state. Like states, they can be discrete or continuous. The robot finding a path in \\nthe maze can perform five discrete actions [up , down, left, right, no change]. The SDC agent, \\non the other hand, can rotate the steering wheel at a continuous range of angles.\\n• Reward R(S,A,S’): Rewards are a scalar value returned by the environment based on the agent’s \\naction(s). Here S is the present state and S’  is the state of the environment after action A is \\ntaken. It is determined by the goal; the agent gets a higher reward if the action brings it near \\nthe goal, and a low (or even negative) reward otherwise. How we define a reward is totally up \\nto us—in the case of the maze, we can define the reward as the Euclidean distance between \\nthe agent’s current position and goal. The SDC agent reward can be that the car is on the road \\n(positive reward) or off the road (negative reward).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4fa7fdef-fc0b-4dd0-99f7-d1efe7d81373', embedding=None, metadata={'page_label': '392', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 392\\n• Policy 𝜋𝜋(𝑆𝑆) : Policy defines a mapping between each state and the action to take in that state. \\nThe policy can be deterministic, that is, for each state, there is a well-defined policy. In the case \\nof the maze robot, a policy can be that if the top block is empty, move up. The policy can also \\nbe stochastic, that is, where an action is taken by some probability. It can be implemented as \\na simple look-up table, or it can be a function dependent on the present state. The policy is \\nthe core of the RL agent. In this chapter, we’ll learn about different algorithms that help the \\nagent to learn the policy.\\n• Return Gt: This is the discounted sum of all future rewards starting from the current time, \\nmathematically defined as:\\n𝐺𝐺𝑡𝑡=∑𝛾𝛾𝑘𝑘𝑅𝑅𝑡𝑡𝑡𝑘𝑘𝑡𝑡∞\\n𝑘𝑘𝑘𝑘 \\n• Here Rt is the reward at time t and 𝛾𝛾  is the discount factor; its value lies between 0 and 1. The \\ndiscount factor determines how important future rewards are in deciding the policy. If it is near \\nzero, the agent gives importance to the immediate rewards. A high discount factor, however, \\nmeans the agent is looking far into the future. It may give up immediate reward in favor of \\nhigh future rewards, just as in the game chess, you may sacrifice a pawn to later checkmate \\nthe opponent.\\n• Value function V(S) : This defines the “goodness” of a state in the long run. It can be thought \\nof as the total amount of reward the agent can expect to accumulate over time, starting from \\nthe state, S. You can think of it as long-term good, as opposed to an immediate but short-lived \\ngood. What do you think is more important, maximizing the immediate reward or the value \\nfunction? You probably guessed right: just as in chess, we sometimes lose a pawn to win the \\ngame a few steps later, and so the agent should try to maximize the value function.\\n• Normally, the value is defined either as the state-value function 𝑉𝑉𝜋𝜋(𝑆𝑆)  or the action-value \\nfunction  𝑄𝑄𝜋𝜋(𝑆𝑆𝑆𝑆𝑆𝑆 , where 𝜋𝜋  is the policy followed. The state-value function is the expected \\nreturn from the state S after following policy 𝜋𝜋 :\\n𝑉𝑉𝜋𝜋(𝑆𝑆)=𝐸𝐸𝜋𝜋[𝐺𝐺𝑡𝑡|𝑆𝑆𝑡𝑡=𝑠𝑠] \\n• Here E is the expectation, and S t=s is the state at time t . The action-value function is the expected \\nreturn from the state S, taking an action A=a and following the policy 𝜋𝜋 :\\n𝑄𝑄𝜋𝜋(𝑆𝑆𝑆𝑆𝑆)=𝐸𝐸𝜋𝜋[𝐺𝐺𝑡𝑡|𝑆𝑆𝑡𝑡= 𝑠𝑠𝑆𝑆𝑆𝑡𝑡=𝑎𝑎] \\n• Model of the environment: This is an optional element. It mimics the behavior of the environment, \\nand it contains the physics of the environment; in other words, it indicates how the environment \\nwill behave. The model of the environment is defined by the transition probability to the next \\nstate. This is an optional component; we can have model-free reinforcement learning as well \\nwhere the transition probability is not needed to define the RL process.\\nIn RL, we assume that the state of the environment follows the Markov property, that is, each state is \\ndependent solely on the preceding state, the action taken from the action space, and the corresponding \\nreward. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37caf109-9203-4f77-9d48-cf45f332bc5c', embedding=None, metadata={'page_label': '393', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 393\\nThat is, if St+1 is the state of the environment at time t+1, then it is a function of St state at time t, At is \\nthe action taken at time t, and Rt is the corresponding reward received at time t, no prior history is \\nneeded. If P(St+1|St) is the transition probability, mathematically the Markov property can be written as:\\n𝑃𝑃(𝑆𝑆𝑡𝑡𝑡𝑡|𝑆𝑆𝑡𝑡)=𝑃𝑃(𝑆𝑆𝑡𝑡𝑡𝑡|𝑆𝑆𝑡,𝑆𝑆2,…,𝑆𝑆𝑡𝑡) \\nAnd thus, RL can be assumed to be a Markov Decision Process (MDP ).\\nDeep reinforcement learning algorithms\\nThe basic idea of Deep Reinforcement Learning (DRL ) is that we can use a deep neural network to \\napproximate either the policy function or the value function. In this chapter, we will be studying some \\npopular DRL algorithms. These algorithms can be classified into two classes, depending upon what \\nthey approximate:\\n• Value-based methods: In these methods, the algorithms take the action that maximizes the \\nvalue  function. The agent here learns to predict how good a given state or action would be. An \\nexample of the value-based method is the Deep Q-Network. Consider, for example, our robot \\nin a maze: assuming that the value of each state is the negative of the number of steps needed \\nto go from that box to the goal, then, at each time step, the agent will choose the action that \\ntakes it to a state with optimal value, as in the following diagram. So, starting from a value of \\n-6, it’ll move to -5, -4, -3, -2, -1, and eventually reach the goal with the value 0: \\nFigure 11.3: Demo value function values for the maze-finding robot\\n• Policy-based methods: In these methods, the algorithms predict the optimal policy (the one \\nthat maximizes the expected return), without maintaining the value function estimates. The \\naim is to find the optimal policy, instead of the optimal action. An example of the policy-based \\nmethod is policy gradients. Here, we approximate the policy function, which allows us to map \\neach state to the best corresponding action. One advantage of policy-based methods over \\nvalue-based is that we can use them even for continuous action spaces.\\nBesides the algorithms approximating either policy or value, there are a few questions we need to \\nanswer to make reinforcement learning work.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02bef6b2-c89e-4db9-b6ce-d8e684db0577', embedding=None, metadata={'page_label': '394', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 394\\nHow does the agent choose its actions, especially when untrained?\\nWhen the agent starts learning, it has no idea what the best way in which to determine an action is, or \\nwhich action will provide the best Q  value. So how do we go about it? We take a leaf out of nature’s book. \\nLike bees and ants, the agent makes a balance between exploring new actions and exploiting learned \\nones. Initially, when the agent starts, it has no idea which action among the possible actions is better, \\nso it makes random choices, but as it learns, it starts making use of the learned policy. This is called \\nthe exploration vs exploitation [2] tradeoff. Using exploration, the agent gathers more information, \\nand later exploits the gathered information to make the best decision.\\nHow does the agent maintain a balance between exploration and \\nexploitation?\\nThere are various strategies; one of the most employed is the epsilon-greedy ( 𝜖𝜖 𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖  ) policy. Here, \\nthe agent explores unceasingly, and depending upon the value of 𝜖𝜖𝜖[0,1] , at each step the agent selects \\na random action with probability 𝜖𝜖 , and with probability 1−𝜖𝜖   selects an action that maximizes the \\nvalue function. Normally, the value of 𝜖𝜖  decreases asymptotically. In Python the 𝜖𝜖 𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖𝜖   policy \\ncan be implemented as:\\n  if np.random.rand() <= epsilon:\\n        a = random.randrange(action_size)\\n  else:\\n        a = np.argmax(model.predict(s))\\nwhere model  is the deep neural network approximating the value/policy function, a is the action chosen \\nfrom the action space of size action_size , and s is the state. Another way to perform exploration is \\nto use noise; researchers have experimented with both Gaussian and Ornstein-Uhlenbeck noise with \\nsuccess.\\nHow to deal with the highly correlated input state space\\nThe input to our RL model is the present state of the environment. Each action results in some change \\nin the environment; however, the correlation between two consecutive states is very high. Now if we \\nmake our network learn based on the sequential states, the high correlation between consecutive inputs \\nresults in what is known as catastrophic forgetting. To mitigate the effect of catastrophic forgetting, \\nin 2018, David Isele and Akansel Cosgun proposed the experience replay method.\\nIn simplest terms, the learning algorithm first stores the MDP tuple—state, action, reward, and next \\nstate <S, A, R, S’>—in a buffer/memory. Once a significant amount of memory is built, a batch is \\nselected randomly to train the agent. The memory is continuously refreshed with new additions and \\nold deletions. The use of experience replay provides three benefits:\\n• First, it allows the same experience to be potentially used in many weight updates, hence \\nincreasing data efficiency.\\n• Second, the random selection of batches of experience removes the correlations between \\nconsecutive states presented to the network for training.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1acfc8d6-ea6c-4da5-8551-f32564b85e24', embedding=None, metadata={'page_label': '395', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 395\\n• Third, it stops any unwanted feedback loops that may arise and cause the network to get stuck \\nin local minima or diverge.\\nA modified version of experience replay is the Prioritized Experience Replay ( PER ). Introduced in 2015 \\nby Tom Schaul et al. [4], it derives from the idea that not all experiences (or, you might say, attempts) \\nare equally important. Some attempts are better lessons than others. Thus, instead of selecting the \\nexperiences randomly, it will be much more efficient to assign higher priority to more educational \\nexperiences in selection for training. In the Schaul paper, it was proposed that experiences in which \\nthe difference between the prediction and target is high should be given priority, as the agent could \\nlearn a lot in these cases.\\nHow to deal with the problem of moving targets\\nUnlike supervised learning, the target is not previously known in RL. With a moving target, the agent \\ntries to maximize the expected return, but the maximum value goes on changing as the agent learns. \\nIn essence, this is like trying to catch a butterfly yet each time you approach it, it moves to a new \\nlocation. The major reason to have a moving target is that the same networks are used to estimate the \\naction and the target values, and this can cause oscillations in learning.\\nA solution to this was proposed by the DeepMind team in their 2015 paper, titled Human-level Control \\nthrough Deep Reinforcement Learning, published in Nature. The solution is that now, instead of a moving \\ntarget, the agent has short-term fixed targets. The agent now maintains two networks, both are exactly \\nthe same in architecture, one called the local network, which is used at each step to estimate the present \\naction, and one the target network, which is used to get the target value. However, both networks \\nhave their own set of weights. At each time step, the local network learns in the direction such that \\nits estimate and target are near to each other. After some number of time steps, the target network \\nweights are updated. The update can be a hard update, where the weights of the local network are \\ncopied completely to the target network after N time steps, or it can be a soft update, in which the \\ntarget network slowly (by a factor of Tau 𝜏𝜏𝜏𝜏[0,1] ) moves its weight toward the local network.\\nReinforcement success in recent years\\nIn the last few years, DRL has been successfully used in a variety of tasks, especially in game playing \\nand robotics. Let us acquaint ourselves with some success stories of RL before learning its algorithms:\\n• AlphaGo Zero: Developed by Google’s DeepMind team, the AlphaGo Zero paper Mastering the \\ngame of Go without any human knowledge starts from an absolutely blank slate (tabula rasa). The \\nAlphaGo Zero uses one neural network to approximate both the move probabilities and value.\\n• This neural network takes as an input the raw board representation. It uses a Monte Carlo tree \\nsearch guided by the neural network to select the moves. The reinforcement learning algorithm \\nincorporates a look-ahead search inside the training loop. It was trained for 40 days using a \\n40-block residual CNN and, over the course of training, it played about 29 million games (a big \\nnumber!). The neural network was optimized on Google Cloud using TensorFlow, with 64 GPU \\nworkers and 19 CPU parameter servers. You can access the paper here: https://www.nature.\\ncom/articles/nature24270 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fff90d0-ec6f-4480-8f14-aa6e152c9d55', embedding=None, metadata={'page_label': '396', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 396\\n• AI-controlled sailplanes: Microsoft has developed a controller system that can run on many \\ndifferent autopilot hardware platforms, such as Pixhawk and Raspberry Pi 3. It can keep the \\nsailplane in the air without using a motor, by autonomously finding and catching rides on \\nnaturally occurring thermals. The controller helps the sailplane to operate on its own by \\ndetecting and using these thermals to travel without the aid of a motor or a person. They \\nimplemented it as a partially observable Markov decision process. They employed Bayesian \\nreinforcement learning and used the Monte Carlo tree search to search for the best action. \\nThey’ve divided the whole system into level planners—a high-level planner that makes a decision \\nbased on experience and a low-level planner that uses Bayesian reinforcement learning to detect \\nand latch onto thermals in real time. You can see the sailplane in action at Microsoft News: \\nhttps://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-\\ntest-ai-controlled-soaring-machine/ .\\n• Locomotion behavior: In the paper Emergence of Locomotion Behaviours in Rich Environments \\n(https://arxiv.org/pdf/1707.02286.pdf ), DeepMind researchers provided the agents with \\nrich and diverse environments. The environments presented a spectrum of challenges at \\ndifferent levels of difficulty. The agent was provided with difficulties in increasing order; this led \\nthe agent to learn sophisticated locomotion skills without performing any reward engineering \\n(that is, designing special reward functions).\\n• Data center cooling using reinforcement learning: Data centers are workhorses of the present \\ndigital/internet revolution. With their large servers and networking devices, they facilitate \\ndata storage, data transfer, and the processing of information over the internet. Data centers \\naccount for about ~1.5% of all global energy consumption and if nothing is done about it, the \\nconsumption will only increase. DeepMind, along with Google Research in 2016, employed \\nreinforcement learning models to reduce the energy consumption of their data centers by 40%. \\nUsing the historical data collected from the  sensors within the data center, they trained a deep \\nneural network to predict future energy efficiency and propose optimal action. You can read \\nthe details of the models and approach in the paper Data center cooling using model-predictive \\ncontrol  (https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1d\\ncc740cc-Paper.pdf ).\\n• Controlling nuclear fusion plasma: A recent (2022) and interesting application of RL is in \\ncontrolling nuclear fusion plasma with the help of reinforcement learning. The results are \\npublished in a Nature paper: Magnetic control of tokamak plasmas through reinforcement learning.\\nIt is really amazing to see how the DRL agent, without any implicit knowledge, learns to perform, and \\neven beat, humans – in many specialized tasks. In the coming sections, we will explore these fabulous \\nDRL algorithms and see them play games with almost human efficiency within a few thousand epochs.\\nSimulation environments for RL\\nAs mentioned earlier, trial and error is an important component of any RL algorithm. Therefore, it \\nmakes sense to train our RL agent firstly in a simulated environment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00fbfbe2-2dd9-4070-8821-9d71b14915cb', embedding=None, metadata={'page_label': '397', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 397\\nToday there exists a large number of platforms that can be used for the creation of an environment. \\nSome popular ones are:\\n• OpenAI Gym: This contains a collection of environments that we can use to train our RL agents. \\nIn this chapter, we’ll be using the OpenAI Gym interface.\\n• Unity ML-Agents SDK: It allows developers to transform games and simulations created \\nusing the Unity editor into environments where intelligent agents can be trained using DRL, \\nevolutionary strategies, or other machine learning methods through a simple-to-use Python \\nAPI. It works with TensorFlow and provides the ability to train intelligent agents for 2D/3D and \\nVR/AR games. You can learn more about it here: https://github.com/Unity-Technologies/\\nml-agents .\\n• Gazebo: In Gazebo, we can build three-dimensional worlds with physics-based simulation. The \\ngym-gazebo  toolkit uses Gazebo along with the Robot Operating System ( ROS ) and the OpenAI \\nGym interface and can be used to train RL agents. To find out more about this, you can refer \\nto the white paper: https://arxiv.org/abs/1608.05742 .\\n• Blender learning environment: This is a Python interface for the Blender game engine, and it \\nalso works with OpenAI Gym. It has at its base Blender: a free 3D modeling software with an \\nintegrated game engine. This provides an easy-to-use, powerful set of tools for creating games. \\nIt provides an interface to the Blender game engine, and the games themselves are designed \\nin Blender. We can then create a custom virtual environment to train an RL agent on a specific \\nproblem ( https://github.com/LouisFoucard/gym-blender ).\\n• Malmo: Built by the Microsoft team, Malmo is a platform for AI experimentation and research \\nbuilt on top of Minecraft. It provides a simple API for creating tasks and missions. You can learn \\nmore about Project Malmo here: https://www.microsoft.com/en-us/research/project/\\nproject-malmo/ .\\nAn introduction to OpenAI Gym\\nWe will be using OpenAI Gym to provide an environment for our agent. OpenAI Gym is an open source \\ntoolkit to develop and compare RL algorithms. It contains a variety of simulated environments that \\ncan be used to train agents and develop new RL algorithms.\\nThe first thing to do is install OpenAI Gym. The following command will install the minimal gym \\npackage:\\npip install gym\\nIf you want to install all (free) gym modules, add [all]  after it:\\npip install gym[all]\\nThe MuJoCo environment requires a purchasing license. For Atari-based games, you will \\nneed to install Atari dependencies (Box2D and ROM):\\npip install box2d-py', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='221a6e9f-8b4a-4b6f-9b3d-49f8b4a630eb', embedding=None, metadata={'page_label': '398', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 398\\nOpenAI Gym provides a variety of environments, from simple text-based to three-dimensional games. \\nThe environments supported can be grouped as follows:\\n• Algorithms: Contains environments that involve performing computations such as addition. \\nWhile we can easily perform the computations on a computer, what makes these problems \\ninteresting as RL problems is that the agent learns these tasks purely by example.\\n• Atari: This environment provides a wide variety of classic Atari/arcade games.\\n• Box2D: Contains robotics tasks in two dimensions such as a car racing agent or bipedal robot \\nwalk.\\n• Classic control: This contains the classical control theory problems, such as balancing a cart \\npole.\\n• MuJoCo : This is proprietary (you can get a one-month free trial). It supports various robot \\nsimulation tasks. The environment includes a physics engine; hence, it’s used for training \\nrobotic tasks.\\n• Robotics: This environment also uses the physics engine of MuJoCo. It simulates goal-based \\ntasks for fetch and shadow-hand robots.\\n• Toy text : A simple text-based environment—very good for beginners.\\nYou can get a complete list of environments from the Gym website: https://gym.openai.com . To find \\na list of all available environments in your installation, you can use the following code:\\nfrom gym import envs\\n   \\nenvall = envs.registry. all()\\nlen(envall)\\nAt the time of writing this book, it resulted in 859, that is, there are 859 different environments present \\nin the gym module. Let us see more details of these environments. Each environment is created by \\nusing the make  function. Associated with each environment is a unique ID, its observation space, its \\naction space, and a default reward range. Gym allows you to access them through dot notation, as \\nshown in the following code. We go through all the environments in the envall  list and note down \\nits unique ID, which is used to create the environment using the make  method, its observation space, \\nreward range, and the action space:\\nfrom tqdm import tqdm\\nList = []\\nfor e in tqdm(envall):\\n    try:\\n        env = e.make()\\n        List.append([e. id, env.observation_space, env.action_space, env.reward_\\nrange])\\n        env.close() \\n    except:\\n        continue   ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='058749b5-f38e-4491-b4dc-f01326d5c2bd', embedding=None, metadata={'page_label': '399', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 399\\nFigure 11.4 shows a random sample from the list:\\nFigure 11.4: Random list of environments available in OpenAI Gym\\nYou can use these commands to find out details about any environment in Gym. For example, the \\nfollowing code prints details of the MountainCar environment:\\nenv = gym.make( \\'MountainCar-v0\\' )\\nprint(f\"The Observation space is        {env.observation_space} \" )\\nprint(f\"Upper Bound for Env Observation {env.observation_space.high} \")\\nprint(f\"Lower Bound for Env Observation {env.observation_space.low} \")\\nprint(f\"Action Space                    {env.action_space} \")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7175686-545c-4446-9c97-39c13bef97b1', embedding=None, metadata={'page_label': '400', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 400\\nenv.seed( 0)\\nobs = env.reset()\\nprint(f\"The initial observation is      {obs}\")\\n# Take a random actionget the new observation space\\nnew_obs, reward, done, info = env.step(env.action_space.sample())\\nprint(f\"The new observation is          {new_obs} \")\\nenv.close()\\nThe core interface provided by OpenAI Gym is the unified environment interface. The agent can \\ninteract with the environment using three basic methods, that is, reset , step , and render . The reset  \\nmethod resets the environment and returns the observation. The step  method steps the environment \\nby one time step and returns new_obs , reward , done , and info . The render  method renders one frame \\nof the environment, like popping a window. Let us try and view some different environments and \\nview their initial frame:\\nPhysics Engine Classic Control Atari\\ne = \\'LunarLander-v2\\'\\nenv = gym.make(e)\\nobs = env.reset() \\nimg = env.\\nrender(mode= \\'rgb_\\narray\\')\\nenv.close()\\nplt.imshow(img)e = \\'CartPole-v0\\'\\nenv = gym.make(e)\\nenv.reset()\\nimg = env.\\nrender(mode= \\'rgb_\\narray\\')\\nenv.close()\\nplt.imshow(img)e = \\'SpaceInvaders-v0\\'\\nenv = gym.make(e)\\nenv.reset()\\nimg = env.\\nrender(mode= \\'rgb_\\narray\\')\\nenv.close()\\nplt.imshow(img)\\nTable 11.1: Different environments of OpenAI Gym and their initial state\\nThe preceding code uses Matplotlib to display the environment; alternatively, you can directly use \\nthe render  method:\\nimport gym\\nenv_name = \\'Breakout-v0\\'', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='438cfec3-568e-4013-a899-1989d8f90f9f', embedding=None, metadata={'page_label': '401', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 401\\nenv = gym.make(env_name)\\nobs = env.reset()\\nenv.render()\\nYou can see the Breakout environment in Figure 11.5; the render  function pops up the environment \\nwindow:\\nFigure 11.5: Initial state of the Breakout environment\\nWe can use env.observation_space  and env.action_space  to find out more about the state space \\nand action space for the Breakout game. The results show the state consists of a three-channel image \\nof size 210 × 160, and the action space is discrete with four possible actions. Once you are done, do \\nnot forget to close OpenAI using:\\nenv.close()\\nRandom agent playing Breakout\\nLet’s have some fun and play the Breakout game. When I first played the game, I had no idea of the \\nrules or how to play, so I randomly chose the control buttons. Our novice agent will do the same; it \\nwill choose the actions randomly from the action space. Gym provides a function called sample() , \\nwhich chooses a random action from the action space – we will be using this function. Also, we can \\nsave a replay of the game, to view it later. There are two ways to save the play, one using Matplotlib \\nand another using an OpenAI Gym Monitor wrapper. Let us first see the Matplotlib method.\\nWe will first import the necessary modules; we will only need gym and matplotlib  for now, as the \\nagent will be playing random moves:\\nimport gym\\nimport matplotlib.pyplot as plt\\nimport matplotlib.animation as animation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5905e5fc-280e-4a85-8469-27926945a561', embedding=None, metadata={'page_label': '402', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Reinforcement Learning 402\\nWe create the Gym environment:\\nenv_name = 'Breakout-v0'\\nenv = gym.make(env_name)\\nNext, we will run the game, one step at a time, choosing a random action, either for 300 steps or until \\nthe game is finished (whichever is earlier). The environment state (observation) space is saved at each \\nstep in the list frames :\\nframes = [] # array to store state space at each step\\nenv.reset()\\ndone = False\\nfor _ in range (300): \\n    #print(done)\\n    frames.append(env.render(mode= 'rgb_array' ))\\n    obs,reward,done, _ = env.step(env.action_space.sample())\\n    if done:\\n        break\\nNow comes the part of combining all the frames into a GIF image using Matplotlib Animation. We \\ncreate an image object, patch, and then define a function that sets image data to a particular frame \\nindex. The function is used by the Matplotlib Animation  class to create an animation, which we finally \\nsave in the file random_agent.gif :\\npatch = plt.imshow(frames[ 0])\\nplt.axis( 'off')\\ndef animate (i):\\n    patch.set_data(frames[i])\\n    anim = animation.FuncAnimation(plt.gcf(), animate, \\\\\\n        frames= len(frames), interval= 10)\\n    anim.save( 'random_agent.gif' , writer= 'imagemagick' )\\nThe code above will generate a GIF image. Below are some screen grabs from the image:\\n \\nFigure 11.6: Some screenshots from the saved GIF image \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d32b34b-6997-4aeb-9ad9-04ec023f5e9a', embedding=None, metadata={'page_label': '403', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 403\\nNow that we are familiar with OpenAI Gym, we’ll move on to wrappers—which you can use to create \\nyour own custom environments.\\nWrappers in Gym\\nGym provides various wrappers for us to modify the existing environment. For example, if you have \\nimage-based inputs with the RGB intensity value lying between 0 and 255, but the RL agent you use is \\na neural network, which works best if the input is in the range 0-1, you can use the Gym wrapper class \\nto preprocess the state space. Below we define a wrapper that concatenates observations:\\nfrom collections import deque\\nfrom gym import spaces\\nimport numpy as np\\n#Class to concat observations\\nclass ConcatObservations (gym.Wrapper):\\n    def __init__ (self, env, n):\\n        gym.Wrapper.__init__(self, env)\\n        shape = env.observation_space.shape\\n        self.n = n\\n        self.frames = deque([], maxlen=n)\\n        self.observation_space = \\\\\\n            spaces.Box(low= 0, high= 255, shape=((n,) + shape), dtype=env.\\nobservation_space.dtype)\\n    def reset(self):  #reset function\\n        obs = self.env.reset()\\n        for _ in range (self.n):\\n            self.frames.append(obs)\\n        return self._get_obs()\\n    def step(self, action): #step function\\n        obs, reward, done, info = self.env.step(action)\\n        self.frames.append(obs)\\n        return self._get_obs(), reward, done, info\\n    def _get_obs (self):\\n        return np.array(self.frames)\\nYou can see that we need to change the default reset  function, step  function, and observation function \\n_get_obs . We also need to modify the default observation space.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1f16116d-3152-4d31-9d39-754c3f081cec', embedding=None, metadata={'page_label': '404', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 404\\nLet us see how it works. If you take the \"BreakoutNoFrameskip-v4\"  environment, then the initial \\nobservation space is 210 x 160 x 3: \\nenv = gym.make( \"BreakoutNoFrameskip-v4\" )\\nprint(f\"The original observation space is  {env.observation_space} \")\\n### OUTPUT:\\n>>>The original observation space is  Box(0, 255, (210, 160, 3), uint8)\\nAnd now if you use the wrapper we just created:\\nenv = ConcatObservations(env, 4)\\nprint(f\"The new observation space is  {env.observation_space} \")\\n### OUTPUT:\\nThe new observation space is  Box(0, 255, (4, 210, 160, 3), uint8)\\nYou can see that now a dimension is added—it has four frames, with each frame of size 210 x 160 x 3. \\nYou can use a wrapper to modify the rewards as well. In this case, you use the superclass RewardWrapper . \\nBelow is sample code that can clip the reward to lie within the range [-10, 10]:\\nclass ClippedRewards (gym.RewardWrapper):\\n    def __init__ (self, env):\\n        gym.RewardWrapper.__init__(self, env)\\n        self.reward_range = (- 10,10)\\n    def reward(self, reward):\\n        \"\"\"Clip to {+10, 0, -10} by its sign.\"\"\"\\n        return reward if reward >= - 10 and reward <= 10 else 10 * \\nnp.sign(reward)\\nLet us try using it in the CartPole environment, which has the reward range [−∞,∞]  :\\nenv = ClippedRewards(gym.make( \"CartPole-v0\" ))\\nprint(f\\'Clipped reward range: {env.reward_range} \\')\\nenv.close()\\n### OUTPUT:\\nClipped reward range: (-10, 10)\\nAnother useful application of wrappers is when you want to save the state space as an agent is learning. \\nNormally, an RL agent requires lots of steps for proper training, and as a result, it is not feasible to \\nstore the state space at each step. Instead, we can choose to store after every 500th step (or any other \\nnumber you wish) in the preceding algorithm. OpenAI Gym provides the Wrapper Monitor  class to \\nsave the game as a video. To do so, we need to first import wrappers, then create the environment, \\nand finally use Monitor .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='340210f8-a0a8-4925-9470-ba17456bc45a', embedding=None, metadata={'page_label': '405', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 405\\nBy default, it will store the video of 1, 8, 27, 64, (episode numbers with perfect cubes), and so on, and \\nthen every 1,000th episode; each training, by default, is saved in one folder. The code to do this is:\\nimport gym\\nenv = gym.make( \"Breakout-v0\" )\\nenv = gym.wrappers.Monitor(env, \\'recording\\' , force= True)\\nobservation = env.reset()\\nfor _ in range (1000):\\n    #env.render()\\n    action = env.action_space.sample()\\n    # your agent here (this takes random actions)\\n    observation, reward, done, info = env.step(action)\\n    if done:\\n        observation = env.reset()\\nenv.close()\\nFor Monitor  to work, we require FFmpeg support. We may need to install it depending upon our OS, \\nif it is missing.\\nThis will save the videos in .mp4  format in the folder recording . An important thing to note here is that \\nyou have to set the force=True  option if you want to use the same folder for the next training session.\\nIf you want to train your agent on Google Colab, you will need to add the following drivers \\nto be able to visualize the Gym output:\\n!pip install pyglet\\n!apt-get install -y xvfb python-opengl > /dev/null 2>&1\\n!pip install gym pyvirtualdisplay > /dev/null 2>&1\\nAfter installing the Python virtual display, you need to start it—Gym uses the virtual display \\nto set observations. The following code can help you in starting a display of size 600 x 400:\\nfrom pyvirtualdisplay import Display\\ndisplay = Display(visible= 0, size=( 600, 400))\\ndisplay.start()\\nAnd to be able to play around with Atari games, use:\\n!wget http://www.atarimania.com/roms/Roms.rar\\n!mkdir /content/ROM/\\n!unrar e /content/Roms.rar /content/ROM/\\n!python -m atari_py.import_roms /content/ROM/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0e8cb681-4486-4b40-b5cc-a27fc29aa68d', embedding=None, metadata={'page_label': '406', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 406\\nDeep Q-networks\\nDeep Q-Networks, DQNs  for short, are deep learning neural networks designed to approximate the \\nQ-function (value-state function). They are one of the most popular value-based reinforcement learning \\nalgorithms. The model was proposed by Google’s DeepMind in NeurIPS 2013, in the paper entitled \\nPlaying Atari with Deep Reinforcement Learning. The most important contribution of this paper was \\nthat they used the raw state space directly as input to the network; the input features were not hand-\\ncrafted as done in earlier RL implementations. Also, they could train the agent with exactly the same \\narchitecture to play different Atari games and obtain state-of-the-art results.\\nThis model is an extension of the simple Q-learning algorithm. In Q-learning algorithms, a Q-table is \\nmaintained as a cheat sheet. After each action, the Q-table is updated using the Bellman equation [5]:\\n𝑄𝑄(𝑆𝑆𝑡𝑡,𝐴𝐴𝑡𝑡)=(1−𝛼𝛼)𝑄𝑄(𝑆𝑆𝑡𝑡,𝐴𝐴𝑡𝑡)+𝛼𝛼𝛼𝛼𝛼 𝑡𝑡𝑡𝑡+𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾 𝐴𝐴𝑄𝑄(𝑆𝑆𝑡𝑡𝑡𝑡,𝐴𝐴𝑡𝑡)) \\n𝛼𝛼  is the learning rate, and its value lies in the range [0,1]. The first term represents the component of the \\nold Q value and the second term the target Q value. Q-learning is good if the number of states and the \\nnumber of possible actions is small, but for large state spaces and action spaces, Q-learning is simply \\nnot scalable. A better alternative would be to use a deep neural network as a function approximator, \\napproximating the target Q-function for each possible action. The weights of the deep neural network \\nin this case store the Q-table information. There is a separate output unit for each possible action. The \\nnetwork takes the state as its input and returns the predicted target Q value for all possible actions. \\nThe question arises: how do we train this network, and what should be the loss function? Well, since \\nour network has to predict the target Q value:\\n𝑄𝑄𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡=𝑅𝑅𝑡𝑡𝑡𝑡+𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾 𝐴𝐴𝑄𝑄(𝑆𝑆𝑡𝑡𝑡𝑡,𝐴𝐴𝑡𝑡) \\nthe loss function should try and reduce the difference between the Q value predicted, Qpredicted , and the \\ntarget Q, Q target. We can do this by defining the loss function as:\\n𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑙 𝑙𝑙 𝜋𝜋[𝑄𝑄𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑆𝑆𝑆𝑆𝑆)−𝑄𝑄𝑝𝑝𝑡𝑡𝑡𝑡𝑝𝑝𝑝𝑝𝑝𝑝𝑡𝑡𝑡𝑡𝑝𝑝(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆)] \\nwhere W is the training parameters of our deep Q network, learned using gradient descent, such that \\nthe loss function is minimized.\\nThe following is the general architecture of a DQN. The network takes the n -dimensional state as input \\nand outputs the Q  value of each possible action in the m -dimensional action space. Each layer (including \\nthe input) can be a convolutional layer (if we are taking the raw pixels as input, convolutional layers \\nmake more sense) or a dense layer:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b3b77d2-69f0-4a20-a256-40c9ecc0681b', embedding=None, metadata={'page_label': '407', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 407\\nFigure 11.7: The figure shows a simple DQN network, the input layer taking State vector S, and the \\noutput predicting Q for all possible actions for the state\\nIn the next section, we will try training a DQN. Our agent task will be to stabilize a pole on a cart. The \\nagent can move the cart left or right to maintain balance.\\nDQN for CartPole\\nCartPole is a classic OpenAI problem with continuous state space and discrete action space. In it, a \\npole is attached by an un-actuated joint to a cart; the cart moves along a frictionless track. The goal is \\nto keep the pole standing on the cart by moving the cart left or right. A reward of +1 is given for each \\ntime step the pole is standing. Once the pole is more than 15 degrees from the vertical, or the cart \\nmoves beyond 2.4 units from the center, the game is over:\\nFigure 11.8: A screenshot from the CartPole Gym environment', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c72f92ec-d755-4088-b7d4-a7b2a8296940', embedding=None, metadata={'page_label': '408', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 408\\nYou can check the leaderboard of OpenAI Gym for some cool entries for the CartPole environment: \\nhttps://github.com/openai/gym/wiki/Leaderboard#cartpole-v0 .\\nWe start with importing the necessary modules. We require gym, obviously, to provide us with the \\nCartPole environment, and tensorflow  to build our DQN network. Besides these, we need the random  \\nand numpy  modules:\\nimport random\\nimport gym\\nimport math\\nimport numpy as np\\nfrom collections import deque\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.optimizers import Adam\\nWe set up the global values for the maximum episodes for which we will be training the agent ( EPOCHS ), \\nthe threshold value when we consider the environment solved ( THRESHOLD ), and a bool to indicate if we \\nwant to record the training or not ( MONITOR ). Please note that as per the official OpenAI documentation, \\nthe CartPole environment is considered solved when the agent is able to maintain the pole in the \\nvertical position for 195 time steps (ticks). In the following code, for the sake of time, we have reduced \\nthe THRESHOLD  to 45:\\nEPOCHS = 1000\\nTHRESHOLD = 45\\nMONITOR = True\\nNow let us build our DQN. We declare a class DQN and in its __init__()  function declare all the \\nhyperparameters and our model. We are also creating the environment inside the DQN class. As you \\ncan see, the class is quite general, and you can use it to train any Gym environment whose state space \\ninformation can be encompassed in a 1D array:\\nclass DQN():\\n    def __init__ (self, env_string, batch_size= 64):\\n        self.memory = deque(maxlen= 100000)\\n        self.env = gym.make(env_string)\\n        input_size = self.env.observation_space.shape[ 0]\\n        action_size = self.env.action_space.n\\n        self.batch_size = batch_size\\n        self.gamma = 1.0\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5009746a-5f82-4a41-b181-158609d136e4', embedding=None, metadata={'page_label': '409', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 409\\n        alpha= 0.01\\n        alpha_decay= 0.01\\n        if MONITOR: self.env = gym.wrappers.Monitor(self.env,\\n        \\'data/\\'+env_string, force= True)\\n        \\n        # Init model\\n        self.model = Sequential()\\n        self.model.add(Dense( 24, input_dim=input_size,\\n        activation= \\'tanh\\'))\\n        self.model.add(Dense( 48, activation= \\'tanh\\'))\\n        self.model.add(Dense(action_size, activation= \\'linear\\' ))\\n        self.model. compile(loss=\\'mse\\', optimizer=Adam(lr=alpha,\\n        decay=alpha_decay))\\nThe DQN that we have built is a three-layered perceptron; in the following output, you can see the \\nmodel summary. We use the Adam optimizer with learning rate decay:\\nModel: \"sequential\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n dense (Dense)               (None, 24)                120       \\n                                                                 \\n dense_1 (Dense)             (None, 48)                1200      \\n                                                                 \\n dense_2 (Dense)             (None, 2)                 98        \\n                                                                 \\n=================================================================\\nTotal params: 1,418\\nTrainable params: 1,418\\nNon-trainable params: 0\\n_________________________________________________________________\\nThe variable list self.memory  will contain our experience replay buffer. We need to add a method for \\nsaving the <S,A,R,S’> tuple into the memory and a method to get random samples from it in batches to \\ntrain the agent. We perform these two functions by defining the class methods remember  and replay :\\ndef remember (self, state, action, reward, next_state, done):\\n        self.memory.append((state, action, reward, next_state, done))\\ndef replay(self, batch_size):\\n        x_batch, y_batch = [], []\\n        minibatch = random.sample(self.memory, min(len(self.memory),', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16e5bbd7-3722-48fe-bf59-0b47be437d96', embedding=None, metadata={'page_label': '410', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 410\\n        batch_size))\\n        for state, action, reward, next_state, done in minibatch:\\n             y_target = self.model.predict(state)\\n             y_target[ 0][action] = reward if done else reward + self.gamma * \\nnp.max(self.model.predict(next_state)[ 0])\\n             x_batch.append(state[ 0])\\n             y_batch.append(y_target[ 0])\\n        \\n        self.model.fit(np.array(x_batch), np.array(y_batch),\\n        batch_size= len(x_batch), verbose= 0)\\nOur agent will use the epsilon-greedy policy when choosing the action. This is implemented in the \\nfollowing method:\\ndef choose_action (self, state, epsilon):\\n        if np.random.random() <= epsilon:\\n            return self.env.action_space.sample()\\n        else:\\n            return np.argmax(self.model.predict(state))\\nNext, we write a method to train the agent. We define two lists to keep track of the scores. First, we \\nfill the experience replay buffer and then we choose some samples from it to train the agent and hope \\nthat the agent will slowly learn to do better:\\ndef train(self):\\n    scores = deque(maxlen= 100)\\n    avg_scores = []\\n    for e in range (EPOCHS):\\n        state = self.env.reset()\\n        state = self.preprocess_state(state)\\n        done = False\\n        i = 0\\n        while not done:\\n            action = self.choose_action(state,self.epsilon)\\n            next_state, reward, done, _ = self.env.step(action)\\n            next_state = self.preprocess_state(next_state)\\n            self.remember(state, action, reward, next_state, done)\\n            state = next_state\\n            self.epsilon = max(self.epsilon_min,\\n            self.epsilon_decay*self.epsilon) # decrease epsilon', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4dd9783f-d81e-43ed-8a47-21898bbecd23', embedding=None, metadata={'page_label': '411', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 11 411\\n            i += 1\\n        scores.append(i)\\n        mean_score = np.mean(scores)\\n        avg_scores.append(mean_score)\\n        if mean_score >= THRESHOLD and e >= 100:\\n            print('Ran {} episodes. Solved after {} trials \\uf0fc'.format (e, e - \\n100))\\n            return avg_scores\\n        if e % 100 == 0:\\n            print('[Episode {}] - Mean survival time over last 100 episodes was \\n{} ticks.' .format(e, mean_score))\\n    self.replay(self.batch_size)\\n    print('Did not solve after {} episodes :(' .format(e))\\n    return avg_scores\\nNow that all necessary functions are done, we just need one more helper function to reshape the state \\nof the CartPole environment so that the input to the model is in the correct shape. The state of the \\nenvironment is described by four continuous variables: cart position ([-2.4-2.4]), cart velocity, pole \\nangle ([-41.8o-41.8o]), and pole velocity :\\ndef preprocess_state (self, state):\\n    return np.reshape(state, [ 1, self.input_size])\\nLet us now instantiate our agent for the CartPole environment and train it:\\nenv_string = 'CartPole-v0'\\nagent = DQN(env_string)\\nscores = agent.train()\\n[Episode 0] - Mean survival time over last 100 episodes was 28.0 ticks.\\n[Episode 100] - Mean survival time over last 100 episodes was 15.71 ticks.\\n[Episode 200] - Mean survival time over last 100 episodes was 27.81 ticks.\\nRan 259 episodes. Solved after 159 trials \\uf0fc\\nLet’s plot the average reward as the agent learns:\\nimport matplotlib.pyplot as plt\\nplt.plot(scores)\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a56a6fb7-040d-4a95-8f54-7313d569ae8f', embedding=None, metadata={'page_label': '412', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 412\\nFigure 11.9 shows the agent being trained on my system. The agent was able to achieve our set threshold \\nof 45 in 254 steps:\\nFigure 11.9: Average agent reward plot\\nOnce the training is done, you can close the environment:\\nagent.env.close()\\nYou can see that starting with no information about how to balance the pole, the agent, using a DQN, \\nis able to balance the pole for more and more time (on average) as it learns. Starting from a blank slate, \\nthe agent is able to build information/knowledge to fulfill the required goal. Remarkable!\\nDQN to play a game of Atari\\nIn the preceding section, we trained a DQN to balance a pole in CartPole. It was a simple problem, \\nand thus we could solve it using a perceptron model. But imagine if the environment state was just \\nthe CartPole visual as we humans see it. With raw pixel values as the input state space, our previous \\nDQN will not work. What we need is a convolutional neural network. Next, we build one based on the \\nseminal paper on DQNs, Playing Atari with Deep Reinforcement Learning.\\nMost of the code will be similar to the DQN for CartPole, but there will be significant changes in the \\nDQN network itself, and how we preprocess the state that we obtain from the environment.\\nFirst, let us see the change in the way state space is processed. Figure 11.10 shows one of the Atari \\ngames, Breakout:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3372887d-315c-4dd7-8663-d97d79f95f70', embedding=None, metadata={'page_label': '413', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 413\\nFigure 11.10: A screenshot of the Atari game, Breakout\\nNow, if you look at the image, not all of it contains relevant information: the top part has redundant \\ninformation about the score, the bottom part has unnecessary blank space, and the image is colored. \\nTo reduce the burden on our model, it is best to remove the unnecessary information, so we crop the \\nimage, convert it to grayscale, and make it a square of size 84 × 84 (as in the paper). Here is the code \\nto preprocess the input raw pixels:\\ndef preprocess_state (self, img):\\n    img_temp = img[ 31:195]  # Choose the important area of the image\\n    img_temp = tf.image.rgb_to_grayscale(img_temp)\\n    img_temp = tf.image.resize(img_temp, [self.IM_SIZE, self.IM_SIZE],\\n    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\\n    img_temp = tf.cast(img_temp, tf.float32)\\n    return img_temp[:,:, 0]\\nAnother important issue is that just by looking at the image at one time step, how can the agent know \\nwhether the ball is going up or down? One way could be to use LSTM along with a CNN to keep a record \\nof the past and hence the ball movement. The paper, however, used a simple technique. Instead of a \\nsingle state frame, it concatenated the state space for the past four time steps together as one input \\nto the CNN; that is, the network sees four past frames of the environment as its input. The following \\nis the code for combining the present and previous states:\\ndef combine_images (self, img1, img2):\\n    if len(img1.shape) == 3 and img1.shape[ 0] == self.m:\\n        im = np.append(img1[ 1:,:, :],np.expand_dims(img2, 0), axis= 2)\\n        return tf.expand_dims(im, 0)\\n    else:\\n        im = np.stack([img1]*self.m, axis = 2)\\n        return tf.expand_dims(im, 0)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6518aa0-c980-45ba-876c-0e4862eab5e5', embedding=None, metadata={'page_label': '414', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Reinforcement Learning 414\\nThe model was defined in the __init__  function. We modify the function to now have a CNN with an \\ninput of (84 × 84 × 4) representing four state frames each of size 84 × 84:\\ndef __init__ (self, env_string,batch_size= 64, IM_SIZE = 84, m = 4):\\n    self.memory = deque(maxlen= 5000)\\n    self.env = gym.make(env_string)\\n    input_size = self.env.observation_space.shape[ 0]\\n    action_size = self.env.action_space.n\\n    self.batch_size = batch_size\\n    self.gamma = 1.0\\n    self.epsilon = 1.0\\n    self.epsilon_min = 0.01\\n    self.epsilon_decay = 0.995\\n    self.IM_SIZE = IM_SIZE\\n    self.m = m\\n    \\n    \\n    alpha= 0.01\\n    alpha_decay= 0.01\\n    if MONITOR: self.env = gym.wrappers.Monitor(self.env, '../data/' +env_\\nstring, force= True)\\n    \\n    # Init model\\n    self.model = Sequential()\\n    self.model.add( Conv2D( 32, 8, ( 4,4), activation= 'relu',padding= 'valid', \\ninput_shape=(IM_SIZE, IM_SIZE, m)))\\n    self.model.add( Conv2D( 64, 4, ( 2,2), activation= 'relu',padding= 'valid'))\\n    self.model.add( Conv2D( 64, 3, ( 1,1), activation= 'relu',padding= 'valid'))\\n    self.model.add(Flatten())\\n    self.model.add(Dense( 512, activation= 'elu'))\\n    self.model.add(Dense(action_size, activation= 'linear' ))\\n    self.model. compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay))\\nLastly, we will need to make a minor change in the train  function. We will need to call the new \\npreprocess  function, along with the combine_images  function to ensure that four frames are \\nconcatenated:\\ndef train(self):\\n    scores = deque(maxlen= 100)\\n    avg_scores = []\\n    \\n    for e in range (EPOCHS):\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06148b74-6b9a-4f41-a308-cc23b06f3c65', embedding=None, metadata={'page_label': '415', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 11 415\\n        state = self.env.reset()\\n        state = self.preprocess_state(state)\\n        state = self.combine_images(state, state)\\n        done = False\\n        i = 0\\n        while not done:\\n            action = self.choose_action(state,self.epsilon)\\n            next_state, reward, done, _ = self.env.step(action)\\n            next_state = self.preprocess_state(next_state)\\n            next_state = self.combine_images(next_state, state)\\n            #print(next_state.shape)\\n            self.remember(state, action, reward, next_state, done)\\n            state = next_state\\n            self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.\\nepsilon) # decrease epsilon\\n            i += reward\\n        scores.append(i)\\n        mean_score = np.mean(scores)\\n        avg_scores.append(mean_score)\\n        if mean_score >= THRESHOLD and e >= 100:\\n            print('Ran {} episodes. Solved after {} trials \\uf0fc'.format (e, e - \\n100))\\n            return avg_scores\\n        if e % 100 == 0:\\n            print('[Episode {}] - Score over last 100 episodes was \\n{}.'.format(e, mean_score))\\n        self.replay(self.batch_size)\\n    \\n    print('Did not solve after {} episodes :(' .format(e))\\n    return avg_scores\\nThat’s all. We can now train the agent for playing Breakout. The complete code is available on GitHub \\nrepository ( https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-\\n3rd-edition/tree/main/Chapter_11 ) of this chapter in the file DQN_Atari_v2.ipynb .\\nDQN variants\\nAfter the unprecedented success of DQNs, the interest in RL increased and many new RL algorithms \\ncame into being. Next, we see some of the algorithms that are based on DQNs. They all use DQNs as \\nthe base and build upon it.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e814009c-8d5a-4b19-b4a9-d1d0864f28de', embedding=None, metadata={'page_label': '416', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 416\\nDouble DQN\\nIn DQNs, the agent uses the same Q  value to both select and evaluate an action. This can cause a \\nmaximization bias in learning. For example, let us consider that for a state, S , all possible actions have \\ntrue Q values of zero. Now, our DQN estimates will have some values above and some values below \\nzero, and since we are choosing the action with the maximum Q  value and later evaluating the Q  value \\nof each action using the same (maximized) estimated value function, we are overestimating Q—or in \\nother words, our agent is over-optimistic. This can lead to unstable training and a low-quality policy. To \\ndeal with this issue, Hasselt et al. from DeepMind proposed the Double DQN algorithm in their paper \\nDeep Reinforcement Learning with Double Q-Learning. In Double DQN, we have two Q-networks with the \\nsame architecture but different weights. One of the Q-networks is used to determine the action using \\nthe epsilon-greedy policy and the other is used to determine its value (Q-target).\\nIf you recall in DQNs, the Q-target was given by:\\n𝑄𝑄𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡=𝑅𝑅𝑡𝑡𝑡𝑡+𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾 𝐴𝐴𝑄𝑄(𝑆𝑆𝑡𝑡𝑡𝑡,𝐴𝐴𝑡𝑡) \\nHere, the action A was selected using the same DQN, Q(S,A; W), where W is the training parameters of \\nthe network; that is, we are writing the Q  value function along with its training parameter to emphasize \\nthe difference between vanilla DQNs and Double DQN:\\n𝑄𝑄𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡=𝑅𝑅𝑡𝑡𝑡𝑡+𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾 𝐴𝐴𝑄𝑄(𝑆𝑆𝑡𝑡𝑡𝑡,𝛾𝛾𝑎𝑎𝑎𝑎𝛾𝛾𝛾𝛾𝛾𝛾𝑡𝑡𝑄𝑄(𝑆𝑆,𝑆𝑆𝑆𝑆𝑆)𝑆𝑆𝑆) \\nIn Double DQN, the equation for the target will now change. Now, the DQN Q(S,A;W) is used for \\ndetermining the action and the DQN Q(S,A;W’) is used for calculating the target (notice the different \\nweights). So, the preceding equation will change to:\\n𝑄𝑄𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡=𝑅𝑅𝑡𝑡𝑡𝑡+𝛾𝛾𝛾𝛾𝛾𝛾𝛾𝛾 𝐴𝐴𝑄𝑄(𝑆𝑆𝑡𝑡𝑡𝑡,𝛾𝛾𝑎𝑎𝑎𝑎𝛾𝛾𝛾𝛾𝛾𝛾𝑡𝑡𝑄𝑄(𝑆𝑆,𝑆𝑆𝑆𝑆𝑆)𝑆𝑆𝑆𝑊) \\nThis simple change reduces the overestimation and helps us to train the agent faster and more reliably.\\nDueling DQN\\nThis architecture was proposed by Wang et al. in their paper Dueling Network Architectures for Deep \\nReinforcement Learning in 2015. Like the DQN and Double DQN, it is also a model-free algorithm.\\nDueling DQN decouples the Q-function into the value function and advantage function. The value \\nfunction, which we discussed earlier, represents the value of the state independent of any action. The \\nadvantage function, on the other hand, provides a relative measure of the utility (advantage/goodness) \\nof action A in the state S. The Dueling DQN uses convolutional networks in the initial layers to extract \\nthe features from raw pixels. However, in the later stages, it is separated into two different networks, \\none approximating the value and another approximating the advantage. This ensures that the network \\nproduces separate estimates for the value function and the advantage function:\\n𝑄𝑄(𝑆𝑆𝑆𝑆𝑆)=𝑆𝑆(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆)+𝑉𝑉𝜋𝜋(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆) \\nHere, 𝜃𝜃  is an array of the training parameters of the shared convolutional network (it is shared by both \\nV and A), and 𝛼𝛼  and 𝛽𝛽  are the training parameters for the Advantage and Value estimator networks. \\nLater, the two networks are recombined using an aggregating layer to estimate the Q value.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a983410-5a56-462c-b796-54b2234c64d1', embedding=None, metadata={'page_label': '417', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 417\\nIn Figure 11.11, you can see the architecture of Dueling DQN:\\nFigure 11.11: Visualizing the architecture of a Dueling DQN\\nYou may be wondering, what is the advantage of doing all of this? Why decompose Q if we will just \\nbe putting it back together? Well, decoupling the value and advantage functions allows us to know \\nwhich states are valuable, without having to take into account the effect of each action for each state. \\nThere are many states that, irrespective of the action taken, are good or bad states: for example, \\nhaving breakfast with your loved ones in a good resort is always a good state, and being admitted to \\na hospital emergency ward is always a bad state. Thus, separating value and advantage allows one \\nto get a more robust approximation of the value function. Next, you can see a figure from the paper \\nhighlighting how in the Atari game Enduro, the value network learns to pay attention to the road, and \\nthe advantage network learns to pay attention only when there are cars immediately in front, so as \\nto avoid a collision:\\nFigure 11.12: In the Atari game Enduro, the value network learns to pay attention to the road (red \\nspot), and the advantage network focuses only when other vehicles are immediately in front. Image \\nsource: https://arxiv.org/pdf/1511.06581.pdf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4859d342-65ed-44c6-94f1-3c7ef61a3a56', embedding=None, metadata={'page_label': '418', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 418\\nThe aggregate layer is implemented in a manner that allows one to recover both V and A from the \\ngiven Q. This is achieved by enforcing that the advantage function estimator has zero advantage at \\nthe chosen action:\\n𝑄𝑄(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆 )=𝑆𝑆(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆)+𝑉𝑉𝜋𝜋(𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆)−𝑚𝑚𝑚𝑚𝑚𝑚𝛼𝛼′𝜖𝜖|𝐴𝐴|𝑆𝑆(𝑆𝑆𝑆𝑆𝑆′𝑆𝑆𝑆𝑆𝑆𝑆) \\nIn the paper, Wang et al. reported that the network is more stable if the max operation is replaced \\nby the average operation. This is so because the speed of change in advantage is now the same as the \\nchange in average, instead of the optimal (max) value.\\nRainbow\\nRainbow is the current state-of-the-art DQN variant. Technically, to call it a DQN variant would be \\nwrong. In essence, it is an ensemble of many DQN variants combined together into a single algorithm. \\nIt modifies the distributional RL [6] loss to multi-step loss and combines it with Double DQN using a \\ngreedy action. Quoting from the paper:\\nRainbow combines six different RL algorithms:\\n• N-step returns\\n• Distributional state-action value learning\\n• Dueling networks\\n• Noisy networks\\n• Double DQN\\n• Prioritized experience replay\\nTill now, we’ve considered value-based reinforcement learning algorithms. In the next section, we \\nwill learn about policy-based reinforcement learning algorithms. \\nDeep deterministic policy gradient\\nThe DQN and its variants have been very successful in solving problems where the state space is \\ncontinuous and action space is discrete. For example, in Atari games, the input space consists of \\nraw pixels, but actions are discrete—[up , down, left, right, no-op ]. How do we solve a problem with \\ncontinuous action space? For instance, say an RL agent driving a car needs to turn its wheels: this \\naction has a continuous action space. The network architecture is a dueling network architecture adapted for use with return \\ndistributions. The network has a shared representation 𝑓𝑓𝑓𝑓(𝑠𝑠) , which is then fed into a \\nvalue stream 𝑣𝑣𝜂𝜂  with N atoms outputs, and into an advantage stream 𝑎𝑎𝑎𝑎  with N atoms×N actions \\noutputs, where a1ξ(fξ(s), a)will denote the output corresponding to atom i and action \\na. For each atom z i, the value and advantage streams are aggregated, as in Dueling \\nDQN, and then passed through a softmax layer to obtain the normalised parametric \\ndistributions used to estimate the returns’ distributions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a2ef2e7-70ee-4ce5-90c6-f8c05d1d10d4', embedding=None, metadata={'page_label': '419', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 419\\nOne way to handle this situation is by discretizing the action space and continuing with a DQN or its \\nvariants. However, a better solution would be to use a policy gradient algorithm. In policy gradient \\nmethods, the policy 𝜋𝜋(𝐴𝐴𝐴𝐴𝐴)  is approximated directly.\\nA neural network is used to approximate the policy; in the simplest form, the neural network learns \\na policy for selecting actions that maximize the rewards by adjusting its weights using the steepest \\ngradient ascent, hence the name: policy gradients.\\nIn this section, we will focus on the Deep Deterministic Policy Gradient (DDPG ) algorithm, another \\nsuccessful RL algorithm by Google’s DeepMind in 2015. DDPG is implemented using two networks; \\none called the actor network and the other called the critic network.\\nThe actor network approximates the optimal policy deterministically, that is, it outputs the most \\npreferred action for any given input state. In essence, the actor is learning. The critic on the other \\nhand evaluates the optimal action value function using the actor’s most preferred action. Before going \\nfurther, let us contrast this with the DQN algorithm that we discussed in the preceding section. In \\nFigure 11.13, you can see the general architecture of DDPG:\\nFigure 11.13: Architecture of the DDPG model\\nOn the left-hand side of Figure 11.13 is the critic network, it takes as input the state vector, S , and \\naction taken, A. The output of the network is the Q value for that state and action. The right-hand \\nfigure shows the actor network. It takes as input the state vector, S, and predicts the optimum action, \\nA, to be taken. In the figure, we have shown both the actor and critic to be of four layers. This is only \\nfor demonstration purposes. \\nThe actor network outputs the most preferred action; the critic takes as input both the input state and \\naction taken and evaluates its Q value. To train the critic network, we follow the same procedure as \\nwith a DQN; that is, we try to minimize the difference between the estimated Q value and the target \\nQ value. The gradient of the Q value over actions is then propagated back to train the actor network. \\nSo, if the critic is good enough, it will force the actor to choose actions with optimal value functions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ce57059-83fc-42a0-8c1c-8e0d533ee8c2', embedding=None, metadata={'page_label': '420', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reinforcement Learning 420\\nSummary\\nReinforcement learning has in recent years seen a lot of progress. To summarize all of that in a single \\nchapter is not possible. However, in this chapter, we focused on the recent successful RL algorithms. \\nThe chapter started by introducing the important concepts in the RL field, its challenges, and the \\nsolutions to move forward. Next, we delved into two important RL algorithms: the DQN and DDPG \\nalgorithms. Toward the end of this chapter, we covered important topics in the field of deep learning.\\nIn the next chapter, we will move on to applying what we have learned to production.\\nReferences\\n1. MIT Technology Review covers OpenAI experiments on reinforcement learning:  https://\\nwww.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-\\ncooperation-after-hide-and-seek-games/\\n2. Coggan, Melanie. (2014). Exploration and Exploitation in Reinforcement Learning. Research \\nsupervised by Prof. Doina Precup, CRA-W DMP Project at McGill University.\\n3. Lin, Long-Ji. (1993). Reinforcement learning for robots using neural networks. No. CMU-CS-93-103. \\nCarnegie-Mellon University Pittsburgh PA School of Computer Science.\\n4. Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. (2015). Prioritized Experience \\nReplay. arXiv preprint arXiv:1511.05952\\n5. Sutton R., Barto A. Chapter 4, Reinforcement Learning. MIT Press: https://web.stanford.edu/\\nclass/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\\n6. Dabney W ., Rowland M., Bellemare M G., and Munos R. (2018). Distributional Reinforcement \\nLearning with Quantile Regression. In Thirty-Second AAAI Conference on Artificial Intelligence.\\n7. Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W ., Horgan, D., Piot, \\nB., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in Deep Reinforcement \\nLearning. In Thirty-Second AAAI Conference on Artificial Intelligence.\\n8. Details about different environments can be obtained from https://www.gymlibrary.ml/  \\n9. Wiki pages are maintained for some environments at https://github.com/openai/gym/wiki\\n10. Details regarding installation instructions and dependencies can be obtained from https://\\ngithub.com/openai/gym\\n11. Link to the paper by DeepMind, Asynchronous Methods for Deep Reinforcement Learning : https://\\narxiv.org/pdf/1602.01783.pdf  \\n12. This is a blog post by Andrej Karpathy on reinforcement learning: http://karpathy.github.\\nio/2016/05/31/rl/\\n13. Glorot X. and Bengio Y. (2010). Understanding the difficulty of training deep feedforward neural \\nnetworks. Proceedings of the Thirteenth International Conference on Artificial Intelligence \\nand Statistics: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\\n14. A good read on why RL is still hard to crack: https://www.alexirpan.com/2018/02/14/rl-\\nhard.html\\n15. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. (2015). \\nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d4d26eb-7549-4eb5-95f7-4948f3646c1d', embedding=None, metadata={'page_label': '421', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11 421\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='109ae700-6992-4214-b214-bf85ffaecc3d', embedding=None, metadata={'page_label': '422', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='099b8237-921d-43e7-b8a7-7c74fa2f7dbe', embedding=None, metadata={'page_label': '423', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12\\nProbabilistic TensorFlow\\nUncertainty is a fact of life; whether you are doing a classification task or a regression task, it is import-\\nant to know how confident your model is in its prediction. Till now, we have covered the traditional \\ndeep learning models, and while they are great at many tasks, they are not able to handle uncertainty. \\nInstead, they are deterministic in nature. In this chapter, you will learn how to leverage TensorFlow \\nProbability to build models that can handle uncertainty, specifically probabilistic deep learning models \\nand Bayesian networks. The chapter will include:\\n• TensorFlow Probability\\n• Distributions, events, and shapes in TensorFlow Probability \\n• Bayesian networks using TensorFlow Probability\\n• Understand uncertainty in machine learning models\\n• Model aleatory and epistemic uncertainty using TensorFlow Probability\\nLet’s start with first understanding TensorFlow Probability.\\nTensorFlow Probability\\nTensorFlow Probability (TFP ), a part of the TensorFlow ecosystem, is a library that provides tools \\nfor developing probabilistic models. It can be used to perform probabilistic reasoning and statistical \\nanalysis. It is built over TensorFlow and provides the same computational advantage. All the code files for this chapter can be found at https://packt.link/dltfchp12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69b45bef-5d26-49e8-a3b4-e12a4570c3f9', embedding=None, metadata={'page_label': '424', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 424\\nFigure 12.1 shows the major components constituting TensorFlow Probability:\\nFigure 12.1: Different components of TensorFlow Probability \\nAt the root, we have all numerical operations supported by TensorFlow, specifically the LinearOperator  \\nclass (part of tf.linalg ) – it contains all the methods that can be performed on a matrix, without \\nthe need to actually materialize the matrix. This provides computationally efficient matrix-free com-\\nputations. TFP includes a large collection of probability distributions and their related statistical \\ncomputations. It also has tfp.bijectors , which offers a wide range of transformed distributions.\\nTensorFlow Probability also provides JointDistribution , which allows the user to draw a joint sam-\\nple and compute a joint log-density (log probability density function). The standard TFP distributions \\nwork on tensors, but JointDistribution  works on the structure of tensors. tfp.layers  provides \\nneural network layers that can be used to extend the standard TensorFlow layers and add uncertainty \\nto them. And finally, it provides a wide range of tools for probabilistic inference. In this chapter, we \\nwill go through some of these functions and classes; let us first start with installation. To install TFP \\nin your working environment, just run:\\npip install tensorflow-probability\\nLet us have some fun with TFP. To use TFP, we will need to import it. Additionally, we are going to do \\nsome plots. So, we import some additional modules:Bijectors encapsulate the change of variables for probability density. That is, when one \\ntransforms one variable from space A to space B, we need a way to map the probability \\ndistributions of the variables as well. Bijectors provide us with all the tools needed to do so.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad025d91-2a50-4c0d-8371-00c605cb42b5', embedding=None, metadata={'page_label': '425', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 12 425\\nimport matplotlib.pyplot as plt\\nimport tensorflow_probability as tfp\\nimport functools, inspect, sys\\nNext, we explore the different classes of distributions available in tfp.distributions :\\ntfd = tfp.distributions\\ndistribution_class =  tfp.distributions.Distribution\\ndistributions = [name for name, obj in inspect.getmembers(tfd)\\n                if inspect.isclass(obj) and issubclass (obj, distribution_\\nclass)]\\nprint(distributions)\\nHere is the output:\\n['Autoregressive', 'BatchBroadcast', 'BatchConcat', 'BatchReshape', 'Bates', \\n'Bernoulli', 'Beta', 'BetaBinomial', 'BetaQuotient', 'Binomial', 'Blockwise', \\n'Categorical', 'Cauchy', 'Chi', 'Chi2', 'CholeskyLKJ', 'ContinuousBernoulli', \\n'DeterminantalPointProcess', 'Deterministic', 'Dirichlet', \\n'DirichletMultinomial', 'Distribution', 'DoublesidedMaxwell', 'Empirical', \\n'ExpGamma', 'ExpInverseGamma', 'ExpRelaxedOneHotCategorical', 'Exponential', \\n'ExponentiallyModifiedGaussian', 'FiniteDiscrete', 'Gamma', 'GammaGamma', \\n'GaussianProcess', 'GaussianProcessRegressionModel', 'GeneralizedExtremeValue', \\n'GeneralizedNormal', 'GeneralizedPareto', 'Geometric', 'Gumbel', 'HalfCauchy', \\n'HalfNormal', 'HalfStudentT', 'HiddenMarkovModel', 'Horseshoe', 'Independent', \\n'InverseGamma', 'InverseGaussian', 'JohnsonSU', 'JointDistribution', \\n'JointDistributionCoroutine', 'JointDistributionCoroutineAutoBatched', \\n'JointDistributionNamed', 'JointDistributionNamedAutoBatched', \\n'JointDistributionSequential', 'JointDistributionSequentialAutoBatched', \\n'Kumaraswamy', 'LKJ', 'LambertWDistribution', 'LambertWNormal', 'Laplace', \\n'LinearGaussianStateSpaceModel', 'LogLogistic', 'LogNormal', 'Logistic', \\n'LogitNormal', 'MarkovChain', 'Masked', 'MatrixNormalLinearOperator', \\n'MatrixTLinearOperator', 'Mixture', 'MixtureSameFamily', 'Moyal', \\n'Multinomial', 'MultivariateNormalDiag', 'MultivariateNormalDiagPlusLowRank', \\n'MultivariateNormalDiagPlusLowRankCovariance', \\n'MultivariateNormalFullCovariance', 'MultivariateNormalLinearOperator', \\n'MultivariateNormalTriL', 'MultivariateStudentTLinearOperator', \\n'NegativeBinomial', 'Normal', 'NormalInverseGaussian', 'OneHotCategorical', \\n'OrderedLogistic', 'PERT', 'Pareto', 'PixelCNN', 'PlackettLuce', \\n'Poisson', 'PoissonLogNormalQuadratureCompound', 'PowerSpherical', \\n'ProbitBernoulli', 'QuantizedDistribution', 'RelaxedBernoulli', \\n'RelaxedOneHotCategorical', 'Sample', 'SigmoidBeta', 'SinhArcsinh', 'Skellam', \\n'SphericalUniform', 'StoppingRatioLogistic', 'StudentT', 'StudentTProcess', \\n'StudentTProcessRegressionModel', 'TransformedDistribution', 'Triangular', \\n'TruncatedCauchy', 'TruncatedNormal', 'Uniform', 'VariationalGaussianProcess', \\n'VectorDeterministic', 'VonMises', 'VonMisesFisher', 'Weibull', \\n'WishartLinearOperator', 'WishartTriL', 'Zipf']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86d8667f-e774-4e94-898f-82eaf1f968d4', embedding=None, metadata={'page_label': '426', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 426\\nYou can see that a rich range of distributions is available in TFP. Let us now try one of the distributions:\\nnormal = tfd.Normal(loc= 0., scale= 1.)\\nThis statement declares that we want to have a normal distribution with mean  (loc) zero and standard \\ndeviation ( scale ) 1. We can generate random samples following this distribution using the sample \\nmethod. The following code snippet generates such N samples and plots them:\\ndef plot_normal (N):\\n  samples = normal.sample(N)\\n  sns.distplot(samples)\\n  plt.title( f\"Normal Distribution with zero mean, and 1 std. dev {N} samples\" )\\n  plt.show()\\nYou can see that as N increases, the plot follows a nice normal distribution:\\nN=100\\nN=1000\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ad8e27e-1c44-4b1d-98f6-abd3fd582a45', embedding=None, metadata={'page_label': '427', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 427\\nN=10000\\nFigure 12.2: Normal distribution from randomly generated samples of sizes 100, 1,000, and 10,000. \\nThe distribution has a mean of zero and a standard deviation of one\\nLet us now explore the different distributions available with TFP.\\nTensorFlow Probability distributions\\nEvery distribution in TFP has a shape, batch, and event size associated with it. The shape is the sample \\nsize; it represents independent and identically distributed draws or observations. Consider the normal \\ndistribution that we defined in the previous section:\\nnormal = tfd.Normal(loc= 0., scale= 1.)\\nThis defines a single normal distribution, with mean zero and standard deviation one. When we use \\nthe sample  function, we do a random draw from this distribution.\\nNotice the details regarding batch_shape  and event_shape  if you print the object normal :\\nprint(normal)\\n>>> tfp.distributions.Normal( \"Normal\" , batch_shape=[], event_shape=[], \\ndtype=float32)\\nLet us try and define a second normal  object, but this time, loc and scale  are lists:\\nnormal_2 = tfd.Normal(loc=[ 0., 0.], scale=[ 1., 3.])\\nprint(normal_2)\\n>>> tfp.distributions.Normal( \"Normal\" , batch_shape=[ 2], event_shape=[], \\ndtype=float32)\\nDid you notice the change in batch_shape ? Now, if we draw a single sample from it, we will draw \\nfrom two normal distributions, one with a mean of zero and standard deviation of one, and the other \\nwith a mean of zero and standard deviation of three. Thus, the batch shape determines the number \\nof observations from the same distribution family. The two normal distributions are independent; \\nthus, it is a batch of distributions of the same family.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e2487b53-4fb3-4780-ac5c-0eb8d2b6dff6', embedding=None, metadata={'page_label': '428', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 428\\nWhat if we need a single normal distribution that is dependent on two variables, each with a different \\nmean? This is made possible using MultivariateNormalDiag , and this influences the event shape – it \\nis the atomic shape of a single draw or observation from this distribution:\\nnormal_3 = tfd.MultivariateNormalDiag(loc = [[ 1.0, 0.3]])\\nprint(normal_3)\\n>>> tfp.distributions.MultivariateNormalDiag( \"MultivariateNormalDiag\" , batch_\\nshape=[1], event_shape=[ 2], dtype=float32)\\nWe can see that in the above output the event_shape  has changed.\\nUsing TFP distributions\\nOnce you have defined a distribution, you can do a lot more. TFP provides a good range of functions \\nto perform various operations. We have already used the Normal  distribution and sample  method. The \\nsection above also demonstrated how we can use TFP for creating univariate, multivariate, or indepen -\\ndent distribution/s. TFP provides many important methods to interact with the created distributions. \\nSome of the important ones include:\\n• sample(n) : It samples n observations from the distribution.\\n• prob(value) : It provides probability (discrete) or probability density (continuous) for the value.\\n• log_prob(values) : Provides log probability or log-likelihood for the values.\\n• mean() : It gives the mean of the distribution.\\n• stddev() : It provides the standard deviation of the distribution.\\nCoin Flip Example\\nLet us now use some of the features of TFP to describe data by looking at an example: the standard \\ncoin-flipping example we are familiar with from our school days. We know that if we flip a coin, there \\nare only two possibilities – we can have either a head or a tail. Such a distribution, where we have only \\ntwo discrete values, is called a Bernoulli distribution. So let us consider different scenarios:\\nScenario 1\\nA fair coin with a 0.5 probability of heads and 0.5 probability of tails.\\nLet us create the distribution:\\ncoin_flip = tfd.Bernoulli(probs= 0.5, dtype=tf.int32)You can have batches of the same type of distribution family, like in the preceding exam-\\nple of having two normal distributions. You cannot create a batch of, say, a normal and \\na Gaussian distribution.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a719a0ea-3eba-4184-96cb-60dad448ecf2', embedding=None, metadata={'page_label': '429', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 429\\nNow get some samples:\\ncoin_flip_data = coin_flip.sample( 2000)\\nLet us visualize the samples:\\nplt.hist(coin_flip_data)\\nFigure 12.3: Distribution of heads and tails from 2,000 observations\\nYou can see that we have both heads and tails in equal numbers; after all, it is a fair coin. The proba -\\nbility of heads and tails as 0.5:\\ncoin_flip.prob( 0) ## Probability of tail\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 0.5>\\nScenario 2\\nA biased coin with a 0.8 probability of heads and 0.2 probability of tails.\\nNow, since the coin is biased, with the probability of heads being 0.8, the distribution would be \\ncreated using:\\nbias_coin_flip = tfd.Bernoulli(probs= 0.8, dtype=tf.int32)\\nNow get some samples:\\nbias_coin_flip_data = bias_coin_flip.sample( 2000)\\nLet us visualize the samples:\\nplt.hist(bias_coin_flip_data)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60025601-d7d9-44ff-8563-c6f48e4f9f83', embedding=None, metadata={'page_label': '430', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Probabilistic TensorFlow 430\\nFigure 12.4: Distribution of heads and tails from 2,000 coin flips of a biased coin\\nWe can see that now heads are much larger in number than tails. Thus, the probability of tails is no \\nlonger 0.5:\\nbias_coin_flip.prob( 0) ## Probability of tail\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 0.19999999 >\\nYou will probably get a number close to 0.2.\\nScenario 3 \\nTwo coins with one biased toward heads with a 0.8 probability, and the other biased toward heads \\nwith a 0.6 probability.\\nNow, we have two independent coins. Since the coins are biased, with the probabilities of heads being \\n0.8 and 0.6 respectively, we create a distribution using:\\ntwo_bias_coins_flip = tfd.Bernoulli(probs=[ 0.8, 0.6], dtype=tf.int32)\\nNow get some samples:\\ntwo_bias_coins_flip_data = two_bias_coins_flip.sample( 2000)\\nLet us visualize the samples:\\nplt.hist(two_bias_coins_flip_data[:, 0], alpha= 0.8, label= 'Coin 1' )\\nplt.hist(two_bias_coins_flip_data[:, 1], alpha= 0.5, label= 'Coin 2' )\\nplt.legend(loc= 'center' )\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18f08644-35e5-4097-b8fc-b8f62a8506d5', embedding=None, metadata={'page_label': '431', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 431\\nFigure 12.5: Distribution of heads and tails from 2,000 flips for two independent coins\\nThe bar in blue corresponds to Coin 1, and the bar in orange corresponds to Coin 2. The brown part \\nof the graphs is the area where the results of the two coins overlap. You can see that for Coin 1, the \\nnumber of heads is much larger as compared to Coin 2, as expected.\\nNormal distribution\\nWe can use the Bernoulli distribution where the data can have only two possible discrete values: \\nheads and tails, good and bad, spam and ham, and so on. However, a large amount of data in our daily \\nlives is continuous in range, with the normal distribution being very common. So let us also explore \\ndifferent normal distributions.\\nMathematically, the probability density function of a normal distribution can be expressed as:\\n𝑓𝑓(𝑥𝑥𝑥𝑥𝑥𝑥𝑥𝑥)=1\\n𝑥𝑥√2𝜋𝜋exp\\u2061(−1\\n2(𝑥𝑥−𝑥𝑥\\n𝑥𝑥)2\\n) \\nwhere 𝜇𝜇  is the mean of the distribution, and 𝜎𝜎  is the standard deviation.\\nIn TFP, the parameter loc represents the mean and the parameter scale  represents the standard \\ndeviation. Now, to illustrate the use of how we can use distribution, let us consider that we want to \\nrepresent the weather data of a location for a particular season, say summer in Delhi, India.\\nUnivariate normal\\nWe can think that weather depends only on temperature. So, by having a sample of temperature in \\nthe summer months over many years, we can get a good representation of data. That is, we can have \\na univariate normal distribution. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac7677c8-3e97-48b9-9a5a-056243365e31', embedding=None, metadata={'page_label': '432', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 432\\nNow, based on weather data, the average high temperature in the month of June in Delhi is 35 degrees \\nCelsius, with a standard deviation of 4 degrees Celsius. So, we can create a normal distribution using:\\ntemperature = tfd.Normal(loc= 35, scale = 4)\\nGet some observation samples from it:\\ntemperature_data = temperature.sample( 1000)\\nAnd let us now visualize it:\\nsns.displot(temperature_data, kde= True)\\nFigure 12.6: Probability density function for the temperature of Delhi in the month of June\\nIt would be good to verify if the mean and standard deviation of our sample data is close to the values \\nwe described.\\nUsing the distribution, we can find the mean and standard deviation using:\\ntemperature.mean()\\n# output\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 35.0>\\ntemperature.stddev()\\n# output\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 4.0>', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd49f981-81bc-4822-862d-db24dd8fcc14', embedding=None, metadata={'page_label': '433', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 433\\nAnd from the sampled data, we can verify using:\\ntf.math.reduce_mean(temperature_data) \\n# output\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 35.00873 >\\ntf.math.reduce_std(temperature_data)\\n# output\\n>>> <tf.Tensor: shape=(), dtype=float32, numpy= 3.9290223 >\\nThus, the sampled data is following the same mean and standard deviation.\\nMultivariate distribution\\nAll is good so far. I show my distribution to a friend working in meteorology, and he says that using \\nonly temperature is not sufficient; the humidity is also important. So now, each weather point de -\\npends on two parameters – the temperature of the day and the humidity of the day. This type of data \\ndistribution can be obtained using the MultivariateNormalDiag  distribution class, as defined in TFP:\\nweather = tfd.MultivariateNormalDiag(loc = [ 35, 56], scale_diag=[ 4, 15])\\nweather_data = weather.sample( 1000)\\nplt.scatter(weather_data[:, 0], weather_data[:, 1], color= \\'blue\\', alpha= 0.4)\\nplt.xlabel( \"Temperature Degree Celsius\" )\\nplt.ylabel( \"Humidity %\" )\\nFigure 12.7, shows the multivariate normal distribution of two variables, temperature and humidity, \\ngenerated using TFP:\\nFigure 12.7: Multivariate normal distribution with the x-axis representing temperature and the y-axis \\nhumidity', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c51bd4a-5007-44cc-a847-710fa4efc405', embedding=None, metadata={'page_label': '434', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 434\\nUsing the different distributions and bijectors available in TFP, we can generate synthetic data that \\nfollows the same joint distribution as real data to train the model.\\nBayesian networks\\nBayesian Networks (BNs ) make use of the concepts from graph theory, probability, and statistics to \\nencapsulate complex causal relationships. Here, we build a Directed Acyclic Graph ( DAG ), where \\nnodes, called factors (random variables), are connected by the arrows representing cause-effect rela -\\ntionships. Each node represents a variable with an associated probability (also called a Conditional \\nProbability Table (CPT)). The links tell us about the dependence of one node over another. Though \\nthey were first proposed by Pearl in 1988, they have regained attention in recent years. The main cause \\nof this renowned interest in BNs is that standard deep learning models are not able to represent the \\ncause-effect relationship.\\nTheir strength lies in the fact that they can be used to model uncertainties combined with expert \\nknowledge and data. They have been employed in diverse fields for their power to do probabilistic \\nand causal reasoning. At the heart of the Bayesian network is Bayes’ rule:\\n𝑃𝑃(𝐴𝐴|𝐵𝐵)=𝑃𝑃(𝐵𝐵|𝐴𝐴)𝑃𝑃(𝐴𝐴)\\n𝑃𝑃(𝐵𝐵) \\nBayes’ rule is used to determine the joint probability of an event given certain conditions. The simplest \\nway to understand the BN is that the BN can determine the causal relationship between the hypothesis \\nand evidence. There is some unknown hypothesis H, about which we want to assess the uncertainty \\nand make some decisions. We start with some prior belief about hypothesis H, and then based on \\nevidence E, we update our belief about H.\\nLet us try to understand it by example. We consider a very standard example: a garden with grass and \\na sprinkler. Now, using common sense, we know that if the sprinkler is on, the grass is wet. Let us now \\nreverse the logic: what if you come back home and find that the grass is wet, what is the probability \\nthat the sprinkler is on, and what is the probability that it actually rained? Interesting, right? Let us \\nadd further evidence – you find that the sky is cloudy. Now, what do you think is the reason for the \\ngrass being wet?\\nThis sort of reasoning based on evidence is encompassed by BNs in the form of DAGs, also called \\ncausal graphs – because they provide an insight into the cause-effect relationship.\\nTo model the problem, we make use of the JointDistributionCoroutine  distribution class. This \\ndistribution allows both the sampling of data and computation of the joint probability from a single \\nmodel specification. Let us make some assumptions to build the model:\\n• The probability that it is cloudy is 0.2\\n• The probability that it is cloudy and it rains is 0.8, and the probability that it is not cloudy but \\nit rains is 0.1\\n• The probability that it is cloudy and the sprinkler is on is 0.1, and the probability that it is not \\ncloudy and the sprinkler is on is 0.5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab3f763d-a092-42c7-8d95-312e1e23bb9e', embedding=None, metadata={'page_label': '435', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 435\\n• Now, for the grass, we have four possibilities:\\nSprinkler Rain Grass Wet\\nF F 0\\nF T 0.8\\nT F 0.9\\nT T 0.99\\nTable 12.1: The conditional probability table for the Sprinkler-Rain-Grass scenario\\nFigure 12.8 shows the corresponding BN DAG:\\nFigure 12.8: Bayesian Network for our toy problem\\nThis information can be represented by the following model:\\nRoot = tfd.JointDistributionCoroutine.Root\\ndef model():\\n  # generate the distribution for cloudy weather\\n  cloudy = yield Root(tfd.Bernoulli(probs= 0.2, dtype=tf.int32))\\n  # define sprinkler probability table\\n  sprinkler_prob = [ 0.5, 0.1]\\n  sprinkler_prob = tf.gather(sprinkler_prob, cloudy)\\n  sprinkler = yield tfd.Bernoulli(probs=sprinkler_prob, dtype=tf.int32)\\n  # define rain probability table\\n  raining_prob = [ 0.1, 0.8]\\n  raining_prob = tf.gather(raining_prob, cloudy)\\n  raining = yield tfd.Bernoulli(probs=raining_prob, dtype=tf.int32)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9434cbff-605a-4270-8aac-bc6bdf03c7fe', embedding=None, metadata={'page_label': '436', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 436\\n  #Conditional Probability table for wet grass\\n  grass_wet_prob = [[ 0.0, 0.8],\\n                    [ 0.9, 0.99]]\\n  grass_wet_prob = tf.gather_nd(grass_wet_prob, _stack(sprinkler, raining))\\n  grass_wet = yield tfd.Bernoulli(probs=grass_wet_prob, dtype=tf.int32)\\nThe above model will function like a data generator. The Root  function is used to tell the node in the \\ngraph without any parent. We define a few utility functions, broadcast  and stack :\\ndef _conform (ts):\\n  \"\"\"Broadcast all arguments to a common shape.\"\"\"\\n  shape = functools.reduce(\\n      tf.broadcast_static_shape, [a.shape for a in ts])\\n  return  [tf.broadcast_to(a, shape) for a in ts]\\ndef _stack(*ts):\\n  return  tf.stack(_conform(ts), axis=- 1)\\nTo do inferences, we make use of the MarginalizableJointDistributionCoroutine  class, as this \\nallows us to compute marginalized probabilities:\\nd = marginalize.MarginalizableJointDistributionCoroutine(model)\\nNow, based on our observations, we can obtain the probability of other factors.\\nCase 1:\\nWe observe that the grass is wet (the observation corresponding to this is 1 – if the grass was dry, we \\nwould set it to 0), we have no idea about the state of the clouds or the state of the sprinkler (the ob -\\nservation corresponding to an unknown state is set to “marginalize”), and we want to know the prob -\\nability of rain (the observation corresponding to the probability we want to find is set to “tabulate”). \\nConverting this into observations:\\nobservations = [ \\'marginalize\\' , # We don\\'t know the cloudy state\\n                \\'tabulate\\' , # We want to know the probability of rain\\n                \\'marginalize\\' , # We don\\'t know the sprinkler state.\\n                1]             # We observed a wet lawn.\\nNow we get the probability of rain using:\\np = tf.exp(d.marginalized_log_prob(observations))\\np = p / tf.reduce_sum(p)\\nThe result is array([0.27761015, 0.72238994], dtype=float32) , that is, there is a 0.722 probability \\nthat it rained.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5edb478e-bfd5-4039-b2ca-c2ec48b52521', embedding=None, metadata={'page_label': '437', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 12 437\\nCase 2:\\nWe observe that the grass is wet, we have no idea about the state of the clouds or rain, and we want to \\nknow the probability of whether the sprinkler is on. Converting this into observations:\\nobservations = [ 'marginalize' ,  \\n                'marginalize' , \\n                'tabulate' ,  \\n                1]\\nThis results in probabilities array([0.61783344, 0.38216656], dtype=float32) , that is, there is a \\n0.382  probability that the sprinkler is on.\\nCase 3:\\nWhat if we observe that there is no rain, and the sprinkler is off? What do you think is the state of the \\ngrass? Logic says the grass should not be wet. Let us confirm this from the model by sending it the \\nobservations:\\nobservations = [ 'marginalize' ,  \\n                 0,\\n                 0, \\n                'tabulate' ]\\nThis results in the probabilities array([1., 0], dtype=float32) , that is, there is a 100% probability \\nthat the grass is dry, just the way we expected.\\nAs you can see, once we know the state of the parents, we do not need to know the state of the parent’s \\nparents – that is, the BN follows the local Markov property. In the example that we covered here, we \\nstarted with the structure, and we had the conditional probabilities available to us. We demonstrate \\nhow we can do inference based on the model, and how despite the same model and CPDs, the evidence \\nchanges the posterior probabilities.\\nHandling uncertainty in predictions using TensorFlow Probability\\nAt the beginning of this chapter, we talked about the uncertainties in prediction by deep learning mod -\\nels and how the existing deep learning architectures are not able to account for those uncertainties. \\nIn this chapter, we will use the layers provided by TFP to model uncertainty.\\nBefore adding the TFP layers, let us first understand the uncertainties a bit. There are two classes of \\nuncertainty.In Bayesian networks, the structure (the nodes and how they are interconnected) and the \\nparameters (the conditional probabilities of each node) are learned from the data. They \\nare referred to as structured learning and parameter learning respectively. Covering the \\nalgorithms for structured learning and parameter learning are beyond the scope of this \\nchapter.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='475ab984-30ed-412b-a44a-b3d07f9697bc', embedding=None, metadata={'page_label': '438', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 438\\nAleatory uncertainty\\nThis exists because of the random nature of the natural processes. It is inherent uncertainty, present \\ndue to the probabilistic variability. For example, when tossing a coin, there will always be a certain \\ndegree of uncertainty in predicting whether the next toss will be heads or tails. There is no way to \\nremove this uncertainty. In essence, every time you repeat the experiment, the results will have \\ncertain variations.\\nEpistemic uncertainty\\nThis uncertainty comes from a lack of knowledge. There can be various reasons for this lack of knowl-\\nedge, for example, an inadequate understanding of the underlying processes, an incomplete knowledge \\nof the phenomena, and so on. This type of uncertainty can be reduced by understanding the reason, \\nfor example, to get more data, we conduct more experiments.\\nThe presence of these uncertainties increases risk. We require a way to quantify these uncertainties \\nand, hence, quantify the risk.\\nCreating a synthetic dataset\\nIn this section, we will learn how to modify the standard deep neural networks to quantify uncer -\\ntainties. Let us start with creating a synthetic dataset. To create the dataset, we consider that output \\nprediction y depends on input x linearly, as given by the following expression:\\n𝑦𝑦𝑖𝑖= 2.7𝑥𝑥 𝑖𝑖+3+0.74𝜀𝜀 \\nHere, 𝜀𝜀𝜀𝜀𝜀𝜀𝜀𝜀𝜀𝜀   follows a normal distribution with mean zero and standard deviation 1 around x. \\nThe function below will generate this synthetic data for us. Do observe that to generate this data, we \\nmade use of the Uniform  distribution and Normal  distributions available as part of TFP distributions:\\ndef create_dataset (n, x_range):\\n    x_uniform_dist = tfd.Uniform(low=x_range[ 0], high=x_range[ 1])\\n    x = x_uniform_dist.sample(n).numpy() [:, np.newaxis] \\n    y_true = 2.7*x+3\\n    eps_uniform_dist = tfd.Normal(loc= 0, scale= 1)\\n    eps = eps_uniform_dist.sample(n).numpy() [:, np.newaxis] * 0.74*x\\n    y = y_true + eps\\n    return x, y, y_true\\ny_true  is the value without including the normal distributed noise 𝜀𝜀 .\\nNow we use it to create a training dataset and a validation dataset:\\nx_train, y_train, y_true = create_dataset( 2000, [-10, 10])\\nx_val, y_val, _ = create_dataset( 500, [-10, 10])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb668f38-848f-40fb-b20e-de7c86910e00', embedding=None, metadata={'page_label': '439', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 12 439\\nThis will give us 2,000 datapoints for training and 500 datapoints for validation. Figure 12.9 shows \\nthe plots of the two datasets, with ground truth (the value of y in the absence of any noise) in the \\nbackground:\\nFigure 12.9: Plot of the synthetic dataset\\nBuilding a regression model using TensorFlow\\nWe can build a simple Keras model to perform the task of regression on the synthetic dataset created \\nin the preceding section:\\n# Model Architecture\\nmodel = Sequential([Dense( 1, input_shape=( 1,))])\\n# Compile \\nmodel.compile(loss='mse', optimizer= 'adam')\\n# Fit\\nmodel.fit(x_train, y_train, epochs= 100, verbose= 1)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bdfafe4-c478-4d78-b39b-bc1ffa07e156', embedding=None, metadata={'page_label': '440', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 440\\nLet us see how good the fitted model works on the test dataset:\\nFigure 12.10: Ground truth and fitted regression line\\nIt was a simple problem, and we can see that the fitted regression line almost overlaps the ground \\ntruth. However, there is no way to tell the uncertainty of predictions.\\nProbabilistic neural networks for aleatory uncertainty\\nWhat if instead of linear regression, we build a model that can fit the distribution? In our synthetic \\ndataset, the source of aleatory uncertainty is the noise, and we know that our noise follows a normal \\ndistribution, which is characterized by two parameters: the mean and standard deviation. So, we can \\nmodify our model to predict the mean and standard deviation distributions instead of actual y values. \\nWe can accomplish this using either the IndependentNormal  TFP layer or the DistributionLambda  \\nTFP layer. The following code defines the modified model architecture:\\nmodel = Sequential([Dense( 2, input_shape = ( 1,)),\\n    tfp.layers.DistributionLambda( lambda t: tfd.Normal(loc=t[..., : 1], \\nscale=0.3+tf.math. abs(t[...,1:])))\\n])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61862a0d-bb39-4f7f-ad45-f3d3bdb6be06', embedding=None, metadata={'page_label': '441', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 12 441\\nWe will need to make one more change. Earlier, we predicted the y value; therefore, the mean square \\nerror loss was a good choice. Now, we are predicting the distribution; therefore, a better choice is the \\nnegative log-likelihood as the loss function:\\n# Define negative loglikelihood loss function\\ndef neg_loglik (y_true, y_pred):\\n    return -y_pred.log_prob(y_true)\\nLet us now train this new model:\\nmodel.compile(loss=neg_loglik, optimizer= 'adam')\\n# Fit\\nmodel.fit(x_train, y_train, epochs= 500, verbose= 1)\\nSince now our model returns a distribution, we require the statistics mean and standard deviation \\nfor the test dataset:\\n# Summary Statistics\\ny_mean = model(x_test).mean()\\ny_std = model(x_test).stddev()\\nNote that the predicted mean now corresponds to the fitted line in the first case. Let us now see the plots:\\nfig = plt.figure(figsize = ( 20, 10))\\nplt.scatter(x_train, y_train, marker= '+', label= 'Training Data' , alpha= 0.5)\\nplt.plot(x_train, y_true, color= 'k', label= 'Ground Truth' )\\nplt.plot(x_test, y_mean, color= 'r', label= 'Predicted Mean' )\\nplt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+ 1*y_std), np.squeeze(y_\\nmean-1*y_std),  alpha= 0.6, label= 'Aleatory Uncertainty (1SD)' )\\nplt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+ 2*y_std), np.squeeze(y_\\nmean-2*y_std),  alpha= 0.4, label= 'Aleatory Uncertainty (2SD)' )\\nplt.title( 'Aleatory Uncertainty' )\\nplt.xlabel( '$x$')\\nplt.ylabel( '$y$')\\nplt.legend()\\nplt.show()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8049fd3-7e19-4374-988f-1a4cac378577', embedding=None, metadata={'page_label': '442', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 442\\nThe following curve shows the fitted line, along with the aleatory uncertainty:\\nFigure 12.11: Modelling aleatory uncertainty using TFP layers\\nYou can see that our model shows less uncertainty near the origin, but as we move further away, the \\nuncertainty increases.\\nAccounting for the epistemic uncertainty\\nIn conventional neural networks, each weight is represented by a single number, and it is updated \\nsuch that the loss of the model with respect to its weight is minimized. We assume that weights so \\nlearned are the optimum weights. But are they? To answer this question, we replace each weight \\nwith a distribution, and instead of learning a single value, we will now make our model learn a set \\nof parameters for each weight distribution. This is accomplished by replacing the Keras Dense  layer \\nwith the DenseVariational  layer. The DenseVariational  layer uses a variational posterior over the \\nweights to represent the uncertainty in their values. It tries to regularize the posterior to be close to \\nthe prior distribution. Hence, to use the DenseVariational  layer, we will need to define two functions, \\none prior generating function and another posterior generating function. We use the posterior and \\nprior functions defined at https://www.tensorflow.org/probability/examples/Probabilistic_\\nLayers_Regression . \\nOur model now has two layers, a DenseVariational  layer followed by a DistributionLambda  layer:\\nmodel = Sequential([\\n  tfp.layers.DenseVariational( 1, posterior_mean_field, prior_trainable, kl_\\nweight=1/x_train.shape[ 0]),\\n  tfp.layers.DistributionLambda( lambda t: tfd.Normal(loc=t, scale= 1)),\\n])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f8cdf3a0-ea24-4167-889d-b738e15b92ce', embedding=None, metadata={'page_label': '443', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 12 443\\nAgain, as we are looking for distributions, the loss function that we use is the negative log-likelihood \\nfunction:\\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate= 0.01), loss=negloglik)\\nWe continue with the same synthetic data that we created earlier and train the model:\\nmodel.fit(x_train, y_train, epochs= 100, verbose= 1)\\nNow that the model has been trained, we make the prediction, and to understand the concept of \\nuncertainty, we make multiple predictions for the same input ranges. We can see the difference in \\nvariance in the result in the following graphs:\\nFigure 12.12: Epistemic uncertainty\\nFigure 12.12 shows two graphs, one when only 200 training data points were used to build the model, \\nand the second when 2,000 data points were used to train the model. We can see that when there is \\nmore data, the variance and, hence, the epistemic uncertainty reduces. Here, overall mean refers to the \\nmean of all the predictions (100 in number), and in the case of ensemble mean, we considered only the \\nfirst 15 predictions. All machine learning models suffer from some level of uncertainty in predicting \\noutcomes. Getting an estimate or quantifiable range of uncertainty in the prediction will help AI users \\nbuild more confidence in their AI predictions and will boost overall AI adoption. \\nSummary\\nThis chapter introduced TensorFlow Probability, the library built over TensorFlow to perform proba -\\nbilistic reasoning and statistical analysis. The chapter started with the need for probabilistic reason-\\ning – the uncertainties both due to the inherent nature of data and due to a lack of knowledge. We \\ndemonstrated how to use TensorFlow Probability distributions to generate different data distributions. \\nWe learned how to build a Bayesian network and perform inference. Then, we built Bayesian neural \\nnetworks using TFP layers to take into account aleatory uncertainty. Finally, we learned how to account \\nfor epistemic uncertainty with the help of the DenseVariational  TFP layer. \\nIn the next chapter, we will learn about TensorFlow AutoML frameworks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ff5fd54-e322-4eea-98cd-02837287b043', embedding=None, metadata={'page_label': '444', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Probabilistic TensorFlow 444\\nReferences\\n1. Dillon, J. V ., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., Patton, B., Alemi, A., \\nHoffman, M., and Saurous, R. A. (2017). TensorFlow distributions. arXiv preprint arXiv:1711.10604.\\n2. Piponi, D., Moore, D., and Dillon, J. V . (2020). Joint distributions for TensorFlow probability. arXiv \\npreprint arXiv:2001.11819.\\n3. Fox, C. R. and Ülkümen, G. (2011). Distinguishing Two Dimensions of Uncertainty , in Essays \\nin Judgment and Decision Making, Brun, W ., Kirkebøen, G. and Montgomery, H., eds. Oslo: \\nUniversitetsforlaget.\\n4. Hüllermeier, E. and Waegeman, W . (2021). Aleatoric and epistemic uncertainty in machine learning: \\nAn introduction to concepts and methods. Machine Learning 110, no. 3: 457–506.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab6fedfa-0437-476c-95a5-edd55c2942c0', embedding=None, metadata={'page_label': '445', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13\\nAn Introduction to AutoML\\nThe goal of AutoML is to enable domain experts who are unfamiliar with machine learning technologies \\nto use ML techniques easily.\\nIn this chapter, we will go through a practical exercise using Google Cloud Platform and do quite a bit \\nof hands-on work after briefly discussing the fundamentals.\\nWe will cover:\\n• Automatic data preparation\\n• Automatic feature engineering\\n• Automatic model generation\\n• AutoKeras\\n• Google Cloud AutoML with its multiple solutions for table, vision, text, translation, and video \\nprocessing\\nLet’s begin with an introduction to AutoML.\\nWhat is AutoML?\\nDuring the previous chapters, we introduced several models used in modern machine learning and deep \\nlearning. For instance, we have seen architectures such as dense networks, CNNs, RNNs, autoencoders, \\nand GANs.\\nTwo observations are in order. First, these architectures are manually designed by deep learning experts \\nand are not necessarily easy to explain to non-experts. Second, the composition of these architectures \\nthemselves was a manual process, which involved a lot of human intuition and trial and error.\\nToday, one primary goal of artificial intelligence research is to achieve Artificial General Intelligence \\n(AGI ) – the intelligence of a machine that can understand and automatically learn any type of work or \\nactivity that a human being can do. It should be noted that many researchers do not believe that AGI \\nis achievable because there is not only one form of intelligence but many forms. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebf5b338-95b5-444f-8591-2099ee7fae4d', embedding=None, metadata={'page_label': '446', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 446\\nPersonally, I tend to agree with this view. See https://twitter.com/ylecun/\\nstatus/1526672565233758213  for Yann LeCun’s position on this subject. However, the reality was \\nvery different before AutoML research and industrial applications started. Indeed, before AutoML, \\ndesigning deep learning architectures was very similar to crafting – the activity or hobby of making \\ndecorative articles by hand.\\nTake, for instance, the task of recognizing breast cancer from X-rays. After reading the previous \\nchapters, you will probably think that a deep learning pipeline created by composing several CNNs may \\nbe an appropriate tool for this purpose. That is probably a good intuition to start with. The problem \\nis that it is not easy to explain to the users of your model why a particular composition of CNN works \\nwell within the breast cancer detection domain. Ideally, you want to provide easily accessible deep \\nlearning tools to the domain experts (in this case, medical professionals) without such a tool requiring \\na strong machine learning background.\\nThe other problem is that it is not easy to understand whether or not there are variants (for example, \\ndifferent compositions) of the original manually crafted model that can achieve better results. Ideally, \\nyou want to provide deep learning tools for exploring the space of variants (for example, different \\ncompositions) in a more principled and automatic way.\\nSo, the central idea of AutoML is to reduce the steep learning curve and the huge costs of handcrafting \\nmachine learning solutions by making the whole end-to-end machine learning pipeline more \\nautomated. To this end, we assume that the AutoML pipeline consists of three macro-steps: data \\npreparation, feature engineering, and automatic model generation, as shown in Figure 13.1:\\nFigure 13.1: Three steps of an AutoML pipeline\\nThroughout the initial part of this chapter, we are going to discuss these three steps in detail. Then, \\nwe will focus on Google Cloud AutoML.\\nAchieving AutoML\\nHow can AutoML achieve the goal of end-to-end automatization? Well, you have probably already \\nguessed that a natural choice is to use machine learning – that’s very cool. AutoML uses ML for \\nautomating ML pipelines.\\nWhat are the benefits? Automating the creation and tuning of machine learning end to end offers \\nsimpler solutions, reduces the time to produce them, and ultimately might produce architectures that \\ncould potentially outperform models that were crafted by hand.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b81258ef-b210-4d18-a732-d9ffe268c40f', embedding=None, metadata={'page_label': '447', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 447\\nIs this a closed research area? Quite the opposite. At the beginning of 2022, AutoML is a very open \\nresearch field, which is not surprising, as the initial paper drawing attention to AutoML was published \\nat the end of 2016.\\nAutomatic data preparation\\nThe first stage of a typical machine learning pipeline deals with data preparation (recall the pipeline \\nin Figure 13.1). There are two main aspects that should be taken into account: data cleansing and data \\nsynthesis:\\nData cleansing is about improving the quality of data by checking for wrong data types, missing values, \\nand errors, and by applying data normalization, bucketization, scaling, and encoding. A robust AutoML \\npipeline should automate all of these mundane but extremely important steps as much as possible.\\nData synthesis is about generating synthetic data via augmentation for training, evaluation, and \\nvalidation. Normally, this step is domain-specific. For instance, we have seen how to generate synthetic \\nCIFAR10-like images (Chapter 4) by using cropping, rotation, resizing, and flipping operations. One \\ncan also think about generating additional images or video via GANs (see Chapter 9) and using the \\naugmented synthetic dataset for training. A different approach should be taken for text, where it is \\npossible to train RNNs (Chapter 5) to generate synthetic text or to adopt more NLP techniques such \\nas BERT, Seq2Seq, or Transformers (see Chapter 6) to annotate or translate text across languages and \\nthen translate it back to the original one – another domain-specific form of augmentation.\\nA different approach is to generate synthetic environments where machine learning can occur. This \\nbecame very popular in reinforcement learning and gaming, especially with toolkits such as OpenAI \\nGym, which aims to provide an easy-to-set-up simulation environment with a variety of different \\n(gaming) scenarios.\\nPut simply, we can say that synthetic data generation is another option that should be provided by \\nAutoML engines. Frequently, the tools used are very domain-specific and what works for image or \\nvideo would not necessarily work in other domains such as text. Therefore, we need a (quite) large \\nset of tools for performing synthetic data generation across domains.\\nAutomatic feature engineering\\nFeature engineering is the second step of a typical machine learning pipeline (see Figure 13.1). It \\nconsists of three major steps: feature selection, feature construction, and feature mapping. Let’s look \\nat each of them in turn:\\nFeature selection aims at selecting a subset of meaningful features by discarding those that are making \\nlittle contribution to the learning task. In this context, “meaningful” truly depends on the application \\nand the domain of your specific problem.\\nFeature construction has the goal of building new derived features, starting from the basic ones. \\nFrequently, this technique is used to allow better generalization and to have a richer representation \\nof the data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0a19ff0-77d0-4017-a183-059773d8b2fc', embedding=None, metadata={'page_label': '448', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 448\\nFeature mapping aims at altering the original feature space by means of a mapping function. This \\ncan be implemented in multiple ways; for instance, it can use autoencoders (see Chapter 8), PCA (see \\nChapter 7), or clustering (see Chapter 7).\\nIn short, feature engineering is an art based on intuition, trial and error, and a lot of human experience. \\nModern AutoML engines aim to make the entire process more automated, requiring less human \\nintervention.\\nAutomatic model generation\\nModel generation and hyperparameter tuning is the typical third macro-step of a machine learning \\npipeline (see Figure 13.1).\\nModel generation consists of creating a suitable model for solving specific tasks. For instance, you \\nwill probably use CNNs for visual recognition, and you will use RNNs for either time series analysis \\nor for sequences. Of course, many variants are possible, each of which is manually crafted through a \\nprocess of trial and error and works for very specific domains.\\nHyperparameter tuning happens once the model is manually crafted. This process is generally very \\ncomputationally expensive and can significantly change the quality of the results in a positive way. \\nThat’s because tuning the hyperparameters can help to optimize our model further.\\nAutomatic model generation is the ultimate goal of any AutoML pipeline. How can this be achieved? \\nOne approach consists of generating the model by combining a set of primitive operations including \\nconvolution, pooling, concatenation, skip connections, recurrent neural networks, autoencoders, and \\npretty much all the deep learning models we have encountered throughout this book. These operations \\nconstitute a (typically very large) search space to be explored, and the goal is to make this exploration \\nas efficient as possible. In AutoML jargon, the exploration is called NAS , or Neural Architecture Search. \\nThe seminal paper on AutoML [1] was produced in November 2016. The key idea (see Figure 13.2) is \\nto use reinforcement learning (RL, see Chapter 11). An RNN acts as the controller, and it generates \\nthe model descriptions of candidate neural networks. RL is used to maximize the expected accuracy \\nof the generated architectures on a validation set.\\nOn the CIFAR-10 dataset, this method, starting from scratch, designed a novel network architecture \\nthat rivals the best human-invented architecture in terms of test set accuracy. The CIFAR-10 model \\nachieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-\\nof-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, the model \\ncan compose a novel recurrent cell that outperforms the widely used LSTM cell (see Chapter 9) and \\nother state-of-the-art baselines. The cell achieves a test set perplexity of 62.4 on the Penn Treebank, \\nwhich is 3.6 better than the previous state-of-the-art model.\\nThe key outcome of the paper is shown in Figure 13.2. A controller network based on RNNs produces a \\nsample architecture A with probability p. This candidate architecture A is trained by a child network to \\nget a candidate accuracy R. Then a gradient of p is computed and scaled by R to update the controller. \\nThis reinforcement learning operation is computed in a cycle a number of times. The process of \\ngenerating an architecture stops if the number of layers exceeds a certain value. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d5674c4-d061-4e66-a3ec-8eee1a6c10a4', embedding=None, metadata={'page_label': '449', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 449\\nThe details of how an RL-based policy gradient method is used by the controller RNN to generate \\nbetter architectures are in [1]. Here we emphasize the fact that NAS uses a meta-modeling algorithm \\nbased on Q-learning with an ϵ-greedy exploration strategy and with experience replay (see Chapter \\n11) to explore the model search space:\\nFigure 13.2: NAS with recurrent neural networks\\nSince the original paper in late 2016, a Cambrian explosion of model generation techniques has been \\nobserved. Initially, the goal was to generate the entire model in one single step. Later, a cell-based \\napproach was proposed where the generation is divided into two macro-steps: first, a cell structure is \\nautomatically built, and then a predefined number of discovered cells are stacked together to generate \\nan entire end-to-end architecture [2]. This Efficient Neural Architecture Search ( ENAS) delivers strong \\nempirical performance using significantly fewer GPU hours compared with all existing automatic \\nmodel design approaches, and notably, is 1,000x less computationally expensive than standard neural \\narchitecture search (in 2018). Here, the primary ENAS goal is to reduce the search space via hierarchical \\ncomposition. Variants of the cell-based approach have been proposed including pure hierarchical \\nmethods where higher-level cells are generated by incorporating lower-level cells iteratively.\\nA completely different approach to NAS is to use transfer learning (see Chapter 5) to transfer the learning \\nof an existing neural network into a new neural network in order to speed up the design [3]. In other \\nwords, we want to use transfer learning in AutoML.\\nAnother approach is based on Genetic Programming (GP ) and Evolutionary Algorithms (EAs), where \\nthe basic operations constituting the model search space are encoded into a suitable representation, \\nand then this encoding is gradually mutated to progressively better models in a way that resembles \\nthe genetic evolution of living beings [4].\\nHyperparameter tuning consists of finding the optimal combination of hyperparameters both related \\nto learning optimization (batch size, learning rate, and so on) and model-specific ones (kernel size; \\nnumber of feature maps and so on for CNNs; or number of neurons for dense or autoencoder networks, \\nand so on). Again, the search space can be extremely large. There are three approaches generally used: \\nBayesian optimization, grid search, and random search.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c979dd13-f389-425e-8bf3-f2c8cc7e712a', embedding=None, metadata={'page_label': '450', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 450\\nBayesian optimization builds a probability model of the objective function and uses it to select the \\nmost promising hyperparameters to evaluate in the true objective function.\\nGrid search divides the search space into a  discrete grid of values and tests all the possible combinations \\nin the grid. For instance, if there are three hyperparameters and a grid with only two candidate values \\nfor each of them, then a total of 2 x 3 = 6 combinations must be checked. There are also hierarchical \\nvariants of grid search, which progressively refine the grid for regions of the search space and provide \\nbetter results. The key idea is to use a coarse grid first, and after finding a better grid region, implement \\na finer grid search on that region.\\nRandom search  performs a random sampling of the parameter search space, and this simple approach \\nhas been proven to work very well in many situations [5].\\nNow that we have briefly discussed the fundamentals, we will do quite a bit of hands-on work on \\nGoogle Cloud. Let’s start.\\nAutoKeras\\nAutoKeras [6] provides functions to automatically search for architecture and hyperparameters of \\ndeep learning models. The framework uses Bayesian optimization for efficient neural architecture \\nsearch. You can install the alpha version by using pip:\\npip3 install autokeras # for 1.19 version\\nThe architecture is explained in Figure 13.3 [6]: \\nFigure 13.3: AutoKeras system overview\\nThe architecture follows these steps:\\n1. The user calls the API.\\n2. The searcher generates neural architectures on the CPU.\\n3. Real neural networks with parameters are built on RAM from the neural architectures.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fcf36804-880a-49b3-b1ad-3bcbe6b05979', embedding=None, metadata={'page_label': '451', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 451\\n4. The neural network is copied to the GPU for training.\\n5. The trained neural networks are saved on storage devices.\\n6. The searcher is updated based on the training results.\\nSteps 2 to 6 will repeat until a time limit is reached.\\nGoogle Cloud AutoML and Vertex AI\\nGoogle Cloud AutoML ( https://cloud.google.com/automl/ ) is a full suite of products for image, video, \\nand text processing. AutoML can be used to train high-quality custom machine learning models with \\nminimal effort and machine learning expertise.\\nVertex AI brings together the Google Cloud services for building ML under one, unified UI and API. \\nIn Vertex AI, you can now easily train, compare, test, and deploy models. Then you can serve a model \\nwith sophisticated ways to monitor and run experiments (see https://cloud.google.com/vertex-ai ).\\nAs of 2022, the suite consists of the following components, which do not require you to know how the \\ndeep learning networks are shaped internally:\\nVertex AI\\n• Unified platform to help you build, deploy, and scale more AI models\\nStructured data\\n• AutoML Tables: Automatically build and deploy state-of-the-art machine learning models on \\nstructured data\\nSight\\n• AutoML Image: Derive insights from object detection and image classification, in the cloud \\nor at the edge\\n• AutoML Video: Enable powerful content discovery and engaging video experiences\\nLanguage\\n• AutoML Text: Reveal the structure and meaning of text through machine learning\\n• AutoML Translation: Dynamically detect and translate between languages\\nIn the remainder of this chapter, we will review three AutoML solutions: AutoML Tables, AutoML \\nText, and AutoML Video.\\nUsing the Google Cloud AutoML Tables solution\\nLet’s see an example of using Google Cloud AutoML Tables. We’ll aim to import some tabular data \\nand train a classifier on that data; we’ll use some marketing data from a bank. Note that this and the \\nfollowing examples might be charged by Google according to different usage criteria (please check \\nonline for the latest cost estimation – see https://cloud.google.com/products/calculator/ ). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7963e8d7-1433-4044-9fef-eb22a0849c34', embedding=None, metadata={'page_label': '452', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 452\\nThe first step required is to enable the Vertex AI API:\\nFigure 13.4: Enable the Vertex AI API\\nWe can then select the TABULAR  dataset from the console (see Figure 13.5). The name of the dataset \\nis bank-marketing.csv :\\nFigure 13.5: Selecting TABULAR datasets', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fbdd9e73-c744-4b41-a901-a174fc79bf8b', embedding=None, metadata={'page_label': '453', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 453\\nOn the next screen, we indicate that we want to load the data from CSV:\\nFigure 13.6: AutoML Tables – loading data from a CSV file\\nNext, we can train a new model, as shown in Figure 13.7:\\nFigure 13.7: Training a new model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c4e10c3-e1b6-4025-9f14-9bff407b0220', embedding=None, metadata={'page_label': '454', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 454\\nSeveral options for training are offered for Classification and Regression:\\nFigure 13.8: Options offered for Classification and Regression\\nLet’s select the target as the Deposit column. The dataset is described at https://archive.ics.uci.\\nedu/ml/datasets/bank+marketing . The data is related to direct marketing campaigns (phone calls) \\nof a Portuguese banking institution. The classification goal is to predict if the client will subscribe to \\na term deposit.\\nSince the selected column is categorical data, AutoML Tables will build a classification model. This will \\npredict the target from the classes in the selected column. The classification is binary: 1 represents \\na negative outcome, meaning that a deposit is not made at the bank; 2 represents a positive outcome, \\nmeaning that a deposit is made at the bank, as shown in Figure 13.9:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a256e1d-f881-4283-9dd4-ea0f332252dd', embedding=None, metadata={'page_label': '455', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 455\\nFigure 13.9: Training a new model with Target column set to Deposit\\nWe can then inspect the dataset (see Figure 13.10), which gives us the opportunity to inspect the dataset \\nwith several features, such as names, type, missing values, distinct values, invalid values, correlation with \\nthe target, mean, and standard deviation:\\nFigure 13.10: AutoML Tables – inspecting the dataset', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b04b6f8e-0e1d-4224-a189-f54b39fc04c7', embedding=None, metadata={'page_label': '456', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 456\\nIt is now time to train the model by using the Train tab. First let’s give a budget for training, as shown \\nin Figure 13.11:\\nFigure 13.11: Setting up the budget for training\\nIn this example, we accept 3  hours as our training budget. During this time, you can go and take \\na coffee whilst AutoML works on your behalf (see Figure 13.12). The training budget is a number \\nbetween 1 and 72 for the maximum number of node hours to spend training your model. If your model \\nstops improving before then, AutoML Tables will stop training and you’ll only be charged the money \\ncorresponding to the actual node budget used:\\nFigure 13.12: AutoML Tables training process', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='31c47a7e-f09b-4715-85ae-35bba17768ec', embedding=None, metadata={'page_label': '457', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 457\\nWhile training, we can check the progress, as shown in Figure 13.13:\\nFigure 13.13: Checking the training progress\\nAfter less than one hour, Google AutoML should send an email to our inbox:\\nFigure 13.14: AutoML Tables: training is concluded, and an email is sent to my account', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd24de46-2b73-462b-baff-d38a699bd0c4', embedding=None, metadata={'page_label': '458', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 458\\nClicking on the suggested URL, it is possible to see the results of our training. The AutoML-generated \\nmodel reached an accuracy of 94% (see Figure 13.15). Remember that accuracy is the fraction \\nof classification predictions produced by the model that were correct on a test, set which is held \\nautomatically. The log-loss (for example, the cross-entropy between the model predictions and the \\nlabel values) is also provided. In the case of log-loss, a lower value indicates a higher-quality model:\\nFigure 13.15: AutoML Tables – analyzing the results of our training\\nIn addition, the Area Under the Receiver Operating Characteristic Curve (AUC ROC ) is represented. \\nThis ranges from zero to one, and a higher value indicates a higher-quality model. This statistic \\nsummarizes an AUC ROC curve, which is a graph showing the performance of a classification model \\nat all classification thresholds. The True Positive Rate (TPR ) (also known as “recall”) is:\\n𝑇𝑇𝑇𝑇𝑇𝑇 = 𝑇𝑇𝑇𝑇\\n𝑇𝑇𝑇𝑇+𝐹𝐹𝐹𝐹 \\nwhere TP is the number of true positives and FN is the number of false negatives. The False Positive \\nRate (FPR ) is:\\n𝐹𝐹𝐹𝐹𝐹𝐹 𝐹𝐹𝐹𝐹𝐹𝐹\\n𝐹𝐹𝐹𝐹+𝑇𝑇𝑇𝑇 \\nwhere FP  is the number of false positives and TN is the number of true negatives.\\nA ROC curve plots TPR vs. FPR at different classification thresholds. In Figure 13.16 you will see the \\nArea Under the Curve (AUC ) for one threshold of a ROC curve:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22948a09-d397-46e0-b3b5-5c3b365ba37c', embedding=None, metadata={'page_label': '459', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 459\\nFigure 13.16: AutoML Tables – deep dive on the results of our training\\nIt is possible to deep dive into the evaluation and access the confusion matrix (see Figure 13.17):\\nFigure 13.17: AutoML Tables – additional deep dive on the results of our training\\nNote that manually crafted models available in https://www.kaggle.com/uciml/adult-census-\\nincome/kernels  get to an accuracy of ˜86-90%. Therefore, our model generated with AutoML is \\ndefinitively a very good result!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f1029f3-9aa7-46ca-a65e-d2f928db45eb', embedding=None, metadata={'page_label': '460', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 460\\nWe can also have a look at the importance of each feature in isolation, as shown in Figure 13.18:\\nFigure 13.18: Specific importance of each feature considered in isolation\\nIf we are happy with our results, we can then deploy the model in production via DEPLOY & TEST (see \\nFigure 13.19). We can decide to create a Docker container deployable at the edge or we can simply use \\nan endpoint. Let’s go for this option and just use the default setting for each available choice:\\nFigure 13.19: AutoML Tables – deploying in production', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='667569b5-fc1c-4951-9dc0-f213bea72415', embedding=None, metadata={'page_label': '461', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 461\\nThen it is possible to make online predictions of income by using a REST API (see https://en.wikipedia.\\norg/wiki/Representational_state_transfer ), using this command for the example we’re looking \\nat in this chapter, as shown in Figure 13.20:\\nFigure 13.20: AutoML Tables – querying the deployed model in production\\nPut simply, we can say that Google Cloud ML is very focused on simplicity of use and efficiency for \\nAutoML. Let’s summarize the main steps required (see Figure 13.21):\\n1. The dataset is imported.\\n2. Your dataset schema and labels are defined.\\n3. The input features are automatically recognized.\\n4. AutoML performs magic by automatically doing feature engineering, creating a model, and \\ntuning the hyperparameters.\\n5. The automatically built model can then be evaluated.\\n6. The model is then deployed in production.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='727e15ec-d2b0-490c-af62-6e55958e55eb', embedding=None, metadata={'page_label': '462', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 462\\nOf course, it is possible to repeat the steps 2-6 by changing the schema and the definition of the labels.\\nFigure 13.21: AutoML Tables – the main steps required\\nIn this section, we have seen an example of AutoML focused on ease of use and efficiency. The progress \\nmade is shown in Faes et al. [7], quoting the paper:\\n”We show, to our knowledge, a first of its kind automated design and implementation \\nof deep learning models for health-care application by non-AI experts, namely physi-\\ncians. Although comparable performance to expert-tuned medical image classification \\nalgorithms was obtained in internal validations of binary and multiple classification \\ntasks, more complex challenges, such as multilabel classification, and external valida-\\ntion of these models was insufficient. We believe that AI might advance medical care \\nby improving efficiency of triage to subspecialists and the personalisation of medicine \\nthrough tailored prediction models. The automated approach to prediction model de -\\nsign improves access to this technology, thus facilitating engagement by the medical \\ncommunity and providing a medium through which clinicians can enhance their un-\\nderstanding of the advantages and potential pitfalls of AI integration.”', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='018ae49b-b632-46c2-a598-59e6afb172f3', embedding=None, metadata={'page_label': '463', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 463\\nIn this case, Cloud AutoML Tables has been used. So, let’s look at another example.\\nUsing the Google Cloud AutoML Text solution\\nIn this section, we are going to build a classifier using AutoML. Let’s create a dataset for text from the \\nVertex AI console. We want to focus on the task of single-label classification:\\nFigure 13.22: AutoML Text classification – creating a dataset\\nWe are going to use a dataset already available online (the happy moments dataset is stored in cloud-\\nml-data/NL-classification/happiness.csv ), load it into a dataset named happiness, and perform \\nsingle-label classification (as shown in Figure 13.23). This can take several minutes or more. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffdcf20a-cc67-4c77-b650-e02b1281b4c8', embedding=None, metadata={'page_label': '464', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 464\\nWe will be emailed once processing completes:\\nFigure 13.23: AutoML Text classification – creating the dataset\\nOnce the dataset is loaded, you should be able to see that each text fragment is annotated with one \\ncategory out of seven, as shown in Figure 13.24:\\nFigure 13.24: AutoML Text classification – a sample of categories', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='84f74c5a-1667-4762-9b8b-536ca3e41a36', embedding=None, metadata={'page_label': '465', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 465\\nIt is now time to start training the model:\\nFigure 13.25: AutoML Text classification – start training\\nBy the end, the model is built, and it achieves a good precision of 90.2% and recall of 86.7%:\\nFigure 13.26: AutoML Text classification – precision and recall\\nWe can also have a look at the precision-recall curve and precision-recall by threshold (see Figure \\n13.27). These curves can be used to calibrate the classifier, calibrating on the threshold (based on the \\nprediction probabilities that are greater than the threshold):\\nFigure 13.27: Precision-recall and Precision-recall by threshold', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52495e18-bb05-43f7-a79e-827d50928c3f', embedding=None, metadata={'page_label': '466', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 466\\nThe confusion matrix is shown in Figure 13.28:\\nFigure 13.28: Confusion matrix for the text classification problem\\nUsing the Google Cloud AutoML Video solution\\nIn this solution, we are going to automatically build a new model for video classification. The intent is \\nto be able to sort different video segments into various categories (or classes) based on their content. \\nThe first step is to create the dataset, as shown in Figure 13.29:\\nFigure 13.29: AutoML Video intelligence – a classification problem', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fa42218-72e3-4ca6-89a5-6a99592995b8', embedding=None, metadata={'page_label': '467', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 467\\nWe are going to use a collection of about 5,000 videos available in a demo already stored in a GCP \\nbucket on automl-video-demo-data/hmdb_split1_5classes_all.csv , as shown in Figure 13.30:\\nFigure 13.30. Importing the demo dataset\\nAs usual, importing will take a while and we will be notified when it is done with an email. Once the \\nvideos are imported, we can preview them with their associated categories:\\nFigure 13.31: AutoML Video intelligence – imported video preview', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6ca6fcd-024c-4634-8fcc-ba89d6f5f604', embedding=None, metadata={'page_label': '468', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 468\\nWe can now start to build a model. There are a number of options including training with AutoML, using \\nAutoML at the edge for models to be exported at the edge, and custom models built on TensorFlow. \\nLet’s use the default, as shown in Figure 13.32:\\nFigure 13.32: AutoML Video intelligence – warning to get more videos\\nIn this case, we decide to run an experiment training with a few labels and divide the dataset into \\n20% training and 80% testing:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d33dda8-9ef3-4183-880a-2443a14eeafb', embedding=None, metadata={'page_label': '469', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 469\\nFigure 13.33: Test and Training dataset split\\nOnce the model is trained, you can access the results from the console (Figure 13.34). In this case, we \\nachieved a precision of 99.5% and a recall of 99.5% even though we were using only 20% of the labels \\nfor training in our experiment. We wanted to keep the training short and still achieve awesome results. \\nYou can play with the model, for instance, increasing the number of labeled videos available, to see \\nhow the performance will change:\\nFigure 13.34: AutoML Video intelligence – evaluating the results', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f28e16de-6ff4-4f15-ab6f-956c4178032b', embedding=None, metadata={'page_label': '470', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 470\\nLet’s have a detailed look at the results. For instance, we can analyze the precision/recall graph for \\ndifferent levels of threshold:\\nFigure 13.35: AutoML Video intelligence – precision and recall\\nThe confusion matrix shows examples of the wrong classification of shots:\\nFigure 13.36: AutoML Video intelligence – confusion matrix\\nCost\\nTraining on GCP has different costs depending on the type of AutoML adopted; for example, training \\nall the solutions presented in this chapter and serving models for testing had a cost of less than $10 in \\n2022. This is, however, not including the initial six hours of free discount that were available for the \\naccount (around $150 were available at the time of writing). Depending on your organizational needs, \\nthis is likely to work out significantly less than the cost of buying expensive on-premises hardware.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b012b3a-6984-4cef-811f-4645b46d518a', embedding=None, metadata={'page_label': '471', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13 471\\nSummary\\nThe goal of AutoML is to enable domain experts who are not familiar with machine learning technologies \\nto use ML techniques easily. The primary goal is to reduce the steep learning curve and the huge costs \\nof handcrafting machine learning solutions by making the whole end-to-end machine learning pipeline \\n(data preparation, feature engineering, and automatic model generation) more automated.\\nAfter reviewing the state-of-the-art solution available at the end of 2022, we discussed how to use Google \\nCloud AutoML both for text, videos, and images, achieving results comparable to the ones achieved \\nwith handcrafted models. AutoML is probably the fastest-growing research topic and interested readers \\ncan find the latest results at https://www.automl.org/ .\\nThe next chapter discusses the math behind deep learning, a rather advanced topic that is recommended \\nif you are interested in understanding what is going on “under the hood” when you play with neural \\nnetworks.\\nReferences\\n1. Zoph, B., Le, Q. V . (2016). Neural Architecture Search with Reinforcement Learning. http://arxiv.\\norg/abs/1611.01578\\n2. Pham, H., Guan, M. Y., Zoph, B., Le, Q. V ., Dean, J. (2018). Efficient Neural Architecture Search \\nvia Parameter Sharing. https://arxiv.org/abs/1802.03268  \\n3. Borsos, Z., Khorlin, A., Gesmundo, A. (2019). Transfer NAS: Knowledge Transfer between Search \\nSpaces with Transformer Agents. https://arxiv.org/abs/1906.08102  \\n4. Lu, Z., Whalen, I., Boddeti V ., Dhebar, Y., Deb, K., Goodman, E., and Banzhaf, W . (2018). NSGA-\\nNet: Neural Architecture Search using Multi-Objective Genetic Algorithm. https://arxiv.org/\\nabs/1810.03522\\n5. Bergstra, J., Bengio, Y. (2012). Random search for hyper-parameter optimization. http://www.\\njmlr.org/papers/v13/bergstra12a.html  \\n6. Jin, H., Song, Q., and Hu, X. (2019). Auto-Keras: An Efficient Neural Architecture Search System. \\nhttps://arxiv.org/abs/1806.10282\\n7. Faes, L., et al. (2019). Automated deep learning design for medical image classification by health-\\ncare professionals with no coding experience: a feasibility study. The Lancet Digital Health Volume \\n1, Issue 5, September 2019. Pages e232-e242. https://www.sciencedirect.com/science/\\narticle/pii/S2589750019301086', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c690803-134d-4055-91bc-4be4eafb45ef', embedding=None, metadata={'page_label': '472', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='An Introduction to AutoML 472\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='512d29aa-4287-4568-901e-10925bdc9ddf', embedding=None, metadata={'page_label': '473', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14\\nThe Math Behind Deep Learning\\nIn this chapter, we discuss the math behind deep learning. This topic is quite advanced and not \\nnecessarily required for practitioners. However, it is recommended reading if you are interested in \\nunderstanding what is going on under the hood when you play with neural networks. \\nHere is what you will learn:\\n• A historical introduction\\n• The concepts of derivatives and gradients\\n• Gradient descent and backpropagation algorithms commonly used to optimize deep learning \\nnetworks\\nLet’s begin!\\nHistory\\nThe basics of continuous backpropagation were proposed by Henry J. Kelley [1] in 1960 using dynamic \\nprogramming. Stuart Dreyfus proposed using the chain rule in 1962 [2]. Paul Werbos was the first \\nto use backpropagation (backprop for short) for neural nets in his 1974 PhD thesis [3]. However, it \\nwasn’t until 1986 that backpropagation gained success with the work of David E. Rumelhart, Geoffrey \\nE. Hinton, and Ronald J. Williams published in Nature [4]. In 1987, Yann LeCun described the modern \\nversion of backprop currently used for training neural networks [5]. \\nThe basic intuition of Stochastic Gradient Descent (SGD ) was introduced by Robbins and Monro in \\n1951 in a context different from neural networks [6]. In 2012 – or 52 years after the first time backprop \\nwas first introduced – AlexNet [7] achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge \\nusing GPUs. According to The Economist [8], Suddenly people started to pay attention, not just within the \\nAI community but across the technology industry as a whole. Innovation in this field was not something \\nthat happened overnight. Instead, it was a long walk lasting more than 50 years!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0efa062-fe77-4af9-9f6a-6b315330a840', embedding=None, metadata={'page_label': '474', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 474\\nSome mathematical tools\\nBefore introducing backpropagation, we need to review some mathematical tools from calculus. Don’t \\nworry too much; we’ll briefly review a few areas, all of which are commonly covered in high school-level \\nmathematics.\\nVectors\\nWe will review two basic  concepts of geometry and algebra that are quite useful for  machine learning: \\nvectors and the cosine of angles. We start by giving an explanation of vectors. Fundamentally, a vector \\nis a list of numbers. Given a vector, we can interpret it as a direction in space. Mathematicians most \\noften write vectors as either a column x  or row vector xT. Given two column vectors u  and v , we can form \\ntheir dot product by computing 𝑢𝑢𝑢𝑢𝑢𝑢 𝑢 𝑢𝑢𝑢𝑇𝑇𝑢𝑢𝑢∑𝑢𝑢𝑖𝑖𝑢𝑢𝑖𝑖𝑖𝑖 . It can be easily proven that 𝑢𝑢𝑢𝑢𝑢𝑢 𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢   \\nwhere 𝜃𝜃  is the angle between the two vectors. \\nHere are two easy questions for you. What is the result when the two vectors are very close? And what \\nis the result when the two vectors are the same? \\nDerivatives and gradients everywhere\\nDerivatives are a powerful mathematical tool. We are going to use derivatives and gradients to optimize \\nour network. Let’s look at the definition. The derivative of a function y = f(x) of a variable x  is a measure \\nof the rate at which the value y of the function changes with respect to the change of the variable x.\\nIf x and y are real numbers, and if the graph of f is plotted against x, the derivative is the “slope” of \\nthis graph at each point. \\nIf the function is linear 𝑦𝑦𝑦𝑦𝑦(𝑥𝑥)𝑦𝑎𝑎𝑥𝑥+𝑏𝑏 , the slope is 𝑎𝑎𝑎∆𝑦𝑦\\n∆𝑥𝑥 . This is a simple result of calculus, which \\ncan be derived by considering that:\\n𝑦𝑦𝑦𝑦(𝑦𝑦)=𝑓𝑓(𝑥𝑥𝑦𝑦𝑥𝑥)=𝑎𝑎(𝑥𝑥𝑦𝑦𝑥𝑥)𝑦𝑏𝑏=𝑎𝑎𝑥𝑥𝑦𝑎𝑎𝑦𝑥𝑥𝑦𝑏𝑏=𝑦𝑦𝑦𝑎𝑎 𝑦𝑥𝑥 \\n∆(𝑦𝑦)= 𝑎𝑎∆(𝑎𝑎) \\n𝑎𝑎𝑎∆𝑦𝑦\\n∆𝑥𝑥 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8631d06d-b64e-4c52-bbbe-2fd7d45187ac', embedding=None, metadata={'page_label': '475', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 475\\nIn Figure 14.1, we show the geometrical meaning of ∆𝑥𝑥 , ∆𝑦𝑦 , and the angle 𝜃𝜃  between the linear func-\\ntion and the x-cartesian axis:\\nFigure 14.1: An example of a linear function and rate of change\\nIf the function is not linear, then computing the rate of change as the mathematical limit value of the \\nratio of the differences ∆𝑦𝑦\\n∆𝑥𝑥  as ∆(𝑥𝑥)  becomes infinitely small. Geometrically, this is the tangent line at \\n(𝑥𝑥𝑥𝑥𝑥 𝑥 𝑥𝑥 (𝑥𝑥))  as shown in Figure 14.2:\\nFigure 14.2: Rate of change for 𝑓𝑓(𝑥𝑥)=𝑥𝑥2  and the tangential line as ∆𝑥𝑥 𝑥 𝑥  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d4e2f75-f23e-401c-87b4-1ed479213998', embedding=None, metadata={'page_label': '476', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 476\\nFor instance, considering 𝑓𝑓(𝑥𝑥)=𝑥𝑥2  and the derivative 𝑓𝑓′(𝑥𝑥)= 2𝑥𝑥   in a given point, say x = 2, we can \\nsee that the derivative is positive 𝑓𝑓′(2)=4 , as shown in Figure 14.3:\\nFigure 14.3: 𝑓𝑓(𝑥𝑥)=𝑥𝑥2  and 𝑓𝑓′(𝑥𝑥)= 2𝑥𝑥  \\nA gradient is a generalization of the derivative for multiple variables. Note that the derivative of a \\nfunction of a single variable is a scalar-valued function, whereas the gradient of a function of several \\nvariables is a vector-valued function. The gradient is denoted with an upside-down delta ∇ , and called \\n“del” or nabla from the Greek alphabet. This makes sense as delta indicates the change in one variable, \\nand the gradient is the change in all variables. Suppose 𝑥𝑥𝑥𝑥𝑚𝑚  (e.g. the space of real numbers with \\nm dimensions) and f maps from ℝ𝑛𝑛  to ℝ ; the gradient is defined as follows:\\n∇(𝑓𝑓)=(𝜕𝜕𝑓𝑓\\n𝜕𝜕𝜕𝜕1,…,𝜕𝜕𝑓𝑓\\n𝜕𝜕𝜕𝜕𝑚𝑚) \\nIn math, a partial derivative 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖  of a function of several variables is its derivative with respect to one \\nof those variables, with the others held constant. \\nNote that it is possible to show that the gradient is a vector (a direction to move) that:\\n• Points in the direction of the greatest increase of a function.\\n• Is 0 at a local maximum or local minimum. This is because if it is 0, it cannot increase or \\ndecrease further.\\nThe proof is left as an exercise to the interested reader. (Hint: consider Figure 14.2 and Figure 14.3.)\\nGradient descent\\nIf the gradient points in the direction of the greatest increase for a function, then it is possible to move \\ntoward a local minimum for the function by simply moving in a direction opposite to the gradient. \\nThat’s the key observation used for gradient descent algorithms, which will be used shortly. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d2b4eb6-0a12-4c69-beae-5020ed332297', embedding=None, metadata={'page_label': '477', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 477\\nAn example is provided in Figure 14.4:\\nFig.14.4: Gradient descent for a function in 3 variables\\nChain rule\\nThe chain rule says that if we have a function y = g(x) and 𝑧𝑧 𝑧 𝑧𝑧𝑧𝑧𝑧(𝑥𝑥))𝑧𝑧𝑧(𝑦𝑦) , then the derivative is \\ndefined as follows:\\n𝑑𝑑𝑑𝑑\\n𝑑𝑑𝑑𝑑=𝑑𝑑𝑑𝑑\\n𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑\\n𝑑𝑑𝑑𝑑 \\nThis chaining can be generalized beyond the scalar case. Suppose 𝑥𝑥𝑥𝑥𝑚𝑚  and 𝑦𝑦𝑦𝑦𝑛𝑛  with g, which \\nmaps from ℝ𝑚𝑚  to ℝ𝑛𝑛 , and f, which maps from ℝ𝑛𝑛  to ℝ . With y = g(x) and z = f(y), we can deduce:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖=∑𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑗𝑗𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑗𝑗 \\nThe generalized chain rule using partial derivatives will be used as a basic tool for the backpropaga -\\ntion algorithm when dealing with functions in multiple variables. Stop for a second and make sure \\nthat you fully understand it.\\nA few differentiation rules\\nIt might be useful to remind ourselves of a few additional differentiation rules that will be used later:\\n• Constant differentiation: c’ = 0, where c is a constant.\\n• Variable differentiation: 𝑑𝑑𝑑𝑑\\n𝑑𝑑𝑑𝑑𝑧𝑧𝑧𝑧  , when deriving the differentiation of a variable.\\n• Linear differentiation: [𝑎𝑎𝑎𝑎(𝑥𝑥𝑥𝑥𝑏𝑏𝑏𝑏(𝑥𝑥𝑥𝑥𝑥 𝑥 𝑥𝑎𝑎𝑎𝑎𝑥(𝑥𝑥𝑥𝑥𝑏𝑏𝑏𝑏𝑥(𝑥𝑥𝑥  \\n• Reciprocal differentiation: [1\\n𝑓𝑓(𝑥𝑥)]′\\n=−𝑓𝑓′(𝑥𝑥)\\n𝑓𝑓(𝑥𝑥)2 \\n• Exponential differentiation: [𝑓𝑓(𝑥𝑥)𝑛𝑛]′=𝑛𝑛𝑛𝑓𝑓(𝑥𝑥)𝑛𝑛𝑛𝑛 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9648f9ec-0b52-4c4b-879b-64500db530a6', embedding=None, metadata={'page_label': '478', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 478\\nMatrix operations\\nThere are many books about matrix calculus. Here we focus only on only a few basic operations \\nused for neural networks. Recall that a matrix 𝑚𝑚𝑚𝑚𝑚   can be used to represent the weights w ij, with \\n0≤𝑖𝑖≤𝑖𝑖  , 0≤𝑗𝑗≤𝑗𝑗  associated with the arcs between two adjacent layers. Note that by adjusting the \\nweights we can control the “behavior” of the network and that a small change in a specific w ij will be \\npropagated through the network following its topology (see Figure 14.5, where the edges in bold are \\nthe ones impacted by the small change in a specific w ij):\\nFigure 14.5: Propagating w ij changes through the network via the edges in bold\\nNow that we have reviewed some basic concepts of calculus, let’s start applying them to deep learning. \\nThe first question is how to optimize activation functions. Well, I am pretty sure that you are thinking \\nabout computing the derivative, so let’s do it!\\nActivation functions\\nIn Chapter 1, Neural Network Foundations with TF, we saw a few activation functions including sigmoid, \\ntanh, and ReLU. In the section below, we compute the derivative of these activation functions.\\nDerivative of the sigmoid\\nRemember that the sigmoid is defined as 𝜎𝜎(𝑧𝑧)=1\\n1+𝑒𝑒−𝑧𝑧  (see Figure 14.6):\\nFigure 14.6: Sigmoid activation function', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15825c97-fc8b-44ac-9d0d-009323b466a8', embedding=None, metadata={'page_label': '479', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 479\\nThe derivative can be computed as follows:\\n𝜎𝜎′(𝑧𝑧)=𝑑𝑑\\n𝑑𝑑𝑑𝑑(1\\n1+𝑒𝑒−𝑧𝑧)=1\\n(1+𝑒𝑒−𝑧𝑧)−2𝑑𝑑\\n𝑑𝑑𝑑𝑑(𝑒𝑒−𝑑𝑑)=𝑒𝑒−𝑧𝑧\\n(1+𝑒𝑒−𝑧𝑧)1\\n(1+𝑒𝑒−𝑧𝑧)=𝑒𝑒−𝑧𝑧+1−1\\n(1+𝑒𝑒−𝑧𝑧)\\n=𝑒𝑒−𝑧𝑧+1−1\\n(1+𝑒𝑒−𝑧𝑧)1\\n(1+𝑒𝑒−𝑧𝑧)=((1+𝑒𝑒−𝑧𝑧)\\n(1+𝑒𝑒−𝑧𝑧)−1\\n(1+𝑒𝑒−𝑧𝑧))1\\n(1+𝑒𝑒−𝑧𝑧)\\n= (1−1\\n(1+𝑒𝑒−𝑧𝑧))(1\\n(1+𝑒𝑒−𝑧𝑧))(1−𝜎𝜎(𝑧𝑧))𝜎𝜎(𝑧𝑧) \\nTherefore the derivative of 𝜎𝜎(𝑧𝑧)  can be computed as a very simple form: 𝜎𝜎′(𝑧𝑧)= (1−𝜎𝜎 (𝑧𝑧))𝜎𝜎(𝑧𝑧) .\\nDerivative of tanh\\nRemember that the arctan function is defined as tanh(𝑧𝑧)=𝑒𝑒𝑧𝑧−𝑒𝑒−𝑧𝑧\\n𝑒𝑒𝑧𝑧+𝑒𝑒−𝑧𝑧  as seen in Figure 14.7:\\nFigure 14.7: Tanh activation function\\nIf you remember that 𝑑𝑑\\n𝑑𝑑𝑑𝑑𝑒𝑒𝑑𝑑=𝑒𝑒𝑑𝑑  and 𝑑𝑑\\n𝑑𝑑𝑑𝑑𝑒𝑒−𝑑𝑑= −𝑒𝑒−𝑑𝑑 , then the derivative is computed as:\\n𝑑𝑑\\n𝑑𝑑𝑑𝑑𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥)=(𝑒𝑒𝑧𝑧+𝑒𝑒−𝑧𝑧)(𝑒𝑒𝑧𝑧+𝑒𝑒−𝑧𝑧)−(𝑒𝑒𝑧𝑧−𝑒𝑒−𝑧𝑧)(𝑒𝑒𝑧𝑧−𝑒𝑒−𝑧𝑧)\\n(𝑒𝑒𝑧𝑧+𝑒𝑒−𝑧𝑧)2=1−(𝑒𝑒𝑧𝑧−𝑒𝑒−𝑧𝑧)2\\n(𝑒𝑒𝑧𝑧+𝑒𝑒−𝑧𝑧)2= 1−𝑡𝑡𝑡𝑡𝑡𝑡𝑡2(𝑑𝑑) \\nTherefore the derivative of tanh(𝑧𝑧)  can be computed as a very simple form: tanh′(𝑧𝑧)= 1−𝑡𝑡𝑡𝑡𝑡𝑡𝑡2(𝑧𝑧) .\\nDerivative of ReLU\\nThe ReLU function is defined as 𝑓𝑓(𝑥𝑥)= max(0,𝑥𝑥)  (see Figure 14.8). The derivative of ReLU is:\\n𝑓𝑓𝑓(𝑥𝑥)={1, 𝑖𝑖𝑓𝑓 𝑥𝑥 𝑥 𝑥\\n𝑥, 𝑜𝑜𝑜𝑜ℎ𝑒𝑒𝑒𝑒𝑒𝑒𝑖𝑖𝑒𝑒𝑒𝑒 \\nNote that ReLU is non-differentiable at zero. However, it is differentiable anywhere else, and the value \\nof the derivative at zero can be arbitrarily chosen to be a 0 or 1, as demonstrated in Figure 14.8:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07305719-1878-4151-8381-d286cdf70465', embedding=None, metadata={'page_label': '480', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 480\\nFigure 14.8: ReLU activation function\\nBackpropagation\\nNow that we have computed the derivative of the activation functions, we can describe the backprop -\\nagation algorithm — the mathematical core of deep learning. Sometimes, backpropagation is called \\nbackprop for short.\\nRemember that a neural network can have multiple hidden layers, as well as one input layer and one \\noutput layer.\\nIn addition to that, recall from Chapter 1, Neural Network Foundations with TF, that backpropagation \\ncan be described as a way of progressively correcting mistakes as soon as they are detected. In order \\nto reduce the errors made by a neural network, we must train the network. The training needs a data -\\nset including input values and the corresponding true output value. We want to use the network for \\npredicting output as close as possible to the true output value. The key intuition of the backpropaga -\\ntion algorithm is to update the weights of the connections based on the measured error at the output \\nneuron(s). In the remainder of this section, we will explain how to formalize this intuition.\\nWhen backpropagation starts, all the weights have some random assignment. Then the net is activated \\nfor each input in the training set; values are propagated forward from the input stage through the hid-\\nden stages to the output stage where a prediction is made (note that we keep the figure below simple \\nby only representing a few values with green dotted lines, but in reality, all the values are propagated \\nforward through the network):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a219c1e3-4c0b-46d5-a88f-23d164ace258', embedding=None, metadata={'page_label': '481', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 481\\nFigure 14.9: Forward step in backpropagation\\nSince we know the true observed value in the training set, it is possible to calculate the error made in \\nthe prediction. The easiest way to think about backtracking is to propagate the error back (see Figure \\n14.10), using an appropriate optimizer algorithm such as gradient descent to adjust the neural network \\nweights, with the goal of reducing the error (again, for the sake of simplicity only a few error values \\nare represented here):\\nFigure 14.10: Backward step in backpropagation\\nThe process of forward propagation from input to output and backward propagation of errors is repeat -\\ned several times until the error goes below a predefined threshold. The whole process is represented in \\nFigure 14.11. A set of features is selected as input to a machine learning model that produces predictions. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f707b5b6-421d-4bb9-b81f-3fe2fc949291', embedding=None, metadata={'page_label': '482', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 482\\nThe predictions are compared with the (true) label, and the resulting loss function is minimized by \\nthe optimizer, which updates the weights of the model:\\nFigure 14.11: Forward propagation and backward propagation\\nLet’s see in detail how the forward and backward steps are realized. It might be useful to have a look \\nback at Figure 14.5  and recall that a small change in a specific w ij will be propagated through the \\nnetwork following its topology (see Figure 14.5, where the edges in bold are the ones impacted by the \\nsmall change in specific weights).\\nForward step\\nDuring the forward steps, the inputs are multiplied with the weights and then all summed together. \\nThen the activation function is applied (see Figure 14.12). This step is repeated for each layer, one \\nafter another. The first layer takes the input features as input and it produces its output. Then, each \\nsubsequent layer takes as input the output of the previous layer:\\nFigure 14.12: Forward propagation \\nIf we look at one single layer, mathematically we have two equations:\\n• The transfer equation 𝑧𝑧𝑧∑𝑤𝑤𝑖𝑖𝑥𝑥𝑖𝑖+𝑏𝑏𝑖𝑖  , where xi are the input values, wi are the weights, and b is \\nthe bias. In vector notation 𝑧𝑧𝑧𝑧𝑧𝑇𝑇𝑥𝑥 . Note that b can be absorbed in the summatory by setting \\n𝑤𝑤0=𝑏𝑏  and 𝑥𝑥0=1 .\\n• The activation function: 𝑦𝑦𝑦𝑦𝑦(𝑧𝑧) , where 𝜎𝜎  is the chosen activation function.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3366a57-6097-4bfa-8094-f63ad30b6185', embedding=None, metadata={'page_label': '483', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 483\\nAn artificial neural network consists of an input layer I, an output layer O, and any number of hidden \\nlayers Hi situated between the input and the output layers. For the sake of simplicity, let’s assume that \\nthere is only one hidden layer, since the results can be easily generalized.\\nAs shown in Figure 14.12, the features x i from the input layer are multiplied by a set of fully connect-\\ned weights w ij connecting the input layer to the hidden layer (see the left side of Figure 14.12). The \\nweighted signals are summed together and with the bias to calculate the result 𝑧𝑧𝑗𝑗=∑𝑤𝑤𝑖𝑖𝑥𝑥𝑖𝑖+𝑏𝑏𝑗𝑗𝑖𝑖  (see \\nthe center of Figure 14.12). The result is passed through the activation function 𝑦𝑦𝑗𝑗=𝜎𝜎𝑗𝑗(𝑧𝑧𝑗𝑗) , which \\nleaves the hidden layer to the output layer (see the right side of Figure 14.12).\\nIn summary, during the forward step we need to run the following operations:\\n1. For each neuron in a layer, multiply each input by its corresponding weight.\\n2. Then for each neuron in the layer, sum all input weights together.\\n3. Finally, for each neuron, apply the activation function on the result to compute the new output.\\nAt the end of the forward step, we get a predicted vector 𝑦𝑦𝑜𝑜  from the output layer o given the input \\nvector x presented at the input layer. Now the question is: how close is the predicted vector 𝑦𝑦𝑜𝑜  to the \\ntrue value vector t?\\nThat’s where the backstep comes in.\\nBackstep\\nTo understand how close the predicted vector 𝑦𝑦𝑜𝑜  is to the true value vector t, we need a function that \\nmeasures the error at the output layer o. That is the loss function defined earlier in the book. There are \\nmany choices for loss function. For instance, we can define the mean squared error defined as follows:\\n𝐸𝐸𝐸1\\n2∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)2\\n𝑜𝑜 \\nNote that E is a quadratic function and, therefore, the difference is quadratically larger when t is far \\naway from 𝑦𝑦𝑜𝑜 , and the sign is not important. Note that this quadratic error (loss) function is not the \\nonly one that we can use. Later in this chapter, we will see how to deal with cross-entropy.\\nNow, remember that the key point is that during the training, we want to adjust the weights of the \\nnetwork to minimize the final error. As discussed, we can move toward a local minimum by moving \\nin the opposite direction to the gradient −∇𝑤𝑤  . Moving in the opposite direction to the gradient is the \\nreason why this algorithm is called gradient descent. Therefore, it is reasonable to define the equation \\nfor updating the weight w ij as follows:\\n𝑤𝑤𝑖𝑖𝑖𝑖←𝑤𝑤𝑖𝑖𝑖𝑖−∇𝑤𝑤𝑖𝑖𝑖𝑖 \\nFor a function in multiple variables, the gradient is computed using partial derivatives. We introduce \\nthe hyperparameter 𝜂𝜂  – or, in ML lingo, the learning rate – to account for how large a step should be \\nin the direction opposite to the gradient.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b255ab1-da4a-4a50-8c47-02dccab1349f', embedding=None, metadata={'page_label': '484', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 484\\nConsidering the error, E, we have the equation: \\n∇𝑤𝑤 𝑤 𝑤𝑤𝑤𝜕𝜕𝜕𝜕\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑖𝑖 \\nThe preceding equation is simply capturing the fact that a slight change will impact the final error, \\nas seen in Figure 14.13:\\nFigure 14.13: A small change in w ij will impact the final error E\\nLet’s define the notation used throughout our equations in the remaining section:\\n• 𝑧𝑧𝑗𝑗  is the input to node j for layer l.\\n• 𝛿𝛿𝑗𝑗  is the activation function for node j in layer l (applied to 𝑧𝑧𝑗𝑗 ).\\n• 𝑦𝑦𝑗𝑗=𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)  is the output of the activation of node j in layer l.\\n• 𝑤𝑤𝑖𝑖𝑖𝑖  is the matrix of weights connecting the neuron i in layer 𝑙𝑙𝑙𝑙   to the neuron j in layer l.\\n• 𝑏𝑏𝑗𝑗  is the bias for unit j in layer l.\\n• 𝑡𝑡𝑜𝑜  is the target value for node o in the output layer.\\nNow we need to compute the partial derivative for the error at the output layer 𝜕𝜕𝜕𝜕  when the weights \\nchange by 𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖 . There are two different cases: \\n• Case 1: Weight update equation for a neuron from hidden (or input) layer to output layer. \\n• Case 2: Weight update equation for a neuron from hidden (or input) layer to hidden layer.\\nWe’ll begin with Case 1.\\nCase 1: From hidden layer to output layer\\nIn this case, we need to consider the equation for a neuron from hidden layer j to output layer o. Ap -\\nplying the definition of E and differentiating we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=𝜕𝜕1\\n2∑(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)2\\n𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)𝜕𝜕(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ee43d5d-b3da-4dc0-847e-ce629beb301e', embedding=None, metadata={'page_label': '485', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 485\\nHere the summation disappears because when we take the partial derivative with respect to the j-th \\ndimension, the only term that is not zero in the error is the j-th. Considering that differentiation is a \\nlinear operation and that 𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑜𝑜=0  – because the true 𝑡𝑡0  value does not depend on 𝑤𝑤𝑗𝑗𝑗𝑗  – we have:\\n𝜕𝜕(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑜𝑜=𝜕𝜕𝑦𝑦𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑜𝑜−0 \\nApplying the chain rule again and remembering that 𝑦𝑦𝑜𝑜=𝛿𝛿𝑜𝑜(𝑧𝑧𝑜𝑜) , we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)𝜕𝜕𝑦𝑦𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)𝜕𝜕𝜕𝜕𝑗𝑗(𝑧𝑧𝑗𝑗)\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)𝜕𝜕′\\n𝑗𝑗(𝑧𝑧𝑗𝑗)𝜕𝜕𝑧𝑧𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗 \\nRemembering that 𝑧𝑧𝑜𝑜=∑𝑤𝑤𝑗𝑗𝑜𝑜𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)+𝑏𝑏𝑜𝑜𝑗𝑗 , we have 𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑜𝑜=𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)  again because when we take the partial \\nderivative with respect to the j-th dimension the only term that is not zero in the error is the j-th. By \\ndefinition 𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)=𝑦𝑦𝑗𝑗 , so putting everything together we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=(𝑦𝑦𝑗𝑗−𝑡𝑡𝑗𝑗)𝛿𝛿′\\n𝑗𝑗(𝑧𝑧𝑗𝑗)𝑦𝑦𝑗𝑗 \\nThe gradient of the error E with respect to the weights w j from the hidden layer j to the output layer o \\nis therefore simply the product of three terms: the difference between the prediction 𝑦𝑦𝑜𝑜  and the true \\nvalue 𝑡𝑡𝑜𝑜 , the derivative 𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)  of the output layer activation function, and the activation output 𝑦𝑦𝑗𝑗  of \\nnode j in the hidden layer. For simplicity we can also define 𝑣𝑣𝑜𝑜=(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)  and get:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗=𝑣𝑣𝑗𝑗𝑦𝑦𝑗𝑗 \\nIn short, for Case 1, the weight update equation for each of the hidden-output connections is: \\n𝑤𝑤𝑗𝑗𝑗𝑗←𝑤𝑤𝑗𝑗𝑗𝑗−𝜂𝜂𝜕𝜕𝜕𝜕\\n𝜕𝜕𝑤𝑤𝑗𝑗𝑗𝑗 \\nNote: if we want to explicitly compute the gradient with respect to the output layer biases, the steps \\nto follow are similar to the ones above with only one difference:\\n𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑜𝑜=𝜕𝜕∑𝑤𝑤𝑗𝑗𝑜𝑜𝛿𝛿𝑗𝑗(𝜕𝜕𝑗𝑗)+𝜕𝜕𝑜𝑜 𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑜𝑜=1 \\nso in this case 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕0=𝑣𝑣𝑜𝑜 .\\nNext, we’ll look at Case 2.\\nCase 2: From hidden layer to hidden layer\\nIn this case, we need to consider the equation for a neuron from a hidden layer (or the input layer) \\nto a hidden layer. Figure 14.13 showed that there is an indirect relationship between the hidden layer \\nweight change and the output error. This makes the computation of the gradient a bit more challeng-\\ning. In this case, we need to consider the equation for a neuron from hidden layer i to hidden layer j. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f14c04be-734c-4b96-a956-6ae08dd34765', embedding=None, metadata={'page_label': '486', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 486\\nApplying the definition of E and differentiating we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=𝜕𝜕1\\n2∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)2\\n𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝜕𝜕(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖𝑜𝑜=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝜕𝜕𝑦𝑦𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖𝑜𝑜 \\nIn this case, the sum will not disappear because the change of weights in the hidden layer is directly \\naffecting the output. Substituting 𝑦𝑦𝑜𝑜=𝛿𝛿𝑜𝑜(𝑧𝑧𝑜𝑜)  and applying the chain rule we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝜕𝜕𝜕𝜕𝑜𝑜(𝑧𝑧𝑜𝑜)\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖𝑜𝑜=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝜕𝜕′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)𝜕𝜕𝑧𝑧𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖𝑜𝑜 \\nThe indirect relation between 𝑧𝑧0  and the internal weights wij (Figure 14.13) is mathematically expressed \\nby the expansion:\\n𝑧𝑧𝑜𝑜=∑𝑤𝑤 𝑗𝑗𝑜𝑜𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)\\n𝑗𝑗+𝑏𝑏𝑜𝑜=∑𝑤𝑤 𝑗𝑗𝑜𝑜𝛿𝛿𝑗𝑗(∑𝑤𝑤 𝑖𝑖𝑗𝑗𝑧𝑧𝑖𝑖\\n𝑖𝑖+𝑏𝑏𝑖𝑖)+𝑏𝑏𝑜𝑜\\n𝑗𝑗 \\nsince 𝑧𝑧𝑗𝑗=∑𝑤𝑤𝑖𝑖𝑗𝑗𝑧𝑧𝑖𝑖𝑖𝑖+𝑏𝑏𝑖𝑖 .\\nThis suggests applying the chain rule again:\\n𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖= \\nApplying the chain rule:\\n=𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑗𝑗𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑗𝑗= \\nSubstituting 𝑧𝑧0 :\\n=𝜕𝜕𝜕𝜕𝑗𝑗𝑤𝑤𝑗𝑗𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑗𝑗= \\nDeriving:\\n=𝑤𝑤𝑗𝑗𝑗𝑗𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑗𝑗= \\nSubstituting 𝑦𝑦𝑗𝑗=𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗) :\\n=𝑤𝑤𝑗𝑗𝑗𝑗𝜕𝜕𝜕𝜕𝑗𝑗(𝑧𝑧𝑗𝑗)\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑗𝑗= ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b28c9fb-a495-4807-b6cb-928a2fe7ac7f', embedding=None, metadata={'page_label': '487', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 487\\nApplying the chain rule:\\n=𝑤𝑤𝑗𝑗𝑗𝑗𝛿𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)𝜕𝜕𝑧𝑧𝑗𝑗\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑗𝑗= \\nSubstituting 𝑧𝑧𝑗𝑗=∑𝑦𝑦𝑖𝑖𝑤𝑤𝑖𝑖𝑗𝑗𝑖𝑖+𝑏𝑏𝑖𝑖 :\\n=𝑤𝑤𝑗𝑗𝑗𝑗𝛿𝛿𝛿𝑗𝑗(𝑧𝑧𝑗𝑗)𝜕𝜕(∑𝑦𝑦𝑖𝑖𝑤𝑤𝑖𝑖𝑗𝑗+𝑏𝑏𝑖𝑖𝑖𝑖)\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑗𝑗= \\nDeriving:\\n𝑤𝑤𝑗𝑗𝑗𝑗𝛿𝛿′\\n𝑗𝑗(𝑧𝑧𝑗𝑗)𝑦𝑦𝑖𝑖 \\nNow we can combine the above two results:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)𝜕𝜕𝑧𝑧𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖𝑜𝑜 \\n𝜕𝜕𝜕𝜕𝑜𝑜\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=𝜕𝜕𝑖𝑖𝑜𝑜𝛿𝛿𝛿𝑖𝑖(𝜕𝜕𝑖𝑖)𝑦𝑦𝑖𝑖 \\nand get:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕 𝑖𝑖𝑖𝑖=∑ (𝑦𝑦𝑜𝑜−𝑡𝑡 𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)\\n𝑜𝑜𝜕𝜕𝑖𝑖𝑜𝑜𝛿𝛿𝛿𝑖𝑖(𝑧𝑧𝑖𝑖)𝑦𝑦𝑖𝑖=𝑦𝑦 𝑖𝑖𝛿𝛿′\\n𝑖𝑖(𝑧𝑧𝑖𝑖)∑ (𝑦𝑦𝑜𝑜−𝑡𝑡 𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)𝜕𝜕𝑖𝑖𝑜𝑜\\n𝑜𝑜 \\nRemembering the definition: 𝑣𝑣𝑜𝑜=(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜) , we get:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=∑(𝑦𝑦𝑜𝑜−𝑡𝑡𝑜𝑜)𝛿𝛿′\\n𝑜𝑜(𝑧𝑧𝑜𝑜)\\n𝑜𝑜𝜕𝜕𝑖𝑖𝑜𝑜𝛿𝛿𝛿𝑖𝑖(𝑧𝑧𝑖𝑖)𝑦𝑦𝑖𝑖=𝑦𝑦𝑖𝑖𝛿𝛿′\\n𝑖𝑖(𝑧𝑧𝑖𝑖)∑𝑣𝑣 𝑜𝑜𝜕𝜕𝑖𝑖𝑜𝑜\\n𝑜𝑜 \\nThis last substitution with 𝑣𝑣𝑜𝑜  is particularly interesting because it backpropagates the signal 𝑣𝑣𝑜𝑜  com-\\nputed in the subsequent layer. The rate of change 𝜕𝜕𝜕𝜕  with respect to the rate of change of the weights \\nwij is therefore the multiplication of three factors: the output activations y i from the layer below, the \\nderivative of hidden layer activation function 𝛿𝛿𝛿𝑗𝑗 , and the sum of the backpropagated signal 𝑣𝑣𝑜𝑜  previ -\\nously computed in the subsequent layer weighted by 𝑤𝑤𝑗𝑗𝑗𝑗 . We can use this idea of backpropagating the \\nerror signal by defining 𝑣𝑣𝑗𝑗=𝛿𝛿′\\n𝑗𝑗(𝑧𝑧𝑗𝑗)∑𝑣𝑣𝑜𝑜𝑤𝑤𝑗𝑗𝑜𝑜𝑜𝑜   and therefore 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖=𝑦𝑦𝑖𝑖𝑣𝑣𝑗𝑗 . This suggests that in order to \\ncalculate the gradients at any layer 𝑙𝑙  in a deep neural network, we can simply multiply the backprop -\\nagated error signal 𝑣𝑣𝑗𝑗  and multiply it by the feed-forward signal 𝑦𝑦𝑙𝑙𝑙𝑙 , arriving at the layer l. Note that \\nthe math is a bit complex but the result is indeed very very simple! The intuition is given in Figure \\n14.14. Given a function 𝑧𝑧𝑧𝑧𝑧(𝑥𝑥𝑥𝑥𝑥) , computed locally to a neuron with the input 𝑥𝑥 , and 𝑦𝑦 , the gradients \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕  are backpropagated. Then, they are combined via the chain rule with the local gradients 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕  and 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕  \\nfor further backpropagation. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0beeaaf2-cd5e-43af-80f7-7bb49d3d60ce', embedding=None, metadata={'page_label': '488', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 488\\nHere, L denotes the error from the generic previous layer:\\nFigure 14.14: An example of the math behind backpropagation\\nNote: if we want to explicitly compute the gradient with respect to the output layer biases, it can be \\nproven that 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖=𝑣𝑣𝑗𝑗 . We leave this as an exercise for you.\\nIn short, for Case 2 (hidden-to-hidden connection) the weight delta is ∆𝑤𝑤 𝑤 𝑤𝑤𝑤𝑤 𝑗𝑗𝑦𝑦𝑖𝑖  and the weight update  \\nequation for each of the hidden-hidden connections is simply:\\n𝑤𝑤𝑖𝑖𝑖𝑖←𝑤𝑤𝑖𝑖𝑖𝑖−𝜂𝜂𝜕𝜕𝜕𝜕\\n𝜕𝜕𝑤𝑤𝑖𝑖𝑖𝑖 \\nWe have arrived at the end of this section and all the mathematical tools are defined to make our final \\nstatement. The essence of the backstep is nothing more than applying the weight update rule one \\nlayer after another, starting from the last output layer and moving back toward the first input layer. \\nDifficult to derive, to be sure, but extremely easy to apply once defined. The whole forward-backward \\nalgorithm at the core of deep learning is then the following:\\n1. Compute the feedforward signals from the input to the output.\\n2. Compute the output error E based on the predictions 𝑦𝑦𝑜𝑜  and the true value 𝑡𝑡𝑜𝑜 .\\n3. Backpropagate the error signals; multiply them with the weights in previous layers and with \\nthe gradients of the associated activation functions.\\n4. Compute the gradients 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕  for all of the parameters 𝜃𝜃  based on the backpropagated error signal \\nand the feedforward signals from the inputs.\\n5. Update the parameters using the computed gradients 𝜃𝜃𝜃𝜃𝜃𝜃𝜃𝜃𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕 .\\nNote that the above algorithm will work for any choice of differentiable error function E and for any \\nchoice of differentiable activation 𝛿𝛿𝑙𝑙  function. The only requirement is that both must be differentiable.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5dc57cbe-6c38-4ba5-9223-87e32a273ec8', embedding=None, metadata={'page_label': '489', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 489\\nGradient descent with backpropagation is not guaranteed to find the global minimum of the loss \\nfunction, but only a local minimum. However, this is not necessarily a problem observed in practical \\napplication.\\nCross entropy and its derivative\\nGradient descent can be used when cross-entropy is adopted as the loss function. As discussed in \\nChapter 1, Neural Network Foundations with TF, the logistic loss function is defined as:\\n𝐸𝐸𝐸𝐸𝐸(𝑐𝑐𝑐𝑐𝑐)𝐸−∑[𝑐𝑐𝑖𝑖ln(𝑐𝑐𝑖𝑖)+(1−𝑐𝑐𝑖𝑖)ln(1−𝑐𝑐𝑖𝑖)]\\n𝑖𝑖 \\nWhere c refers to one-hot-encoded classes (or labels) whereas p  refers to softmax-applied probabilities. \\nSince cross-entropy is applied to softmax-applied probabilities and to one-hot-encoded classes, we \\nneed to take into account the chain rule for computing the gradient with respect to the final weights \\nscore i. Mathematically, we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝜕𝜕𝜕𝜕𝑖𝑖\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖 \\nComputing each part separately, let’s start from 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖 :\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖=𝜕𝜕(−∑[𝑐𝑐𝑖𝑖ln(𝜕𝜕𝑖𝑖)+(1−𝑐𝑐𝑖𝑖)ln(𝑖𝑖−𝜕𝜕𝑖𝑖)])\\n𝜕𝜕𝜕𝜕𝑖𝑖=𝜕𝜕(−[𝑐𝑐𝑖𝑖ln(𝜕𝜕𝑖𝑖)+(1−𝑐𝑐𝑖𝑖)ln(𝑖𝑖−𝜕𝜕𝑖𝑖)])\\n𝜕𝜕𝜕𝜕𝑖𝑖 \\n(noting that for a fixed 𝜕𝜕𝜕𝜕𝑖𝑖  all the terms in the sum are constant except the chosen one).\\nTherefore, we have:\\n−𝜕𝜕𝜕𝜕𝑖𝑖ln𝑝𝑝𝑖𝑖\\n𝜕𝜕𝑝𝑝𝑖𝑖−𝜕𝜕(1−𝜕𝜕𝑖𝑖)ln(1−𝑝𝑝𝑖𝑖)\\n𝜕𝜕𝑝𝑝𝑖𝑖=−𝜕𝜕𝑖𝑖\\n𝑝𝑝𝑖𝑖−(1−𝜕𝜕𝑖𝑖)\\n(1−𝑝𝑝𝑖𝑖)𝜕𝜕(1−𝑝𝑝𝑖𝑖)\\n𝜕𝜕𝑝𝑝𝑖𝑖 \\n(applying the partial derivative to the sum and considering that ln′(𝑥𝑥)=1\\n𝑥𝑥 )\\nTherefore, we have:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖=−𝑐𝑐𝑖𝑖\\n𝜕𝜕𝑖𝑖+(1−𝑐𝑐𝑖𝑖)\\n(1−𝜕𝜕𝑖𝑖) \\nNow let’s compute the other part 𝜕𝜕𝜕𝜕𝑖𝑖\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖  where p i is the softmax function defined as:\\n𝜎𝜎𝜎𝜎𝜎𝑗𝑗)=𝑒𝑒𝑥𝑥𝑗𝑗\\n∑𝑒𝑒𝑥𝑥𝑖𝑖𝑖𝑖 \\nThe derivative is:\\n𝜕𝜕𝜕𝜕(𝑥𝑥𝑗𝑗)\\n𝜕𝜕𝑥𝑥𝑘𝑘=𝜕𝜕(𝑥𝑥 𝑗𝑗) (1 − 𝜕𝜕(𝑥𝑥 𝑗𝑗)) , 𝑖𝑖𝑖𝑖 𝑗𝑗=𝑗𝑗 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='075dceaf-249c-4cae-aa2a-46cc45d2d1f1', embedding=None, metadata={'page_label': '490', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 490\\nand \\n𝜕𝜕𝜕𝜕(𝑥𝑥𝑗𝑗)\\n𝜕𝜕𝑥𝑥𝑘𝑘= −𝜕𝜕 (𝑒𝑒𝑥𝑥𝑗𝑗)𝜕𝜕(𝑒𝑒𝑥𝑥𝑘𝑘), 𝑖𝑖𝑖𝑖 𝑗𝑗 𝑗 𝑗𝑗  \\nUsing the Kronecker delta 𝛿𝛿𝑖𝑖𝑖𝑖={1, 𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓=𝑓𝑓,\\n0,𝑓𝑓𝑜𝑜ℎ𝑒𝑒𝑓𝑓𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒  we have:\\n𝜕𝜕𝜕𝜕(𝑥𝑥𝑗𝑗)\\n𝜕𝜕𝑥𝑥𝑘𝑘=𝜕𝜕(𝑥𝑥𝑗𝑗)(𝛿𝛿𝑖𝑖𝑗𝑗−𝜕𝜕(𝑥𝑥𝑗𝑗)) \\nTherefore, considering that we are computing the partial derivative, all the components are zeroed \\nwith the exception of only one, and we have:\\n𝜕𝜕𝜕𝜕𝑖𝑖\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖=𝜕𝜕𝑖𝑖(1−𝜕𝜕𝑖𝑖) \\nCombining the results, we have:\\n 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝜕𝜕𝜕𝜕𝑖𝑖\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖=[−𝜕𝜕𝑖𝑖\\n𝜕𝜕𝑖𝑖+(1−𝜕𝜕𝑖𝑖)\\n(1−𝜕𝜕𝑖𝑖)][𝑝𝑝𝑖𝑖(1−𝑝𝑝 𝑖𝑖)]                   \\n                         = −𝜕𝜕𝑖𝑖[𝜕𝜕𝑖𝑖(1−𝜕𝜕𝑖𝑖)]\\n𝜕𝜕𝑖𝑖+ (1−𝜕𝜕𝑖𝑖)𝜕𝜕𝑖𝑖(1−𝜕𝜕𝑖𝑖)\\n(1−𝜕𝜕𝑖𝑖)= −𝑐𝑐 𝑖𝑖(1−𝑝𝑝 𝑖𝑖)+(1−𝑐𝑐 𝑖𝑖)𝑝𝑝𝑖𝑖\\n= −𝑐𝑐 𝑖𝑖+𝑐𝑐 𝑖𝑖𝑝𝑝𝑖𝑖+𝑝𝑝 𝑖𝑖−𝑐𝑐 𝑖𝑖𝑝𝑝𝑖𝑖=𝑝𝑝 𝑖𝑖−𝑐𝑐 𝑖𝑖                     \\nWhere ci denotes the one-hot-encoded classes and p i refers to the softmax probabilities. In short, the \\nderivative is both elegant and easy to compute:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕 𝑖𝑖=𝑝𝑝𝑖𝑖−𝜕𝜕𝑖𝑖 \\nBatch gradient descent, stochastic gradient descent, and mini-\\nbatch\\nIf we generalize the previous discussion, then we can state that the problem of optimizing a neural \\nnetwork consists of adjusting the weights w of the network in such a way that the loss function is \\nminimized. Conveniently, we can think about the loss function in the form of a sum, as in this form \\nit’s indeed representing all the loss functions commonly used:\\n𝑄𝑄(𝑤𝑤)=1\\n𝑛𝑛∑𝑄𝑄𝑖𝑖(𝑤𝑤)𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nIn this case, we can perform a derivation using steps very similar to those discussed previously, fol-\\nlowing the update rule, where 𝜂𝜂  is the learning rate and ∇  is the gradient:\\n𝑤𝑤 𝑤 𝑤𝑤 𝑤𝑤𝑤𝑤𝑤𝑤 (𝑤𝑤)𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑖𝑖(𝑤𝑤)𝑛𝑛\\n𝑖𝑖𝑖𝑖 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13b3bedb-8384-4cbd-b082-c1fcc93cf3c4', embedding=None, metadata={'page_label': '491', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 491\\nIn many cases, evaluating the above gradient might require an expensive evaluation of the gradients \\nfrom all summand functions. When the training set is very large, this can be extremely expensive. If \\nwe have three million samples, we have to loop through three million times or use the dot product. \\nThat’s a lot! How can we simplify this? There are three types of gradient descent, each different in the \\nway they handle the training dataset.\\nBatch gradient descent\\nBatch Gradient Descent (BGD ) computes the change of error but updates the whole model only once \\nthe entire dataset has been evaluated. Computationally it is very efficient, but it requires that the \\nresults for the whole dataset be held in the memory.\\nStochastic gradient descent\\nInstead of updating the model once the dataset has been evaluated, Stochastic Gradient Descent  \\n(SGD ) does so after every single training example. The key idea is very simple: SGD samples a subset \\nof summand functions at every step.\\nMini-batch gradient descent\\nMini-Batch Gradient Descent ( MBGD ) is very frequently used in deep learning. MBGD (or mini-batch) \\ncombines BGD and SGD in one single heuristic. The dataset is divided into small batches of about size \\nbs, generally 64 to 256. Then each of the batches is evaluated separately.\\nNote that bs  is another hyperparameter to fine-tune during training. MBGD lies between the extremes \\nof BGD and SGD – by adjusting the batch size and the learning rate parameters, we sometimes find \\na solution that descends closer to the global minimum than what can be achieved by either of the \\nextremes. \\nIn contrast with gradient descent, where the cost function is minimized more smoothly, the mini-batch \\ngradient has a bit more of a noisy and bumpy descent, but the cost function still trends downhill. The \\nreason for the noise is that mini-batches are a sample of all the examples and this sampling can cause \\nthe loss function to oscillate.\\nThinking about backpropagation and ConvNets\\nIn this section, we will examine backprop and ConvNets. For the sake of simplicity, we will focus on an \\nexample of convolution with input X  of size 3x3, one single filter W  of size 2x2 with no padding, stride 1, \\nand no dilation (see Chapter 3, Convolutional Neural Networks). The generalization is left as an exercise.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfecb976-7fb8-45bb-a413-c89275f54deb', embedding=None, metadata={'page_label': '492', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 492\\nThe standard convolution operation is represented in Figure 14.15. Simply put, the convolutional \\noperation is the forward pass:\\nInput\\nX11\\nX12\\nX13\\nX21\\nX22\\nX23\\nX31\\nX32\\nX33Weights\\nW11\\nW12\\nW21\\nW22Convolution\\nW11X11+W12X12+W21X21+W22X22\\nW11X12+W12X13+W21X21+W22X23\\nW11X21+W12X22+W21X31+W22X32\\nW11X22+W12X23+W21X32+W22X33\\nFigure 14.15: Forward pass for our ConvNet toy example \\nFollowing the examination of Figure 14.15, we can now focus our attention on the backward pass for \\nthe current layer. The key assumption is that we receive a backpropagated signal 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝑖𝑖𝑖𝑖  as input, and we \\nneed to compute 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖  and 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖 . This computation is left as an exercise, but please note that each weight \\nin the filter contributes to each pixel in the output map or, in other words, any change in a weight of \\na filter affects all the output pixels. \\nThinking about backpropagation and RNNs\\nRemember from Chapter 5, Recurrent Neural Networks, the basic equation for an RNN is \\n𝑠𝑠𝑡𝑡= tanh(𝑈𝑈 𝑥𝑥𝑡𝑡+𝑊𝑊𝑠𝑠𝑡𝑡𝑡𝑡) , the final prediction is 𝑦𝑦𝑦𝑡𝑡= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (𝑉𝑉𝑠𝑠𝑡𝑡)  at step t, the correct value is y t, and \\nthe error E  is the cross-entropy. Here U , V, and W  are learning parameters used for the RNN’s equations. \\nThese equations can be visualized as shown in Figure 14.16, where we unroll the recurrency. The core \\nidea is that total error is just the sum of the errors at each time step. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da2a93cb-0b5c-433d-ba94-32142b303cd6', embedding=None, metadata={'page_label': '493', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 493\\nIf we used SGD, we need to sum the errors and the gradients at each time step for one given training \\nexample:\\nFigure 14.16: RNN unrolled with equations\\nWe are not going to write all the tedious math behind all the gradients but rather focus only on a \\nfew peculiar cases. For instance, with math computations similar to the ones made in the previous \\nsections, it can be proven by using the chain rule that the gradient for V depends only on the value at \\nthe current time step s 3, y3 and 𝑦𝑦𝑦3 :\\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕3𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=(𝜕𝜕𝜕3−𝜕𝜕3)𝑠𝑠3 \\nHowever, 𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕  has dependencies carried across time steps because, for instance, 𝑠𝑠3= tanh(𝑈𝑈 𝑥𝑥𝑡𝑡+𝑊𝑊𝑠𝑠2)  \\ndepends on s 2, which depends on W 2 and s 1. As a consequence, the gradient is a bit more complicated \\nbecause we need to sum up the contributions of each time step:\\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=∑𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕3𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕𝑘𝑘𝜕𝜕𝜕𝜕𝑘𝑘\\n𝜕𝜕𝜕𝜕3\\n𝑘𝑘𝑘𝑘 \\nTo understand the preceding equation, imagine that we are using the standard backpropagation al-\\ngorithm used for traditional feedforward neural networks but for RNNs. We need to additionally add \\nthe gradients of W across time steps. That’s because we can effectively make the dependencies across \\ntime explicit by unrolling the RNN. This is the reason why backprop for RNNs is frequently called \\nBackpropagation Through Time (BPTT ). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1aace575-12a3-46e8-a6db-ed1ce73275c6', embedding=None, metadata={'page_label': '494', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 494\\nThe intuition is shown in Figure 14.17, where the backpropagated signals are represented:\\n \\nFigure 14.17: RNN equations and backpropagated signals \\nI hope that you have been following up to this point because now the discussion will be slightly more \\ndifficult. If we consider:\\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=∑𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕3𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕𝑘𝑘𝜕𝜕𝜕𝜕𝑘𝑘\\n𝜕𝜕𝜕𝜕3\\n𝑘𝑘𝑘𝑘 \\nthen we notice that 𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕𝑘𝑘  should be again computed with the chain rule, producing a number of multi-\\nplications. In this case, we take the derivative of a vector function with respect to a vector, so we need \\na matrix whose elements are all the pointwise derivatives (in math, this matrix is called a Jacobian). \\nMathematically, it can be proven that:\\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕𝑘𝑘=∏𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗3\\n𝑗𝑗𝑗𝑘𝑘𝑗𝑗 \\nTherefore, we have: \\n𝜕𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕=∑𝜕𝜕𝜕𝜕3\\n𝜕𝜕 𝜕𝜕𝜕3𝜕𝜕 𝜕𝜕𝜕3\\n𝜕𝜕𝜕𝜕3(∏𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕𝑗𝑗𝑗𝑗3\\n𝑗𝑗𝑗𝑗𝑗𝑗𝑗)𝜕𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕3\\n𝑗𝑗𝑗𝑘 \\nThe multiplication in the above equation is particularly problematic since both the sigmoid and tanh \\nget saturated at both ends and their derivative goes to 0. When this happens, they drive other gradients \\nin previous layers toward 0. This makes the gradient vanish completely after a few time steps and the \\nnetwork stops learning from “far away.”\\nChapter 5, Recurrent Neural Networks, discussed how to use Long Short-Term Memory ( LSTM ) and \\nGated Recurrent Units (GRUs ) to deal with the problem of vanishing gradients and efficiently learn \\nlong-range dependencies. In a similar way, the gradient can explode when one single term in the \\nmultiplication of the Jacobian matrix becomes large. Chapter 5  discussed how to use gradient clipping \\nto deal with this problem.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e19913a9-71c5-4258-99d5-249260ee8765', embedding=None, metadata={'page_label': '495', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 495\\nWe have now concluded this journey, and you should now understand how backpropagation works \\nand how it is applied in neural networks for dense networks, CNNs, and RNNs. In the next section, we \\nwill discuss how TensorFlow computes gradients, and why this is useful for backpropagation.\\nA note on TensorFlow and automatic differentiation\\nTensorFlow can automatically calculate derivatives, a feature called automatic differentiation. This \\nis achieved by using the chain rule. Every node in the computational graph has an attached gradient \\noperation for calculating the derivatives of input with respect to output. After that, the gradients with \\nrespect to parameters are automatically computed during backpropagation.\\nAutomatic differentiation is a very important feature because you do not need to hand-code new vari-\\nations of backpropagation for each new model of a neural network. This allows for quick iteration \\nand running many experiments faster.\\nSummary\\nIn this chapter, we discussed the math behind deep learning. Put simply, a deep learning model com-\\nputes a function given an input vector to produce the output. The interesting part is that it can literally \\nhave billions of parameters (weights) to be tuned. Backpropagation is a core mathematical algorithm \\nused by deep learning for efficiently training artificial neural networks, following a gradient descent \\napproach that exploits the chain rule. The algorithm is based on two steps repeated alternatively: the \\nforward step and the backstep. \\nDuring the forward step, inputs are propagated through the network to predict the outputs. These \\npredictions might be different from the true values given to assess the quality of the network. In other \\nwords, there is an error and our goal is to minimize it. This is where the backstep plays a role, by ad-\\njusting the weights of the network to minimize the error. The error is computed via loss functions such \\nas Mean Squared Error (MSE ), or cross-entropy for non-continuous values such as Boolean (Chapter \\n1, Neural Network Foundations with TF). A gradient-descent-optimization algorithm is used to adjust \\nthe weight of neurons by calculating the gradient of the loss function. Backpropagation computes \\nthe gradient, and gradient descent uses the gradients for training the model. A reduction in the error \\nrate of predictions increases accuracy, allowing machine learning models to improve. SGD is the \\nsimplest thing you could possibly do by taking one step in the direction of the gradient. This chapter \\ndoes not cover the math behind other optimizers such as Adam and RMSProp (Chapter 1). However, \\nthey involve using the first and the second moments of the gradients. The first moment involves the \\nexponentially decaying average of the previous gradients, and the second moment involves the expo -\\nnentially decaying average of the previous squared gradients.\\nThere are three big properties of our data that justify using deep learning; otherwise, we might as \\nwell use regular machine learning: \\n• Very-high-dimensional input (text, images, audio signals, videos, and temporal series are \\nfrequently a good example).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88703005-7e51-4b04-bbf5-3568533ba8fb', embedding=None, metadata={'page_label': '496', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Math Behind Deep Learning 496\\n• Dealing with complex decision surfaces that cannot be approximated with a low-order poly -\\nnomial function.\\n• Having a large amount of training data available. \\nDeep learning models can be thought of as a computational graph made up of stacking together several \\nbasic components such as dense networks (Chapter 1), CNNs (Chapter 3), embeddings (Chapter 4), RNNs \\n(Chapter 5 ), GANs ( Chapter 9 ), autoencoders ( Chapter 8 ) and, sometimes, adopting shortcut connections \\nsuch as “peephole,” “skip,” and “residual” because they help data flow a bit more smoothly. Each node \\nin the graph takes tensors as input and produces tensors as output. As discussed, training happens \\nby adjusting the weights in each node with backprop, where the key idea is to reduce the error in the \\nfinal output node(s) via gradient descent. GPUs and TPUs (Chapter 15) can significantly accelerate the \\noptimization process since it is essentially based on (hundreds of) millions of matrix computations.\\nThere are a few other mathematical tools that might be helpful to improve your learning process. \\nRegularization (L1, L2, and Lasso (Chapter 1)) can significantly improve learning by keeping weights \\nnormalized. Batch normalization (Chapter 1) helps to basically keep track of the mean and the standard \\ndeviation of your dataset across multiple deep layers. The key idea is to have data resembling a normal \\ndistribution while it flows through the computational graph. Dropout (Chapters 1, 3, 5, 6, 9, and 20) \\nhelps by introducing some elements of redundancy in your computation; this prevents overfitting \\nand allows better generalization.\\nThis chapter has presented the mathematical foundation behind intuition. As discussed, this topic is \\nquite advanced and not necessarily required for practitioners. However, it is recommended reading \\nif you are interested in understanding what is going on “under the hood” when you play with neural \\nnetworks.\\nThe next chapter will introduce the Tensor Processing Unit (TPU ), a special chip developed at Google \\nfor ultra-fast execution of many mathematical operations described in this chapter.\\nReferences\\n1. Kelley, Henry J. (1960). Gradient theory of optimal flight paths. ARS Journal. 30 (10): 947–954. \\nBibcode:1960ARSJ...30.1127B. doi:10.2514/8.5282.\\n2. Dreyfus, Stuart. (1962). The numerical solution of variational problems. Journal of Mathematical \\nAnalysis and Applications. 5 (1): 30–45. doi:10.1016/0022-247x(62)90004-5.\\n3. Werbos, P. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sci-\\nences. PhD thesis, Harvard University.\\n4. Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986-10-09). Learning represen-\\ntations by back-propagating errors. Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. \\ndoi:10.1038/323533a0.\\n5. LeCun, Y. (1987). Modèles Connexionnistes de l’apprentissage (Connectionist Learning Models), Ph.D. \\nthesis, Université P. et M. Curie.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b62c5158-6b85-4639-810e-748d6931e404', embedding=None, metadata={'page_label': '497', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14 497\\n6. Herbert Robbins and Sutton Monro. (1951). A Stochastic Approximation Method. The Annals of \\nMathematical Statistics, Vol. 22, No. 3. pp. 400–407.\\n7. Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (June 2017). ImageNet classification \\nwith deep convolutional neural networks  (PDF). Communications of the ACM. 60 (6): 84–90. \\ndoi:10.1145/3065386. ISSN 0001-0782. \\n8. From not working to neural networking. The Economist. (25 June 2016)\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db5557ae-dc4a-43b4-84bd-8b7a127875ec', embedding=None, metadata={'page_label': '498', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09648921-4924-4cbd-9b17-940784868841', embedding=None, metadata={'page_label': '499', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15\\nTensor Processing Unit\\nThis chapter introduces the Tensor Processing Unit  (TPU ), a special chip developed at Google for \\nultra-fast execution of neural network mathematical operations. As with Graphics Processing Units \\n(GPUs ), the idea here is to have a special processor focusing only on very fast matrix operations, with no \\nsupport for all the other operations normally supported by Central Processing Units ( CPUs ). However, \\nthe additional improvement with TPUs is to remove from the chip any hardware support for graphics \\noperations normally present in GPUs (rasterization, texture mapping, frame buffer operations, and so \\non). Think of a TPU as a special purpose co-processor specialized for deep learning, being focused on \\nmatrix or tensor operations. In this chapter, we will compare CPUs and GPUs with the four generations \\nof TPUs and with Edge TPUs. All these accelerators are available as of April 2022. The chapter will \\ninclude code examples of using TPUs.\\nIn this chapter, you will learn the following:\\n• C/G/T processing units\\n• Four generations of TPUs and Edge TPUs\\n• TPU performance\\n• How to use TPUs with Colab\\nSo with that, let’s begin.\\nC/G/T processing units\\nIn this section we discuss CPUs, GPUs, and TPUs. Before discussing TPUs, it will be useful for us to \\nreview CPUs and GPUs.\\nCPUs and GPUs\\nYou are probably somewhat familiar with the concept of a CPU, a general-purpose chip sitting in each \\ncomputer, tablet, and smartphone. CPUs are in charge of all of the computations: from logical controls, \\nto arithmetic, to register operations, to operations with memory, and many others. CPUs are subject \\nto the well-known Moore’s law [1], which states that the number of transistors in a dense integrated \\ncircuit doubles about every two years. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd099b7e-46dd-486b-a075-87800d7a4dfd', embedding=None, metadata={'page_label': '500', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 500\\nMany people believe that we are currently in an era where this trend cannot be sustained for long, and \\nindeed it has already declined during the past decade. Therefore, we need some additional technology \\nif we want to support the demand for faster and faster computation to process the ever-growing amount \\nof data that is available out there.\\nOne improvement came from GPUs, special-purpose chips that are perfect for fast graphics operations \\nsuch as matrix multiplication, rasterization, frame-buffer manipulation, texture mapping, and many \\nothers. In addition to computer graphics where matrix multiplications are applied to pixels of images, \\nGPUs turned out to be a great match for deep learning. This is a funny story of serendipity (serendipity \\nis the occurrence and development of events by chance in a happy or beneficial way) – a great example \\nof technology created for one goal and then being met with staggering success in a domain completely \\nunrelated to the one it was originally envisioned for.\\nTPUs\\nOne problem encountered in using GPUs for deep learning is that these chips are made for graphics \\nand gaming, not only for fast matrix computations. This would of course be the case, given that the \\nG in GPU stands for Graphics! GPUs led to unbelievable improvements in deep learning but, in the \\ncase of tensor operations for neural networks, large parts of the chip are not used at all. For deep \\nlearning, there is no need for rasterization, no need for frame-buffer manipulation, and no need for \\ntexture mapping. The only thing that is necessary is a very efficient way to compute matrix and tensor \\noperations. It should be no surprise that GPUs are not necessarily the ideal solution for deep learning, \\nsince CPUs and GPUs were designed long before deep learning became successful.\\nBefore going into technical details, let’s first discuss the fascinating genesis of Tensor Processing \\nUnit version 1, or TPU v1. In 2013, Jeff Dean, the Chief of the Brain Division at Google, estimated \\n(see Figure 15.1) that if all the people owning a mobile phone were talking in calls for only 3 minutes \\nmore per day, then Google would have needed two or three times more servers to process this data. \\nThis would have been an unaffordable case of success-disaster, i.e., where great success has led to \\nproblems that cannot be properly managed. It was clear that neither CPUs nor GPUs were a suitable \\nsolution. So, Google decided that they needed something completely new – something that would allow \\na 10x growth in performance with no significant cost increase. That’s how TPU v1 was born! What is \\nimpressive is that it took only 15 months from initial design to production. You can find more details \\nabout this story in Jouppi et al., 2014 [3], where a detailed report about different inference workloads \\nseen at Google in 2013 is also reported:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ddfa8227-cd53-4123-9f92-d5d2987a5932', embedding=None, metadata={'page_label': '501', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 501\\nFigure 15.1: Different inference workloads seen at Google in 2013 (source [3])\\nLet’s talk a bit about the technical details. A TPU v1 is a special device (or an Application-Specific \\nIntegrated Circuit, or ASIC  for short) designed for super-efficient tensor operations. TPUs follow \\nthe philosophy less is more. This philosophy has an important consequence: TPUs do not have all the \\ngraphic components that are needed for GPUs. Because of this, they are both very efficient from an \\nenergy consumption perspective, and frequently much faster than GPUs. So far, there have been four \\ngenerations of TPUs. Let’s review them.\\nFour generations of TPUs, plus Edge TPU\\nAs discussed, TPUs are domain-specific processors expressly optimized for matrix operations. Now, \\nyou might remember that the basic operation of matrix multiplication is a dot product between a line \\nfrom one matrix and a column from the other matrix. For instance, given a matrix multiplication \\n𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌  , computing Y[i, 0] is:\\n𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌 𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌 𝑌𝑌𝑌𝑌𝑌𝑌...𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑋𝑋𝑌𝑌𝑌𝑌𝑌𝑋𝑋𝑌𝑌𝑌 \\nThe sequential implementation of this operation is time-consuming for large matrices. A brute-force \\ncomputation has a time complexity of O(n3) for n x n matrices so it’s not feasible for running large \\ncomputations.\\nFirst generation TPU\\nThe first generation TPU (TPU v1) was announced in May 2016 at Google I/O. TPU v1 [1] supports matrix \\nmultiplication using 8-bit arithmetic. TPU v1 is specialized for deep learning inference but it does \\nnot work for training. For training there is a need to perform floating-point operations, as discussed \\nin the following paragraphs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b3da0f8-3a60-4b85-a888-5cfabd23c0a4', embedding=None, metadata={'page_label': '502', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 502\\nA key function of TPU is the “systolic” matrix multiplication. Let’s see what this means. Remember \\nthat the core of deep learning is a core product 𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌  , where, for instance, the basic operation \\nto compute Y[i, 0] is:\\n𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌 𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌 𝑛𝑛𝑌𝑌𝑌𝑌𝑌𝑛𝑛𝑌𝑌𝑌 \\n“Systolic” matrix multiplication allows multiple Y[i, j] values to be computed in parallel. Data flows in \\na coordinated manner and, indeed, in medicine the term “systolic” refers to heart contractions and \\nhow blood flows rhythmically in our veins. Here systolic refers to the data flow that pulses inside the \\nTPU. It can be proven that a systolic multiplication algorithm is less expensive than the brute-force \\none [2]. A TPU v1 has a Matrix Multiply Unit (MMU) running systolic multiplications on 256 x 256 \\ncores so that 65,536 multiplications can be computed in parallel in one single shot. In addition, a \\nTPU v1 sits in a rack and it is not directly accessible. Instead, a CPU acts as the host controlling data \\ntransfer and sending commands to the TPU for performing tensor multiplications, for computing \\nconvolutions, and for applying activation functions. The CPU ↔  TPU v1 communication happens via \\na standard PCIe 3.0 bus. From this perspective, a TPU v1 is closer in spirit to a Floating-Point Unit \\n(FPU ) coprocessor than it is to a GPU. However, a TPU v1 has the ability to run whole inference models \\nto reduce dependence on the host CPU. Figure 15.2 represents TPU v1, as shown in [3]. As you see in \\nthe figure, the processing unit is connected via a PCI port, and it fetches weights via a standard DDR4 \\nDRAM chip. Multiplication happens within the MMU with systolic processing. Activation functions \\nare then applied to the results. The MMU and the unified buffer for activations take up a lot of space. \\nThere is an area where the activation functions are computed.\\nFigure 15.2: TPU v1 design schema (source [3])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a9adbad-122a-48cf-b6a6-ba24604696b6', embedding=None, metadata={'page_label': '503', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 503\\nTPU v1s are manufactured on a 28 nm process node with a die size of ≤ 331 mm2, a clock speed of 700 \\nMHz, 28 MiB of on-chip memory, 4 MiB of 32-bit accumulators, and a 256 x 256 systolic array of 8-bit \\nmultipliers. For this reason, we can get 700 MHz*65,536 (multipliers) →  92 Tera operations/sec. This \\nis an amazing performance for matrix multiplications; Figure 15.3 shows the TPU circuit board and \\nflow of data for the systolic matrix multiplication performed by the MMU. In addition, TPU v1 has an \\n8 GiB of dual-channel 2133 MHz DDR3 SDRAM offering 34 GB/s of bandwidth. The external memory \\nis standard, and it is used to store and fetch the weights used during the inference. Notice also that \\nTPU v1 has a thermal design power of 28–40 watts, which is certainly low consumption compared to \\nGPUs and CPUs. Moreover, TPU v1s are normally mounted in a PCI slot used for SATA disks so they do \\nnot require any modification in the host server [3]. Up to four cards can be mounted on each server. \\nFigure 15.3 shows a TPU v1 card and the process of systolic computation:\\nFigure 15.3: On the left you can see a TPU v1 board, and on the right an example of how the data is \\nprocessed during the systolic computation\\nIf you want to have a look at TPU performance compared to GPUs and CPUs, you can refer to [3] and \\nsee (in a log-log scale graph) that the performance is two orders of magnitude higher than a Tesla K80 \\nGPU. The graph shows a “rooftop” performance, which is growing until the point where it reaches the \\npeak and then it is constant. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8d1da3f8-a7dc-428c-a97e-17b7825c6009', embedding=None, metadata={'page_label': '504', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 504\\nThe higher the roof, the better performance is:\\nFigure 15.4: TPU v1 peak performance can be up to 3x higher than a Tesla K80\\nSecond generation TPU\\nThe second generation TPUs (TPU2s) were announced in 2017. In this case, the memory bandwidth is \\nincreased to 600 GB/s and performance reaches 45 TFLOPS. Four TPU2s are arranged in a module with \\n180 TFLOPS of performance. Then 64 modules are grouped into a pod with 11.5 PFLOPS of performance. \\nTPU2s adopt floating-point arithmetic and therefore they are suitable for both training and inference.\\nA TPU2 has an MNU for matrix multiplications of 128*128 cores and a Vector Processing Unit (VPU ) \\nfor all other tasks such as applying activations etc. The VPU handles float32 and int32 computations. \\nThe MXU on the other hand operates in a mixed precision 16–32 bit floating-point format.\\nEach TPU v2 chip has two cores, and up to four chips are mounted on each board. In TPU v2, Google \\nadopted a new floating-point model called bfloat16 The idea is to sacrifice some resolution but still \\nbe very good for deep learning. This reduction in resolution allows us to improve the performance \\nof the TPU2s, which are more power-efficient than the v1s. Indeed, It can be proven that a smaller \\nmantissa helps reduce the physical silicon area and multiplier power. Therefore, the bfloat16 uses \\nthe same standard IEEE 754 single-precision floating-point format, but it truncates the mantissa field \\nfrom 23 bits to just 7 bits. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96cad37f-2df1-4f54-8910-81a95eb5f143', embedding=None, metadata={'page_label': '505', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 505\\nPreserving the exponent bits allows the format to keep the same range as the 32-bit single precision. \\nThis allows for relatively simpler conversion between the two data types:\\nFigure 15.5: Cloud TPU v2 and Cloud TPU v3\\nGoogle offers access to these TPU v2 and TPU v3 via Google Compute Engine (GCE), and via Google \\nKubernetes Engine (GKE ). Plus, it is possible to use them for free via Colab.\\nThird generation TPU\\nThe third generation TPUs (TPU3) were announced in 2018 [4]. TPU3s are 2x faster than TPU2 and they \\nare grouped in 4x larger pods. In total, this is an 8x performance increase. Cloud TPU v3 Pods can \\ndeliver more than 100 petaflops of computing power. On the other hand, Cloud TPU v2 Pods released \\nin alpha in 2018 can achieve 11.5 petaflops – another impressive improvement. As of 2019, both TPU2 \\nand TPU3 are in production at different prices:\\nFigure 15.6: Google announced TPU v2 and v3 Pods in beta at the Google I/O 2019\\nA TPU v3 board has four TPU chips, eight cores, and liquid cooling. Google has adopted ultra-high-\\nspeed interconnect hardware derived from supercomputer technology for connecting thousands of \\nTPUs with very low latency. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e76b5eab-fb73-4d63-a777-b87a9728befc', embedding=None, metadata={'page_label': '506', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 506\\nEach time a parameter is updated on a single TPU, all the others are informed via a reduce-all algorithm \\ntypically adopted for parallel computation. So, you can think about a TPU v3 as one of the fastest \\nsupercomputers available today for matrix and tensor operations, with thousands of TPUs inside it.\\nFourth generation TPUs\\nGoogle’s fourth generation TPU ASIC has more than double the matrix multiplication TFLOPs of TPU \\nv3, a considerable boost in memory bandwidth, and more advances in interconnect technology. Each \\nTPU v4 chip provides more than 2x the compute power of a TPU v3 chip – up to 275 peak TFLOPS. \\nEach TPU v4 Pod delivers 1.1 exaflops/s of peak performance. Google claims that TPU v4 Pods are used \\nextensively to develop research breakthroughs such as MUM and LaMDA, and improve core products \\nsuch as Search, Assistant, and Translate (see https://blog.google/technology/developers/io21-\\nhelpful-google/ ). As of April 2022, TPU v4s are only available in preview (Figure 15.7):\\nFigure 15.7: A TPU v4 chip and a portion of a TPU v4 Pod – source: https://twitter.com/google/\\nstatus/1394785686683783170 \\nIn this section, we have introduced four generations of TPUs. Before concluding, I wanted to mention \\nthat it is possible to save money by using preemptible Cloud TPUs for fault-tolerant machine learning \\nworkloads. These workloads include but are not limited to long training runs with checkpointing or \\nbatch prediction on large datasets.\\nEdge TPU\\nIn addition to the three generations of TPUs already discussed, in 2018 Google announced a special \\ngeneration of TPUs running on the edge. This TPU is particularly appropriate for the Internet of Things \\n(IoT ) and for supporting TensorFlow Lite on mobile and IoT. An individual Edge TPU can perform \\n4 trillion (fixed-point) operations per second (4 TOPS), using only 2 watts of power. An Edge TPU is \\ndesigned for small, low-power devices and is ideal for on-device ML, with it being fast and power-\\nefficient. Edge TPUs support the TensorFlow Lite development framework (see Figure 15.8). At the end of \\n2019, Google announced the Pixel 4 smartphone  containing an Edge TPU called the Pixel Neural Core:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17e99d94-ce03-4a00-9b5f-a61e4ca412d4', embedding=None, metadata={'page_label': '507', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 507\\nFigure 15.8: Two Edge TPUs on one penny – source: https://coral.ai/docs/edgetpu/faq/#what-is-\\nthe-edge-tpu \\nWith this we conclude the introduction to TPU v1, v2, v3, v4, and Edge TPU. In the next section we \\nwill briefly discuss performance.\\nTPU performance\\nDiscussing performance is always difficult because it is important to first define the metrics that we \\nare going to measure, and the set of workloads that we are going to use as benchmarks. For instance, \\nGoogle reported an impressive linear scaling for TPU v2 used with ResNet-50 [4] (see Figure 15.9 and \\nFigure 15.10):\\nFigure 15.9: Linear scalability in the number of TPUs v2 when increasing the number of images', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7d8e84c-6cb6-4403-9b85-169b62e224bf', embedding=None, metadata={'page_label': '508', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 508\\nIn addition, you can find online a comparison of ResNet-50 [4] where a full Cloud TPU v2 Pod is >200x \\nfaster than a V100 NVIDIA Tesla GPU for ResNet-50 training:\\nFigure 15.10: A full Cloud TPU v2 Pod is >200x faster than a V100 NVIDIA Tesla GPU for training a \\nResNet-50 model\\nAccording to Google, TPU v4 givse top-line results for MLPerf1.0 [5] when compared with NVIDIA  \\nA100 GPUs (see Figure 15.11). Indeed, these accelerators are designed by keeping in mind the latest \\nlarge models encompassing billions and sometimes trillions of parameters (think about GPT-3, T5, \\nand the Switch Transformer):\\nFigure 15.11: MLPerf 1.0 TPU v4 Pod performance – source: https://cloud.google.com/blog/products/\\nai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1dd61446-ac27-412a-b0f6-3f786f8af51d', embedding=None, metadata={'page_label': '509', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 509\\nHow to use TPUs with Colab\\nIn this section, we show how to use TPUs with Colab. Just point your browser to https://colab.\\nresearch.google.com/  and change the runtime from the Runtime menu as shown in Figure 15.12. \\nFirst, you’ll need to enable TPUs for the notebook, then navigate to Edit→Notebook settings and select \\nTPU  from the Hardware accelerator drop-down box:\\nFigure 15.12: Setting TPU as the hardware accelerator\\nChecking whether TPUs are available\\nFirst of all, let’s check if there is a TPU available, by using this simple code fragment that returns the \\nIP address assigned to the TPU. Communication between the CPU and TPU happens via gRPC  (gRPC \\nRemote Procedure Call), which is a modern, open-source, high-performance Remote Procedure Call \\n(RPC) framework that can run in any environment:\\n%tensorflow_version 2.x\\nimport tensorflow as tf\\nprint(\"Tensorflow version \"  + tf.__version__)\\ntry:\\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\\n  print(\\'Running on TPU \\' , tpu.cluster_spec().as_dict()[ \\'worker\\' ])\\nexcept ValueError:\\n  raise BaseException( \\'ERROR: Not connected to a TPU runtime; please see the \\nprevious cell in this notebook for instructions!\\' )\\ntf.config.experimental_connect_to_cluster(tpu)\\ntf.tpu.experimental.initialize_tpu_system(tpu)\\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d07aabe4-c66c-4b8e-b774-159efbc1e38b', embedding=None, metadata={'page_label': '510', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 510\\nYou should see something like the following:\\nTensorflow version 2.8.0\\nRunning on TPU  [\\'10.36.66.50:8470\\']\\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\\nINFO:tensorflow:Initializing the TPU system: grpc://10.36.66.50:8470\\nINFO:tensorflow:Initializing the TPU system: grpc://10.36.66.50:8470\\nINFO:tensorflow:Finished initializing TPU system.\\nINFO:tensorflow:Finished initializing TPU system.\\nWARNING:absl:\\'tf.distribute.experimental.TPUStrategy\\' is deprecated, please use  \\nthe non experimental symbol \\'tf.distribute.TPUStrategy\\' instead.\\nINFO:tensorflow:Found TPU system:\\nINFO:tensorflow:Found TPU system:\\nINFO:tensorflow:*** Num TPU Cores: 8\\nINFO:tensorflow:*** Num TPU Cores: 8\\nINFO:tensorflow:*** Num TPU Workers: 1\\nINFO:tensorflow:*** Num TPU Workers: 1\\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\\nWe’ve confirmed that a TPU is available! \\nKeras MNIST TPU end-to-end training\\nReferring to the notebook available on Google Research Colab (see https://colab.research.google.\\ncom/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-\\ndata-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7 ), we can check how TPUs or \\nGPUs are detected with this code snippet, which uses either TPUs or GPUs as a fallback:\\ntry: # detect TPUs\\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU \\ndetection\\n    strategy = tf.distribute.TPUStrategy(tpu)\\nexcept ValueError: # detect GPUs\\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\\n    #strategy = tf.distribute.get_strategy() # default strategy that works on \\nCPU and single GPU\\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for \\nclusters of multi-GPU machines\\nprint(\"Number of accelerators: \" , strategy.num_replicas_in_sync)\\nNote that the strategy tf.distribute.TPUStrategy(tpu)  is the only change you need in code for \\nsynchronous training on TPUs and TPU Pods. Then, to run TF2 programs on TPUs, you can either use \\n.compile  or .fit  APIs in tf.keras  with TPUStrategy . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfbd73ac-bb75-4c28-984f-e2df2280fb38', embedding=None, metadata={'page_label': '511', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 511\\nIf you want you can write your own customized training loop by calling strategy.run  directly (see \\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy ).\\nUsing pretrained TPU models\\nGoogle offers a collection of models pretrained with TPUs available in the GitHub tensorflow/tpu  \\nrepository ( https://github.com/tensorflow/tpu ). Models include image recognition, object detection, \\nlow-resource models, machine translation and language models, speech recognition, and image \\ngeneration. Whenever it is possible, my suggestion is to start with a pretrained model [6], and then \\nfine-tune it or apply some form of transfer learning. As of April 2022, the following models are available:\\nImage Recognition, \\nSegmentation, and MoreMachine Translation and \\nLanguage ModelsSpeech Recognition Image \\nGeneration\\nImage Recognition\\nAmoebaNet-D\\nResNet-50/101/152/2000\\nInception v2/v3/v4\\nObject Detection\\nRetinaNet\\nMask R-CNN\\nImage Segmentation\\nMask R-CNN\\nDeepLab\\nRetinaNet\\nLow-Resource Models\\nMnasNet\\nMobileNet\\nSqueezeNetMachine Translation\\n(transformer based)\\nSentiment Analysis\\n(transformer based)\\nQuestion Answer\\nBERTASR \\nTransformerImage \\nTransformer\\nDCGAN\\nGAN\\nTable 15.1: State-of-the-art collection of models pretrained with TPUs available on GitHub', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66282031-892e-4437-bbbe-7b21e3787169', embedding=None, metadata={'page_label': '512', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 512\\nThe best way to play with the repository is to clone it on the Google Cloud console and use the \\nenvironment available at https://github.com/tensorflow/tpu/blob/master/README.md . You should \\nbe able to browse what is shown in Figure 15.13:\\nFigure 15.13: Cloud TPUs\\nIf you click the button OPEN IN GOOGLE CLOUD SHELL, then the system will clone the Git repository \\ninto your cloud shell and then open the shell (see Figure 15.14):\\nFigure 15.14: Google Cloud Shell with the TPU Git repository cloned on your behalf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='abb4bbc0-1c90-4d7b-941f-d12975d7b82a', embedding=None, metadata={'page_label': '513', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15 513\\nFrom there, you can play with a nice Google Cloud TPU demo for training a ResNet-50 on MNIST with \\na TPU flock – a Compute Engine VM and Cloud TPU pair (see Figure 15.15):\\nFigure 15.15: Google Cloud TPU demo for training a ResNet-50 on MNIST with a TPU flock\\nI will leave this training demo for you if you are interested in looking it up.\\nSummary\\nTPUs are very special ASIC chips developed at Google for executing neural network mathematical \\noperations in an ultra-fast manner. The core of the computation is a systolic multiplier that computes \\nmultiple dot products (row * column) in parallel, thus accelerating the computation of basic deep \\nlearning operations. Think of a TPU as a special-purpose co-processor for deep learning that is \\nfocused on matrix or tensor operations. Google has announced four generations of TPUs so far, plus \\nan additional Edge TPU for IoT. Cloud TPU v1 is a PCI-based specialized co-processor, with 92 teraops \\nand inference only. Cloud TPU v2 achieves 180 teraflops and it supports training and inference. Cloud \\nTPU v2 Pods released in alpha in 2018 can achieve 11.5 petaflops. Cloud TPU v3 achieves 420 teraflops \\nwith both training and inference support. Cloud TPU v3 Pods can deliver more than 100 petaflops of \\ncomputing power. Each TPU v4 chip provides more than 2x the compute power of a TPU v3 chip – up \\nto 275 peak TFLOPS. Each TPU v4 Pod delivers 1.1 exaflops/s of peak performance.\\nThat’s a world-class supercomputer for tensor operations!\\nIn the next chapter, we will see some other useful deep learning libraries.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='055f1d7e-c8d4-4013-9d07-15afbf8d71ee', embedding=None, metadata={'page_label': '514', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tensor Processing Unit 514\\nReferences\\n1. Moore’s law: https://en.wikipedia.org/wiki/Moore%27s_law\\n2. Milovanović, I. Ž. et al. (May 2010). Forty-three ways of systolic matrix multiplication. Article in \\nInternational Journal of Computer Mathematics 87(6):1264–1276.\\n3. Jouppi, N. P. et al. (June 2014). In-Datacenter Performance Analysis of a Tensor Processing Unit. \\n44th International Symposium on Computer Architecture (ISCA).\\n4. Google TPU v2 performance: https://storage.googleapis.com/nexttpu/index.html\\n5. MLPerf site: https://mlperf.org/\\n6. A collection of models pretrained with TPU: https://cloud.google.com/tpu\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a3950cb-bf50-404e-b354-b81352b95315', embedding=None, metadata={'page_label': '515', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16\\nOther Useful Deep Learning \\nLibraries\\nTensorFlow from Google is not the only framework available for deep learning tasks. There is a good \\nrange of libraries and frameworks available, each with its special features, capabilities, and use cases. \\nIn this chapter, we will explore some of the popular deep learning libraries and compare their features. \\nThe chapter will include:\\n• Hugging Face\\n• H2O\\n• PyTorch\\n• ONNX\\n• Open AI \\nLet’s begin!\\nHugging Face\\nHugging Face is not new for us; Chapter 6, Transformers, introduced us to the library. Hugging Face is an \\nNLP-centered startup, founded by Delangue and Chaumond in 2016. It has, in a short time, established \\nitself as one of the best tools for all NLP-related tasks. The AutoNLP and accelerated inference API are \\navailable for a price. However, its core NLP libraries datasets, tokenizers, Accelerate, and transformers \\n(Figure 16.1) are available for free. It has built a cool community-driven open-source platform. All the code files for this chapter can be found at https://packt.link/dltfchp16 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7be72e64-acdc-4c17-a089-7a4dd8d40f31', embedding=None, metadata={'page_label': '516', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 516\\nFigure 16.1: NLP libraries from Hugging Face \\nThe core of the Hugging  Face ecosystem is its transformers library. The Tokenizers and Datasets libraries \\nsupport the Transformers library. To use these libraries, we need to install them first. Transformers \\ncan be installed using a simple pip install  command:\\npip install transformers\\nSome of the out-of-the-box models available with Hugging Face are text summarization, question \\nanswering, text classification, audio classification, automatic speech recognition, feature extraction, \\nimage classification, and translation. In Figure 16.2, we can see the result of the out-of-the-box \\nsummarization model available with Hugging Face.\\nFigure 16.2: Out-of-the-box text summarization using Hugging Face', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b53cc5a-a0b4-4b39-b2d5-a8d43716d4b4', embedding=None, metadata={'page_label': '517', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16 517\\nBesides these out-of-the-box models, we can use the large number of models and datasets available at \\nHugging Face Hub and can use them with PyTorch, TensorFlow, and JAX to build customized models. \\nOpenAI\\nOpenAI is another well-known name for people working in the field of reinforcement learning. Their \\nGym module is a standard toolkit used by developers across the globe for developing and comparing \\nreinforcement learning algorithms. In Chapter 11, Reinforcement Learning, we have already covered \\nthe Gym module in detail. In this chapter, we will explore two more offerings by OpenAI.\\nOpenAI GPT-3 API\\n“OpenAI GPT3 is a machine learning platform that allows developers to build custom algorithms for \\ndeep learning. This platform was released in December of 2017 and has been widely used by businesses \\nand individuals in the field of artificial intelligence. One of the primary reasons that GPT3 has been so \\nsuccessful is because it is easy to use and has a wide range of features. This platform is able to learn \\nfrom data and can be used for a variety of tasks, including deep learning, natural language processing, \\nand image recognition. GPT3 is also popular because it is open source and can be used by anyone. This \\nmakes it an ideal platform for anyone who wants to learn about deep learning and the various ways \\nthat it can be used. Overall, GPT3 is a powerful and easy-to-use machine learning platform that has \\nbeen widely used by businesses and individuals in the field of artificial intelligence.”\\nThis is the text generated by the OpenAI GPT-3 API, when asked to write on GPT-3 itself ( https://\\nbeta.openai.com/playground ): \\nFigure 16.3: Text generation using OpenAI GPT-3 API', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6576d90e-cb59-4e8a-8233-5e8340382cea', embedding=None, metadata={'page_label': '518', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 518\\nThe OpenAI GPT-3 API offers the following tasks:\\n• Text Completion: Here, the GPT-3 API is used to generate or manipulate text and even code. \\nYou can use it to write a tagline, an introduction, or an essay, or you can leave a sentence half-\\nwritten and ask it to complete it. People have used it to generate stories and advertisement leads.\\n• Semantic Search : This allows you to do a semantic search over a set of documents. For example, \\nyou can upload documents using the API; it can handle up to 200 documents, where each file \\ncan be a maximum of 150 MB in size, and the total limited to 1 GB at any given time. The API \\nwill take your query and rank the documents based on the semantic similarity score (ranges \\nnormally between 0-300). \\n• Question Answering: This API uses the documents uploaded as the source of truth; the API \\nfirst searches the documents for relevance to the question. Then it ranks them based on the \\nsemantic relevance and finally, answers the question. \\n• Text Classification: The text classification endpoint of OpenAI GPT-3 takes as input a labeled \\nset of examples and then uses the labels in it to label the query text. There are a lot of examples \\nwhere this feature has been used to perform sentiment analysis.\\nInitially, the OpenAI GPT-3 was available only after applying for it, but now, anyone can use the API; \\nthere is no longer a waitlist.\\nOpenAI DALL-E 2\\nThe GPT-3 API by OpenAI deals with all things related to NLP; DALL-E 2 goes a step further. DALL-E \\nwas originally released by OpenAI in January 2021. It claims to produce photorealistic images based \\non the textual description provided to the model. It can also make realistic edits to existing images; \\nyou can use it to add or remove objects and elements from the image, and when it does so, it considers \\nthe effect on shadows, reflections, and texture. Figure 16.4 shows some of the remarkable feats by \\nDALL-E 2. In the figures on the top row, I gave DALL-E 2 a text describing what I want: “Albert Einstein \\nflying on dinosaur over the Amazon Forest.” It generated a cartoon-like image. The images in the lower \\nrow are generated using the image-editor feature of DALL-E 2. I added the image on the left, and it \\ngenerated four variations. The variations look very realistic if you ignore that the faces are blurred: \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='338841e7-88b2-43f2-a4a7-162a32a2e697', embedding=None, metadata={'page_label': '519', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16 519\\nFigure 16.4: On top is the image generated by DALL-E 2, and below are the images edited by DALL-E 2\\nAt the time of writing this book (August 2022), DALL-E 2 is not available for public use. But imagine \\nthe possibilities for artists and professionals working in creating digital media once the model is \\navailable as an API.\\nOpenAI Codex\\nWhen a student starts with their first lessons of programming, as a teacher, I often recommend that \\nthey think of a program as a set of instructions – the only important thing to master is writing those \\ninstructions as clearly as possible in whatever language you know.\\nWell, Codex makes it happen, you just need to give it the instructions of what you want to achieve, and \\nit will generate the respective code for you. \\nOpenAI launches it as a general-purpose programming model, and it has been trained on publicly \\navailable GitHub codes.\\nBelow are a few snippets of the task and corresponding code generated by Codex:\\n\"\"\" Ask the user for their name and say \"Hello\" in Python \"\"\"  \\nprint(\"Hello, world!\" ) \\nname = input(\"Please type your name: \" ) \\nprint(\"Hello, \"  + name + \"!\")\\n\"\"\" Write a program in Python to find sum of n fibonacci sequences \"\"\"  \\ndef Fibonacci (n): \\n    if n < 0: \\n        print (\"Incorrect input\" ) \\n    elif  n == 1: \\n       return 0 \\n    elif  n == 2: \\n        return 1 \\n    else : \\n        return Fibonacci(n - 1) + Fibonacci(n - 2) \\nn = int(input(\"Enter the number of terms: \" )) ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f8d7e06e-ce10-47d8-8376-51fc46502d24', embedding=None, metadata={'page_label': '520', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 520\\n# check if the number of terms is valid \\nif n <= 0: \\n    print(\"Please enter a positive integer\" ) \\nelse: \\n    print(\"Fibonacci sequence: \" ) \\n    for I in range (1, n + 1): \\n        print(Fibonacci(n), end= \\' \\')\\nThe first task, as you can see, is done flawlessly. In the second task, we asked it to find the sum of the \\nFibonacci sequence; instead, it generated the Fibonacci sequence, which is a more common problem. \\nThis tells us that while it is great at doing run-of-the-mill jobs, the need for real programmers is still \\nthere.\\nPyTorch\\nLike TensorFlow, PyTorch is a full-fledged deep learning framework. In AI-based social groups, you \\nwill often find die-hard fans of PyTorch and TensorFlow arguing that theirs is best. PyTorch, developed \\nby Facebook (Meta now), is an open-source deep learning framework. Many researchers prefer it for \\nits flexible and modular approach. PyTorch also has stable support for production deployment. Like \\nTF, the core of PyTorch is its tensor processing library and its automatic differentiation engine. In a \\nC++ runtime environment, it leverages TorchScript for an easy transition between graph and eager \\nmode. The major feature that makes PyTorch popular is its ability to use dynamic computation, i.e., \\nits ability to dynamically build the computational graph – this gives the programmer flexibility to \\nmodify and inspect the computational graphs anytime.\\nThe PyTorch library consists of many modules, which are used as building blocks to make complex \\nmodels. Additionally, PyTorch also provides convenient functions to transfer variables and models \\nbetween different devices viz CPU, GPU, or TPU. Of special mention are the following three powerful \\nmodules:\\n• NN Module: This is the base class where all layers and functions to build a deep learning \\nnetwork are. Below, you can see the code snippet where the NN module is used to build a \\nnetwork. The network can then be instantiated using the statement net = My_Net(1,10,5) ; \\nthis creates a network with one input channel, 10 output neurons, and a kernel of size 5x5:\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nclass My_Net(nn.Module):\\n    def __init__ (self, input_channel, output_neurons, kernel_size):\\n        super(My_Net, self).__init__()\\n        self.conv1 = nn.Conv2d(input_channel, 6, kernel_size)\\n        self.conv2 = nn.Conv2d( 6, 16, 5)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42477c22-e8b0-46be-a693-8c09ca55b803', embedding=None, metadata={'page_label': '521', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16 521\\n        self.fc1 = nn.Linear( 16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear( 120, 84)\\n        self.fc3 = nn.Linear( 84,output_neurons)\\n    def forward (self, x):\\n        x = F.max_pool2d(F.relu(self.conv1(x)), ( 2, 2))\\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\\n        x = x.view(- 1, self.num_flat_features(x))\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n    def num_flat_features (self, x):\\n        size = x.size()[ 1:]  \\n        num_features = 1\\n        for s in size:\\n            num_features *= s\\n        return num_features\\nHere is a summary of the network:\\nMy_Net(\\n    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\\n    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1,\\n    1))\\n    (fc1): Linear(in_features=400, out_features=120,\\n    bias=True)\\n    (fc2): Linear(in_features=120, out_features=84,\\n    bias=True)\\n    (fc3): Linear(in_features=84, out_features=10,\\n    bias=True)\\n)\\n• Autograd Module: This is the heart of PyTorch. The module provides classes and functions \\nthat are used for implementing automatic differentiation. The module creates an acyclic graph \\ncalled the dynamic computational graph; the leaves of this graph are the input tensors, and the \\nroot is the output tensors. It calculates a gradient by tracing the root to the leaf and multiplying \\nevery gradient in the path using the chain rule. The following code snippet shows how to use \\nthe Autograd module for calculating gradients. The backward()  function computes the gradient \\nof the loss with respect to all the tensors whose requires_grad  is set to True . So suppose you \\nhave a variable w, then after the call to backward(),  the tensor w.grad  will give us the gradient \\nof the loss with respect to w. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9ae71f1-4c59-43c1-9f69-6d9fe5b57a46', embedding=None, metadata={'page_label': '522', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 522\\nWe can then use this to update the variable w as per the learning rule: \\nloss = (y_true – y_pred). pow(2).sum()\\nloss.backward()\\n# Here the autograd is used to compute the backward pass. \\nWith torch.no_grad():\\n    W = w – lr_rate * w.grad\\n    w.grad = None # Manually set to zero after updating\\n• Optim Module: The Optim module implements various optimization algorithms. Some of the \\noptimizer algorithms available in Optim are SGD, AdaDelta, Adam, SparseAdam, AdaGrad, and \\nLBFGS. One can also use the Optim module to create complex optimizers. To use the Optim \\nmodule, one just needs to construct an optimizer object that will hold the current state and \\nwill update the parameters based on gradients.\\nPyTorch is used by many companies for their AI solutions. Tesla uses PyTorch for AutoPilot. The Tesla \\nAutopilot, which uses the footage from eight cameras around the vehicle, passes that footage through \\n48 neural networks for object detection, semantic segmentation, and monocular depth estimation. \\nThe system provides level 2 vehicle automation. They take video from all eight cameras to generate \\nroad layout, any static infrastructure (e.g., buildings and traffic/electricity poles), and 3D objects (other \\nvehicles, persons on the road, and so on). The networks are trained iteratively in real time. While \\na little technical, this 2019 talk by Andrej Karpathy, Director of AI at Tesla, gives a bird’s-eye view \\nof Autopilot and its capabilities: https://www.youtube.com/watch?v=oBklltKXtDE&t=670s . Uber’s \\nPyro, a probabilistic deep learning library, and OpenAI are other examples of big AI companies using \\nPyTorch for research and development.\\nONNX\\nOpen Neural Network Exchange (ONNX ) provides an open-source format for AI models. It supports \\nboth deep learning models and traditional machine learning models. It is a format designed to represent \\nany type of model, and it achieves this by using an intermediate representation of the computational \\ngraph created by different frameworks.  It supports PyTorch, TensorFlow, MATLAB, and many more \\ndeep learning frameworks. Thus, using ONNX, we can easily convert models from one framework \\nto another. This helps in reducing the time from research to deployment. For example, you can use \\nONNX to convert a PyTorch model to ONNX.js form, which can then be directly deployed on the web.\\nH2O.ai\\nH2O is a fast, scalable machine learning and deep learning framework developed by H2O.ai, released \\nunder the open-source Apache license. According to the company website, as of the time of writing \\nthis book, more than 20,000 organizations use H2O for their ML/deep learning needs. The company \\noffers many products like H2O AI cloud, H2O Driverless AI, H2O wave, and Sparkling Water. In this \\nsection, we will explore its open-source product, H2O. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b0b0850-1fe5-4136-8a71-aee7581238fc', embedding=None, metadata={'page_label': '523', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 16 523\\nIt works on big data infrastructure on Hadoop, Spark, or Kubernetes clusters and it can also work in \\nstandalone mode. It makes use of distributed systems and in-memory computing, which allows it to \\nhandle a large amount of data in memory, even with a small cluster of machines. It has an interface \\nfor R, Python, Java, Scala, and JavaScript, and even has a built-in web interface. \\nH2O includes a large number of statistical-based ML algorithms such as generalized linear modeling, \\nNaive Bayes, random forest, gradient boosting, and all major deep learning algorithms. The best part \\nof H2O is that one can build thousands of models, compare the results, and even do hyperparameter \\ntuning with only a few lines of code. H2O also has better data pre-processing tools. \\nH2O requires Java, therefore, ensure that Java is installed on your system. You can install H2O to work \\nin Python using PyPi, as shown in the following code: \\npip install h2o\\nH2O AutoML \\nOne of the most exciting features of H2O is AutoML, the automatic ML. It is an attempt to develop a \\nuser-friendly ML interface that can be used by beginners and non-experts. H2O AutoML automates \\nthe process of training and tuning a large selection of candidate models. Its interface is designed \\nso that users just need to specify their dataset, input and output features, and any constraints they \\nwant on the number of total models trained, or time constraints. The rest of the work is done by the \\nAutoML itself, in the specified time constraint; it identifies the best performing models and provides a \\nleaderboard. It has been observed that usually, the Stacked Ensemble model, the ensemble of all the \\npreviously trained models, occupies the top position on the leaderboard. There is a large number of \\noptions that advanced users can use; details of these options and their various features are available \\nat http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html .\\nTo learn more about H2O, visit their website: http://h2o.ai . \\nAutoML using H2O\\nLet us try H2O AutoML on a synthetically created dataset. We use the scikit-learn make_circles  method \\nto create the data and save it as a CSV file:\\nfrom sklearn.datasets import make_circles\\nimport pandas as pd\\nX, y = make_circles(n_samples= 1000, noise= 0.2, factor= 0.5, random_state= 9)\\ndf = pd.DataFrame(X, columns=[ 'x1','x2'])\\ndf['y'] = y\\ndf.head()\\ndf.to_csv( 'circle.csv' , index= False, header= True)\\nBefore we can use H2O, we need to initiate its server, which is done using the init()  function:\\nimport h2o\\nh2o.init()\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80671af1-9871-4e54-bfdd-8e0b44759486', embedding=None, metadata={'page_label': '524', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 524\\nThe following shows the output we will receive after initializing the H2O server:\\nChecking whether there is an H2O instance running at http://localhost:54321 \\n..... not found.\\nAttempting to start a local H2O server...\\n  Java Version: openjdk version \"11.0.15\" 2022-04-19; OpenJDK Runtime \\nEnvironment (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1); OpenJDK 64-Bit Server \\nVM (build 11.0.15+10-Ubuntu-0ubuntu0.18.04.1, mixed mode, sharing)\\n  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/\\nh2o.jar\\n  Ice root: /tmp/tmpm2fsae68\\n  JVM stdout: /tmp/tmpm2fsae68/h2o_unknownUser_started_from_python.out\\n  JVM stderr: /tmp/tmpm2fsae68/h2o_unknownUser_started_from_python.err\\n  Server is running at http://127.0.0.1:54321\\nConnecting to H2O server at http://127.0.0.1:54321 ... successful.\\nH2O_cluster_uptime:    05 secs\\nH2O_cluster_timezone:    Etc/UTC\\nH2O_data_parsing_timezone:    UTC\\nH2O_cluster_version:    3.36.1.1\\nH2O_cluster_version_age:    27 days \\nH2O_cluster_name:    H2O_from_python_unknownUser_45enk6\\nH2O_cluster_total_nodes:    1\\nH2O_cluster_free_memory:    3.172 Gb\\nH2O_cluster_total_cores:    2\\nH2O_cluster_allowed_cores:    2\\nH2O_cluster_status:    locked, healthy\\nH2O_connection_url:    http://127.0.0.1:54321\\nH2O_connection_proxy:    {\"http\": null, \"https\": null}\\nH2O_internal_security:    False\\nPython_version:    3.7.13 final\\nWe read the file containing the synthetic data that we created earlier. Since we want to treat the \\nproblem as a classification problem, whether the points lie in a circle or not, we redefine our label \\n\\'y\\' as asfactor()  – this will tell the H2O AutoML module to treat the variable y as categorical, and \\nthus the problem as classification. The dataset is split into training, validation, and test datasets in a \\nratio of 60:20:20: \\nclass_df = h2o.import_file( \"circle.csv\" ,\\\\\\n                           destination_frame= \"circle_df\" )\\nclass_df[ \\'y\\'] = class_df[ \\'y\\'].asfactor()\\ntrain_df,valid_df,test_df = class_df.split_frame(ratios=[ 0.6, 0.2],\\\\\\n                                                 seed= 133)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5451692e-c7b2-415a-8da5-2ee2b3d82e9d', embedding=None, metadata={'page_label': '525', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 16 525\\nAnd now we invoke the AutoML module from H2O and train on our training dataset. AutoML will search \\na maximum of 10 models, but you can change the parameter max_models  to increase or decrease the \\nnumber of models to test:\\nfrom h2o.automl import H2OAutoML as AutoML\\naml = AutoML(max_models = 10, max_runtime_secs= 100, seed=2)\\naml.train(training_frame= train_df, \\\\\\n          validation_frame=valid_df, \\\\\\n          y = 'y', x=['x1','x2'])\\nFor each of the models, it gives a performance summary, for example, in Figure 16.5, you can see the \\nevaluation summary for a binomial GLM:\\nFigure 16.5: Performance summary of one of the models by H2O AutoML\\nYou can check the performance of all the models evaluated by H2O AutoML on a leaderboard:\\nlb = aml.leaderboard\\nlb.head()\\nHere is the snippet of the leaderboard:\\nmodel_id     auc    logloss    aucpr    mean_per_class_error    rmse    mse\\nStackedEnsemble_BestOfFamily_1_AutoML_2_20220511_61356    0.937598    0.315269    \\n0.940757    0.117037    0.309796    0.0959735\\nStackedEnsemble_AllModels_1_AutoML_2_20220511_61356     0.934905    0.323695    \\n0.932648    0.120348    0.312413    0.0976021\\nXGBoost_2_AutoML_2_20220511_61356     0.93281     0.322668    0.938299    \\n0.122004    0.313339    0.0981811\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7aafbd0-ee4d-46d1-bcfb-d4a7797941e0', embedding=None, metadata={'page_label': '526', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 526\\nXGBoost_3_AutoML_2_20220511_61356     0.932392    0.330866    0.929846    \\n0.130168    0.319367    0.101995 \\nGBM_2_AutoML_2_20220511_61356     0.926839    0.353181    0.923751    0.141713    \\n0.331589    0.109951 \\nXRT_1_AutoML_2_20220511_61356     0.925743    0.546718    0.932139    0.154774    \\n0.331096    0.109625 \\nGBM_3_AutoML_2_20220511_61356     0.923935    0.358691    0.917018    0.143374    \\n0.334959    0.112197 \\nDRF_1_AutoML_2_20220511_61356     0.922535    0.705418    0.921029    0.146669    \\n0.333494    0.111218 \\nGBM_4_AutoML_2_20220511_61356     0.921954    0.36403     0.911036    0.151582    \\n0.336908    0.113507 \\nXGBoost_1_AutoML_2_20220511_61356     0.919142    0.365454    0.928126    \\n0.130227    0.336754    0.113403\\nH2O model explainability\\nH2O provides a convenient wrapper for a number of explainability methods and their visualizations \\nusing a single function explain()  with a dataset and model. To get explainability on our test data for \\nthe models tested by AutoML, we will use aml.explain() . Below, we use the explain  module for the \\nStackedEnsemble_BestOfFamily  model – the topmost in the leaderboard (we are continuing with the \\nsame data that we created in the previous section):\\nexa = aml.leader.explain(test_df)\\nThe results are:\\nFigure 16.6: A confusion matrix on test dataset generated by H2O explain module\\nThe ground truth is displayed in rows and the prediction by the model in columns. For our data, 0 was \\npredicted correctly 104 times, and 1 was predicted correctly 88 times.\\nPartial dependence plots\\nPartial Dependence Plots (PDP ) provide a graphical depiction of the marginal effect of a variable on \\nthe response of the model. It can tell us about the relationship between the output label and the input \\nfeature. Figure 16.7 shows the PDP plots as obtained from the H2O explain  module on our synthetic \\ndataset:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96fe19cd-8a3e-4d5c-8712-18cbe446410b', embedding=None, metadata={'page_label': '527', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16 527\\nFigure 16.7: PDP for input features x 1 and x 2\\nFor building PDP plots for each feature, H2O considers the rest of the features as constant. So, in the \\nPDP plot for x 1 (x 2), the feature x 2 (x 1) is kept constant and the mean response is measured, as x 1 (x 2) \\nis varied.  The graph shows that both features play an important role in determining if the point is a \\ncircle or not, especially for values lying between [-0.5, 0.5].\\nVariable importance heatmap\\nWe can also check the importance of variables across different models: \\nFigure 16.8: Variable importance heatmap for input features x 1 and x 2\\nFigure 16.8 shows how much importance was given to the two input features by different algorithms. \\nWe can see that the models that gave almost equal importance to the two features are doing well on the \\nleaderboard, while GLM_1, which treated both features quite differently, has only about 41% accuracy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27345cbe-e166-4fad-9265-a5ccc78d5ca2', embedding=None, metadata={'page_label': '528', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Useful Deep Learning Librarie 528\\nModel correlation\\nThe prediction between different models is correlated; we can check this correlation:\\nFigure 16.9: Model correlation\\nFigure 16.9 shows the model correlation; it shows the correlation between the predictions on the test \\ndataset for different models. It measures the frequency of identical predictions to calculate correlations. \\nAgain, we can see that except for GLM_1, most other models perform almost equally, with accuracy \\nranging from 84-93% on the leaderboard.\\nWhat we have discussed here is just the tip of the iceberg; each of the frameworks listed here has \\nentire books on their features and applications. Depending upon your use case, you should choose \\nthe respective framework. If you are building a model for production, TensorFlow is a better choice \\nfor both web-based and Edge applications. If you are building a model where you need better control \\nof training and how the gradients are updated, then PyTorch is better suited. If you need to work \\ncross-platform very often, ONNX can be useful. And finally, H2O and platforms like OpenAI GPT-3 \\nand DALL-E 2 provide a low-threshold entry into the field of artificial intelligence and deep learning.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f0f5123-ceaa-4a67-8fd0-8608622eadc4', embedding=None, metadata={'page_label': '529', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16 529\\nSummary\\nIn this chapter, we briefly covered the features and capabilities of some other popular deep learning \\nframeworks, libraries, and platforms. We started with Hugging Face, a popular framework for NLP. \\nThen we explored OpenAI’s GPT-3 and DALL-E 2, both very powerful frameworks. The GPT-3 API can \\nbe used for a variety of NLP-related tasks, and DALL-E 2 uses GPT-3 to generate images from textual \\ndescriptions. Next, we touched on the PyTorch framework. According to many people, PyTorch and \\nTensorFlow are equal competitors, and PyTorch indeed has many features comparable to TensorFlow. \\nIn this chapter, we briefly talked about some important features like the NN module, Optim module, \\nand Autograd module of PyTorch. We also discussed ONNX, the open-source format for deep learning \\nmodels, and how we can use it to convert the model from one framework to another. Lastly, the chapter \\nintroduced H2O and its AutoML and explain  modules. \\nIn the next chapter, we will learn about graph neural networks.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfaf2d6a-7292-48b7-99eb-51c992212fc0', embedding=None, metadata={'page_label': '530', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='978ec253-0222-4e03-833b-98b4be56395d', embedding=None, metadata={'page_label': '531', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17\\nGraph Neural Networks\\nIn this chapter, we will look at a r elatively new class of neural networks, the Graph Neural Network \\n(GNN ), which is ideally suited for processing graph data. Many real-life problems in areas such as \\nsocial media, biochemistry, academic literature, and many others are inherently “graph-shaped,” \\nmeaning that their inputs are composed of data that can best be represented as graphs. We will \\ncover what graphs are from a mathematical point of view, then explain the intuition behind “graph \\nconvolutions,” the main idea behind GNNs. We will then describe a few popular GNN layers that are \\nbased on variations of the basic graph convolution technique. We will describe three major applications \\nof GNNs, covering node classification, graph classification, and edge prediction, with examples using \\nTensorFlow and the Deep Graph Library ( DGL ). DGL provides the GNN layers we have just mentioned \\nplus many more. In addition, it also provides some standard graph datasets, which we will use in the \\nexamples. Following on, we will show how you could build a DGL-compatible dataset from your own \\ndata, as well as your own layer using DGL’s low-level message-passing API. Finally, we will look at \\nsome extensions of graphs, such as heterogeneous graphs and temporal graphs.\\nWe will cover the following topics in this chapter:\\n• Graph basics\\n• Graph machine learning\\n• Graph convolutions\\n• Common graph layers\\n• Common graph applications\\n• Graph customizations\\n• Future directions\\nLet’s begin with the basics.All the code files for this chapter can be found at https://packt.link/dltfchp17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dcb525ef-a35d-4dda-84c5-514c1aebfc9c', embedding=None, metadata={'page_label': '532', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 532\\nGraph basics\\nMathematically speaking, a graph G  is a data structure consisting of a set of vertices (also called nodes) \\nV, connected to each other by a set of edges E, i.e:\\n𝐺𝐺 𝐺 𝐺𝐺𝐺𝐺𝐺𝐺𝐺 \\nA graph can be equivalently represented as an adjacency matrix A of size (n, n) where n is the number \\nof vertices in the set V . The element A[I, j] of this adjacency matrix represents the edge between \\nvertex i and vertex j. Thus the element A[I, j] = 1 if there is an edge between vertex i and vertex j, and 0 \\notherwise. In the case of weighted graphs, the edges might have their own weights, and the adjacency \\nmatrix will reflect that by setting the edge weight to the element A[i, j]. Edges may be directed or \\nundirected. For example, an edge representing the friendship between a pair of nodes x  and y  is \\nundirected, since x is friends with y implies that y is friends with x. Conversely, a directed edge can \\nbe one in a follower network (social media), where x following y does not imply that y follows x. For \\nundirected graphs, A[I, j] = A[j, i].\\nAnother interesting property of the adjacency matrix A is that An, i.e., the product of A taken n times, \\nexposes n-hop connections between nodes.\\nThe graph-to-matrix equivalence is bi-directional, meaning the adjacency matrix can be converted back \\nto the graph representation without any loss of information. Since Machine Learning (ML) methods, \\nincluding Deep Learning (DL ) methods, consume input data in the form of tensors, this equivalence \\nmeans that graphs can be efficiently represented as inputs to all kinds of machine learning algorithms.\\nEach node can also be associated with its own feature vector, much like records in tabular input. \\nAssuming a feature vector of size f, the set of nodes X can be represented as (n, f). It is also possible \\nfor edges to have their own feature vectors. Because of the equivalence between graphs and matrices, \\ngraphs are usually represented by libraries as efficient tensor-based structures. We will examine this \\nin more detail later in this chapter.\\nGraph machine learning\\nThe goal of any ML exercise is to learn a mapping F from an input space X to an output space y. Early \\nmachine learning methods required feature engineering to define the appropriate features, whereas \\nDL methods can infer the features from the training data itself. DL works by hypothesizing a model \\nM with random weights 𝛳𝛳 , formulating the task as an optimization problem over the parameters 𝛳𝛳 :\\nmin\\n𝜃𝜃ℒ(𝑦𝑦𝑦𝑦𝑦(𝑦𝑦𝑦𝑦 \\nand using gradient descent to update the model weights over multiple iterations until the parameters \\nconverge:\\n𝜃𝜃 𝜃𝜃𝜃𝜃𝜃𝜃𝜃 𝜃𝜃ℒ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee6f1b45-8f2e-4859-8db5-8f6ad3046ec3', embedding=None, metadata={'page_label': '533', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 533\\nNot surprisingly, GNNs follow this basic model as well.\\nHowever, as you have seen in previous chapters, ML and DL are often optimized for specific structures. \\nFor example, you might instinctively choose a simple FeedForward Network ( FFN ) or “dense” network \\nwhen working with tabular data, a Convolutional Neural Network ( CNN) when dealing with image data, \\nand a Recurrent Neural Network ( RNN ) when dealing with sequence data like text or time series. Some \\ninputs may reduce to simpler structures such as pixel lattices or token sequences, but not necessarily \\nso. In their natural form, graphs are topologically complex structures of indeterminate size and are \\nnot permutation invariant (i.e., instances are not independent of each other).\\nFor these reasons, we need special tooling to deal with graph data. We will introduce in this chapter \\nthe DGL, a cross-platform graph library that supports users of MX-Net, PyTorch, and TensorFlow \\nthrough the use of a configurable backend and is widely considered one of the most powerful and \\neasy-to-use graph libraries available.\\nGraph convolutions – the intuition behind GNNs\\nThe convolution operator, which effectively allows values of neighboring pixels on a 2D plane to be \\naggregated in a specific way, has been successful in deep neural networks for computer vision. The \\n1-dimensional variant has seen similar success in natural language processing and audio processing \\nas well. As you will recall from Chapter 3 , Convolutional Neural Networks , a network applies convolution \\nand pooling operations across successive layers and manages to learn enough global features across \\na sufficiently large number of input pixels to succeed at the task it is trained for.\\nExamining the analogy from the other end, an image (or each channel of an image) can be thought \\nof as a lattice-shaped graph where neighboring pixels link to each other in a specific way. Similarly, \\na sequence of words or audio signals can be thought of as another linear graph where neighboring \\ntokens are linked to each other. In both cases, the deep learning architecture progressively applies \\nconvolutions and pooling operations across neighboring vertices of the input graph until it learns to \\nperform the task, which is generally classification. Each convolution step encompasses an additional \\nlevel of neighbors. For example, the first convolution merges signals from distance 1 (immediate) \\nneighbors of a node, the second merges signals from distance 2 neighbors, and so on.\\nFigure 17.1 shows the equivalence between a 3 x 3 convolution in a CNN and the corresponding “graph \\nconvolution” operation. The convolution operator applies the filter, essentially a set of nine learnable \\nmodel parameters, to the input and combines them via a weighted sum. You can achieve the same \\neffect by treating the pixel neighborhood as a graph of nine nodes centered around the middle pixel. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90d3ef61-bc1d-4f54-a70a-d0e7cc0a116c', embedding=None, metadata={'page_label': '534', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 534\\nA graph convolution on such a structure would just be a weighted sum of the node features, the same \\nas the convolution operator in the CNN:\\nFigure 17.1: Parallels between convolutions in images and convolutions in graphs. Image source: \\nCS-224W machine learning with Graphs, Stanford Univ.\\nThe corresponding equations for the convolution operation on the CNN and the graph convolution \\nare shown below. As you can see, on CNN, the convolution can be considered as a weighted linear \\ncombination of the input pixel and each of its neighbors. Each pixel brings its own weight in the \\nform of the filter being applied. On the other hand, the graph convolution is also a weighted linear \\ncombination of the input pixel and an aggregate of all its neighbors. The aggregate effect of all neighbors \\nis averaged into the convolution output:\\nℎ𝑣𝑣(𝑙𝑙𝑙𝑙𝑙= 𝜎𝜎( 𝜎 𝜎𝜎𝑙𝑙𝑢𝑢ℎ𝑢𝑢(𝑙𝑙𝑙+𝐵𝐵 𝑙𝑙ℎ𝑣𝑣(𝑙𝑙𝑙\\n𝑢𝑢𝑢𝑢𝑢 (𝑣𝑣𝑙𝑙      … (𝐶𝐶𝐶𝐶𝐶𝐶 𝑙 \\nℎ𝑣𝑣(𝑙𝑙𝑙𝑙𝑙= 𝜎𝜎(𝜎𝜎 𝑙𝑙∑ℎ𝑢𝑢(𝑙𝑙𝑙\\n|𝑁𝑁(𝑁𝑁𝑙|\\n𝑢𝑢𝑢𝑢𝑢 (𝑣𝑣𝑙+𝐵𝐵 𝑙𝑙ℎ𝑣𝑣(𝑙𝑙𝑙𝑙      … (𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 ℎ𝑙 \\nGraph convolutions are thus a variation of convolutions that we are already familiar with. In the \\nfollowing section, we will see how these convolutions can be composed to build different kinds of \\nGCN layers.\\nCommon graph layers\\nAll the graph layers that we discuss in this section use some variation of the graph convolution operation \\ndescribed above. Contributors to graph libraries such as DGL provide prebuilt versions of many of \\nthese layers within a short time of it being proposed in an academic paper, so you will realistically \\nnever have to implement one of these. The information here is mainly for understanding how things \\nwork under the hood.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d1bdbad-a532-4076-b527-fe2826c34459', embedding=None, metadata={'page_label': '535', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 535\\nGraph convolution network\\nThe Graph Convolution Network  (GCN ) is the graph convolution layer proposed by Kipf and Welling [1]. \\nIt was originally presented as a scalable approach for semi-supervised learning on graph-structured \\ndata. They describe the GCN as an operation over the node feature vectors X  and the adjacency matrix \\nA of the underlying graph and point out that this can be exceptionally powerful when the information \\nin A is not present in the data X, such as citation links between documents in a citation network, or \\nrelations in a knowledge graph.\\nGCNs combine the value of each node’s feature vector with those of its neighbors using some weights \\n(initialized to random values). Thus, for every node, the sum of the neighboring node’s features is \\nadded. This operation can be represented as follows:\\n𝑋𝑋𝑖𝑖′= 𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑢 𝑋𝑋𝑖𝑖,𝑢𝑢𝑎𝑎𝑎𝑎𝑎𝑎𝑢𝑢𝑎𝑎𝑢𝑢𝑢𝑢𝑢𝑢𝑢𝑎𝑋𝑋𝑗𝑗,𝑗𝑗 𝑗 𝑗𝑗𝑢𝑗𝑗𝑗𝑗𝑗𝑗  \\nHere the update and aggregate are different kinds of summation functions. This sort of projection on \\nnode features is called a message-passing mechanism. A single iteration of this message passing is \\nequivalent to a graph convolution over each node’s immediate neighbors. If we wish to incorporate \\ninformation from more distant nodes, we can repeat this operation several times.\\nThe following equation describes the output of the GCN at layer (l+1) at node i. Here, N(i) is the set of \\nneighbors of node I (including itself), c ij is the product of the square root of node degrees, and sigma \\nis an activation function. The b(l) term is an optional bias term:\\nℎ𝑖𝑖(𝑙𝑙𝑙𝑙𝑙= 𝜎𝜎(𝜎𝜎(𝑙𝑙𝑙+∑1\\n𝑐𝑐𝑖𝑖𝑖𝑖𝑖𝑖𝑗𝑗𝑗(𝑖𝑖𝑙ℎ𝑖𝑖(𝑙𝑙𝑙𝑊𝑊(𝑙𝑙𝑙𝑙 \\nNext up, we will look at the graph attention network, a variant of the GCN where the coefficients are \\nlearned via an attentional mechanism instead of being explicitly defined.\\nGraph attention network\\nThe Graph Attention Network ( GAT ) layer was proposed by Velickovic, et al. [2]. Like the GCN, the GAT \\nperforms local averaging of its neighbors’ features. The difference is instead of explicitly specifying \\nthe normalization term c ij, the GAT allows it to be learned using self-attention over the node features \\nto do so. The corresponding normalization term is written as 𝛼𝛼  for the GAT, which is computed based \\non the hidden features of the neighboring nodes and the learned attention vector. Essentially, the idea \\nbehind the GAT is to prioritize feature signals from similar neighbor nodes compared to dissimilar ones.\\nEvery neighbor 𝑗𝑗𝑗𝑗  neighborhood N (i) of node i  sends its own vector of attentional coefficients 𝛼𝛼𝑖𝑖𝑖𝑖 . The \\nfollowing set of equations describes the output of the GAT at layer (i+1) for node i. The attention 𝛼𝛼  is \\ncomputed using Bahdanau’s attention model using a feedforward network:\\nℎ𝑖𝑖(𝑙𝑙𝑙𝑙)=∑𝛼𝛼 𝑖𝑖𝑖𝑖𝑊𝑊(𝑙𝑙)ℎ𝑖𝑖(𝑙𝑙)\\n𝑖𝑖𝑗𝑗𝑗(𝑖𝑖) \\n𝛼𝛼𝑖𝑖𝑖𝑖𝑙𝑙= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑖𝑖𝑖𝑖𝑙𝑙) ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e2ddbbb-cb7e-4820-bf45-806366fcf9a3', embedding=None, metadata={'page_label': '536', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 536\\n𝑒𝑒𝑖𝑖𝑖𝑖𝑙𝑙= 𝐿𝐿𝑒𝑒𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿 𝑒𝑒𝐿𝐿𝐿𝐿𝐿𝐿𝐿 𝐿𝑇𝑇[𝑊𝑊𝑊𝑖𝑖||𝑊𝑊𝑊𝑖𝑖]) \\nGCN and GAT architectures are suitable for small to medium-sized networks. The GraphSAGE \\narchitecture, described in the next section, is more suitable for larger networks.\\nGraphSAGE (sample and aggregate)\\nSo far, the convolutions we have considered require that all nodes in the graph be present during the \\ntraining, and are therefore transductive and do not naturally generalize to unseen nodes. Hamilton, Ying, \\nand Leskovec [3] proposed GraphSAGE, a general, inductive framework that can generate embeddings \\nfor previously unseen nodes. It does so by sampling and aggregating from a node’s local neighborhood. \\nGraphSAGE has proved successful at node classification on temporally evolving networks such as \\ncitation graphs and Reddit post data.\\nGraphSAGE samples a subset of neighbors instead of using them all. It can define a node neighborhood \\nusing random walks and sum up importance scores to determine the optimum sample. An aggregate \\nfunction can be one of MEAN, GCN, POOL, and LSTM. Mean aggregation simply takes the element-wise \\nmean of the neighbor vectors. The LSTM aggregation is more expressive but is inherently sequential \\nand not symmetric; it is applied on an unordered set derived from a random permutation of the node’s \\nneighbors. The POOL aggregation is both symmetric and trainable; here, each neighbor vector is \\nindependently fed through a fully connected neural network and max pooling is applied across the \\naggregate information across the neighbor set. \\nThis set of equations shows how the output for node i at layer (l+1) is generated from node i and its \\nneighbors N(i) at layer l:\\nℎ𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑙𝑙𝑙𝑙𝑁= 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 𝑁𝑎ℎ𝑗𝑗𝑙𝑙∀𝑗𝑗 𝑗 𝑗𝑗𝑁𝑗𝑗𝑁𝑗𝑁 \\nℎ𝑖𝑖(𝑙𝑙𝑙𝑙𝑙= 𝜎𝜎(𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎𝜎 (ℎ𝑖𝑖𝑙𝑙,ℎ𝑁𝑁(𝑖𝑖𝑙(𝑙𝑙𝑙𝑙𝑙𝑙𝑙 \\nℎ𝑖𝑖(𝑙𝑙𝑙𝑙𝑙= 𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛(ℎ𝑖𝑖(𝑙𝑙𝑙𝑙𝑙𝑙 \\nNow that we have seen strategies for handling large networks using GNNs, we will look at strategies \\nfor maximizing the representational (and therefore the discriminative) power of GNNs, using the \\ngraph isomorphism network.\\nGraph isomorphism network\\nXu, et al. [4] proposed the Graph Isomorphism Network (GIN ) as a graph layer with more expressive \\npower compared to the ones available. Graph layers with high expressive power should be able to \\ndistinguish between a pair of graphs that are topologically similar but not identical. They showed that \\nGCNs and GraphSAGE are unable to distinguish certain graph structures. They also showed that SUM \\naggregation is better than MEAN and MAX aggregation in terms of distinguishing graph structures. \\nThe GIN layer thus provides a better way to represent neighbor’s aggregation compared to GCNs and \\nGraphSAGE.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0b8e190-d6f7-4064-b87d-443840510129', embedding=None, metadata={'page_label': '537', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 537\\nThe following equation shows the output at node i and layer (l+1). Here, the function f θ is a callable \\nactivation function, aggregate is an aggregation function such as SUM, MAX, or MEAN, and 𝜀𝜀  is a \\nlearnable parameter that will be learned over the course of the training:\\nℎ𝑖𝑖(𝑙𝑙𝑙𝑙𝑙=𝑓𝑓𝜃𝜃((1+𝜖𝜖𝑙ℎ𝑖𝑖𝑙𝑙+𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 (ℎ𝑗𝑗𝑙𝑙,𝑗𝑗 𝑗 𝑗𝑗(𝑗𝑗𝑙𝑙𝑙 \\nHaving been introduced to several popular GNN architectures, let us now direct our attention to the \\nkind of tasks we can do with GNNs.\\nCommon graph applications\\nWe will now look at some common applications of GNNs. Typically, applications fall into one of the \\nthree major classes listed below. In this section, we will see code examples on how to build and train \\nGNNs for each of these tasks, using TensorFlow and DGL:\\n• Node classification\\n• Graph classification\\n• Edge classification (or link prediction)\\nThere are other applications of GNNs as well, such as graph clustering or generative graph models, \\nbut they are less common and we will not consider them here.\\nNode classification\\nNode classification is a popular task  on graph data. Here, a model is trained to predict the node category. \\nNon-graph classification methods can use the node feature vectors alone to do so, and some pre-GNN \\nmethods such as DeepWalk and node2vec can use the adjacency matrix alone, but GNNs are the \\nfirst class of techniques that can use both the node feature vectors and the connectivity information \\ntogether to do node classification.\\nEssentially, the idea is to apply one or more graph convolutions (as described in the previous section) \\nto all nodes of a graph, to project the feature vector of the node to a corresponding output category \\nvector that can be used to predict the node category. Our node classification example will use the \\nCORA dataset, a collection of 2,708 scientific papers classified into one of seven categories. The papers \\nare organized into a citation network, which contains 5,429 links. Each paper is described by a word \\nvector of size 1,433.\\nWe first set up our imports. If you have not already done so, you will need to install the DGL library into \\nyour environment with pip install dgl . You will also need to set the environment variable DGLBACKEND  \\nto TensorFlow. On the command line, this is achieved by the command export DGLBACKEND=tensorflow , \\nand in a notebook environment, you can try using the magic %env DGLBACKEND=tensorflow :\\nimport dgl\\nimport dgl.data\\nimport matplotlib.pyplot as plt\\nimport numpy as np', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9734d18-1fb4-486b-8827-49baf898c0c8', embedding=None, metadata={'page_label': '538', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 538\\nimport os\\nimport tensorflow as tf\\nimport tensorflow_addons as tfa\\nfrom dgl.nn.tensorflow import GraphConv\\nThe CORA dataset is pre-packaged as a DGL dataset, so we load the dataset into memory using the \\nfollowing call:\\ndataset = dgl.data.CoraGraphDataset()\\nThe first time this is called, it will log that it is downloading and extracting to a local file. Once done, \\nit will print out some useful statistics about the CORA dataset. As you can see, there are 2,708 nodes \\nand 10,566 edges in the graph. Each node has a feature vector of size 1,433 and a node is categorized \\nas being in one of seven classes. In addition, we see that it has 140 training samples, 500 validation \\nsamples, and 1,000 test samples:\\n  NumNodes: 2708\\n  NumEdges: 10556\\n  NumFeats: 1433\\n  NumClasses: 7\\n  NumTrainingSamples: 140\\n  NumValidationSamples: 500\\n  NumTestSamples: 1000\\nDone saving data into cached files.\\nSince this is a graph dataset, it is expected to contain data pertaining to a set of graphs. However, \\nCORA is a single citation graph. You can verify this by len(dataset) , which will give you 1. This also \\nmeans that downstream code will work on the graph given by dataset[0]  rather than on the complete \\ndataset. The node features will be contained in the dictionary dataset[0].ndata  as key-value pairs, \\nand the edge features in dataset[0].edata . The ndata  contains the keys train_mask , val_mask , and \\ntest_mask , which are Boolean masks signifying which nodes are part of the train, validation, and \\ntest splits, respectively, and a feat  key, which contains the feature vector for each node in the graph.\\nWe will build a NodeClassifier  network with two GraphConv  layers. Each layer will compute a new \\nnode representation by aggregating neighbor information. GraphConv  layers are just simple tf.keras.\\nlayers.Layer  objects and can therefore be stacked. The first GraphConv  layer projects the incoming \\nfeature size (1,433) to a hidden feature vector of size 16, and the second GraphConv  layer projects the \\nhidden feature vector to an output category vector of size 2, from which the category is read.\\nNote that GraphConv  is just one of many graph layers that we can drop into the NodeClassifier  model. \\nDGL makes available a variety of graph convolution layers that can be used to replace GraphConv  if \\nneeded:\\nclass NodeClassifier (tf.keras.Model):\\n  def __init__ (self, g, in_feats, h_feats, num_classes):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0b3df62-3227-4d16-a4bb-02f478c5bb1c', embedding=None, metadata={'page_label': '539', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 539\\n    super(NodeClassifier, self).__init__()\\n    self.g = g\\n    self.conv1 = GraphConv(in_feats, h_feats, activation=tf.nn.relu)\\n    self.conv2 = GraphConv(h_feats, num_classes)\\n  def call(self, in_feat):\\n    h = self.conv1(self.g, in_feat)\\n    h = self.conv2(self.g, h)\\n    return h\\ng = dataset[ 0]\\nmodel = NodeClassifier(\\n  g, g.ndata[ \"feat\"].shape[ 1], 16, dataset.num_classes)\\nWe will train this model with the CORA dataset using the code shown below. We will use the AdamW  \\noptimizer (a variation of the more popular Adam optimizer that results in models with better \\ngeneralization capabilities), with a learning rate of 1e-2 and weight decay of 5e-4. We will train for \\n200 epochs. Let us also detect if we have a GPU available, and if so, assign the graph to the GPU. \\nTensorFlow will automatically move the model to the GPU if the GPU is detected:\\ndef set_gpu_if_available ():\\n  device = \"/cpu:0\"\\n  gpus = tf.config.list_physical_devices( \"GPU\")\\n  if len (gpus) > 0:\\n    device = gpus[ 0]\\n  return  device\\ndevice = set_gpu_if_available()\\ng = g.to(device)\\nWe also define a do_eval()  method that computes the accuracy given the features and the Boolean \\nmask for the split being evaluated:\\ndef do_eval (model, features, labels, mask):\\n  logits = model(features, training= False)\\n  logits = logits[mask]\\n  labels = labels[mask]\\n  preds = tf.math.argmax(logits, axis= 1)\\n  acc = tf.reduce_mean(tf.cast(preds == labels, dtype=tf.float32))\\n  return  acc.numpy().item()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed85c802-d035-4c5d-9a1a-b5988f443493', embedding=None, metadata={'page_label': '540', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 540\\nFinally, we are ready to set up and run our training loop as follows:\\nNUM_HIDDEN = 16\\nLEARNING_RATE = 1e-2\\nWEIGHT_DECAY = 5e-4\\nNUM_EPOCHS = 200\\nwith tf.device(device):\\n  feats = g.ndata[ \"feat\"]\\n  labels = g.ndata[ \"label\"]\\n  train_mask = g.ndata[ \"train_mask\" ]\\n  val_mask = g.ndata[ \"val_mask\" ]\\n  test_mask = g.ndata[ \"test_mask\" ]\\n  in_feats = feats.shape[ 1]\\n  n_classes = dataset.num_classes\\n  n_edges = dataset[ 0].number_of_edges()\\n  model = NodeClassifier(g, in_feats, NUM_HIDDEN, n_classes)\\n  loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True)\\n  optimizer = tfa.optimizers.AdamW(\\n    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\\n  best_val_acc, best_test_acc = 0, 0\\n  history = []\\n  for epoch in range (NUM_EPOCHS):\\n    with tf.GradientTape() as tape:\\n      logits = model(feats)\\n      loss = loss_fcn(labels[train_mask], logits[train_mask])\\n      grads = tape.gradient(loss, model.trainable_weights)\\n      optimizer.apply_gradients( zip(grads, model.trainable_weights))\\n    \\n    val_acc = do_eval(model, feats, labels, val_mask)\\n    history.append((epoch + 1, loss.numpy().item(), val_acc))\\n    if epoch % 10 == 0:\\n      print( \"Epoch {:3d} | train loss: {:.3f} | val acc: {:.3f}\" .format(\\n         epoch, loss.numpy().item(), val_acc))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9e1634d-5751-4015-b4f2-c9a0180749c5', embedding=None, metadata={'page_label': '541', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 541\\nThe output of the training run shows the training loss decreasing from 1.9 to 0.02  and the validation \\naccuracy increasing from 0.13  to 0.78 :\\nEpoch   0 | train loss: 1.946 | val acc: 0.134\\nEpoch  10 | train loss: 1.836 | val acc: 0.544\\nEpoch  20 | train loss: 1.631 | val acc: 0.610\\nEpoch  30 | train loss: 1.348 | val acc: 0.688\\nEpoch  40 | train loss: 1.032 | val acc: 0.732\\nEpoch  50 | train loss: 0.738 | val acc: 0.760\\nEpoch  60 | train loss: 0.504 | val acc: 0.774\\nEpoch  70 | train loss: 0.340 | val acc: 0.776\\nEpoch  80 | train loss: 0.233 | val acc: 0.780\\nEpoch  90 | train loss: 0.164 | val acc: 0.780\\nEpoch 100 | train loss: 0.121 | val acc: 0.784\\nEpoch 110 | train loss: 0.092 | val acc: 0.784\\nEpoch 120 | train loss: 0.073 | val acc: 0.784\\nEpoch 130 | train loss: 0.059 | val acc: 0.784\\nEpoch 140 | train loss: 0.050 | val acc: 0.786\\nEpoch 150 | train loss: 0.042 | val acc: 0.786\\nEpoch 160 | train loss: 0.037 | val acc: 0.786\\nEpoch 170 | train loss: 0.032 | val acc: 0.784\\nEpoch 180 | train loss: 0.029 | val acc: 0.784\\nEpoch 190 | train loss: 0.026 | val acc: 0.784\\nWe can now evaluate our trained node classifier against the hold-out test split:\\ntest_acc = do_eval(model, feats, labels, test_mask)\\nprint(\"Test acc: {:.3f}\" .format(test_acc))\\nThis prints out the overall accuracy of the model against the hold-out test split:\\nTest acc: 0.779\\nGraph classification\\nGraph classification is done by predicting some attribute of the entire graph by aggregating all node \\nfeatures and applying one or more graph convolutions to it. This could be useful, for example, when \\ntrying to classify molecules during drug discovery as having a particular therapeutic property. In this \\nsection, we will showcase graph classification using an example.\\nIn order to run the example, please make sure DGL is installed and set to use the TensorFlow backend; \\nrefer to the previous section on node classification for information on how to do this. To begin the \\nexample, let us import the necessary libraries:\\nimport dgl.data\\nimport tensorflow as tf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f96912b1-010e-4865-94c5-16eed58d0da5', embedding=None, metadata={'page_label': '542', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 542\\nimport tensorflow_addons as tfa\\nfrom dgl.nn import GraphConv\\nfrom sklearn.model_selection import train_test_split\\nWe will use the protein dataset from DGL. The dataset is a set of graphs, each with node features and \\na single label. Each graph represents a protein molecule and each node in the graph represents an \\natom in the molecule. Node features list the chemical properties of the atom. The label indicates if \\nthe protein molecule is an enzyme:\\ndataset = dgl.data.GINDataset( \"PROTEINS\" , self_loop= True)\\nprint(\"node feature dimensionality:\" , dataset.dim_nfeats)\\nprint(\"number of graph categories:\" , dataset.gclasses)\\nprint(\"number of graphs in dataset:\" , len(dataset))\\nThe call above downloads the protein dataset locally and prints out some information about the dataset. \\nAs you can see, each node has a feature vector of size 3, the number of graph categories is 2 (enzyme \\nor not), and the number of graphs in the dataset is 1113 :\\nnode feature dimensionality: 3\\nnumber of graph categories: 2\\nnumber of graphs in dataset: 1113\\nWe will first split the dataset into training, validation, and test. We will use the training dataset to train \\nour GNN, validate using the validation dataset, and publish the results of our final model against the \\ntest dataset:\\ntv_dataset, test_dataset = train_test_split(\\n  dataset, shuffle= True, test_size= 0.2)\\ntrain_dataset, val_dataset = train_test_split(\\n  tv_dataset, test_size= 0.1)\\nprint(len(train_dataset), len(val_dataset), len(test_dataset))\\nThis splits the dataset into a training, validation, and test split of 801, 89, and 223 graphs, respectively. \\nSince our datasets are large, we need to train our network using mini-batches so as not to overwhelm \\nGPU memory. So, this example will also demonstrate mini-batch processing using our data.\\nNext, we define our GNN for graph classification. This consists of two GraphConv  layers stacked \\ntogether that will encode the nodes into their hidden representations. Since the objective is to predict \\na single category for each graph, we need to aggregate all the node representations into a graph-level \\nrepresentation, which we do by averaging the node representations using dgl.mean_nodes() :\\nclass GraphClassifier (tf.keras.Model):\\n  def __init__ (self, in_feats, h_feats, num_classes):\\n    super(GraphClassifier, self).__init__()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01d2b1ec-bc84-4f86-88ee-c2c82a50c4ff', embedding=None, metadata={'page_label': '543', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 543\\n    self.conv1 = GraphConv(in_feats, h_feats, activation=tf.nn.relu)\\n    self.conv2 = GraphConv(h_feats, num_classes)\\n  def call(self, g, in_feat):\\n    h = self.conv1(g, in_feat)\\n    h = self.conv2(g, h)\\n    g.ndata[ \"h\"] = h\\n    return dgl.mean_nodes(g, \"h\")\\nFor the training, we set the training parameters and the do_eval()  function:\\nHIDDEN_SIZE = 16\\nBATCH_SIZE = 16\\nLEARNING_RATE = 1e-2\\nNUM_EPOCHS = 20\\ndevice = set_gpu_if_available()\\ndef do_eval (model, dataset):\\n  total_acc, total_recs = 0, 0\\n  indexes = tf.data.Dataset.from_tensor_slices( range(len(dataset)))\\n  indexes = indexes.batch(batch_size=BATCH_SIZE)\\n  for batched_indexes in indexes:\\n    graphs, labels = zip(*[dataset[i] for i in batched_indexes])\\n    batched_graphs = dgl.batch(graphs)\\n    batched_labels = tf.convert_to_tensor(labels, dtype=tf.int64)\\n    batched_graphs = batched_graphs.to(device)\\n    logits = model(batched_graphs, batched_graphs.ndata[ \"attr\"])\\n    batched_preds = tf.math.argmax(logits, axis= 1)\\n    acc = tf.reduce_sum(tf.cast(batched_preds == batched_labels,\\n                                dtype=tf.float32))\\n    total_acc += acc.numpy().item()\\n    total_recs += len(batched_labels)\\n  return  total_acc / total_recs\\nFinally, we define and run our training loop to train our GraphClassifier  model. We use the Adam  \\noptimizer with a learning rate of 1e-2  and the SparseCategoricalCrossentropy  as the loss function, \\ntraining, or 20 epochs:\\nwith tf.device(device):\\n  model = GraphClassifier(', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b422f35d-6933-4244-8f92-a901b9ff1902', embedding=None, metadata={'page_label': '544', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 544\\n    dataset.dim_nfeats, HIDDEN_SIZE, dataset.gclasses)\\n  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\\n  loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(\\n    from_logits= True)\\n  train_indexes = tf.data.Dataset.from_tensor_slices(\\n    range(len(train_dataset)))\\n  train_indexes = train_indexes.batch(batch_size=BATCH_SIZE)\\n  for epoch in range (NUM_EPOCHS):\\n    total_loss = 0\\n    for batched_indexes in train_indexes:\\n      with tf.GradientTape() as tape:\\n        graphs, labels = zip(*[train_dataset[i] for i in batched_indexes])\\n        batched_graphs = dgl.batch(graphs)\\n        batched_labels = tf.convert_to_tensor(labels, dtype=tf.int32)\\n        batched_graphs = batched_graphs.to(device)\\n        logits = model(batched_graphs, batched_graphs.ndata[ \"attr\"])\\n        loss = loss_fcn(batched_labels, logits)\\n        grads = tape.gradient(loss, model.trainable_weights)\\n        optimizer.apply_gradients( zip(grads, model.trainable_weights))\\n        total_loss += loss.numpy().item()\\n  \\n    val_acc = do_eval(model, val_dataset)\\n    print( \"Epoch {:3d} | train_loss: {:.3f} | val_acc: {:.3f}\" .format(\\n        epoch, total_loss, val_acc))\\nThe output shows that the loss decreases and validation accuracy increases as the GraphClassifier  \\nmodel is trained over 20 epochs:\\nEpoch   0 | train_loss: 34.401 | val_acc: 0.629\\nEpoch   1 | train_loss: 33.868 | val_acc: 0.629\\nEpoch   2 | train_loss: 33.554 | val_acc: 0.618\\nEpoch   3 | train_loss: 33.184 | val_acc: 0.640\\nEpoch   4 | train_loss: 32.822 | val_acc: 0.652\\nEpoch   5 | train_loss: 32.499 | val_acc: 0.663\\nEpoch   6 | train_loss: 32.227 | val_acc: 0.663\\nEpoch   7 | train_loss: 32.009 | val_acc: 0.697\\nEpoch   8 | train_loss: 31.830 | val_acc: 0.685\\nEpoch   9 | train_loss: 31.675 | val_acc: 0.685\\nEpoch  10 | train_loss: 31.580 | val_acc: 0.685\\nEpoch  11 | train_loss: 31.525 | val_acc: 0.708', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='971c3b73-e215-4d72-84b7-16534039734c', embedding=None, metadata={'page_label': '545', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 545\\nEpoch  12 | train_loss: 31.485 | val_acc: 0.708\\nEpoch  13 | train_loss: 31.464 | val_acc: 0.708\\nEpoch  14 | train_loss: 31.449 | val_acc: 0.708\\nEpoch  15 | train_loss: 31.431 | val_acc: 0.708\\nEpoch  16 | train_loss: 31.421 | val_acc: 0.708\\nEpoch  17 | train_loss: 31.411 | val_acc: 0.708\\nEpoch  18 | train_loss: 31.404 | val_acc: 0.719\\nEpoch  19 | train_loss: 31.398 | val_acc: 0.719\\nFinally, we evaluate the trained model against our hold-out test dataset:\\ntest_acc = do_eval(model, test_dataset)\\nprint(\"test accuracy: {:.3f}\" .format(test_acc))\\nThis prints out the accuracy of the trained GraphClassifier  model against the held-out test split:\\ntest accuracy: 0.677\\nThe accuracy shows that the model can successfully identify a molecule as an enzyme or non-enzyme \\nslightly less than 70% of the time.\\nLink prediction\\nLink prediction is a type of edge classification problem, where the task is to predict if an edge exists \\nbetween two given nodes in the graph. \\nMany applications, such as social recommendation, knowledge graph completion, etc., can be \\nformulated as link prediction, which predicts whether an edge exists between a pair of nodes. In this \\nexample, we will predict if a citation relationship, either citing or cited, exists between two papers \\nin a citation network.\\nThe general approach would be to treat all edges in the graph as positive examples and sample a \\nnumber of non-existent edges as negative examples and train the link prediction classifier for binary \\nclassification (edge exists or not) on these positive and negative examples.\\nBefore running the example, please make sure DGL is installed and set to use the TensorFlow backend; \\nrefer to the Node classification section for information on how to do this. Let us start by importing the \\nnecessary libraries:\\nimport dgl\\nimport dgl.data\\nimport dgl.function as fn\\nimport tensorflow as tf\\nimport itertools\\nimport numpy as np\\nimport scipy.sparse as sp', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50393c40-b2ac-4742-abc1-ba29086383f1', embedding=None, metadata={'page_label': '546', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 546\\nfrom dgl.nn import SAGEConv\\nfrom sklearn.metrics import roc_auc_score\\nFor our data, we will reuse the CORA citation graph from the DGL datasets that we had used for our \\nnode classification example earlier. We already know what the dataset looks like, so we won’t dissect it \\nagain here. If you would like to refresh your memory, please refer to the node classification example \\nfor the relevant details:\\ndataset = dgl.data.CoraGraphDataset()\\ng = dataset[ 0]\\nNow, let us prepare our data. For training our link prediction model, we need a set of positive edges \\nand a set of negative edges. Positive edges are one of the 10,556 edges that already exist in the CORA \\ncitation graph, and negative edges are going to be 10,556 node pairs without connecting edges sampled \\nfrom the rest of the graph. In addition, we need to split both the positive and negative edges into \\ntraining, validation, and test splits:\\nu, v = g.edges()\\n# positive edges\\neids = np.arange(g.number_of_edges())\\neids = np.random.permutation(eids)\\ntest_size = int(len(eids) * 0.2)\\nval_size = int((len(eids) - test_size) * 0.1)\\ntrain_size = g.number_of_edges() - test_size - val_size\\nu = u.numpy()\\nv = v.numpy()\\ntest_pos_u = u[eids[ 0:test_size]]\\ntest_pos_v = v[eids[ 0:test_size]]\\nval_pos_u = u[eids[test_size:test_size + val_size]]\\nval_pos_v = v[eids[test_size:test_size + val_size]]\\ntrain_pos_u = u[eids[test_size + val_size:]]\\ntrain_pos_v = v[eids[test_size + val_size:]]\\n# negative edges\\nadj = sp.coo_matrix((np.ones( len(u)), (u, v)))\\nadj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\\nneg_u, neg_v = np.where(adj_neg != 0)\\nneg_eids = np.random.choice( len(neg_u), g.number_of_edges())\\ntest_neg_u = neg_u[neg_eids[:test_size]]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e0e2fbb-3f11-4209-abc4-ff5b3e3f80d9', embedding=None, metadata={'page_label': '547', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 17 547\\ntest_neg_v = neg_v[neg_eids[:test_size]]\\nval_neg_u = neg_u[neg_eids[test_size:test_size + val_size]]\\nval_neg_v = neg_v[neg_eids[test_size:test_size + val_size]]\\ntrain_neg_u = neg_u[neg_eids[test_size + val_size:]]\\ntrain_neg_v = neg_v[neg_eids[test_size + val_size:]]\\n# remove edges from training graph\\ntest_edges = eids[:test_size]\\nval_edges = eids[test_size:test_size + val_size]\\ntrain_edges = eids[test_size + val_size:]\\ntrain_g = dgl.remove_edges(g, np.concatenate([test_edges, val_edges]))\\nWe now construct a GNN that will compute the node representation using two GraphSAGE  layers, each  \\nlayer computing the node representation by averaging its neighbor information:\\nclass LinkPredictor (tf.keras.Model):\\n  def __init__ (self, g, in_feats, h_feats):\\n    super(LinkPredictor, self).__init__()\\n    self.g = g\\n    self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\\n    self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\\n  def call(self, in_feat):\\n    h = self.conv1(self.g, in_feat)\\n    h = self.relu1(h)\\n    h = self.conv2(self.g, h)\\n    return h\\nHowever, link prediction requires us to compute representations of pairs of nodes, DGL recommends \\nthat you treat the pairs of nodes as another graph since you can define a pair of nodes as an edge. \\nFor link prediction, we will have a positive graph containing all the positive examples as edges, and \\na negative graph containing all the negative examples as edges. Both positive and negative graphs \\ncontain the same set of nodes as the original graph:\\ntrain_pos_g = dgl.graph((train_pos_u, train_pos_v), \\n  num_nodes=g.number_of_nodes())\\ntrain_neg_g = dgl.graph((train_neg_u, train_neg_v), \\n  num_nodes=g.number_of_nodes())\\nval_pos_g = dgl.graph((val_pos_u, val_pos_v), \\n  num_nodes=g.number_of_nodes())\\nval_neg_g = dgl.graph((val_neg_u, val_neg_v), \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d4eceb5-6f3f-4cf1-bb5c-fb06d79e1b92', embedding=None, metadata={'page_label': '548', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 548\\n  num_nodes=g.number_of_nodes())\\ntest_pos_g = dgl.graph((test_pos_u, test_pos_v), \\n  num_nodes=g.number_of_nodes())\\ntest_neg_g = dgl.graph((test_neg_u, test_neg_v), \\n  num_nodes=g.number_of_nodes())\\nNext, we will define a predictor class that will take the set of node representations from the \\nLinkPredictor  class and use the DGLGraph.apply_edges  method to compute edge feature scores, \\nwhich are the dot product of the source node features and the destination node features (both output \\ntogether from the LinkPredictor  in this case):\\nclass DotProductPredictor (tf.keras.Model):\\n  def call(self, g, h):\\n    with g.local_scope():\\n      g.ndata[ \\'h\\'] = h\\n      # Compute a new edge feature named \\'score\\' by a dot-product \\n      # between the source node feature \\'h\\' and destination node \\n      # feature \\'h\\'.\\n      g.apply_edges(fn.u_dot_v( \\'h\\', \\'h\\', \\'score\\' ))\\n      # u_dot_v returns a 1-element vector for each edge so you \\n      # need to squeeze it.\\n      return g.edata[ \\'score\\'][:, 0]\\nYou can also build a custom predictor such as a multi-layer perceptron with two dense layers, as the \\nfollowing code shows. Note that the apply_edges  method describes how the edge score is calculated:\\nclass MLPPredictor (tf.keras.Model):\\n  def __init__ (self, h_feats):\\n    super().__init__()\\n    self.W1 = tf.keras.layers.Dense(h_feats, activation=tf.nn.relu)\\n    self.W2 = tf.keras.layers.Dense( 1)\\n  def apply_edges (self, edges):\\n    h = tf.concat([edges.src[ \"h\"], edges.dst[ \"h\"]], axis= 1)\\n    return {\\n      \"score\": self.W2(self.W1(h))[:, 0]\\n    }\\n  def call(self, g, h):\\n    with g.local_scope():\\n      g.ndata[ \\'h\\'] = h\\n      g.apply_edges(self.apply_edges)\\n      return g.edata[ \\'score\\']', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7debf0f7-cfc9-4d27-a8e7-ff3221db7890', embedding=None, metadata={'page_label': '549', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 549\\nWe instantiate the LinkPredictor  model we defined earlier, select the Adam  optimizer, and declare our \\nloss function to be BinaryCrossEntropy  (since our task is binary classification). The predictor head \\nthat we will use in our example is the DotProductPredictor . However, the MLPPredictor  can be used \\nas a drop-in replacement instead; just replace the pred  variable below to point to the MLPPredictor  \\ninstead of the DotProductPredictor :\\nHIDDEN_SIZE = 16\\nLEARNING_RATE = 1e-2\\nNUM_EPOCHS = 100\\nmodel = LinkPredictor(train_g, train_g.ndata[ \\'feat\\'].shape[ 1], \\n    HIDDEN_SIZE)\\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\\nloss_fcn = tf.keras.losses.BinaryCrossentropy(from_logits= True)\\npred = DotProductPredictor()\\nWe also define a couple of convenience functions for our training loop. The first one computes the \\nloss between the scores returned from the positive graph and the negative graphs, and the second \\ncomputes the Area Under the Curve (AUC ) from the two scores. AUC is a popular metric to evaluate \\nbinary classification models:\\ndef compute_loss (pos_score, neg_score):\\n    scores = tf.concat([pos_score, neg_score], axis= 0)\\n    labels = tf.concat([\\n      tf.ones(pos_score.shape[ 0]),\\n      tf.zeros(neg_score.shape[ 0])\\n    ], axis= 0\\n)\\n    return loss_fcn(labels, scores)\\ndef compute_auc (pos_score, neg_score):\\n    scores = tf.concat([pos_score, neg_score], axis= 0).numpy()\\n    labels = tf.concat([\\n      tf.ones(pos_score.shape[ 0]),\\n      tf.zeros(neg_score.shape[ 0])\\n    ], axis= 0).numpy()\\n    return roc_auc_score(labels, scores)\\nWe now train our LinkPredictor  GNN for 100 epochs of training, using the following training loop:\\nfor epoch in range (NUM_EPOCHS):\\n  in_feat = train_g.ndata[ \"feat\"]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dff6a8d2-9e0d-4ceb-b6fb-3ba91cef6185', embedding=None, metadata={'page_label': '550', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 550\\n  with tf.GradientTape() as tape:\\n    h = model(in_feat)\\n    pos_score = pred(train_pos_g, h)\\n    neg_score = pred(train_neg_g, h)\\n    loss = compute_loss(pos_score, neg_score)\\n    grads = tape.gradient(loss, model.trainable_weights)\\n    optimizer.apply_gradients( zip(grads, model.trainable_weights))\\n  val_pos_score = pred(val_pos_g, h)\\n  val_neg_score = pred(val_neg_g, h)\\n  val_auc = compute_auc(val_pos_score, val_neg_score)\\n  if epoch % 5 == 0:\\n    print( \"Epoch {:3d} | train_loss: {:.3f}, val_auc: {:.3f}\" .format(\\n      epoch, loss, val_auc))\\nThis returns the following training logs:\\nEpoch   0 | train_loss: 0.693, val_auc: 0.566\\nEpoch   5 | train_loss: 0.681, val_auc: 0.633\\nEpoch  10 | train_loss: 0.626, val_auc: 0.746\\nEpoch  15 | train_loss: 0.569, val_auc: 0.776\\nEpoch  20 | train_loss: 0.532, val_auc: 0.805\\nEpoch  25 | train_loss: 0.509, val_auc: 0.820\\nEpoch  30 | train_loss: 0.492, val_auc: 0.824\\nEpoch  35 | train_loss: 0.470, val_auc: 0.833\\nEpoch  40 | train_loss: 0.453, val_auc: 0.835\\nEpoch  45 | train_loss: 0.431, val_auc: 0.842\\nEpoch  50 | train_loss: 0.410, val_auc: 0.851\\nEpoch  55 | train_loss: 0.391, val_auc: 0.859\\nEpoch  60 | train_loss: 0.371, val_auc: 0.861\\nEpoch  65 | train_loss: 0.350, val_auc: 0.861\\nEpoch  70 | train_loss: 0.330, val_auc: 0.861\\nEpoch  75 | train_loss: 0.310, val_auc: 0.862\\nEpoch  80 | train_loss: 0.290, val_auc: 0.860\\nEpoch  85 | train_loss: 0.269, val_auc: 0.856\\nEpoch  90 | train_loss: 0.249, val_auc: 0.852\\nEpoch  95 | train_loss: 0.228, val_auc: 0.848\\nWe can now evaluate the trained model against the hold-out test set:\\npos_score = tf.stop_gradient(pred(test_pos_g, h))\\nneg_score = tf.stop_gradient(pred(test_neg_g, h))\\nprint(\\'Test AUC\\' , compute_auc(pos_score, neg_score))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba17aea5-4b0b-465d-9a84-84d7b6d3a96a', embedding=None, metadata={'page_label': '551', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 551\\nThis returns the following test AUC for our LinkPredictor  GNN:\\nTest AUC 0.8266960571287392\\nThis is quite impressive as it implies that the link predictor can correctly predict 82% of the links \\npresented as ground truths in the test set.\\nGraph customizations\\nWe have seen how to build and train GNNs for common graph ML tasks. However, for convenience, we \\nhave chosen to use prebuilt DGL graph convolution layers in our models. While unlikely, it is possible \\nthat you might need a layer that is not provided with the DGL package. DGL provides a message passing \\nAPI to allow you to build custom graph layers easily. In the first part of this section, we will look at an \\nexample where we use the message-passing API to build a custom graph convolution layer.\\nWe have also loaded datasets from the DGL data package for our examples. It is far more likely that \\nwe will need to use our own data instead. So, in the second part of this section, we will see how to \\nconvert our own data into a DGL dataset.\\nCustom layers and message passing\\nAlthough DGL provides many graph layers out of the box, there may be cases where the ones provided \\ndon’t meet our needs exactly and we need to build your own. \\nFortunately, all these graph layers are based on a common underlying concept of message passing \\nbetween nodes in the graph. So, in order to build a custom GNN layer, you need to understand how \\nthe message-passing paradigm works. This paradigm is also known as the Message Passing Neural \\nNetwork (MPNN) framework [5]:\\n𝑚𝑚𝑢𝑢𝑢𝑢𝑢(𝑙𝑙𝑙=𝑀𝑀(𝑙𝑙𝑙(ℎ𝑢𝑢(𝑙𝑙𝑙𝑙𝑙,ℎ𝑢𝑢(𝑙𝑙𝑙𝑙𝑙,𝑒𝑒𝑢𝑢𝑢𝑢𝑢(𝑙𝑙𝑙𝑙𝑙𝑙 \\n𝑚𝑚𝑣𝑣(𝑙𝑙𝑙=∑𝑚𝑚𝑢𝑢𝑢𝑣𝑣(𝑙𝑙𝑙\\n𝑢𝑢𝑢𝑢𝑢(𝑣𝑣𝑙 \\nℎ𝑣𝑣(𝑙𝑙𝑙=𝑈𝑈(𝑙𝑙𝑙(ℎ𝑣𝑣(𝑙𝑙𝑙𝑙𝑙,𝑚𝑚𝑣𝑣(𝑙𝑙𝑙𝑙 \\nEach node u  in the graph has a hidden state (initially its feature vector) represented by h u. For each node \\nu and v, where nodes u and v are neighbors, i.e., connected by an edge e u->v, we apply some function \\nM called the message function. The message function M is applied to every node on the graph. We then \\naggregate the output of M for all nodes with the output of all their neighboring nodes to produce the \\nmessage m. Here ∑  is called the reduce function. Note that even though we represent the reduce function \\nby the summation symbol ∑ , it can be any aggregation function. Finally, we update the hidden state \\nof node v using the obtained message and the previous state of the node. The function U applied at \\nthis step is called the update function.\\nThe message-passing algorithm is repeated a specific number of times. After that, we reach the readout \\nphase where we extract the feature vector from each node that represents the entire graph. For example, \\nthe final feature vector for a node might represent the node category in the case of node classification.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9c2e0ad5-826b-473f-bfd2-6c2105d5b81f', embedding=None, metadata={'page_label': '552', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 552\\nIn this section, we will use the MPNN framework to implement a GraphSAGE layer. Even though DGL \\nprovides the dgl.nn.SAGEConv , which implements this already, this is an example to illustrate the \\ncreation of custom graph layers using MPNN. The message-passing steps of a GraphSAGE layer are \\ngiven by:\\nℎ𝑁𝑁𝑁𝑁𝑁𝑁𝑘𝑘← 𝐴𝐴𝐴𝐴𝐴𝐴𝑁ℎ𝑢𝑢𝑘𝑘𝑘𝑘,∀𝑢𝑢 𝑢 𝑢𝑢𝑁𝑢𝑢𝑁𝑁 \\nℎ𝑣𝑣𝑘𝑘← 𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑘𝑘.𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶 𝑅ℎ𝑣𝑣𝑘𝑘𝑘𝑘,ℎ𝑁𝑁𝑅𝑣𝑣)𝑘𝑘)) \\nThe code to implement our custom GraphSAGE layer using MPNN is shown below. The DGL function \\nupdate_all  call allows you to specify a message_fn  and a reduce_fn , which are also DGL built-in \\nfunctions, and the tf.concat  and Dense  layers represent the final update function:\\nimport dgl\\nimport dgl.data\\nimport dgl.function as fn\\nimport tensorflow as tf\\nclass CustomGraphSAGE (tf.keras.layers.Layer):\\n  def __init__ (self, in_feat, out_feat):\\n    super(CustomGraphSAGE, self).__init__()\\n    # A linear submodule for projecting the input and neighbor \\n    # feature to the output.\\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\\n  def call(self, g, h):\\n    with g.local_scope():\\n        g.ndata[ \"h\"] = h\\n        # update_all is a message passing API.\\n        g.update_all(message_func=fn.copy_u( \\'h\\', \\'m\\'),\\n                     reduce_func=fn.mean( \\'m\\', \\'h_N\\'))\\n        h_N = g.ndata[ \\'h_N\\']\\n        h_total = tf.concat([h, h_N], axis= 1)\\n        return self.linear(h_total)\\nHere, we see that the update_all  function specifies a message_func , which just copies the node’s current \\nfeature vector to a message vector m, and then averages all the message vectors in the neighborhood \\nof each node. As you can see, this faithfully follows the first GraphSAGE equation above. DGL provides \\nmany such built-in functions ( https://docs.dgl.ai/api/python/dgl.function.html ).\\nOnce the neighborhood vector h_N  is computed in the first step, it is concatenated with the input \\nfeature vector h, and then passed through a Dense  layer with a ReLU activation, as described by the \\nsecond equation for GraphSAGE above. We have thus implemented the GraphSAGE layer with our \\nCustomGraphSAGE  object.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50a7ffea-16be-4388-bd6c-b4441a36d2da', embedding=None, metadata={'page_label': '553', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 17 553\\nThe next step is to put it into a GNN to see how it works. The following code shows a CustomGNN  model \\nthat uses two layers of our custom SAGEConv  implementation:\\nclass CustomGNN (tf.keras.Model):\\n  def __init__ (self, g, in_feats, h_feats, num_classes):\\n    super(CustomGNN, self).__init__()\\n    self.g = g\\n    self.conv1 = CustomGraphSAGE(in_feats, h_feats)\\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\\n    self.conv2 = CustomGraphSAGE(h_feats, num_classes)\\n  def call(self, in_feat):\\n    h = self.conv1(self.g, in_feat)\\n    h = self.relu1(h)\\n    h = self.conv2(self.g, h)\\n    return h\\nWe will run it to do node classification against the CORA dataset, details of which should be familiar \\nfrom previous examples.\\nThe above code assumes an unweighted graph, i.e., edges between nodes have the same weight. This \\ncondition is true for the CORA dataset, where each edge represents a citation from one paper to another. \\nHowever, we can imagine scenarios where edges may be weighted based on how many times some edge \\nhas been invoked, for example, an edge that connects a product and a user for user recommendations. \\nThe only change we need to make to handle weighted edges is to allow the weight to play a part in \\nour message function. That is, if an edge between our node u and a neighbor node v occurs k times, \\nwe should consider that edge k times. The code below shows our custom GraphSAGE layer with the \\nability to handle weighted edges:\\nclass CustomWeightedGraphSAGE (tf.keras.layers.Layer):\\n  def __init__ (self, in_feat, out_feat):\\n    super(CustomWeightedGraphSAGE, self).__init__()\\n    # A linear submodule for projecting the input and neighbor \\n    # feature to the output.\\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\\n  def call(self, g, h, w):\\n    with g.local_scope():\\n      g.ndata[ 'h'] = h\\n      g.edata[ 'w'] = w\\n      g.update_all(message_func=fn.u_mul_e( 'h', 'w', 'm'),\\n                   reduce_func=fn.mean( 'm', 'h_N'))\\n      h_N = g.ndata[ 'h_N']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b563b7a8-202d-491a-88de-0deea3db3b3f', embedding=None, metadata={'page_label': '554', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 554\\n      h_total = tf.concat([h, h_N], axis= 1)\\n      return self.linear(h_total)\\nThis code expects an additional edge property w, which contains the edge weights, which you can \\nsimulate on the CORA dataset by:\\ng.edata[ \"w\"] = tf.cast(\\n   tf.random.uniform((g.num_edges(), 1), minval= 3, maxval= 10, \\n                     dtype=tf.int32),\\n   dtype=tf.float32)\\nThe message_func  in CustomWeightedGraphSAGE  has changed from simply copying the feature vector \\nh to the message vector m, to multiplying h and w to produce the message vector m. Everything else is \\nthe same as in CustomGraphSAGE . The new CustomWeightedGraphSAGE  layer can now be simply dropped \\ninto the calling class CustomGNN  where CustomGraphSAGE  was originally being called.\\nCustom graph dataset\\nA more common use case that you are likely to face is to use your own data to train a GNN model. \\nObviously, in such cases, you cannot use a DGL-provided dataset (as we have been using in all our \\nexamples so far) and you must wrap your data into a custom graph dataset. \\nYour custom graph dataset should inherit from the dgl.data.DGLDataset  object provided by DGL and \\nimplement the following methods:\\n• __getitem__(self, i)  – retrieve the i-th example from the dataset. The retrieved example \\ncontains a single DGL graph and its label if applicable.\\n• __len__(self)  – the number of examples in the dataset.\\n• process(self)  – defines how to load and process raw data from the disk.\\nAs we have seen before, node classification and link prediction operate on a single graph, and graph \\nclassification operates on a set of graphs. While the approach is largely identical for both cases, there \\nare some concerns specific to either case, so we will provide an example to do each of these below.\\nSingle graphs in datasets\\nFor our example, we will choose Zachary’s Karate Club graph, which represents the members of a \\nKarate Club observed over three years. Over time, there was a disagreement between an administrator \\n(Officer) and the instructor (Mr. Hi), and the club members split and reformed under the Officer and \\nMr. Hi (shown below as blue and red nodes, respectively). The Zachary Karate Club network is available \\nfor download from the NetworkX library:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='274ccb7a-53ea-4e3a-9f14-7d7b71b9a625', embedding=None, metadata={'page_label': '555', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 555\\nFigure 17.2: Graph representation of the Karate Club Network\\nThe graph contains 34 nodes labeled with one of “Officer” or “Mr. Hi” depending on which group \\nthey ended up in after the split. It contains 78 edges, which are undirected and unweighted. An edge \\nbetween a pair of members indicates that they interact with each other outside the club. To make this \\ndataset more realistic for GNN usage, we will attach a 10-dimensional random feature vector to each \\nnode, and an edge weight as an edge feature. Here is the code to convert the Karate Club graph into a \\nDGL dataset that you can then use for downstream node or edge classification tasks:\\nclass KarateClubDataset (DGLDataset):\\n  def __init__ (self):\\n    super().__init__(name= \"karate_club\" )\\n  def __getitem__ (self, i):\\n    return self.graph\\n  def __len__ (self):\\n    return 1\\n  def process (self):\\n    G = nx.karate_club_graph()\\n    nodes = [node for node in G.nodes]\\n    edges = [edge for edge in G.edges]\\n    node_features = tf.random.uniform(\\n        ( len(nodes), 10), minval= 0, maxval= 1, dtype=tf.dtypes.float32)\\n    label2int = { \"Mr. Hi\" : 0, \"Officer\" : 1}', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d550a2b3-7ab0-4f8a-b8ee-b233b31c74ee', embedding=None, metadata={'page_label': '556', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 556\\n    node_labels = tf.convert_to_tensor(\\n        [label2int[G.nodes[node][ \"club\"]] for node in nodes])\\n    edge_features = tf.random.uniform(\\n        ( len(edges), 1), minval= 3, maxval= 10, dtype=tf.dtypes.int32)\\n    edges_src = tf.convert_to_tensor([u for u, v in edges])\\n    edges_dst = tf.convert_to_tensor([v for u, v in edges])\\n    self.graph = dgl.graph((edges_src, edges_dst), num_nodes= len(nodes))\\n    self.graph.ndata[ \"feat\"] = node_features\\n    self.graph.ndata[ \"label\"] = node_labels\\n    self.graph.edata[ \"weight\" ] = edge_features\\n    # assign masks indicating the split (training, validation, test)\\n    n_nodes = len(nodes)\\n    n_train = int(n_nodes * 0.6)\\n    n_val = int(n_nodes * 0.2)\\n    train_mask = tf.convert_to_tensor(\\n      np.hstack([np.ones(n_train), np.zeros(n_nodes - n_train)]),\\n      dtype=tf. bool)\\n    val_mask = tf.convert_to_tensor(\\n      np.hstack([np.zeros(n_train), np.ones(n_val), \\n                 np.zeros(n_nodes - n_train - n_val)]),\\n      dtype=tf. bool)\\n    test_mask = tf.convert_to_tensor(\\n      np.hstack([np.zeros(n_train + n_val), \\n                 np.ones(n_nodes - n_train - n_val)]),\\n      dtype=tf. bool)\\n    self.graph.ndata[ \"train_mask\" ] = train_mask\\n    self.graph.ndata[ \"val_mask\" ] = val_mask\\n    self.graph.ndata[ \"test_mask\" ] = test_mask\\nMost of the logic is in the process  method. We call the NetworkX method to get the Karate Club as a \\nNetworkX graph, then convert it to a DGL graph object with node features and labels. Even though \\nthe Karate Club graph does not have node and edge features defined, we manufacture some random \\nnumbers and set them to these properties. Note that this is only for purposes of this example, to show \\nwhere these features would need to be updated if your graph had node and edge features. Note that \\nthe dataset contains a single graph.\\nIn addition, we also want to split the graph into training, validation, and test splits for node classification \\npurposes. For that, we assign masks indicating whether a node belongs to one of these splits. We do this \\nrather simply by splitting the nodes in the graph 60/20/20 and assigning Boolean masks for each split.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e28fb1f8-69a3-4b25-9f06-037f07274533', embedding=None, metadata={'page_label': '557', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 17 557\\nIn order to instantiate this dataset from our code, we can say:\\ndataset = KarateClubDataset()\\ng = dataset[ 0]\\nprint(g)\\nThis will give us the following output (reformatted a little for readability). The two main structures \\nare the ndata_schemas  and edata_schemas , accessible as g.ndata  and g.edata , respectively. Within \\nndata_schemas , we have keys that point to the node features ( feats ), node labels ( label ), and the \\nmasks to indicate the training, validation, and test splits ( train_mask , val_mask , and test_mask ), \\nrespectively. Under edata_schemas , there is the weight  attribute that indicates the edge weights:\\nGraph(num_nodes=34, \\n      num_edges=78,\\n      ndata_schemes={\\n        'feat': Scheme(shape=(10,), dtype=tf.float32),\\n        'label': Scheme(shape=(), dtype=tf.int32),\\n        'train_mask': Scheme(shape=(), dtype=tf.bool),\\n        'val_mask': Scheme(shape=(), dtype=tf.bool),\\n        'test_mask': Scheme(shape=(), dtype=tf.bool)\\n      }\\n      edata_schemes={\\n         'weight': Scheme(shape=(1,), dtype=tf.int32)\\n      }\\n)\\nPlease refer to the examples on node classification and link prediction for information on how to use \\nthis kind of custom dataset.\\nSet of multiple graphs in datasets\\nDatasets that support the graph classification task will contain multiple graphs and their associated \\nlabels, one per graph. For our example, we will consider a hypothetical dataset of molecules represented \\nas graphs, and the task would be to predict if the molecule is toxic or not (a binary prediction).\\nWe will use the NetworkX method random_regular_graph()  to generate synthetic graphs with a \\nrandom number of nodes and node degree. To each node of each graph, we will attach a random \\n10-dimensional feature vector. Each node will have a label (0 or 1) indicating if the graph is toxic. Note \\nthat this is just a simulation of what real data might look like. With real data, the structure of each \\ngraph and the values of the node vectors, which are random in our case, will have a real impact on \\nthe target variable, i.e., the toxicity of the molecule. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf629372-254c-4cf5-be99-2d0968e4fbf0', embedding=None, metadata={'page_label': '558', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 558\\nThe figure below shows some examples of what the synthetic “molecules” might look like:\\nFigure 17.3: Some examples of random regular graphs generated using NetworkX\\nHere is the code to convert a set of random NetworkX graphs into a DGL graph dataset for graph \\nclassification. We will generate 100 such graphs and store them in a list in the form of a DGL dataset:\\nfrom networkx.exception import NetworkXError\\nclass SyntheticDataset (DGLDataset):\\n  def __init__ (self):\\n    super().__init__(name= \"synthetic\" )\\n  def __getitem__ (self, i):\\n    return self.graphs[i], self.labels[i]\\n  def __len__ (self):\\n    return len(self.graphs)\\n  def process (self):\\n    self.graphs, self.labels = [], []\\n    num_graphs = 0\\n    while(True):\\n      d = np.random.randint( 3, 10)\\n      n = np.random.randint( 5, 10)\\n      if ((n * d) % 2) != 0:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4defc6df-afe9-49b1-9796-3f4acb399c80', embedding=None, metadata={'page_label': '559', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 559\\n        continue\\n      if n < d:\\n        continue\\n      try:\\n        g = nx.random_regular_graph(d, n)\\n      except NetworkXError:\\n        continue\\n      g_edges = [edge for edge in g.edges]\\n      g_src = [u for u, v in g_edges]\\n      g_dst = [v for u, v in g_edges]\\n      g_num_nodes = len(g.nodes)\\n      label = np.random.randint( 0, 2)\\n      # create graph and add to list of graphs and labels\\n      dgl_graph = dgl.graph((g_src, g_dst), num_nodes=g_num_nodes)\\n      dgl_graph.ndata[ \"feats\"] = tf.random.uniform(\\n          (g_num_nodes, 10), minval= 0, maxval= 1, dtype=tf.dtypes.float32)\\n      self.graphs.append(dgl_graph)\\n      self.labels.append(label)\\n      num_graphs += 1\\n      if num_graphs > 100:\\n        break\\n    self.labels = tf.convert_to_tensor(self.labels, dtype=tf.dtypes.int64)\\nOnce created, we can then call it from our code as follows:\\ndataset = SyntheticDataset()\\ngraph, label = dataset[ 0]   \\nprint(graph)\\nprint(\"label:\" , label)\\nThis produces the following output for the first graph in the DGL dataset (reformatted slightly for \\nreadability). As you can see, the first graph in the dataset has 6 nodes and 15 edges and contains a \\nfeature vector (accessible using the feats  key) of size 10. The label is a 0-dimensional tensor (i.e., a \\nscalar) of type long ( int64 ):\\nGraph(num_nodes=6, num_edges=15,\\n      ndata_schemes={\\n        \\'feats\\': Scheme(shape=(10,), dtype=tf.float32)}\\n      edata_schemes={})\\nlabel: tf.Tensor(0, shape=(), dtype=int64)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8012388-dab4-433b-9911-1bec4aec9521', embedding=None, metadata={'page_label': '560', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 560\\nAs before, in order to see how you would use this custom dataset for some task such as graph \\nclassification, please refer to the example on graph classification earlier in this chapter.\\nFuture directions\\nGraph neural networks are a rapidly evolving discipline. We have covered working with static \\nhomogeneous graphs on various popular graph tasks so far, which covers many real-world use cases. \\nHowever, it is likely that some graphs are neither homogeneous nor static, and neither can they be \\neasily reduced to this form. In this section, we will look at our options for dealing with heterogenous \\nand temporal graphs.\\nHeterogeneous graphs\\nHeterogeneous gr aphs [7], also called heterographs, differ from the graphs we have seen so far in \\nthat they may contain different kinds of nodes and edges. These different types of nodes and edges \\nmight also contain different types of attributes, including possible representations with different \\ndimensions. Popular examples of heterogeneous graphs are citation graphs that contain authors and \\npapers, recommendation graphs that contain users and products, and knowledge graphs that can \\ncontain many different types of entities.\\nYou can use the MPNN framework on heterogeneous graphs by manually implementing message and \\nupdate functions individually for each edge type. Each edge type is defined by the triple (source node \\ntype, edge type, and destination node type). However, DGL provides support for heterogeneous graphs \\nusing the dgl.heterograph()  API, where a graph is specified as a series of graphs, one per edge type.\\nTypical learning tasks associated with heterogeneous graphs are similar to their homogeneous \\ncounterparts, namely node classification and regression, graph classification, and edge classification/\\nlink prediction. A popular graph layer for working with heterogeneous graphs is the Relational GCN \\nor R-GCN , available as a built-in layer in DGL.\\nTemporal Graphs\\nTemporal Graphs [8] is a framework developed at Twitter to handle dynamic graphs that change over \\ntime. While GNN models have primarily focused on static graphs that do not change over time, adding \\nthe time dimension allows us to model many interesting phenomena in social networks, financial \\ntransactions, and recommender systems, all of which are inherently dynamic. In such systems, it is \\nthe dynamic behavior that conveys the important insights.\\nA dynamic graph can be represented as a stream of timed events, such as additions and deletions of \\nnodes and edges. This stream of events is fed into an encoder network that learns a time-dependent \\nencoding for each node in the graph. A decoder is trained on this encoding to support some specific \\ntask such as link prediction at a future point in time. There is currently no support in the DGL library \\nfor Temporal Graphs, mainly because it is a very rapidly evolving research area.\\nAt a high level, a Temporal Graph Network ( TGN ) encoder works by creating a compressed representation \\nof the nodes based on their interaction and updates over time. The current state of each node is stored \\nin TGN memory and acts as the hidden state s t of an RNN; however, we have a separate state vector \\nst(t) for each node i and time point t. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8facb38e-4975-42ff-88dc-134b6f48c5a6', embedding=None, metadata={'page_label': '561', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17 561\\nA message function similar to what we have seen in the MPNN framework computes two messages m i \\nand mj for a pair of nodes i  and j  using the state vectors and their interaction as input. The message and \\nstate vectors are then combined using a memory updater, which is usually implemented as an RNN. \\nTGNs have been found to outperform their static counterparts on the tasks of future edge prediction \\nand dynamic node classification both in terms of accuracy and speed.\\nSummary\\nIn this chapter, we have covered graph neural networks, an exciting set of techniques to learn not \\nonly from node features but also from the interaction between nodes. We have covered the intuition \\nbehind why graph convolutions work and the parallels between them and convolutions in computer \\nvision. We have described some common graph convolutions, which are provided as layers by DGL. \\nWe have demonstrated how to use the DGL for popular graph tasks of node classification, graph \\nclassification, and link prediction. In addition, in the unlikely event that our needs are not met by \\nstandard DGL graph layers, we have learned how to implement our own graph convolution layer using \\nDGL’s message-passing framework. We have also seen how to build DGL datasets for our own graph \\ndata. Finally, we look at some emerging directions of graph neural networks, namely heterogeneous \\ngraphs and temporal graphs. This should equip you with skills to use GNNs to solve interesting problems \\nin this area.\\nIn the next chapter, we will turn our attention to learning about some best ML practices associated \\nwith deep learning projects.\\nReferences\\n1. Kipf, T. and Welling, M. (2017). Semi-supervised Classification with Graph Convolutional Networks . \\nArxiv Preprint, arXiv: 1609.02907 [cs.LG]. Retrieved from https://arxiv.org/abs/1609.02907\\n2. Velickovic, P., et al. (2018). Graph Attention Networks. Arxiv Preprint, arXiv 1710.10903 [stat.ML]. \\nRetrieved from https://arxiv.org/abs/1710.10903  \\n3. Hamilton, W . L., Ying, R., and Leskovec, J. (2017). Inductive Representation Learning on \\nLarge Graphs . Arxiv Preprint, arXiv: 1706.02216 [cs.SI]. Retrieved from https://arxiv.org/\\nabs/1706.02216  \\n4. Xu, K., et al. (2018). How Powerful are Graph Neural Networks?. Arxiv Preprint, arXiv: 1810.00826 \\n[cs.LG]. Retrieved from https://arxiv.org/abs/1810.00826  \\n5. Gilmer, J., et al. (2017). Neural Message Passing for Quantum Chemistry. Arxiv Preprint, arXiv: \\n1704.01212 [cs.LG]. Retrieved from https://arxiv.org/abs/1704.01212  \\n6. Zachary, W . W . (1977). An Information Flow Model for Conflict and Fission in Small Groups. Journal \\nof Anthropological Research. Retrieved from https://www.journals.uchicago.edu/doi/\\nabs/10.1086/jar.33.4.3629752  \\n7. Pengfei, W . (2020). Working with Heterogeneous Graphs in DGL. Blog post. Retrieved from https://\\nwww.jianshu.com/p/767950b560c4  \\n8. Bronstein, M. (2020). Temporal Graph Networks. Blog post. Retrieved from https://\\ntowardsdatascience.com/temporal-graph-networks-ab8f327f2efe  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='43125b41-19ec-4167-b870-23a66e15fa8d', embedding=None, metadata={'page_label': '562', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Graph Neural Networks 562\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1097c314-d3a9-4b49-9148-a1d7eab952e3', embedding=None, metadata={'page_label': '563', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18\\nMachine Learning Best Practices\\nMachine learning is much more than building and training models. Till now in this book, we focused \\non different deep learning algorithms and introduced the latest algorithms, their power, and their \\nlimitations. In this chapter, we change our focus, from the ML/DL algorithms to the practices that can \\nmake us better machine learning engineers and scientists.\\nThe chapter will include:\\n• The need for best practices for AI/ML\\n• Data best practices\\n• Model best practices\\nThe need for best practices\\nToday, deep learning algorithms are not just an active research area but part and parcel of many \\ncommercial systems and products. Figure 18.1 shows the investment in AI start-ups in the last five \\nyears. You can see that the interest in AI start-ups is continuously increasing. From healthcare to virtual \\nassistants, from room cleaning robots to self-driving cars, AI today is the driving force behind many \\nof the recent important technological advances. AI is deciding whether a person should be hired, or \\nshould be given a loan. AI is creating the feeds you see on social media. There are Natural Language \\nProcessing ( NLP ) bots generating content, images, faces – anything you can think of – there is someone \\ntrying to put AI into it. Since most teams consist of multiple team members working cross-domain, \\nit is important to build best practices. What should be the best practices? Well, there is no definitive \\nanswer to this question as best practices in ML depend on the specific problem domain and dataset. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96052f45-57d9-4d51-bdc7-ea91969bb4c4', embedding=None, metadata={'page_label': '564', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machine Learning Best Practices 564\\nHowever, in this chapter we will provide some general tips for best practices in machine learning:\\nFigure 18.1: Investment in AI start-ups in the last five years (2017–2022)\\nBelow are a few reasons why having best practices in machine learning is important:\\n• It can ensure that models are built in a way that is both effective and efficient.\\n• It can help to avoid issues such as overfitting, which can lead to poor performance on unseen \\ndata.\\n• It can ensure that the models are interpretable and can be easily explained to non-technical \\naudiences.\\n• It can help to promote reproducibility in machine learning research.\\nIn the coming sections, you will be introduced to some best practices as advocated by the FAANG  \\n(Facebook , Amazon, Apple , Netflix, and Google) companies and AI influencers. Following this advice \\ncan help you avoid common mistakes that can lead to inaccurate or poor results. These best practices \\nwill help ensure that your AI services are accurate and reliable. And finally, best practices can help \\nyou optimize your AI services for performance and efficiency.\\nData best practices\\nData is becoming increasingly important in today’s world. Not just people in the field of AI but various \\nworld leaders are calling data “the new gold” or “the new oil” – basically the commodity that will drive \\nthe economy around the world. Data is helping in decision making processes, managing transport, \\ndealing with supply chain issues, supporting healthcare, and so on. The insights derived from data \\ncan help businesses improve their efficiency and performance.\\nMost importantly, data can be used to create new knowledge. In business, for example, data can be \\nused to identify new trends. In medicine, data can be used to uncover new relationships between \\ndiseases and to develop new treatments. However, our models are only as good as the data they are \\ntrained on. And therefore, the importance of data is likely to continue to increase in the future. As \\ndata becomes more accessible and easier to use, it will become increasingly important in a variety of \\nfields. Let us now see some common bottlenecks and the best way to deal with them.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50ca69cc-8c8e-4ef7-aa67-7bff6132d715', embedding=None, metadata={'page_label': '565', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 18 565\\nFeature selection\\nThe first step when we start with any AI/ML problem is to propose a hypothesis: what are the input \\nfeatures that can help us in classifying or predicting our output? Choosing the right features is essential \\nfor any machine learning model, but it can be difficult to know which ones to choose. If you include \\ntoo many irrelevant features in your model, your results will be inaccurate. If you include too few \\nfeatures, your model may not be able to learn from your data. Thus, feature selection is a critical step \\nin machine learning that helps you reduce noise and improve the accuracy of your models:\\n• As a rule, before using any feature engineering, one should start with directly observed and \\nreported features instead of learned features. Learned features are the features generated \\neither by an external system (like a clustering algorithm) or by employing a deep model itself. \\nSimplifying can help you achieve a solid baseline performance, after which you can experiment \\nwith more esoteric strategies.\\n• Remove the features that you are not using. Unused features create technical debt. They make \\nyour code more difficult to read and maintain and can also lead to unexpected bugs and security \\nvulnerabilities. Of course, it can be difficult to keep track of which features are being used \\nand which are not. However, do not drop the features arbitrarily; perform data analysis and \\nexploration carefully – understand the features. A good way to do this would be to assign an \\nowner to each feature. The feature owner would be responsible for maintaining the feature and \\ndocumenting its rationale so that the knowledge can be shared across teams. This also means \\nthat whenever a feature owner leaves the team, ownership is transferred to other members. \\nBy taking the time to understand and remove unused features, you can keep your code clean \\nand avoid accumulating technical debt.\\n• Often, we think more features equal a better model, but that is far from true. Instead of using \\nmillions of features you do not understand, it is better to work with specific features; you can \\nuse the method of regularization to remove the features that apply to too few examples.\\n• You can also combine and modify the features to create new features. There are a variety \\nof ways you can combine and modify. For example, you can discretize continuously valued \\nfeatures into many discrete features. You can also create synthetic new features by crossing \\n(multiplying) two or more existing features. For example, if you have the features “height” and \\n“weight,” you can create a new feature called “BMI” by combining those two features. Feature \\ncrossing can provide predictive abilities beyond what those features can provide individually. \\nTwo features that are each somewhat predictive of the desired outcome may be much more \\npredictive when combined. This is because the combined feature captures information that \\nis not captured by either individual feature. Feature crossing is a powerful tool that can help \\nto improve the accuracy of predictive models.\\nFeatures and data\\nOne of the problems when we move from learning data science to solving real problems is the lack \\nof data. Despite the internet, mobile, and IoT devices generating loads of data, getting good-quality \\nlabeled data is a big hurdle. The cost of annotation is normally as high as it is time-consuming and \\nrequires subject matter expertise. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='099600be-41da-47f9-b9d0-40399eb85a8c', embedding=None, metadata={'page_label': '566', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machine Learning Best Practices 566\\nThus, we need to ensure we have sufficient data to train the model. As a rule of thumb, the number of \\ninput features (n) that a model can learn is roughly proportional to the amount of data (N) you have \\n(n << N). A few tips that can be followed in such a situation are:\\n• Scale model learning to the size of the data. For example, if we have only 1,000 labeled samples, \\nthen use highly human-engineered features. A good number would be to have a dozen well-\\nselected features for 1,000 labeled samples. But if we have millions of examples, then we can \\nafford to have about a hundred thousand features. And assuming we have billions of data \\nsamples, we can build a model with millions of features. \\n• If we have too much data, we do not arbitrarily drop it; instead, we can use Importance Weight \\nSampling ( https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/\\nfinal_reports/report247.pdf ). The idea is to assign a weight of importance to each sample \\nbased on some distributional feature that captures similarity to the specialized domain data.\\n• Another way to deal with a lack of sufficient data is using data augmentation. Initially proposed \\nfor image data by H. S. Baird in his article, Document image analysis  [7], it has proven to be a good \\nway to increase image data by making use of simple image transformations, like horizontal flips, \\nvertical flips, rotation, translation, etc. Most deep learning frameworks have data generators, \\nwhich you can use to perform this augmentation on the go, as shown in Figure 18.2:\\nFigure 18.2: Original and augmented images\\nWhile augmenting image data is readily available in all the major deep learning frameworks, augmenting \\ntextual data and audio data is not that straightforward. Next, we present some of the techniques you \\ncan use to augment textual and speech data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='347e65fe-7ddd-491d-ab1c-4b22b0a69f03', embedding=None, metadata={'page_label': '567', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 18 567\\nAugmenting textual data\\nSome of the simple ways that we can use to augment textual data are:\\n• Synonym replacement: In this, random words from the sentence are chosen and replaced by \\ntheir synonyms using WordNet. For example, if we have the sentence “This book focuses on \\ndeep learning using TensorFlow and Keras and is meant for both novices and experts,” we \\ncan choose the two words in bold for synonym replacement, resulting in this sentence: “This \\nbook centers on deep learning using TensorFlow and Keras and is meant for both beginners \\nand experts.”\\n• Back translation: The method was proposed by Sennrich et al. in 2016. The basic idea is that a \\nsentence is translated into another language and then translated back to the original language. \\nWe can use language translation APIs or Python modules like googletrans . The following code \\nsnippet translates a sentence from English to German and back. For the code to work, we need \\nto have googletrans  installed:\\nfrom googletrans import Translator\\ntranslator = Translator()\\ntext = 'I am going to the market for a walk'\\ntranslated = translator.translate(text, src= 'en', dest='de')\\nsynthetic_text = translator.translate(translated.text, src= 'de', dest='en')\\nprint(f'text: {text}\\\\nTranslated: {translated.text} \\\\nSynthetic Text: \\n{synthetic_text.text} ')\\nNow we have two sentences “I am going to the market” and “I walk to the market” belonging to the \\nsame class. Figure 18.3 details the process of data augmentation using back translation:\\nFigure 18.3: Data augmentation using back translation\\nIn the review paper A survey of Data Augmentation Approaches for NLP, the authors provide an \\nextensive list of many other augmentation methods. This paper provides an in-depth analysis of data \\naugmentation for NLP.\\nIn recent years, with the success of large language models and transformers, people have experimented \\nwith using them for the task of data augmentation. In the paper entitled Data augmentation using pre-\\ntrained transformers, by the Amazon Alexa AI team, the authors demonstrate how by using only 10 \\ntraining samples per class they can generate synthetic data using the pretrained transformers. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41514017-fb3b-4809-b554-71bf9bea5656', embedding=None, metadata={'page_label': '568', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machine Learning Best Practices 568\\nThey experimented with three different pretrained models: an autoencoder LM BERT, an autoregressive \\nLM GPT2, and the pretrained seq2seq  model BART. Figure 18.4 shows their algorithm for generating \\nsynthetic data using pretrained models:\\n \\nFigure 18.4: Algorithm for generating synthetic textual data using pretrained transformers\\nSpeech data can also be augmented using techniques like:\\n• Time warping: Here a random point is selected and data is warped to either left or right with a \\ndistance w. The distance w is not fixed, and instead is chosen from a uniform distribution [0, W ].\\n• Frequency Masking : Here a range of frequency channels [f 0, f0+f)] are masked; the choice \\nof frequency f 0 and f depends upon the number of frequency channels and frequency mask \\nparameter F. \\n• Time Masking: In this case, the consecutive time steps are masked.\\nThese techniques were proposed by the Google Team in 2019, in their paper SpecAugment: A simple \\ndata augmentation method for Automatic Speech Recognition.\\nModel best practices\\nModel accuracy and performance are critical to success for any machine learning and deep learning \\nproject. If a model is not accurate enough, the associated business use case will not be successful. \\nTherefore, it is important to focus on model accuracy and performance to increase the chances of \\nsuccess. There are a number of factors that impact model accuracy and performance, so it is important \\nto understand all of them in order to optimize accuracy and performance. Below we list some of the \\nmodel best practices that can help us leverage best from our model development workflow.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99563bb4-0103-4d1a-84cb-1fe1139e6bed', embedding=None, metadata={'page_label': '569', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 18 569\\nBaseline models\\nA baseline model is  a tool used in machine learning to evaluate other models. It is usually the simplest \\npossible model, and acts as a comparison point for more complex  models. The goal is to see if the more \\ncomplex models are actually providing any improvements over the baseline model. If not, then there \\nis no point in using the more complex model. Baseline models can also be used to help detect data \\nleakage. Data leakage occurs when information from the test set bleeds into the training set, resulting \\nin overfitting. By comparing the performance of the baseline model to other models, it is possible to \\ndetect when data leakage has occurred. Baseline models are an essential part of machine learning \\nand provide a valuable perspective on the performance of more complex models. Thus, whenever \\nwe start working on a new problem, it is good to think of the simplest model that can fit the data and \\nget a baseline.\\nOnce we have built a satisfactory baseline model, we need to carefully review it.\\nReview the initial hypothesis about the dataset and the choice of our initial algorithms. For example, \\nmaybe when we first began working with the data, we hypothesized that the patterns we are observing \\nwould be best explained by a Gaussian Mixture Model (GMM ). However, after further exploration, \\nwe may find that the GMM is not able to capture the underlying structure of the data accurately. In \\nthat case we will need to rethink our strategy. Ultimately, our choice of algorithms is dictated by the \\nnature of the data itself.\\nConfirm if the model is overfitting or underfitting. If the model is overfitting, try more data, reduce \\nmodel complexity, increase batch size, or include regularization methods like ridge, lasso, or dropout. \\nIf the model is underfitting, try increasing model complexity, adding more features, and training for \\nmore epochs.\\nAnalyze the model based on its performance metrics. For example, if we have made a classification \\nmodel, analyze its confusion metrics and its precision/recall as per the business use case. Identify which \\nclass model is not predicting correctly; this should give us an insight into the data for those classes.\\nPerform hyperparameter tuning to get a strong baseline model. It is important that we establish a \\nstrong baseline model because it serves as a benchmark for future model improvements. The baseline \\nshould incorporate all the business and technical requirements, and test the data engineering and \\nmodel deployment pipelines. By taking the time to develop a strong baseline, we can ensure that \\nour machine learning project is on the right track from the start. Furthermore, a good baseline can \\nhelp us identify potential areas of improvement as we iterate on our models. As such, it is well worth \\ninvesting the time and effort to create a robust baseline model.\\nPretrained models, model APIs, and AutoML\\nWhen we want to launch a commercial product, time and energy are often two of the most important \\nfactors. When working on a new project, it can be very time-consuming to train a baseline model \\nfrom scratch. However, there are now a number of sources where we can find pretrained models that \\ncan save us a lot of time and effort. These include GitHub, Kaggle, and various cloud-based APIs from \\ncompanies like Amazon, Google, OpenAI, and Microsoft. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b4ff45e-698e-4cc0-8d4b-b91413724dde', embedding=None, metadata={'page_label': '570', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machine Learning Best Practices 570\\nIn addition, there are specialized start-ups like Scale AI and Hugging Face that offer pretrained models \\nfor a variety of different tasks. By taking advantage of these resources, we can quickly get our machine \\nlearning projects up and running without having to spend a lot of time training a model from scratch. \\nSo, if our problem is a standard classification or regression problem, or we have structured tabular \\ndata available, we can make use of either pretrained models, or APIs provided by companies like \\nAmazon, Google, and Microsoft. Using these approaches can save us valuable time and energy and \\nallow us to get started with our project quickly.\\nAnother solution that is evolving is using AutoML, or Automatic Machine Learning. Using AutoML, we \\ncan create custom models that are more tailored to a company’s specific needs. If you are limited in \\nterms of organizational knowledge and resources, we can still take advantage of machine learning at \\nscale by utilizing AutoML. This solution has already been helping companies large and small to meet \\ntheir business goals in a more efficient and accurate manner. In the future, it is likely that AutoML \\nwill only become more prevalent and popular as awareness of its capabilities grows.\\nModel evaluation and validation\\nIn this section, we talk about ways of evaluating our model. Here we are not talking about conventional \\nmachine learning metrics, but instead focusing on the experience of the end user:\\n• User experience techniques: When our model is near production, we should test it further. \\nCrowdsourcing is a great way to get feedback from our audience before we release the product. \\nWe can either pay people or use live experiments on real users in order for them to give \\nvaluable opinions about what works best. We can create user personas early in a process, that \\nis, create hypothetical users – for example, if we are a team with an age group lying between \\n19–40 and we build a recommender system, we can create a user persona for someone in their \\nsixties. Later, we can perform usability testing by bringing in actual people and watching their \\nreactions to our site.\\n• Use model deltas: When we’re releasing a new model, one of the best ways to measure its \\nsuccess is by calculating how different it is from the one in production. For example, if our \\nranking algorithm has been giving better results than expected but not as much so that people \\nwould notice, or care, then we should run both models on samples through the entire system \\nwith weights given by position rank. If we find that the difference between the two queries is \\nvery small, then we know that there will be little change. However, if the difference is large, \\nwe should ensure that the change is good. In this case we should explore the queries where \\nsymmetric differences are high; this will enable you to understand the change qualitatively.\\n• Utilitarian power is more important than predictive power: We may have a model with the \\nhighest accuracy and best prediction. But that is not the end; the question is what we do with \\nthat prediction. For example, if we build a model to semantically rank the documents, then \\nthe quality of the final ranking matters more than the prediction. Let us consider another \\nexample: let us say you built a spam filter, and our model predicts the probability whether \\nthe given message is spam or ham; we follow it with a cut-off on what text is blocked. In such \\na situation, what we allow to pass through matters most. So, it is possible that we get a model \\nwith better log loss, but still no improvement in overall performance. In such a case, we should \\nlook for other features to improve the performance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b3103be-7204-45d0-b037-5e7aae0cfd16', embedding=None, metadata={'page_label': '571', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 18 571\\n• Look for patterns in measured errors: In training samples, check the ones the model is not \\nable to predict correctly. Explore features that we have not yet considered yet; can they improve \\nthe prediction for the incorrect samples? Do not be very specific with features; we can add a \\ndozen of them, and let the model decide what can be done with them. To visualize errors in \\nclassification problems, we can use a confusion matrix, and in regression tasks, we can look \\nfor cases where the loss is high.\\n• Test on unseen data: To measure the performance of our model, test it on the data that is \\ngathered after the model has been trained; this way we will get an estimate of the performance \\nin production. This may result in reduced performance, but the reduction should not be severe. \\nPerformance monitoring is a crucial part of model development. The performance between training \\nand production data can vary drastically, which means that we must continuously monitor the behavior \\nof deployed models to make sure they’re not doing anything unexpected in our system. We should build \\na monitoring pipeline that continuously monitors performance, quality and skew metrics, fairness \\nmetrics, model explanations, and user interactions.\\nModel improvements\\nOnce a reliable model is built and deployed, the work is far from over. The model may need to be \\nchanged for various reasons, such as data drift or concept drift. Data drift occurs when the distribution \\nof data changes over time, and concept drift occurs when the properties of dependent (labeled) variables \\nchange over time. To account for these changes, the model must be retrained on new data and updated \\naccordingly. This process can be time-consuming and expensive, but it is essential to maintaining a \\nhigh-performing machine learning model. However, before we jump into model improvement, it is \\nimportant to identify and measure the reasons for low performance – “measure first, optimize second”:\\nData drift: The performance of a machine learning model can vary depending on when it is trained \\nand when it is deployed. This is because the data used during training and serving can be different. \\nTo avoid this problem, it is important to log the features at the time of deployment. This can allow \\nus to monitor the variation in serving data (data in production). Once the data drift (the difference \\nbetween training data and serving data) crosses a threshold, we should retrain the model with new \\ndata. This will ensure that the model is trained on the same data that it will be deployed on, and thus \\nimprove its performance.\\nTraining-serving skew: Training-serving skew can be a major problem for machine learning models. \\nIf there is a discrepancy between how the model is trained and how it is used in the real world, this \\ncan lead to poor performance and inaccuracies. There are three main causes of training-serving \\nskew: a discrepancy between the data used in training and serving, a change in the data between \\ntraining and serving, and a feedback loop between the model and the algorithm. For example, if we \\nhave built a recommender system to recommend movies, we can then retrain the recommender later \\nbased on the movies users saw from the recommended list. The first two causes can be addressed by \\ncareful data management, while the third cause requires special attention when designing machine \\nlearning models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='174429f5-e9bb-48b8-bee8-932ac8980d1a', embedding=None, metadata={'page_label': '572', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Machine Learning Best Practices 572\\nIt is possible that even after sufficient experimentation, we find that with the present features we \\ncannot improve the model performance any further. However, to stay in business, continuous growth \\nis necessary. Thus, when we find that our model performance has plateaued, it is time to look for new \\nsources for improvements, instead of working with the existing features.\\nThe software development process is never really “done.” Even after a product is launched, there are \\nalways going to be new features that could be added or existing features that could be improved. The \\nsame is true for machine learning models. Even after a model is “finished” and deployed to production, \\nthere will always be new data that can be used to train a better model. And as data changes over time, \\nthe model will need to be retrained on new data to remain accurate. Therefore, it’s important to think \\nof machine learning models as being in a constant state of flux. It’s never really “done” until you stop \\nworking on it.\\nAs we build our model, it is important to think about how easy it is to add or remove features. Can we \\neasily create a fresh copy of the pipeline and verify its correctness? Is it possible to have two or three \\ncopies of the model running in parallel? These are all important considerations when building our \\nmodel. By thinking about these things upfront, we can save ourselves a lot of time and effort down \\nthe line.\\nSummary\\nIn this chapter, we focused on the strategies and rules to follow to get the best performance from your \\nmodels. The list here is not exhaustive, and since AI technology is still maturing, in the years to come \\nwe may see more rules and heuristics emerge. Still, if you follow the advice in the chapter, you will \\nbe able to move from the alchemical nature of AI models to more reliable, robust, and reproducible \\nbehavior.\\nIn the next chapter, we will explore the TensorFlow ecosystem and see how we can integrate all that \\nis covered in this book into practical business applications.\\nReferences\\n1. Soni, N., Sharma, E. K., Singh, N., and Kapoor, A. (2020). Artificial intelligence in business: from \\nresearch and innovation to market deployment. Procedia Computer Science, 167, 2200–2210.\\n2. Feng, S. Y., Gangal, V ., Wei, J., Chandar, S., Vosoughi, S., Mitamura, T., and Hovy, E. (2021). A \\nsurvey of data augmentation approaches for NLP. arXiv preprint arXiv:2105.03075.\\n3. Sennrich, R., Haddow, B., and Birch, A. (2016). Improving Neural Machine Translation Models \\nwith Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for \\nComputational Linguistics (Volume 1: Long Papers), pages 86–96, Berlin, Germany. Association \\nfor Computational Linguistics.\\n4. Kumar, V ., Choudhary, A., and Cho, E. (2020). Data augmentation using pre-trained transformer \\nmodels. arXiv preprint arXiv:2003.02245.\\n5. Park, D. S., Chan, W ., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D., and Le, Q. V . (2019). \\nSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. arXiv \\npreprint arXiv:1904.08779.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2398d7af-a0e0-435d-ae1d-8ee0ebed4552', embedding=None, metadata={'page_label': '573', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 18 573\\n6. Rules of Machine Learning: Best practices for ML engineering. Martin Zinkewich. https://\\ndevelopers.google.com/machine-learning/guides/rules-of-ml\\n7. Baird, H. S. (1995). Document image analysis. Chapter: Document Image Defect Models, pages \\n315–325. IEEE Computer Society Press, Los Alamitos, CA, USA.\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0ec1c51-c9c9-4eb3-9834-27477825d59a', embedding=None, metadata={'page_label': '574', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54546d09-f5b0-484e-8f93-9427bf38c76c', embedding=None, metadata={'page_label': '575', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19\\nTensorFlow 2 Ecosystem\\nIn this chapter, we will learn about the different components of the TensorFlow ecosystem. The \\nchapter will elaborate upon TensorFlow Hub – a repository for pretrained deep learning models – and \\nTensorFlow Datasets – a collection of ready-to-use datasets for ML tasks. TensorFlow JS, the solution for \\ntraining and deploying ML models on the web, will be introduced. We will also learn about TensorFlow \\nLite, an open-source deep learning framework for mobile and edge devices. Some examples of Android, \\niOS, and Raspberry Pi applications will be discussed, together with examples of deploying pretrained \\nmodels such as MobileNet v1, v2, v3 (image classification models designed for mobile and embedded \\nvision applications), PoseNet for pose estimation (a vision model that estimates the poses of people in \\nimage or video), DeepLab segmentation (an image segmentation model that assigns semantic labels \\n(for example, dog, cat, and car) to every pixel in the input image), and MobileNet SSD object detection \\n(an image classification model that detects multiple objects with bounding boxes). The chapter will \\nconclude with an example of federated learning, a decentralized machine learning framework that \\nis thought to respect user privacy. The chapter includes:\\n• TensorFlow Hub\\n• TensorFlow Datasets\\n• TensorFlow Lite and using it for mobile and edge applications\\n• Federated learning at edge\\n• TensorFlow JS\\n• Using Node.js with TensorFlow models\\nLet’s begin with TensorFlow Hub.All the code files for this chapter can be found at https://packt.link/dltfchp19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb7766e0-5dcc-4cae-a2bb-c93f0d87683d', embedding=None, metadata={'page_label': '576', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 576\\nTensorFlow Hub\\nEven if you have a powerful computer, training a machine learning model can take days or weeks. And \\nonce you’ve trained the model, deploying it to different devices can be difficult and time-consuming. \\nDepending upon the platform you want to deploy, you might need it in different formats.\\nYou can think of TensorFlow Hub as a library with many pretrained models. It contains hundreds \\nof trained, ready-to-deploy deep learning models. TensorFlow Hub provides pretrained models for \\nimage classification, image segmentation, object detection, text embedding, text classification, video \\nclassification and generation, and much more. The models in TF Hub are available in SavedModel, \\nTFLite, and TF.js formats. We can use these pretrained models directly for inference or fine-tune them. \\nWith its growing community of users and developers, TensorFlow Hub is the go-to place for finding \\nand sharing machine learning models. To use TensorFlow Hub, we first need to install it:\\npip install tensorflow_hub\\nOnce installed, we can import it simply using:\\nimport tensorflow_hub as hub\\nand load the model using the load  function:\\nmodel = hub.load(handle)\\nHere handle  is a string, which contains the link of the model we wants to use. If we want to use it as \\npart of our existing model, we can wrap it as a Keras layer:\\nhub.KerasLayer(\\n    handle,\\n    trainable= False,\\n    arguments= None,\\n    _sentinel= None,\\n    tags= None,\\n    signature= None,\\n    signature_outputs_as_dict= None,\\n    output_key= None,\\n    output_shape= None,\\n    load_options= None,\\n    **kwargs\\n)\\nBy changing the parameter trainable  to True , we can fine-tune the model for our specific data.\\nFigure 19.1 shows the easy-to-use web interface to select different models at the tfhub.dev  site. Using \\nthe filters, we can easily find a model to solve our problem. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='240d53b8-704b-4258-936a-f1de29b7b180', embedding=None, metadata={'page_label': '577', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 577\\nWe can choose which type and format we need, as well as who published it!\\nFigure 19.1: The tfhub.dev site showing different filters\\nUsing pretrained models for inference\\nLet us see how you can leverage pretrained models from TensorFlow Hub. We will consider an example \\nof image classification:\\n1. Let us import the necessary modules:\\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nimport matplotlib.pyplot as plt\\nimport numpy as np', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91d7ad39-7f4f-4aff-b3a2-d106bd941d86', embedding=None, metadata={'page_label': '578', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 578\\n2. We define a function for loading an image from a URL. The functions get the image from the \\nweb, and we reshape it by adding batch indexes for inference. Also, the image is normalized \\nand resized according to the pretrained model chosen:\\ndef load_image_from_url (img_url, image_size):\\n  \"\"\"Get the image from url. The image return has shape [1, height, \\nwidth, num_channels].\"\"\"\\n  response = requests.get(img_url, headers={ \\'User-agent\\' : \\'Colab Sample \\n(https://tensorflow.org)\\' })\\n  image = Image. open(BytesIO(response.content))\\n  image = np.array(image)\\n  # reshape image\\n  img_reshaped = tf.reshape(image, [ 1, image.shape[ 0], image.shape[ 1], \\nimage.shape[ 2]]) \\n  # Normalize by convert to float between [0,1]\\n  image = tf.image.convert_image_dtype(img_reshaped, tf.float32) \\n  image_padded = tf.image.resize_with_pad(image, image_size, image_size)\\n  return  image_padded, image\\n3. Another helper function to show the image:\\ndef show_image (image, title= \\'\\'):\\n  image_size = image.shape[ 1]\\n  w = (image_size * 6) // 320\\n  plt.figure(figsize=(w, w))\\n  plt.imshow(image[ 0], aspect= \\'equal\\')\\n  plt.axis( \\'off\\')\\n  plt.title(title)\\n  plt.show()\\n4. The model we are using is EfficientNet-B2 ( https://arxiv.org/abs/1905.11946 ) trained on \\nthe ImageNet dataset. It gives better accuracy, is smaller in size, and gives faster inference. For \\nconvenience, we choose images to be resized to 330 x 330 pixels. We use the helper function \\ndefined in step 2 to download the image from Wikimedia:\\nimage_size = 330\\nprint(f\"Images will be converted to {image_size} x{image_size} \")\\nimg_url =  \"https://upload.wikimedia.org/wikipedia/commons/c/c6/Okonjima_\\nLioness.jpg\"\\nimage, original_image = load_image_from_url(img_url, image_size) \\nshow_image(image, \\'Scaled image\\' )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62459377-1a7b-4269-a7bd-ecc2388ac553', embedding=None, metadata={'page_label': '579', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 579\\nFigure 19.2: The image taken from the web for classification, scaled to size 330 x 330 pixels\\n5. For completeness, we also get all the labels of ImageNet data so that we can infer the label \\nfrom the model prediction; we download it from a public repository of TensorFlow:\\nlabels_file = \"https://storage.googleapis.com/download.tensorflow.org/\\ndata/ImageNetLabels.txt\"\\n#download labels and creates a maps\\ndownloaded_file = tf.keras.utils.get_file( \"labels.txt\" , origin=labels_\\nfile)\\nclasses = []\\nwith open(downloaded_file) as f:\\n  labels = f.readlines()\\n  classes = [l.strip() for l in labels]\\n6. Now that all the ingredients are ready, we download the model from tfhub.dev :\\nclassifier = hub.load( \"https://tfhub.dev/tensorflow/efficientnet/b2/\\nclassification/1\" )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f6f5c54-4184-4ff5-b89d-c3316952d758', embedding=None, metadata={'page_label': '580', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 580\\n7. We get the softmax probabilities for all the classes for the image downloaded in step 5:\\nprobabilities = tf.nn.softmax(classifier(image)).numpy()\\n8. Let us see the top prediction:\\ntop_5 = tf.argsort(probabilities, axis=- 1, direction= \"DESCENDING\" )[0]\\n[:5].numpy()\\nshow_image(image, f\\'{classes[top_5[ 0]+1]}: {probabilities[ 0][top_5]\\n[0]:.4f}\\')\\nFigure 19.3: The image with the label prediction of lion\\nSo, as we can see, in a few lines of code we get a perfect inference – the image is of a lioness, and the \\nclosest label for it in the ImageNet dataset is that of a lion , which the model has correctly predicted. \\nBy using the pretrained models of TF Hub, we can focus on our product workflow, and get better \\nmodels and faster production.\\nTensorFlow Datasets\\nTensorFlow Datasets ( TFDS) is a powerful tool for anyone working with machine learning. It provides \\na collection of ready-to-use datasets that can be easily used with TensorFlow or any other Python ML \\nframework. All datasets are exposed as tf.data.Datasets , making it easy to use them in your input \\npipeline. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8c3a83a-0b1f-4ca4-947b-02ae9271130d', embedding=None, metadata={'page_label': '581', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 581\\nWith TFDS, you can quickly get started with your machine learning projects and save time by not \\nhaving to collect and prepare your own data. The library currently contains a wide variety of datasets, \\nincluding image classification, object detection, text classification, and more. In addition, the library \\nprovides tools for creating new datasets from scratch, which can be useful for researchers or developers \\nwho need to create custom datasets for their own projects. TFDS is open source and released under \\nthe Apache 2.0 license. To be able to use TFDS, you will need to install it:\\npip install tensorflow-datasets\\nOnce installed, you can import it as:\\nimport tensorflow_datasets as tfds\\nAt the time of writing this book, TFDS contained 224 public datasets for a large range of tasks:\\ndatasets = tfds.list_builders()\\nprint(f\"TFDS contains {len(datasets)}  datasets\" )\\n### Output\\nTFDS contains 224 datasets\\nIn this section, we will introduce you to TFDS and show how it can simplify your training process by \\nexploring its underlying structure as well as providing some best practices for loading large amounts \\ninto machine learning models efficiently.\\nLoad a TFDS dataset\\nEach dataset in TFDS is identified by its unique name, and associated with each dataset is also a \\npublisher and dataset version. To get the data, you can use the TFDS load  function (it is a powerful \\nfunction with a lot of flexibility; you can read more about the function at https://www.tensorflow.\\norg/datasets/api_docs/python/tfds/load ):\\ntfds.load(\\n    name: str,\\n    *,\\n    split: Optional[Tree[splits_lib.SplitArg]] = None,\\n    data_dir: Optional[ str] = None,\\n    batch_size: tfds.typing.Dim = None,\\n    shuffle_files: bool = False,\\n    download: bool = True,\\n    as_supervised: bool = False,\\n    decoders: Optional[TreeDict[decode.partial_decode.DecoderArg]] =\\nNone,\\n    read_config: Optional[tfds.ReadConfig] = None,\\n    with_info: bool = False,\\n    builder_kwargs: Optional[Dict[ str, Any]] = None,\\n    download_and_prepare_kwargs: Optional[Dict[ str, Any]] = None,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0f0ea39-ffb0-4d94-a98b-74554076c2f2', embedding=None, metadata={'page_label': '582', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 582\\n    as_dataset_kwargs: Optional[Dict[ str, Any]] = None,\\n    try_gcs: bool = False\\n)\\nYou only need to specify the dataset name; the rest of the parameters are optional. You can read more \\nabout the optional arguments from TFDS docs. For example, below, we are downloading the famous \\nMNIST dataset:\\ndata, info = tfds.load(name= \"mnist\", as_supervised= True, split=[ \\'train\\', \\n\\'test\\'], with_info= True)\\nThe preceding statement downloads both the training and test dataset of MNIST into the variable data. \\nSince the as_supervised  flag is set to True , the labels are downloaded with the data, and the detailed \\ninformation about the dataset is downloaded in info .\\nLet us first check the info:\\nprint(info)\\n### output\\ntfds.core.DatasetInfo(\\n    name=\\'mnist\\',\\n    version=3.0.1,\\n    description=\\'The MNIST database of handwritten digits.\\',\\n    homepage=\\'http://yann.lecun.com/exdb/mnist/\\',\\n    features=FeaturesDict({\\n        \\'image\\': Image(shape=(28, 28, 1), dtype=tf.uint8),\\n        \\'label\\': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\\n    }),\\n    total_num_examples=70000,\\n    splits={\\n        \\'test\\': 10000,\\n        \\'train\\': 60000,\\n    },\\n    supervised_keys=(\\'image\\', \\'label\\'),\\n    citation=\"\"\"@article{lecun2010mnist,\\n      title={MNIST handwritten digit database},\\n      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\\n      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\\n      volume={2},\\n      year={2010}\\n    }\"\"\",\\n    redistribution_info=,\\n)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='771712dd-29cc-40ad-9d1f-4f4d65334ead', embedding=None, metadata={'page_label': '583', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 583\\nSo, we can see that the information is quite extensive. It tells us about the splits and the total number \\nof samples in each split, the keys available if used for supervised learning, the citation details, and \\nso on. The variable data here is a list of two TFDS dataset objects – the first one corresponding to the \\ntest dataset and the second one corresponding to the train dataset. TFDS dataset objects are dict  by \\ndefault. Let us take one single sample from the train dataset and explore:\\ndata_train = data[ 1].take( 1)\\nfor sample, label in data_train:\\n  print(sample.shape)\\n  print(label)\\n### output\\n(28, 28, 1)\\ntf.Tensor(2, shape=(), dtype=int64)\\nYou can see that the sample is an image of handwritten digits of the shape 28 x 28 x 1 and its label \\nis 2. For image data, TFDS also has a method show_examples , which you can use to view the sample \\nimages from the dataset: \\nfig = tfds.show_examples(data[ 0], info)\\nFigure 19.4: Sample from test dataset of MNIST dataset\\nBuilding data pipelines using TFDS\\nLet us build a complete end-to-end example using the TFDS data pipeline:\\n1. As always, we start with importing the necessary modules. Since we will be using TensorFlow \\nto build the model, and TFDS for getting the dataset, we are including only these two for now:\\nimport tensorflow as tf\\nimport tensorflow_datasets as tfds', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8f16066-9a38-42d7-8c57-8610b725ab20', embedding=None, metadata={'page_label': '584', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 584\\n2. Using the Keras Sequential API, we build a simple convolutional neural network with three \\nconvolutional layers and two dense layers:\\nmodel = tf.keras.models.Sequential([ \\n  tf.keras.layers.Conv2D( 16, (3,3), activation= \\'relu\\', input_shape=( 300, \\n300, 3)), \\n  tf.keras.layers.MaxPooling2D( 2, 2),\\n  tf.keras.layers.Conv2D( 32, (3,3), activation= \\'relu\\'), \\n  tf.keras.layers.MaxPooling2D( 2,2), \\n  tf.keras.layers.Conv2D( 64, (3,3), activation= \\'relu\\'), \\n  tf.keras.layers.MaxPooling2D( 2,2), \\n  tf.keras.layers.Flatten(), \\n  tf.keras.layers.Dense( 256, activation= \\'relu\\'), \\n  tf.keras.layers.Dense( 1, activation= \\'sigmoid\\' )\\n])\\n3. We will be building a binary classifier, so we choose binary cross entropy as the loss function, \\nand Adam as the optimizer:\\nmodel.compile(optimizer= \\'Adam\\', loss=\\'binary_\\ncrossentropy\\' ,metrics=[ \\'accuracy\\' ])\\n4. Next, we come to the dataset. We are using the horses_or_humans  dataset, so we use the tfds.\\nload  function to get the training and validation data:\\ndata = tfds.load( \\'horses_or_humans\\' , split= \\'train\\', as_supervised= True) \\nval_data = tfds.load( \\'horses_or_humans\\' , split= \\'test\\', as_\\nsupervised= True)\\n5. The images need to be normalized; additionally, for better performance, we will augment the \\nimages while training:\\ndef normalize_img (image, label):\\n  \"\"\"Normalizes images: \\'uint8\\' -> \\'float32\\'.\"\"\"\\n  return  tf.cast(image, tf.float32) / 255., label\\ndef augment_img (image, label):\\n  image, label = normalize_img(image, label)\\n  image = tf.image.random_flip_left_right(image)\\n  return  image, label\\n6. So now we build the pipeline; we start with cache  for better memory efficiency, apply the pre-\\nprocessing steps (normalization and augmentation), ensure that data is shuffled while training, \\ndefine the batch size, and use prefetch  so that the next batch is brought in as the present \\nbatch is being trained on. We repeat the same steps for the validation data. The difference is \\nthat validation data need not be augmented or shuffled:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19e9ae0c-6a87-49e7-88b6-d201a1151042', embedding=None, metadata={'page_label': '585', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 585\\ndata = data.cache()\\ndata = data. map(augment_img, num_parallel_calls=tf.data.AUTOTUNE)\\ntrain_data = data.shuffle( 1024).batch( 32)\\ntrain_data = train_data.prefetch(tf.data.AUTOTUNE)\\nval_data = val_data. map(normalize_img, num_parallel_calls=tf.data.\\nAUTOTUNE)\\nval_data = val_data.batch( 32)\\nval_data = val_data.cache()\\nval_data = val_data.prefetch(tf.data.AUTOTUNE)\\n7. And finally, we train the model:\\n%time history = model.fit(train_data, epochs= 10, validation_data=val_\\ndata, validation_steps= 1)\\nPlay around with different parameters of the data pipeline and see how it affects the training time. For \\nexample, try removing prefetch  and cache  and not specifying num_parallel_calls .\\nTensorFlow Lite\\nTensorFlow Lite is a lightweight platform designed by TensorFlow. This platform is focused on mobile \\nand embedded devices such as Android, iOS, and Raspberry Pi. The main goal is to enable machine \\nlearning inference directly on the device by putting a lot of effort into three main characteristics: (1) \\na small binary and model size to save on memory, (2) low energy consumption to save on the battery, \\nand (3) low latency for efficiency. It goes without saying that battery and memory are two important \\nresources for mobile and embedded devices. To achieve these goals, Lite uses a number of techniques \\nsuch as quantization, FlatBuffers, mobile interpreter, and mobile converter, which we are going to \\nreview briefly in the following sections.\\nQuantization\\nQuantization refers to a set of techniques that constrains an input made of continuous values (such as \\nreal numbers) into a discrete set (such as integers). The key idea is to reduce the space occupancy of \\nDeep Learning ( DL) models by representing the internal weight with integers instead of real numbers. \\nOf course, this implies trading space gains for some amount of performance of the model. However, \\nit has been empirically shown in many situations that a quantized model does not suffer from a \\nsignificant decay in performance. TensorFlow Lite is internally built around a set of core operators \\nsupporting both quantized and floating-point operations.\\nModel quantization is a toolkit for applying quantization. This operation is applied to the representations \\nof weights and, optionally, to the activations for both storage and computation. There are two types \\nof quantization available:\\n• Post-training quantization quantizes weights and the result of activations post-training.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='67e2520f-71a8-4dd0-995b-c2063a0b2fb2', embedding=None, metadata={'page_label': '586', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 586\\n• Quantization-aware training allows for the training of networks that can be quantized with \\nminimal accuracy drop (only available for specific CNNs). Since this is a relatively experimental \\ntechnique, we are not going to discuss it in this chapter, but the interested reader can find \\nmore information in [1].\\nTensorFlow Lite supports reducing the precision of values from full floats to half-precision floats \\n(float16 ) or 8-bit integers. TensorFlow reports multiple trade-offs in terms of accuracy, latency, \\nand space for selected CNN models (see Figure 19.5, source: https://www.tensorflow.org/lite/\\nperformance/model_optimization ):\\nFigure 19.5: Trade-offs for various quantized CNN models\\nFlatBuffers\\nFlatBuffers ( https://google.github.io/flatbuffers/ ) is an open-source format optimized to \\nserialize data on mobile and embedded devices. The format was originally created at Google for game \\ndevelopment and other performance-critical applications. FlatBuffers supports access to serialized \\ndata without parsing/unpacking for fast processing. The format is designed for memory efficiency and \\nspeed by avoiding unnecessary multiple copies in memory. FlatBuffers works across multiple platforms \\nand languages such as C++, C#, C, Go, Java, JavaScript, Lobster, Lua, TypeScript, PHP, Python, and Rust.\\nMobile converter\\nA model generated with TensorFlow needs to be converted into a TensorFlow Lite model. The converter \\ncan introduce optimizations for improving the binary size and performance. For instance, the converter \\ncan trim away all the nodes in a computational graph that are not directly related to inference but \\ninstead are needed for training.\\nMobile optimized interpreter\\nTensorFlow Lite runs on a highly optimized interpreter that is used to optimize the underlying \\ncomputational graphs, which in turn are used to describe the machine learning models. Internally, \\nthe interpreter uses multiple techniques to optimize the computational graph by inducing a static \\ngraph order and by ensuring better memory allocation. The interpreter core takes ~100 kb alone or \\n~300 kb with all supported kernels.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3779c0bc-2551-446b-b7a9-3a48bd4eafef', embedding=None, metadata={'page_label': '587', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 587\\nSupported platforms\\nOn Android, the TensorFlow Lite inference can be performed using either Java or C++. On iOS, \\nTensorFlow Lite inference can run in Swift and Objective-C. On Linux platforms (such as Raspberry \\nPi), inferences run in C++ and Python. TensorFlow Lite for microcontrollers is an experimental port of \\nTensorFlow Lite designed to run machine learning models on microcontrollers based on Arm Cortex-M \\n(https://developer.arm.com/ip-products/processors/cortex-m ) and series processors, including \\nArduino Nano 33 BLE Sense ( https://store.arduino.cc/nano-33-ble-sense-with-headers ), \\nSparkFun Edge (https://www.sparkfun.com/products/15170 ), and the STM32F746 Discovery  kit \\n(https://www.st.com/en/evaluation-tools/32f746gdiscovery.html ). These microcontrollers are \\nfrequently used for IoT applications.\\nArchitecture\\nThe architecture of TensorFlow Lite is described in Figure 19.6 (from https://www.tensorflow.org/\\nlite/convert/index ). As you can see, both tf �keras (for example, TensorFlow 2.x) and low-Level APIs  \\nare supported. A standard TensorFlow 2.x model can be converted by using TFLite Converter and then \\nsaved in a TFLite FlatBuffer format (named .tflite ), which is then executed by the TFLite interpreter  \\non available devices (GPUs and CPUs) and on native device APIs. The concrete function in Figure 19.6 \\ndefines a graph that can be converted to a TensorFlow Lite model or be exported to a SavedModel:\\nFigure 19.6: TensorFlow Lite internal architectureComputational graphs are the graphical representation of the learning algorithm; here, \\nnodes describe the operations to be performed and edges connecting the nodes represent \\nthe flow of data. These graphs provide the deep learning frameworks with performance \\nefficiency, which we are not able to achieve if we construct a neural network in pure NumPy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58972473-c8ab-43f4-9a3f-625ed6502cdf', embedding=None, metadata={'page_label': '588', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 588\\nUsing TensorFlow Lite\\nUsing TensorFlow Lite involves the following steps:\\n1. Model selection: A standard TensorFlow 2.x model is selected for solving a specific task. This \\ncan be either a custom-built model or a pretrained model.\\n2. Model conversion: The selected model is converted with the TensorFlow Lite converter, \\ngenerally invoked with a few lines of Python code.\\n3. Model deployment: The converted model is deployed on the chosen device, either a phone or \\nan IoT device, and then run by using the TensorFlow Lite interpreter. As discussed, APIs are \\navailable for multiple languages.\\n4. Model optimization: The model can be optionally optimized by using the TensorFlow Lite \\noptimization framework.\\nA generic example of an application\\nIn this section, we are going to see how to convert a model to TensorFlow Lite and then run it. Note that \\ntraining can still be performed by TensorFlow in the environment that best fits your needs. However, \\ninference runs on the mobile device. Let’s see how with the following code fragment in Python:\\nimport tensorflow as tf\\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\\ntflite_model = converter.convert()\\nopen(\"converted_model.tflite\" , \"wb\").write(tflite_model)\\nThe code is self-explanatory. A standard TensorFlow 2.x model is opened and converted by using \\ntf.lite.TFLiteConverter.from_saved_model(saved_model_dir) . Pretty simple! Note that no specific \\ninstallation is required. We simply use the tf.lite  API (https://www.tensorflow.org/api_docs/\\npython/tf/lite ). It is also possible to apply a number of optimizations. For instance, post-training \\nquantization can be applied by default:\\nimport tensorflow as tf\\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\ntflite_quant_model = converter.convert()\\nopen(\"converted_model.tflite\" , \"wb\").write(tflite_quant_model)\\nOnce the model is converted, it can be copied onto the specific device. Of course, this step is different \\nfor each different device. Then the model can run by using the language you prefer. For instance, in \\nJava the invocation happens with the following code snippet:\\ntry (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) {\\n  interpreter.run( input, output);\\n}', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e85e93bc-e0a4-48fa-a253-6c9c03e743de', embedding=None, metadata={'page_label': '589', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 589\\nAgain, pretty simple! What is very useful is that the same steps can be followed for a heterogeneous \\ncollection of mobile and IoT devices.\\nUsing GPUs and accelerators\\nModern phones frequently have accelerators on board that allow floating-point matrix operations \\nto be performed faster. In this case, the interpreter can use the concept of Delegate, and specifically, \\nGpuDelegate() , to use GPUs. Let’s look at an example in Java:\\nGpuDelegate delegate = new GpuDelegate();\\nInterpreter.Options options = (new Interpreter.Options()).\\naddDelegate(delegate);\\nInterpreter interpreter = new Interpreter(tensorflow_lite_model_file, options);\\ntry {\\n  interpreter.run( input, output);\\n}\\nAgain, the code is self-commenting. A new GpuDelegate()  is created and then it is used by the \\ninterpreter to run the model on a GPU.\\nAn example of an application\\nIn this section, we are going to use TensorFlow Lite for building an example application that is later \\ndeployed on Android. We will use Android Studio ( https://developer.android.com/studio/ ) to \\ncompile the code. The first step is to clone the repo with:\\ngit clone https://github.com/tensorflow/examples\\nThen we open an existing project (see Figure 19.7) with the path examples/lite/examples/image_\\nclassification/android .\\nThen you need to install Android Studio from https://developer.android.com/studio/install  and \\nan appropriate distribution of Java. In my case, I selected the Android Studio macOS distribution and \\ninstalled Java via brew  with the following command:\\nbrew tap adoptopenjdk/openjdk\\nbrew cask install  homebrew/cask-versions/adoptopenjdk8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff4e43f6-828b-4bd1-8b38-2464e00a841a', embedding=None, metadata={'page_label': '590', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 590\\nAfter that, you can launch the sdkmanager  and install the required packages. In my case, I decided to \\nuse the internal emulator and deploy the application on a virtual device emulating a Google Pixel 3 \\nXL. The required packages are reported in Figure 19.7:\\nFigure 19.7: Required packages to use a Google Pixel 3 XL emulator\\nThen, start Android Studio and select Open an existing Android Studio project, as shown in Figure 19.8:\\nFigure 19.8: Opening a new Android project', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2efd8b45-7706-41c6-9ad2-a57b491383c7', embedding=None, metadata={'page_label': '591', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 591\\nOpen Adv Manager  (under the Tool  menu) and follow the instructions for how to create a virtual \\ndevice, like the one shown in Figure 19.9:\\nFigure 19.9: Creating a virtual device\\nNow that you have the virtual device ready, let us dive into the TensorFlow Lite models and see how \\nwe can use them.\\nPretrained models in TensorFlow Lite\\nFor many interesting use cases, it is possible to use a pretrained model that is already suitable for \\nmobile computation. This is a field of active research with new proposals coming pretty much every \\nmonth. Pretrained TensorFlow Lite models are available on TensorFlow Hub; these models are ready \\nto use (https://www.tensorflow.org/lite/models/ ). As of August 2022, these include:\\n• Image classification: Used to  identify multiple classes of objects such as places, plants, animals, \\nactivities, and people.\\n• Object detection: Used to detect multiple objects with bounding boxes.\\n• Audio speech synthesis: Used to generate speech from text.\\n• Text embedding: Used to embed textual data.\\n• Segmentations: Identifies the shape of objects together with semantic labels for people, places, \\nanimals, and many additional classes.\\n• Style transfers: Used to apply artistic styles to any given image.\\n• Text classification: Used to assign different categories to textual content.\\n• Question and answer: Used to provide answers to questions provided by users.\\nIn this section, we will discuss some of the optimized pretrained models available in TensorFlow Lite \\nout of the box as of August 2022. These models can be used for a large number of mobile and edge \\ncomputing use cases. Compiling the example code is pretty simple.\\nYou just import a new project from each example directory and Android Studio will use Gradle ( https://\\ngradle.org/ ) for synching the code with the latest version in the repo and for compiling. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e79fd355-f887-4a71-a8d2-f144426e0fb3', embedding=None, metadata={'page_label': '592', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 592\\nIf you compile all the examples, you should be able to see them in the emulator (see Figure 19.10). \\nRemember to select Build | Make Project, and Android Studio will do the rest:\\nFigure 19.10: Emulated Google Pixel 3 XL with TensorFlow Lite example applications\\nImage classification\\nAs of August 2022, the list of available models for pretrained classification is rather large, and it offers \\nthe opportunity to trade space, accuracy, and performance as shown in Figure 19.11 (source: https://\\nwww.tensorflow.org/lite/models/trained ):Edge computing is a distributed computing model that brings computation and data closer \\nto the location where it is needed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55d70ba0-6310-43c8-bcec-2cf5bf0b36ac', embedding=None, metadata={'page_label': '593', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 593\\nFigure 19.11: Space, accuracy, and performance trade-offs for various mobile models\\nMobileNet V1 is a quantized CNN model described in Benoit Jacob [2]. MobileNet V2 is an advanced \\nmodel proposed by Google [3]. Online, you can also find floating-point models, which offer the best \\nbalance between model size and performance. Note that GPU acceleration requires the use of floating-\\npoint models. Note that recently, AutoML models for mobile have been proposed based on an automated \\nmobile neural architecture search (MNAS) approach [4], beating the models handcrafted by humans.\\nWe discussed AutoML in Chapter 13, An Introduction to AutoML, and the interested reader can refer to \\nMNAS documentation in the references [4] for applications to mobile devices.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc431a2f-39b2-42cc-8467-323938cac855', embedding=None, metadata={'page_label': '594', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 594\\nObject detection\\nTensorFlow Lite format models are included in TF Hub. There is a large number of pretrained models \\nthat can detect multiple objects within an image, with bounding boxes. Eighty different classes of \\nobjects are recognized. The network is based on a pretrained quantized COCO SSD MobileNet V1 \\nmodel. For each object, the model provides the class, the confidence of detection, and the vertices of \\nthe bounding boxes ( https://tfhub.dev/s?deployment-format=lite&module-type=image-object-\\ndetection ).\\nPose estimation\\nTF Hub has a TensorFlow Lite format pretrained model for detecting parts of human bodies in an \\nimage or a video. For instance, it is possible to detect noses, left/right eyes, hips, ankles, and many other \\nparts. Each detection comes with an associated confidence score ( https://tfhub.dev/s?deployment-\\nformat=lite&module-type=image-pose-detection ).\\nSmart reply\\nTF Hub also has a TensorFlow Lite format pretrained model for generating replies to chat messages. \\nThese replies are contextualized and similar to what is available on Gmail ( https://tfhub.dev/\\ntensorflow/lite-model/smartreply/1/default/1 ).\\nSegmentation\\nThere are pretrained models ( https://tfhub.dev/s?deployment-format=lite&module-type=image-\\nsegmentation ) for image segmentation, where the goal is to decide what the semantic labels (for \\nexample, person, dog, and cat) assigned to every pixel in the input image are. Segmentation is based \\non the DeepLab algorithm [5].\\nStyle transfer\\nTensorFlow Lite also supports artistic style transfer (see Chapter 20, Advanced Convolutional Neural \\nNetworks) via a combination of a MobileNet V2-based neural network, which reduces the input style \\nimage to a 100-dimension style vector, and a style transform model, which applies the style vector to a \\ncontent image to create the stylized image ( https://tfhub.dev/s?deployment-format=lite&module-\\ntype=image-style-transfer ).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c526ea2c-989b-49b5-8898-374c9f0253d6', embedding=None, metadata={'page_label': '595', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 595\\nText classification\\nThere are models for text classification and sentiment analysis ( https://tfhub.dev/s?deployment-\\nformat=lite&module-type=text-classification ) trained on the Large Movie Review Dataset v1.0 \\n(http://ai.stanford.edu/~amaas/data/sentiment/ ) with IMDb movie reviews that are positive or \\nnegative. An example of text classification is given in Figure 19.12:\\nFigure 19.12: An example of text classification on Android with TensorFlow Lite', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5c48c4a-3046-4e75-9f1e-8295b873ddf5', embedding=None, metadata={'page_label': '596', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 596\\nLarge language models\\nThere are pretrained large language models based on transformer architecture ( https://tfhub.\\ndev/s?deployment-format=lite&q=bert ). The models are based on a compressed variant of BERT \\n[6] (see Chapter 6, Transformers) called MobileBERT [7], which runs 4x faster and has a 4x smaller size. \\nAn example of Q&A is given in Figure 19.13:\\nFigure 19.13: An example of Q&A on Android with TensorFlow Lite and BERT\\nA note about using mobile GPUs\\nThis section concludes the overview of pretrained models for mobile devices and IoT. Note that modern \\nphones are equipped with internal GPUs. For instance, on Pixel 3, TensorFlow Lite GPU inference \\naccelerates inference to 2–7x faster than CPUs for many models (see Figure 19.14, source: https://\\nblog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html ):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95c82afc-836c-42a8-8558-514641ba2179', embedding=None, metadata={'page_label': '597', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 597\\nFigure 19.14: GPU speed-up over CPU for various learning models running on various phones\\nAn overview of federated learning at the edge\\nAs discussed, edge computing is a distributed computing model that brings computation and data \\ncloser to the location where it is needed.\\nNow, let’s introduce Federated Learning (FL ) [8] at the edge, starting with two use cases.\\nSuppose you built an app for playing music on mobile devices and then you want to add recommendation \\nfeatures aimed at helping users to discover new songs they might like. Is there a way to build a \\ndistributed model that leverages each user’s experience without disclosing any private data?\\nSuppose you are a car manufacturer producing millions of cars connected via 5G networks, and then \\nyou want to build a distributed model for optimizing each car’s fuel consumption. Is there a way to \\nbuild such a model without disclosing the driving behavior of each user?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='275ff1f8-8ac9-4f50-b5c8-286201a42006', embedding=None, metadata={'page_label': '598', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 598\\nTraditional machine learning requires you to have a centralized repository for training data either on \\nyour desktop, in your data center, or in the cloud. Federated learning pushes the training phase at the \\nedge by distributing the computation among millions of mobile devices. These devices are ephemeral \\nin that they are not always available for the learning process, and they can disappear silently (for \\ninstance, a mobile phone can be switched off all of a sudden). The key idea is to leverage the CPUs \\nand the GPU of each mobile phone that is made available for an FL computation. Each mobile device \\nthat is part of the distributed FL training downloads a (pretrained) model from a central server, and it \\nperforms local optimization based on the local training data collected on each specific mobile device. \\nThis process is similar to the transfer learning process (see Chapter 20, Advanced Convolutional Neural \\nNetworks), but it is distributed at the edge. Each locally updated model is then sent back by millions \\nof edge devices to a central server to build an averaged shared model.\\nOf course, there are many issues to be considered. Let’s review them:\\n• Battery usage: Each mobile device that is part of an FL computation should save as much as \\npossible on local battery usage.\\n• Encrypted communication: Each mobile device belonging to an FL computation has to use \\nencrypted communication with the central server to update the locally built model.\\n• Efficient communication: Typically, deep learning models are optimized with optimization \\nalgorithms such as SGD (see Chapter 1, Neural Network Foundations with TF, and Chapter 14, The \\nMath Behind Deep Learning). However, FL works with millions of devices and there is, therefore, \\na strong need to minimize the communication patterns. Google introduced a Federated \\nAveraging algorithm [8], which is reported to reduce the amount of communication 10x–100x \\nwhen compared with vanilla SGD. Plus, compression techniques [9] reduce communication \\ncosts by an additional 100x with random rotations and quantization.\\n• Ensure user privacy: This is probably the most important point. All local training data acquired \\nat the edge must stay at the edge. This means that the training data acquired on a mobile device \\ncannot be sent to a central server. Equally important, any user behavior learned in locally \\ntrained models must be anonymized so that it is not possible to understand any specific action \\nperformed by specific individuals.\\nFigure 19.15 shows a typical FL architecture [10]. An FL server sends a model and a training plan to \\nmillions of devices. The training plan includes information on how frequently updates are expected \\nand other metadata.\\nEach device runs the local training and sends a model update back to the global services. Note that \\neach device has an FL runtime providing federated learning services to an app process that stores \\ndata in a local example store. The FL runtime fetches the training examples from the example store:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='702233b8-8e96-4c84-8fdc-74b5495faaa9', embedding=None, metadata={'page_label': '599', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 599\\nFigure 19.15: An example of federated learning architecture\\nTensorFlow FL APIs\\nThe TensorFlow Federated (TTF) platform has two layers:\\n• Federated learning ( FL), as discussed earlier, is a high-level interface that works well with \\ntf.keras  and non- tf.keras  models. In the majority of situations, we will use this API for \\ndistributed training that is privacy-preserving.\\n• Federated core ( FC), a low-level interface that is highly customizable and allows you to interact \\nwith low-level communications and with federated algorithms. You will need this API only if \\nyou intend to implement new and sophisticated distributed learning algorithms. This topic is \\nrather advanced, and we are not going to cover it in this book. If you wish to learn more, you \\ncan find more information online ( https://www.tensorflow.org/federated/federated_core ).\\nThe FL API has three key parts:\\n1. Models: Used to wrap existing models for enabling federating learning. This can be achieved \\nvia the tff.learning.from_keras_model() , or via the subclassing of tff.learning.Model() . \\nFor instance, you can have the following code fragment:\\nkeras_model = …\\nkeras_model. compile(...)\\nkeras_federated_model = tff.learning.from_compiled_keras_model(keras_\\nmodel, ..)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbf5ec53-a8e1-4d59-a7e6-26d7245c779b', embedding=None, metadata={'page_label': '600', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 600\\n2. Builders: This is the layer where the federated computation happens. There are two phases: \\ncompilation, where the learning algorithm is serialized into an abstract representation of the \\ncomputation, and execution, where the represented computation is run.\\n3. Datasets: This is a large collection of data that can be used to simulate federated learning \\nlocally – a useful step for initial fine-tuning.\\nWe conclude this overview by mentioning that you can find a detailed description of the APIs online and \\nalso a number of coding examples ( https://www.tensorflow.org/federated/federated_learning ). \\nStart by using the Colab notebook made available by Google ( https://colab.research.google.com/\\ngithub/tensorflow/federated/blob/v0.10.1/docs/tutorials/federated_learning_for_image_\\nclassification.ipynb ). The framework allows us to simulate the distributed training before running \\nit in a real environment. The library in charge of FL learning is tensorflow_federated . Figure 19.16 \\ndiscusses all the steps used in federated learning with multiple nodes, and it might be useful to better \\nunderstand what has been discussed in this section: \\nFigure 19.16: An example of federated learning with multiple nodes (source: https://upload.wikimedia.\\norg/wikipedia/commons/e/e2/Federated_learning_process_central_case.png)\\nThe next section will introduce TensorFlow.js, a variant of TensorFlow that can be used natively in \\nJavaScript.\\nTensorFlow.js\\nTensorFlow.js is a JavaScript library for machine learning models that can work either in vanilla mode \\nor via Node.js. In this section, we are going to review both of them.\\nVanilla TensorFlow.js\\nTensorFlow.js is a JavaScript library for training and using machine learning models in a browser. It \\nis derived from deeplearn.js, an open-source, hardware-accelerated library for doing deep learning \\nin JavaScript, and is now a companion library to TensorFlow.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4f22163-ec38-4349-91cf-5354e65fd3b8', embedding=None, metadata={'page_label': '601', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 601\\nThe most common use of TensorFlow.js is to make pretrained ML/DL models available on the browser. \\nThis can help in situations where it may not be feasible to send client data back to the server due to \\nnetwork bandwidth or security concerns. However, TensorFlow.js is a full-stack ML platform, and it is \\npossible to build and train an ML/DL model from scratch, as well as fine-tune an existing pretrained \\nmodel with new client data.\\nAn example of a TensorFlow.js application is the TensorFlow Projector ( https://projector.tensorflow.\\norg), which allows a client to visualize their own data (as word vectors) in 3-dimensional space, using \\none of several dimensionality reduction algorithms provided. There are a few other examples of \\nTensorFlow.js applications listed on the TensorFlow.js demo page ( https://www.tensorflow.org/\\njs/demos ).\\nSimilar to TensorFlow, TensorFlow.js also provides two main APIs – the Ops API, which exposes low-\\nlevel tensor operations such as matrix multiplication, and the Layers API, which exposes Keras-style \\nhigh-level building blocks for neural networks.\\nAt the time of writing, TensorFlow.js runs on three different backends. The fastest (and also the most \\ncomplex) is the WebGL backend, which provides access to WebGL’s low-level 3D graphics APIs and \\ncan take advantage of GPU hardware acceleration. The other popular backend is the Node.js backend, \\nwhich allows the use of TensorFlow.js in server-side applications. Finally, as a fallback, there is the \\nCPU-based implementation in plain JavaScript that will run in any browser.\\nIn order to gain a better understanding of how to write a TensorFlow.js application, we will walk \\nthrough an example of classifying MNIST digits using a CNN provided by the TensorFlow.js team \\n(https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html ).\\nThe steps here are similar to a normal supervised model development pipeline – load the data, define, \\ntrain, and evaluate the model.\\nJavaScript works inside a browser environment, within an HTML page. The HTML file (named index.\\nhtml ) below represents this HTML page. Notice the two imports for TensorFlow.js ( tf.min.js ) and the \\nTensorFlow.js visualization library ( tfjs-vis.umd.min.js ) – these provide library functions that we \\nwill use in our application. The JavaScript code for our application comes from data.js  and script.\\njs files, located in the same directory as our index.html  file:\\n<!DOCTYPE html>\\n<html>\\n<head>\\n  <meta charset =\"utf-8\" >\\n  <meta http-equiv =\"X-UA-Compatible\"  content =\"IE=edge\" >\\n  <meta name=\"viewport\"  content =\"width=device-width, initial-scale=1.0\" >\\n  <!-- Import TensorFlow.js -->\\n  <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.\\njs\"></script>\\n  <!-- Import tfjs-vis -->', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c10dab8b-929e-46e5-b0c0-22b18a7e9d16', embedding=None, metadata={'page_label': '602', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 602\\n  <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.0.2/dist/\\ntfjs-vis.umd.min.js\" ></script>\\n  <!-- Import the data file -->\\n  <script src=\"data.js\"  type=\"module\" ></script>\\n  <!-- Import the main script file -->\\n  <script src=\"script.js\"  type=\"module\" ></script>\\n</head>\\n<body>\\n</body>\\n</html>\\nFor deployment, we will deploy these three files ( index.html , data.js , and script.js ) on a web \\nserver, but for development, we can start a web server up by calling a simple one bundled with the \\nPython distribution. This will start up a web server on port 8000  on localhost , and the index.html  \\nfile can be rendered on the browser at http://localhost:8000 :\\npython -m http.server\\nThe next step is to load the data. Fortunately, Google provides a JavaScript script that we have called \\ndirectly from our index.html  file. It downloads the images and labels from GCP storage and returns \\nshuffled and normalized batches of image and label pairs for training and testing. We can download \\nthis to the same folder as the index.html  file using the following command:\\nwget -cO - https://storage.googleapis.com/tfjs-tutorials/mnist_data.js > data.\\njs\\nModel definition, training, and evaluation code is all specified inside the script.js  file. The function \\nto define and build the network is shown in the following code block. As you can see, it is very similar \\nto the way you would build a sequential model with tf.keras . The only difference is the way you \\nspecify the arguments, as a dictionary of name-value pairs instead of a list of parameters. The model \\nis a sequential model, that is, a list of layers. Finally, the model is compiled with the Adam optimizer:\\nfunction getModel() {\\n  const IMAGE_WIDTH = 28;\\n  const IMAGE_HEIGHT = 28;\\n  const IMAGE_CHANNELS = 1;  \\n  const NUM_OUTPUT_CLASSES = 10;\\n  \\n  const model = tf.sequential();\\n  model.add(tf.layers.conv2d({For Windows users, you will need to first download Wget: https://eternallybored.\\norg/misc/wget/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8ca26f6-3b90-43cb-8335-3bc2fbf0fd07', embedding=None, metadata={'page_label': '603', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 19 603\\n    inputShape: [IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS],\\n    kernelSize: 5,\\n    filters: 8,\\n    strides: 1,\\n    activation: 'relu',\\n    kernelInitializer: 'varianceScaling'\\n  }));\\n  model.add(tf.layers.maxPooling2d({\\n    poolSize: [ 2, 2], strides: [ 2, 2]\\n  }));\\n  model.add(tf.layers.conv2d({\\n    kernelSize: 5,\\n    filters: 16,\\n    strides: 1,\\n    activation: 'relu',\\n    kernelInitializer: 'varianceScaling'\\n  }));\\n  model.add(tf.layers.maxPooling2d({\\n    poolSize: [ 2, 2], strides: [ 2, 2]\\n  }));\\n  model.add(tf.layers.flatten());\\n  model.add(tf.layers.dense({\\n    units: NUM_OUTPUT_CLASSES,\\n    kernelInitializer: 'varianceScaling' ,\\n    activation: 'softmax'\\n  }));\\n  const optimizer = tf.train.adam();\\n  model. compile({\\n    optimizer: optimizer,\\n    loss: 'categoricalCrossentropy' ,\\n    metrics: [ 'accuracy' ],\\n  });\\n  return  model;\\n}\\nThe model is then trained for 10 epochs with batches from the training dataset and validated inline \\nusing batches from the test dataset. A best practice is to create a separate validation dataset from \\nthe training set. However, to keep our focus on the more important aspect of showing how to use \\nTensorFlow.js to design an end-to-end DL pipeline, we are using the external data.js  file provided \\nby Google, which provides functions to return only a training and test batch. In our example, we will \\nuse the test dataset for validation as well as evaluation later. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a033eae-0854-454d-b7e2-9b1083c40922', embedding=None, metadata={'page_label': '604', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"TensorFlow 2 Ecosystem 604\\nThis is likely to give us better accuracies compared to what we would have achieved with an unseen \\n(during training) test set, but that is unimportant for an illustrative example such as this one:\\nasync function train(model, data) {\\n  const metrics = [ 'loss', 'val_loss' , 'acc', 'val_acc' ];\\n  const container = {\\n    name: 'Model Training' , tab: 'Model', styles: { height: '1000px'  }\\n  };\\n  const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);\\n  \\n  const BATCH_SIZE = 512;\\n  const TRAIN_DATA_SIZE = 5500;\\n  const TEST_DATA_SIZE = 1000;\\n  const [trainXs, trainYs] = tf.tidy(() => {\\n    const d = data.nextTrainBatch(TRAIN_DATA_SIZE);\\n    return [\\n      d.xs.reshape([TRAIN_DATA_SIZE, 28, 28, 1]),\\n      d.labels\\n    ];\\n  });\\n  const [testXs, testYs] = tf.tidy(() => {\\n    const d = data.nextTestBatch(TEST_DATA_SIZE);\\n    return [\\n      d.xs.reshape([TEST_DATA_SIZE, 28, 28, 1]),\\n      d.labels\\n    ];\\n  });\\n  return  model.fit(trainXs, trainYs, {\\n    batchSize: BATCH_SIZE,\\n    validationData: [testXs, testYs],\\n    epochs: 10,\\n    shuffle: true,\\n    callbacks: fitCallbacks\\n  });\\n}\\nOnce the model finishes training, we want to make predictions and evaluate the model on its predictions. \\nThe following functions will do the predictions and compute the overall accuracy for each of the classes \\nover all the test set examples, as well as produce a confusion matrix across all the test set samples:\\nconst classNames = [\\n  'Zero' , 'One', 'Two', 'Three' , 'Four' , \\n  'Five' , 'Six', 'Seven' , 'Eight' , 'Nine' ];\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d57644be-76d7-4f20-9e0c-e4a9f29d0c57', embedding=None, metadata={'page_label': '605', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 19 605\\nfunction doPrediction(model, data, testDataSize = 500) {\\n  const IMAGE_WIDTH = 28;\\n  const IMAGE_HEIGHT = 28;\\n  const testData = data.nextTestBatch(testDataSize);\\n  const testxs = testData.xs.reshape(\\n    [testDataSize, IMAGE_WIDTH, IMAGE_HEIGHT, 1]);\\n  const labels = testData.labels.argMax([- 1]);\\n  const preds = model.predict(testxs).argMax([- 1]);\\n  testxs.dispose();\\n  return  [preds, labels];\\n}\\nasync function showAccuracy(model, data) {\\n  const [preds, labels] = doPrediction(model, data);\\n  const classAccuracy = await tfvis.metrics.perClassAccuracy(\\n    labels, preds);\\n  const container = {name: 'Accuracy' , tab: 'Evaluation' };\\n  tfvis.show.perClassAccuracy(container, classAccuracy, classNames);\\n  labels.dispose();\\n}\\nasync function showConfusion(model, data) {\\n  const [preds, labels] = doPrediction(model, data);\\n  const confusionMatrix = await tfvis.metrics.confusionMatrix(\\n    labels, preds);\\n  const container = {name: 'Confusion Matrix' , tab: 'Evaluation' };\\n  tfvis.render.confusionMatrix(\\n      container, {values: confusionMatrix}, classNames);\\n  labels.dispose();\\n}\\nFinally, the run()  function will call all these functions in sequence to build an end-to-end ML pipeline:\\nimport {MnistData} from './data.js' ;\\nasync function run() { \\n  const data = new MnistData();\\n  await data.load();\\n  await showExamples(data);\\n  const model = getModel();\\n  tfvis.show.modelSummary({name: 'Model Architecture' , tab: 'Model'}, model);\\n  await train(model, data);\\n  await showAccuracy(model, data);\\n  await showConfusion(model, data);\\n}\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0392f94b-19fc-4731-b8b7-3e21de4cd4a1', embedding=None, metadata={'page_label': '606', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"TensorFlow 2 Ecosystem 606\\ndocument.addEventListener( 'DOMContentLoaded' , run);\\nRefreshing the browser location, http://localhost:8000/index.html , will invoke the run()  method \\nabove. Figure 19.17 shows the model architecture and the plots of the progress of the training.\\nOn the left are the loss and accuracy values on the validation dataset observed at the end of each batch, \\nand on the right are the same loss and accuracy values observed on the training dataset (blue) and \\nvalidation dataset (red) at the end of each epoch:\\nFigure 19.17: Model loss and accuracy as it is being trained\\nIn addition, the following figure shows the accuracies across different classes for predictions from our \\ntrained model on the test dataset, as well as the confusion matrix of predicted versus actual classes \\nfor test dataset samples:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d402ae0b-91f9-4fe4-949e-8604a0004a8f', embedding=None, metadata={'page_label': '607', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 607\\nFigure 19.18: Confusion metrics and accuracy for each class as obtained by the trained model\\nReaders might enjoy seeing this live example from the TensorFlow team training a TFJS model on the \\nMNIST dataset: https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html .\\nWe have seen how to use TensorFlow.js within the browser. The next section will explain how to \\nconvert a model from Keras into TensorFlow.js.\\nConverting models\\nSometimes it is convenient to convert a model that has already been created with tf.keras . This is \\nvery easy and can be done offline with the following command, which takes a Keras model from /\\ntmp/model.h5  and outputs a JavaScript model into /tmp/tfjs_model :\\ntensorflowjs_converter --input_format=keras /tmp/model.h5 /tmp/tfjs_model\\nTo be able to use this command, you will need a Python environment with TensorFlow JS installed using:\\npip install tensorflowjs\\nThis will install the above converter. The next section will explain how to use pretrained models in \\nTensorFlow.js.\\nPretrained models\\nTensorFlow.js comes with a significant number of pretrained models for deep learning with image, \\nvideo, and text. The models are hosted on npm, so it’s very simple to use them if you are familiar with \\nNode.js development.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b08841f4-d779-4afe-963e-38cd4b5a6db1', embedding=None, metadata={'page_label': '608', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 608\\nTable 19.1 summarizes some of the pretrained models available as of August 2022 (source: https://\\ngithub.com/tensorflow/tfjs-models ):\\nImages\\nModel Details Install\\nMobileNet (https://github.\\ncom/tensorflow/tfjs-\\nmodels/tree/master/\\nmobilenet )Classify images with labels from the \\nImageNet database.npm i @tensorflow-\\nmodels/mobilenet\\nPoseNet (https://github.\\ncom/tensorflow/tfjs-\\nmodels/tree/master/\\nposenet )A machine learning model that allows \\nfor real-time human pose estimation in \\nthe browser; see a detailed description \\nhere: https://medium.com/tensorflow/\\nreal-time-human-pose-estimation-\\nin-the-browser-with-tensorflow-js-\\n7dd0bc881cd5 .npm i @tensorflow-\\nmodels/posenet\\nCoco SSD (https://github.\\ncom/tensorflow/tfjs-\\nmodels/tree/master/coco-\\nssd)Object detection model that aims to \\nlocalize and identify multiple objects in \\na single image; based on the TensorFlow \\nobject detection API (https://github.\\ncom/tensorflow/models/blob/master/\\nresearch/object_detection/README.md ).npm i @tensorflow-\\nmodels/coco-ssd\\nBodyPix (https://github.\\ncom/tensorflow/tfjs-\\nmodels/tree/master/body-\\npix)Real-time person and body-part \\nsegmentation in the browser using \\nTensorFlow.js.npm i @tensorflow-\\nmodels/body-pix\\nDeepLab v3(https://\\ngithub.com/tensorflow/\\ntfjs-models/tree/master/\\ndeeplab )Semantic segmentation.npm i @tensorflow-\\nmodels/deeplab\\nAudio\\nModel Details Install\\nSpeech Commands (https://\\ngithub.com/tensorflow/\\ntfjs-models/tree/master/\\nspeech-commands )Classify 1-second audio snippets from the \\nspeech commands dataset (https://github.\\ncom/tensorflow/docs/blob/master/\\nsite/en/r1/tutorials/sequences/audio_\\nrecognition.md ).npm i @tensorflow-\\nmodels/speech-\\ncommands', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='be9110e5-5d1d-4587-aeff-43ecd74b8dd6', embedding=None, metadata={'page_label': '609', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 609\\nText\\nModel Details Install\\nUniversal Sentence Encoder \\n(https://github.com/\\ntensorflow/tfjs-models/\\ntree/master/universal-\\nsentence-encoder )Encode text into a 512-dimensional \\nembedding to be used as inputs to natural \\nlanguage processing tasks such as sentiment \\nclassification and textual similarity.npm i @tensorflow-\\nmodels/universal-\\nsentence-encoder\\nText Toxicity (https://\\ngithub.com/tensorflow/\\ntfjs-models/tree/master/\\ntoxicity )Score the perceived impact a comment might \\nhave on a conversation, from “Very toxic” to \\n“Very healthy”.npm i @tensorflow-\\nmodels/toxicity\\nGeneral Utilities\\nModel Details Install\\nKNN Classifier (https://\\ngithub.com/tensorflow/\\ntfjs-models/tree/master/\\nknn-classifier )This package provides a utility for creating \\na classifier using the K-nearest neighbors \\nalgorithm; it can be used for transfer \\nlearning.npm i @tensorflow-\\nmodels/knn-\\nclassifier\\nTable 19.1: A list of some of the pretrained models on TensorFlow.js\\nEach pretrained model can be directly used from HTML. For instance, this is an example with the \\nKNN classifier:\\n<html>\\n  <head>\\n    <!-- Load TensorFlow.js -->\\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\" ></script>\\n    <!-- Load MobileNet -->\\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet\" ></\\nscript>\\n    <!-- Load KNN Classifier -->\\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/knn-\\nclassifier\" ></script>\\n  </head>\\nThe next section will explain how to use pretrained models in Node.js.\\nNode.js\\nIn this section, we will give an overview of how to use TensorFlow with Node.js. Let’s start.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72b88dcd-ac78-4a3f-b242-d870d9321e4e', embedding=None, metadata={'page_label': '610', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"TensorFlow 2 Ecosystem 610\\nThe CPU package is imported with the following line of code, which will work for all macOS, Linux, \\nand Windows platforms:\\nimport * as tf from '@tensorflow/tfjs-node'\\nThe GPU package is imported with the following line of code (as of November 2019, this will work \\nonly on a GPU in a CUDA environment):\\nimport * as tf from '@tensorflow/tfjs-node-gpu'\\nAn example of Node.js code for defining and compiling a simple dense model is reported below. The \\ncode is self-explanatory:\\nconst model = tf.sequential();\\nmodel.add(tf.layers.dense({ units: 1, inputShape: [ 400] }));\\nmodel.compile({\\n  loss: 'meanSquaredError' ,\\n  optimizer: 'sgd',\\n  metrics: [ 'MAE']\\n});\\nTraining can then start with the typical Node.js asynchronous invocation:\\nconst xs = tf.randomUniform([ 10000, 400]);\\nconst ys = tf.randomUniform([ 10000, 1]);\\nconst valXs = tf.randomUniform([ 1000, 400]);\\nconst valYs = tf.randomUniform([ 1000, 1]);\\nasync function train() {\\n  await model.fit(xs, ys, {\\n    epochs: 100,\\n    validationData: [valXs, valYs],\\n  });\\n}\\ntrain();\\nIn this section, we have discussed how to use TensorFlow.js with both vanilla JavaScript and Node.js \\nusing sample applications for both the browser and backend computation.\\nSummary\\nIn this chapter, we have discussed different components of the TensorFlow ecosystem. We started \\nwith TensorFlow Hub, the place where many pretrained models are available. Next, we talked about \\nthe TensorFlow Datasets and learned how to build a data pipeline using TFDS. We learned how to \\nuse TensorFlow Lite for mobile devices and IoT and deployed real applications on Android devices. \\nThen, we also talked about federated learning for distributed learning across thousands (millions) of \\nmobile devices, taking into account privacy concerns. The last section of the chapter was devoted to \\nTensorFlow.js for using TensorFlow with vanilla JavaScript or with Node.js.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97da9f69-4f28-4cff-adeb-91decd6f8563', embedding=None, metadata={'page_label': '611', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 19 611\\nThe next chapter is about advanced CNNs, where you will learn some advanced CNN architectures \\nand their applications.\\nReferences\\n1. Quantization-aware training: https://github.com/tensorflow/tensorflow/tree/r1.13/\\ntensorflow/contrib/quantize\\n2. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. \\n(Submitted on 15 Dec 2017). Quantization and Training of Neural Networks for Efficient Integer-\\nArithmetic-Only Inference. https://arxiv.org/abs/1712.05877\\n3. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L-C. (Submitted on 13 Jan 2018 (v1), \\nlast revised 21 Mar 2019 (v4)). MobileNetV2: Inverted Residuals and Linear Bottlenecks. https://\\narxiv.org/abs/1806.08342\\n4. Tan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M., Howard, A., and Le, Q. V . MnasNet: \\nPlatform-Aware Neural Architecture Search for Mobile. https://arxiv.org/abs/1807.11626\\n5. Chen, L-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. (May 2017). DeepLab: \\nSemantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected \\nCRFs. https://arxiv.org/pdf/1606.00915.pdf\\n6. Devlin, J., Chang, M-W ., Lee, K., and Toutanova, K. (Submitted on 11 Oct 2018 (v1), last revised \\n24 May 2019 v2). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. \\nhttps://arxiv.org/abs/1810.04805\\n7. Anonymous authors, Paper under double-blind review. (modified: 25 Sep 2019). MOBILEBERT: \\nTASK-AGNOSTIC COMPRESSION OF BERT BY PROGRESSIVE KNOWLEDGE TRANSFER. \\nICLR 2020 Conference Blind Submission Readers: Everyone. https://openreview.net/\\npdf?id=SJxjVaNKwB  \\n8. McMahan, H. B., Moore, E., Ramage, D., Hampson, and S., Arcas, B. A. y. (Submitted on 17 Feb \\n2016 (v1), last revised 28 Feb 2017 (this version, v3)). Communication-Efficient Learning of Deep \\nNetworks from Decentralized Data. https://arxiv.org/abs/1602.05629\\n9. Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. (Submitted \\non 18 Oct 2016 (v1), last revised 30 Oct 2017 (this version, v2)). Federated Learning: Strategies for \\nImproving Communication Efficiency. https://arxiv.org/abs/1610.05492\\n10. Bonawitz, K. et al. (22 March 2019). TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM \\nDESIGN . https://arxiv.org/pdf/1902.01046.pdf', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eaf3a55e-5c04-4413-9f95-77a59ad54f14', embedding=None, metadata={'page_label': '612', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TensorFlow 2 Ecosystem 612\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c54c9465-4b30-4b12-a6d0-f1373bbeb771', embedding=None, metadata={'page_label': '613', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20\\nAdvanced Convolutional Neural \\nNetworks\\nIn this chapter, we will see some more advanced uses for Convolutional Neural Networks (CNNs). We \\nwill explore:\\n• How CNNs can be applied within the areas of computer vision, video, textual documents, \\naudio, and music\\n• How to use CNNs for text processing\\n• What capsule networks are\\n• Computer vision\\nLet’s start by using CNNs for complex tasks.\\nComposing CNNs for complex tasks\\nWe have discussed CNNs quite extensively in Chapter 3, Convolutional Neural Networks, and at this point, \\nyou are probably convinced about the effectiveness of the CNN architecture for image classification \\ntasks. What you may find surprising, however, is that the basic CNN architecture can be composed \\nand extended in various ways to solve a variety of more complex tasks. In this section, we will look \\nat the computer vision tasks mentioned in Figure 20.1 and show how they can be solved by turning \\nCNNs into larger and more complex architectures.All the code files for this chapter can be found at https://packt.link/dltfchp20 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5ada98f-d429-4f02-b4c4-7d446b86ad1e', embedding=None, metadata={'page_label': '614', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 614\\nFigure 20.1: Different Computer Vision Tasks – source: Introduction to Artificial Intelligence and \\nComputer Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-\\nintelligence-and-computer-vision-revolution) \\nClassification and localization\\nIn the classification and localization task, not only do you have to report the class of object found in \\nthe image, but also the coordinates of the bounding box where the object appears in the image. This \\ntype of task assumes that there is only one instance of the object in an image.\\nThis can be achieved by attaching a “regression head” in addition to the “classification head” in a \\ntypical classification network. Recall that in a classification network, the final output of convolution \\nand pooling operations, called the feature map, is fed into a fully connected network that produces \\na vector of class probabilities. This fully connected network is called the classification head, and it is \\ntuned using a categorical loss function (L c) such as categorical cross-entropy.\\nSimilarly, a regression head is another fully connected network that takes the feature map and produces \\na vector (x , y, w, h) representing the top left x  and y  coordinates, and the width and height of the \\nbounding box. It is tuned using a continuous loss function (L R) such as mean squared error. The entire \\nnetwork is tuned using a linear combination of the two losses, i.e.,\\n𝐿𝐿 𝐿𝐿𝐿𝐿𝐿 𝑐𝑐+(1−𝐿𝐿 𝛼𝐿𝐿𝑟𝑟 \\nHere, 𝛼𝛼  is a hyperparameter and can take a value between 0 and 1. Unless the value is determined by \\nsome domain knowledge about the problem, it can be set to 0.5.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='194392f1-6417-4b75-ba66-c7e81e1b0c78', embedding=None, metadata={'page_label': '615', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 615\\nFigure 20.2 shows a typical classification and localization network architecture:\\nFigure 20.2: Network architecture for image classification and localization\\nAs you can see, the only difference with respect to a typical CNN classification network is the additional \\nregression head at the top right-hand side.\\nSemantic segmentation\\nAnother class of problem that builds on the basic classification idea is “semantic segmentation.” Here \\nthe aim is to classify every single pixel on the image as belonging to a single class.\\nAn initial method of implementation could be to build a classifier network for each pixel, where the \\ninput is a small neighborhood around each pixel. In practice, this approach is not very performant, so \\nan improvement over this implementation might be to run the image through convolutions that will \\nincrease the feature depth, while keeping the image width and height constant. Each pixel then has \\na feature map that can be sent through a fully connected network that predicts the class of the pixel. \\nHowever, in practice, this is also quite expensive and is not normally used.\\nA third approach is to use a CNN encoder-decoder network, where the encoder decreases the \\nwidth and height of the image but increases its depth (number of features), while the decoder uses \\ntransposed convolution operations to increase its size and decrease its depth. Transposed convolution \\n(or upsampling) is the process of going in the opposite direction of a normal convolution. Input to \\nthis network is the image and the output is the segmentation map. A popular implementation of this \\nencoder-decoder architecture is the U-Net (a good implementation is available at https://github.\\ncom/jakeret/tf_unet ), originally developed for biomedical image segmentation, which has additional \\nskip connections between corresponding layers of the encoder and decoder. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bbed26c-73aa-437b-99de-95da2c835ea8', embedding=None, metadata={'page_label': '616', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 616\\nFigure 20.3 shows the U-Net architecture:\\nFigure 20.3: U-Net architecture\\nObject detection\\nThe object detection task is similar to the classification and localization task. The big difference is that \\nnow there are multiple objects in the image, and for each one of them, we need to find the class and \\nthe bounding box coordinates. In addition, neither the number of objects nor their size is known in \\nadvance. As you can imagine, this is a difficult problem, and a fair amount of research has gone into it.\\nA first approach to the problem might be to create many random crops of the input image and, for \\neach crop, apply the classification and localization network we described earlier. However, such an \\napproach is very wasteful in terms of computing and unlikely to be very successful.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94d789de-60a2-41e9-8bf0-cd149fb093f9', embedding=None, metadata={'page_label': '617', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 617\\nA more practical approach would be to use a tool such as Selective Search (Selective Search for Object \\nRecognition, by Uijlings et al., http://www.huppelen.nl/publications/selectiveSearchDraft.pdf ), \\nwhich uses traditional computer vision techniques to find areas in the image that might contain \\nobjects. These regions are called “region proposals,” and the network to detect them is called Region-\\nbased CNN, or R-CNN. In the  original R-CNN, the regions were resized and fed into a network to \\nyield image vectors. These vectors were then classified with an SVM-based classifier (see https://\\nen.wikipedia.org/wiki/Support-vector_machine ), and the bounding boxes proposed by the external \\ntool were corrected using a linear regression network over the image vectors. An R-CNN network can \\nbe represented conceptually as shown in Figure 20.4:\\nFigure 20.4: R-CNN network\\nThe next iteration of the R-CNN network is called the Fast R-CNN. The Fast R-CNN still gets its region \\nproposals from an external tool, but instead of feeding each region proposal through the CNN, the \\nentire image is fed through the CNN and the region proposals are projected onto the resulting feature \\nmap. Each region of interest is fed through a Region Of Interest (ROI ) pooling layer and then to a fully \\nconnected network, which produces a feature vector for the ROI.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b1e6d3e7-11cf-4d65-b794-f3d1854958d9', embedding=None, metadata={'page_label': '618', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 618\\nROI pooling is a widely used operation in object detection tasks using CNNs. The ROI pooling layer \\nuses max pooling to convert the features inside any valid region of interest into a small feature map \\nwith a fixed spatial extent of H x W (where H and W are two hyperparameters). The feature vector is \\nthen fed into two fully connected networks, one to predict the class of the ROI and the other to correct \\nthe bounding box coordinates for the proposal. This is illustrated in Figure 20.5:\\nFigure 20.5: Fast R-CNN network architecture\\nThe Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called the Faster R-CNN \\n(an implementation is at https://github.com/tensorpack/tensorpack/tree/master/examples/\\nFasterRCNN ), removes the external region proposal mechanism and replaces it with a trainable \\ncomponent, called the Region Proposal Network (RPN ), within the network itself. The output of this \\nnetwork is combined with the feature map and passed in through a similar pipeline to the Fast R-CNN \\nnetwork, as shown in Figure 20.6. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a31706f-02b2-46a1-887e-bd1a7fd8c55b', embedding=None, metadata={'page_label': '619', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 619\\nThe Faster R-CNN network is about 10x faster than the Fast R-CNN network, making it approximately \\n250x faster than an R-CNN network:\\nFigure 20.6: Faster R-CNN network architecture\\nAnother somewhat different class of object detection networks are Single Shot Detectors (SSD ) such \\nas YOLO  (You Only Look Once). In these cases, each image is split into a predefined number of parts \\nusing a grid. In the case of YOLO, a 7 x 7 grid is used, resulting in 49 sub-images. A predetermined \\nset of crops with different aspect ratios are applied to each sub-image. Given B bounding boxes and C \\nobject classes, the output for each image is a vector of size (7∗7∗(5𝐵𝐵 𝐵𝐵𝐵𝐵𝐵 . Each bounding box has \\na confidence and coordinates (x, y, w, h), and each grid has prediction probabilities for the different \\nobjects detected within them.\\nThe YOLO network is a CNN, which does this transformation. The final predictions and bounding \\nboxes are found by aggregating the findings from this vector. In YOLO, a single convolutional network \\npredicts the bounding boxes and the related class probabilities. YOLO is the faster solution for object \\ndetection. An implementation is at https://www.kaggle.com/aruchomu/yolo-v3-object-detection-\\nin-tensorflow .\\nInstance segmentation\\nInstance segmentation is similar to semantic segmentation – the process of associating each pixel of \\nan image with a class label – with a few important distinctions. First, it needs to distinguish between \\ndifferent instances of the same class in an image. Second, it is not required to label every single pixel \\nin the image. In some respects, instance segmentation is also similar to object detection, except that \\ninstead of bounding boxes, we want to find a binary mask that covers each object.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0538da37-952a-4353-88b5-57b63b86e0a9', embedding=None, metadata={'page_label': '620', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 620\\nThe second definition leads to the intuition behind the Mask R-CNN network. The Mask R-CNN is a \\nFaster R-CNN with an additional CNN in front of its regression head, which takes as input the bounding \\nbox coordinates reported for each ROI and converts it to a binary mask [11]:\\nFigure 20.7: Mask R-CNN architecture\\nIn April 2019, Google released Mask R-CNN in open source, pretrained with TPUs. This is available at \\nhttps://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/\\nmask_rcnn/mask_rcnn_demo.ipynb .\\nI suggest playing with the Colab notebook to see what the results are. In Figure 20.8 , we see an example \\nof image segmentation:\\nFigure 20.8: An example of image segmentation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d91d8af2-573e-48d6-819c-e9fa53b55cc4', embedding=None, metadata={'page_label': '621', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 621\\nGoogle also released another model trained on TPUs called DeepLab, and you can see an image ( Figure \\n20.9) from the demo. This is available at\\nhttps://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/\\ndeeplab_demo.ipynb#scrollTo=edGukUHXyymr :\\nFigure 20.9: An example of image segmentation\\nIn this section, we have covered, at a somewhat high level, various network architectures that are \\npopular in computer vision. Note that all of them are composed by the same basic CNN and fully \\nconnected architectures. This composability is one of the most powerful features of deep learning. \\nHopefully, this has given you some ideas for networks that could be adapted for your own computer \\nvision use cases.\\nApplication zoos with tf.Keras and TensorFlow Hub\\nOne of the nice things about transfer learning is that it is possible to reuse pretrained networks to save \\ntime and resources. There are many collections of ready-to-use networks out there, but the following \\ntwo are the most used.\\nKeras Applications\\nKeras Applications (Keras Applications are available at https://www.tensorflow.org/api_docs/\\npython/tf/keras/applications ) includes models for image classification with weights trained on \\nImageNet (Xception, VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2, \\nMobileNet, MobileNetV2, DenseNet, and NASNet). In addition, there are a few other reference \\nimplementations from the community for object detection and segmentation, sequence learning, \\nreinforcement learning (see Chapter 11), and GANs (see Chapter 9).\\nTensorFlow Hub\\nTensorFlow Hub (available at https://www.tensorflow.org/hub ) is an alternative collection of  \\npretrained models. TensorFlow Hub includes modules for text classification, sentence encoding (see \\nChapter 4), image classification, feature extraction, image generation with GANs, and video classification. \\nCurrently, both Google and DeepMind contribute to TensorFlow Hub.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1439614c-a1e6-47e6-ab99-24e26d00f161', embedding=None, metadata={'page_label': '622', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 622\\nLet’s look at an example of using TF.Hub . In this case, we have a simple image classifier using \\nMobileNetv2:\\nimport matplotlib.pylab as plt\\nimport tensorflow as tf\\nimport tensorflow_hub as hub\\nimport numpy as np\\nimport PIL.Image as Image\\nclassifier_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/\\nclassification/2\"  #@param {type:\"string\"}\\nIMAGE_SHAPE = ( 224, 224)\\n# wrap the hub to work with tf.keras\\nclassifier = tf.keras.Sequential([\\n    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+( 3,))\\n])\\ngrace_hopper = tf.keras.utils.get_file( \\'image.jpg\\' ,\\'https://storage.googleapis.\\ncom/download.tensorflow.org/example_images/grace_hopper.jpg\\' )\\ngrace_hopper = Image. open(grace_hopper).resize(IMAGE_SHAPE)\\ngrace_hopper = np.array(grace_hopper)/ 255.0\\nresult = classifier.predict(grace_hopper[np.newaxis, ...])\\npredicted_class = np.argmax(result[ 0], axis=- 1)\\nprint (predicted_class)\\nPretty simple indeed. Just remember to use hub.KerasLayer()  for wrapping any Hub layer. In this \\nsection, we have discussed how to use TensorFlow Hub.\\nNext, we will focus on other CNN architectures.\\nAnswering questions about images (visual Q&A)\\nOne of the nice things about neural networks is that different media types can be combined together \\nto provide a unified interpretation. For instance, Visual Question Answering (VQA ) combines image \\nrecognition and text natural language processing. Training can use VQA (VQA is available at https://\\nvisualqa.org/ ), a dataset containing open-ended questions about images. These questions require \\nan understanding of vision, language, and common knowledge to be answered. The following images \\nare taken from a demo available at https://visualqa.org/ .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf30fc69-e78e-4856-817e-3e8db47ef59c', embedding=None, metadata={'page_label': '623', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 623\\nNote the question at the top of the image, and the subsequent answers:\\nFigure 20.10: Examples of visual question and answers ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='553c6531-3cb6-4e22-ad3d-0e61cc090352', embedding=None, metadata={'page_label': '624', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Convolutional Neural Networks 624\\nIf you want to start playing with VQA, the first thing is to get appropriate training datasets such as the \\nVQA dataset, the CLEVR dataset (available at https://cs.stanford.edu/people/jcjohns/clevr/ ), or \\nthe FigureQA dataset (available at https://datasets.maluuba.com/FigureQA ); alternatively, you can \\nparticipate in a Kaggle VQA challenge (available at https://www.kaggle.com/c/visual-question-\\nanswering ). Then you can build a model that is the combination of a CNN and an RNN and start \\nexperimenting. For instance, a CNN can be something like this code fragment, which takes an image \\nwith three channels (224 x 224) as input and produces a feature vector for the image:\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\n# IMAGE\\n#\\n# Define CNN for visual processing\\ncnn_model = models.Sequential()\\ncnn_model.add(layers.Conv2D( 64, (3, 3), activation= 'relu', padding= 'same', \\n        input_shape=( 224, 224, 3)))\\ncnn_model.add(layers.Conv2D( 64, (3, 3), activation= 'relu'))\\ncnn_model.add(layers.MaxPooling2D( 2, 2))\\ncnn_model.add(layers.Conv2D( 128, (3, 3), activation= 'relu', padding= 'same'))\\ncnn_model.add(layers.Conv2D( 128, (3, 3), activation= 'relu'))\\ncnn_model.add(layers.MaxPooling2D( 2, 2))\\ncnn_model.add(layers.Conv2D( 256, (3, 3), activation= 'relu', padding= 'same'))\\ncnn_model.add(layers.Conv2D( 256, (3, 3), activation= 'relu'))\\ncnn_model.add(layers.Conv2D( 256, (3, 3), activation= 'relu'))\\ncnn_model.add(layers.MaxPooling2D( 2, 2))\\ncnn_model.add(layers.Flatten())\\ncnn_model.summary()\\n#define the visual_model with proper input\\nimage_input = layers.Input(shape=( 224, 224, 3))\\nvisual_model = cnn_model(image_input)\\nText can be encoded with an RNN; for now, think of it as a black box taking a text fragment (the \\nquestion) in input and producing a feature vector for the text:\\n# TEXT\\n#\\n#define the RNN model for text processing\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='36f80715-a825-48e3-aec5-4adddb997675', embedding=None, metadata={'page_label': '625', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 20 625\\nquestion_input = layers.Input(shape=( 100,), dtype= 'int32')\\nemdedding = layers.Embedding(input_dim= 10000, output_dim= 256, \\n    input_length= 100)(question_input)\\nencoded_question = layers.LSTM( 256)(emdedding)\\nThen the two feature vectors (one for the image, and one for the text) are combined into one joint \\nvector, which is provided as input to a dense network to produce the combined network:\\n# combine the encoded question and visual model\\nmerged = layers.concatenate([encoded_question, visual_model])\\n#attach a dense network at the end\\noutput = layers.Dense( 1000, activation= 'softmax' )(merged)\\n#get the combined model\\nvqa_model = models.Model(inputs=[image_input, question_input], outputs=output)\\nvqa_model.summary()\\nFor instance, if we have a set of labeled images, then we can learn what the best questions and answers \\nare for describing an image. The number of options is enormous! If you want to know more, I suggest \\nthat you investigate Maluuba, a start-up providing the FigureQA dataset with 100,000 figure images and \\n1,327,368 question-answer pairs in the training set. Maluuba has been recently acquired by Microsoft, \\nand the lab is advised by Yoshua Bengio, one of the fathers of deep learning.\\nIn this section, we have discussed how to implement visual Q&A. The next section is about style transfer, \\na deep learning technique used for training neural networks to create art.\\nCreating a DeepDream network\\nAnother interesting application of CNNs is DeepDream, a computer vision program created by Google \\n[8] that uses a CNN to find and enhance patterns in images. The result is a dream-like hallucinogenic \\neffect. Similar to the previous example, we are going to use a pretrained network to extract features. \\nHowever, in this case, we want to “enhance” patterns in images, meaning that we need to maximize \\nsome functions. This tells us that we need to use a gradient ascent and not a descent. First, let’s see an \\nexample from Google gallery (available at https://colab.research.google.com/github/tensorflow/\\ndocs/blob/master/site/en/tutorials/generative/deepdream.ipynb ) where the classic Seattle \\nlandscape is “incepted” with hallucinogenic dreams such as birds, cards, and strange flying objects. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4b13ea3-a140-4dd8-8b2f-f431c05dfb99', embedding=None, metadata={'page_label': '626', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 626\\nGoogle released the DeepDream code as open source (available at https://github.com/google/\\ndeepdream ), but we will use a simplified example made by a random forest (available at https://www.\\ntensorflow.org/tutorials/generative/deepdream ):\\nFigure 20.11: DeepDreaming Seattle\\nLet’s start with some image preprocessing:\\n# Download an image and read it into a NumPy array, \\ndef download (url):\\n  name = url.split( \"/\")[-1]\\n  image_path = tf.keras.utils.get_file(name, origin=url)\\n  img = image.load_img(image_path)\\n  return  image.img_to_array(img)\\n# Scale pixels to between (-1.0 and 1.0)\\ndef preprocess (img):\\n  return  (img / 127.5) - 1\\n  \\n# Undo the preprocessing above\\ndef deprocess (img):\\n  img = img.copy()\\n  img /= 2.\\n  img += 0.5\\n  img *= 255.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='837c08bd-4880-4dd0-9b4f-a16cb492d54f', embedding=None, metadata={'page_label': '627', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 20 627\\n  return  np.clip(img, 0, 255).astype( 'uint8')\\n# Display an image\\ndef show(img):\\n  plt.figure(figsize=( 12,12))\\n  plt.grid( False)\\n  plt.axis( 'off')\\n  plt.imshow(img)\\n# https://commons.wikimedia.org/wiki/File:Flickr_-_Nicholas_T_-_Big_Sky_(1).jpg\\nurl = 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flickr_-_\\nNicholas_T_-_Big_Sky_%281%29.jpg/747px-Flickr_-_Nicholas_T_-_Big_Sky_%281%29.\\njpg'\\nimg = preprocess(download(url))\\nshow(deprocess(img))\\nNow let’s use the Inception pretrained network to extract features. We use several layers, and the goal \\nis to maximize their activations. The tf.keras  functional API is our friend here:\\n# We'll maximize the activations of these layers\\nnames = [ 'mixed2' , 'mixed3' , 'mixed4' , 'mixed5' ]\\nlayers = [inception_v3.get_layer(name).output for name in names]\\n# Create our feature extraction model\\nfeat_extraction_model = tf.keras.Model(inputs=inception_v3. input, \\noutputs=layers)\\ndef forward (img):\\n  \\n  # Create a batch\\n  img_batch = tf.expand_dims(img, axis= 0)\\n  \\n  # Forward the image through Inception, extract activations\\n  # for the layers we selected above\\n  return  feat_extraction_model(img_batch)\\nThe loss function is the mean of all the activation layers considered, normalized by the number of \\nunits in the layer itself:\\ndef calc_loss (layer_activations):\\n  \\n  total_loss = 0\\n  \\n  for act in layer_activations:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37117277-36ba-4e22-9213-5781d5a459f1', embedding=None, metadata={'page_label': '628', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 628\\n    \\n    # In gradient ascent, we\\'ll want to maximize this value\\n    # so our image increasingly \"excites\" the layer\\n    loss = tf.math.reduce_mean(act)\\n    # Normalize by the number of units in the layer\\n    loss /= np.prod(act.shape)\\n    total_loss += loss\\n  return  total_loss\\nNow let’s run the gradient ascent:\\nimg = tf.Variable(img)\\nsteps = 400\\nfor step in range (steps):\\n  \\n  with tf.GradientTape() as tape:\\n    activations = forward(img)\\n    loss = calc_loss(activations)\\n    \\n  gradients = tape.gradient(loss, img)\\n  # Normalize the gradients\\n  gradients /= gradients.numpy().std() + 1e-8 \\n  \\n  # Update our image by directly adding the gradients\\n  img.assign_add(gradients)\\n  \\n  if step % 50 == 0:\\n    clear_output()\\n    print (\"Step %d, loss %f\"  % (step, loss))\\n    show(deprocess(img.numpy()))\\n    plt.show()\\n# Let\\'s see the result\\nclear_output()\\nshow(deprocess(img.numpy()))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9106c919-2c1b-4145-9b25-f7b65bf2ddd8', embedding=None, metadata={'page_label': '629', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 629\\nThis transforms the image on the left into the psychedelic image on the right:\\nFigure 20.12: DeepDreaming of a green field with clouds\\nInspecting what a network has learned\\nA particularly interesting research effort is being devoted to understand what neural networks \\nare actually learning in order to be able to recognize images so well. This is called neural network \\n“interpretability.” Activation atlases is a promising recent technique that aims to show the feature \\nvisualizations of averaged activation functions. In this way, activation atlases produce a global map \\nseen through the eyes of the network. Let’s look at a demo available at https://distill.pub/2019/\\nactivation-atlas/ :\\nFigure 20.13: Examples of inspections', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d442b47-9f84-44c9-93cb-8ec7c2bc450f', embedding=None, metadata={'page_label': '630', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 630\\nIn this image, an InceptionV1 network used for vision classification reveals many fully realized features, \\nsuch as electronics, screens, a Polaroid camera, buildings, food, animal ears, plants, and watery \\nbackgrounds. Note that grid cells are labeled with the classification they give the most support for. Grid \\ncells are also sized according to the number of activations that are averaged within. This representation \\nis very powerful because it allows us to inspect the different layers of a network and how the activation \\nfunctions fire in response to the input.\\nIn this section, we have seen many techniques to process images with CNNs. Next, we’ll move on to \\nvideo processing.\\nVideo\\nIn this section, we are going to discuss how to use CNNs with videos and the different techniques \\nthat we can use.\\nClassifying videos with pretrained nets in six different ways\\nClassifying videos is an area of active research because of the large amount of data needed for \\nprocessing this type of media. Memory requirements are frequently reaching the limits of modern \\nGPUs and a distributed form of training on multiple machines might be required. Researchers are \\ncurrently exploring different directions of investigation, with increasing levels of complexity from \\nthe first approach to the sixth, as described below. Let’s review them:\\n• The first approach consists of classifying one video frame at a time by considering each one \\nof them as a separate image processed with a 2D CNN. This approach simply reduces the \\nvideo classification problem to an image classification problem. Each video frame “emits” a \\nclassification output, and the video is classified by taking into account the more frequently \\nchosen category for each frame.\\n• The second approach consists of creating one single network where a 2D CNN is combined \\nwith an RNN (see Chapter 9, Generative Models). The idea is that the CNN will take into account \\nthe image components and the RNN will take into account the sequence information for each \\nvideo. This type of network can be very difficult to train because of the very high number of \\nparameters to optimize.\\n• The third approach is to use a 3D ConvNet, where 3D ConvNets are an extension of 2D ConvNets \\noperating on a 3D tensor (time, image width, and image height). This approach is another \\nnatural extension of image classification. Again, 3D ConvNets can be hard to train.\\n• The fourth approach is based on a clever idea: instead of using CNNs directly for classification, \\nthey can be used for storing offline features for each frame in the video. The idea is that feature \\nextraction can be made very efficient with transfer learning, as shown in a previous recipe. \\nAfter all the features are extracted, they can be passed as a set of inputs into an RNN, which \\nwill learn sequences across multiple frames and emit the final classification.\\n• The fifth approach is a simple variant of the fourth, where the final layer is an MLP instead \\nof an RNN. In certain situations, this approach can be simpler and less expensive in terms of \\ncomputational requirements.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9cbd4842-ff08-4489-a71c-0fadeba9619e', embedding=None, metadata={'page_label': '631', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 631\\n• The sixth approach is a variant of the fourth, where the phase of feature extraction is realized \\nwith a 3D CNN that extracts spatial and visual features. These features are then passed into \\neither an RNN or an MLP.\\nDeciding upon the best approach is strictly dependent on your specific application, and there is \\nno definitive answer. The first three approaches are generally more computationally expensive and \\nless clever, while the last three approaches are less expensive, and they frequently achieve better \\nperformance.\\nSo far, we have explored how CNNs can be used for image and video applications. In the next section, \\nwe will apply these ideas within a text-based context.\\nText documents\\nWhat do text and images have in common? At first glance, very little. However, if we represent a \\nsentence or a document as a matrix, then this matrix is not much different from an image matrix \\nwhere each cell is a pixel. So, the next question is, how can we represent a piece of text as a matrix? \\nWell, it is pretty simple: each row of a matrix is a vector that represents a basic unit for the text. Of \\ncourse, now we need to define what a basic unit is. A simple choice could be to say that the basic \\nunit is a character. Another choice would be to say that a basic unit is a word; yet another choice is \\nto aggregate similar words together and then denote each aggregation (sometimes called cluster or \\nembedding) with a representative symbol.\\nNote that regardless of the specific choice adopted for our basic units, we need to have a 1:1 mapping \\nfrom basic units into integer IDs so that the text can be seen as a matrix. For instance, if we have a \\ndocument with 10 lines of text and each line is a 100-dimensional embedding, then we will represent \\nour text with a matrix of 10 x 100. In this very particular “image,” a “pixel” is turned on if that sentence, \\nX, contains the embedding, represented by position Y. You might also notice that a text is not really a \\nmatrix but more a vector because two words located in adjacent rows of text have very little in common. \\nIndeed, this is a major difference when compared with images, where two pixels located in adjacent \\ncolumns are likely to have some degree of correlation.\\nNow you might wonder: I understand that we represent the text as a vector but, in doing so, we lose the \\nposition of the words. This position should be important, shouldn’t it? Well, it turns out that in many \\nreal applications, knowing whether a sentence contains a particular basic unit (a char, a word, or \\nan aggregate) or not is pretty useful information even if we don’t keep track of where exactly in the \\nsentence this basic unit is located.\\nFor instance, CNNs achieve pretty good results for sentiment analysis, where we need to understand if a \\npiece of text has a positive or a negative sentiment; for spam detection, where we need to understand if \\na piece of text is useful information or spam; and for topic categorization, where we need to understand \\nwhat a piece of text is all about. However, CNNs are not well suited for a Part of Speech (POS) analysis, \\nwhere the goal is to understand what the logical role of every single word is (for example, a verb, an \\nadverb, a subject, and so on). CNNs are also not well suited for entity extraction, where we need to \\nunderstand where relevant entities are located in sentences. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49c75e6b-cd48-4d6e-855f-3469849e7442', embedding=None, metadata={'page_label': '632', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 632\\nIndeed, it turns out that a position is pretty useful information for the last two use cases. 1D ConvNets \\nare very similar to 2D ConvNets. However, the former operates on a single vector, while the latter \\noperates on matrices.\\nUsing a CNN for sentiment analysis\\nLet’s have a look at the code. First of all, we load the dataset with tensorflow_datasets . In this case \\nwe use IMDB, a collection of movie reviews:\\nimport tensorflow as tf\\nfrom tensorflow.keras import datasets, layers, models, preprocessing\\nimport tensorflow_datasets as tfds\\nmax_len = 200\\nn_words = 10000\\ndim_embedding = 256\\nEPOCHS = 20\\nBATCH_SIZE = 500\\ndef load_data ():\\n    #load data\\n    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_\\nwords)\\n    # Pad sequences with max_len\\n    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\\n    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\\n    return (X_train, y_train), (X_test, y_test)\\nThen we build a suitable CNN model. We use embeddings (see Chapter 4, Word Embeddings) to map \\nthe sparse vocabulary typically observed in documents into a dense feature space of dimensions \\ndim_embedding . Then we use Conv1D , followed by a GlobalMaxPooling1D  for averaging, and two Dense  \\nlayers – the last one has only one neuron firing binary choices (positive or negative reviews):\\ndef build_model ():\\n    model = models.Sequential()\\n    #Input - Embedding Layer\\n    # the model will take as input an integer matrix of size (batch, input_\\nlength)\\n    # the model will output dimension (input_length, dim_embedding)\\n    # the largest integer in the input should be no larger\\n    # than n_words (vocabulary size).\\n    model.add(layers.Embedding(n_words,\\n        dim_embedding, input_length=max_len))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91eea060-7553-4e8c-908d-38b8d9de42b7', embedding=None, metadata={'page_label': '633', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 20 633\\n    model.add(layers.Dropout( 0.3))\\n    model.add(layers.Conv1D( 256, 3, padding= 'valid', \\n        activation= 'relu'))\\n    #takes the maximum value of either feature vector from each of the n_words \\nfeatures\\n    model.add(layers.GlobalMaxPooling1D())\\n    model.add(layers.Dense( 128, activation= 'relu'))\\n    model.add(layers.Dropout( 0.5))\\n    model.add(layers.Dense( 1, activation= 'sigmoid' ))\\n    return model\\n(X_train, y_train), (X_test, y_test) = load_data()\\nmodel=build_model()\\nmodel.summary()\\nThe model has more than 2,700,000 parameters, and it is summarized as follows:\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n embedding (Embedding)       (None, 200, 256)          2560000   \\n                                                                 \\n dropout (Dropout)           (None, 200, 256)          0         \\n                                                                 \\n conv1d (Conv1D)             (None, 198, 256)          196864    \\n                                                                 \\n global_max_pooling1d (Globa  (None, 256)              0         \\n lMaxPooling1D)                                                  \\n                                                                 \\n dense (Dense)               (None, 128)               32896     \\n                                                                 \\n dropout_1 (Dropout)         (None, 128)               0         \\n                                                                 \\n dense_1 (Dense)             (None, 1)                 129       \\n                                                                 \\n=================================================================\\nTotal params: 2,789,889\\nTrainable params: 2,789,889\\nNon-trainable params: 0\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da96a3e3-6df6-4720-952c-eb9a00ec5cdf', embedding=None, metadata={'page_label': '634', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 634\\nThen we compile and fit the model with the Adam optimizer and binary cross-entropy loss:\\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\" ,\\n  metrics = [ \"accuracy\" ]\\n)\\nscore = model.fit(X_train, y_train,\\n  epochs= EPOCHS,\\n  batch_size = BATCH_SIZE,\\n  validation_data = (X_test, y_test)\\n)\\nscore = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\\nprint(\"\\\\nTest score:\" , score[ 0])\\nprint(\\'Test accuracy:\\' , score[ 1])\\nThe final accuracy is 88.21%, showing that it is possible to successfully use CNNs for textual processing:\\nEpoch 19/20\\n25000/25000 [==============================] - 135s 5ms/sample - loss: 7.5276e-\\n04 - accuracy: 1.0000 - val_loss: 0.5753 - val_accuracy: 0.8818\\nEpoch 20/20\\n25000/25000 [==============================] - 129s 5ms/sample - loss: 6.7755e-\\n04 - accuracy: 0.9999 - val_loss: 0.5802 - val_accuracy: 0.8821\\n25000/25000 [==============================] - 23s 916us/sample - loss: 0.5802 \\n- accuracy: 0.8821\\nTest score: 0.5801781857013703\\nTest accuracy: 0.88212\\nNote that many other non-image applications can also be converted to an image and classified \\nusing CNNs (see, for instance, https://becominghuman.ai/sound-classification-using-images-\\n68d4770df426 ). \\nAudio and music\\nWe have used CNNs for images, videos, and texts. Now let’s have a look at how variants of CNNs can \\nbe used for audio.\\nSo, you might wonder why learning to synthesize audio is so difficult. Well, each digital sound we hear \\nis based on 16,000 samples per second (sometimes 48K or more), and building a predictive model \\nwhere we learn to reproduce a sample based on all the previous ones is a very difficult challenge.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c21c374-e3c1-4296-92eb-2cf7a159c84e', embedding=None, metadata={'page_label': '635', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 635\\nDilated ConvNets, WaveNet, and NSynth\\nWaveNet is a deep generative model for producing raw audio waveforms. This breakthrough technology \\nwas introduced (available at https://deepmind.com/blog/wavenet-a-generative-model-for-raw-\\naudio/ ) by Google DeepMind for teaching computers how to speak. The results are truly impressive, \\nand online you can find examples of synthetic voices where the computer learns how to talk with the \\nvoice of celebrities such as Matt Damon. There are experiments showing that WaveNet improved the \\ncurrent state-of-the-art Text-to-Speech (TTS ) systems, reducing the difference with respect to human \\nvoices by 50% for both US English and Mandarin Chinese. The metric used for comparison is called \\nMean Opinion Score (MOS ), a subjective paired comparison test. In the MOS tests, after listening to \\neach sound stimulus, the subjects were asked to rate the naturalness of the stimulus on a five-point \\nscale from “Bad” (1) to “Excellent” (5).\\nWhat is even cooler is that DeepMind demonstrated that WaveNet can be also used to teach computers \\nhow to generate the sound of musical instruments such as piano music.\\nNow some definitions. TTS systems are typically divided into two different classes: concatenative and \\nparametric.\\nConcatenative TTS is where single speech voice fragments are first memorized and then recombined \\nwhen the voice has to be reproduced. However, this approach does not scale because it is possible to \\nreproduce only the memorized voice fragments, and it is not possible to reproduce new speakers or \\ndifferent types of audio without memorizing the fragments from the beginning.\\nParametric TTS is where a model is created to store all the characteristic features of the audio to \\nbe synthesized. Before WaveNet, the audio generated with parametric TTS was less natural than \\nconcatenative TTS. WaveNet enabled significant improvement by modeling directly the production \\nof audio sounds, instead of using intermediate signal processing algorithms as in the past.\\nIn principle, WaveNet can be seen as a stack of 1D convolutional layers with a constant stride of one and \\nwith no pooling layers. Note that the input and the output have by construction the same dimension, \\nso ConvNets are well suited to modeling sequential data such as audio sounds. However, it has been \\nshown that in order to reach a large size for the receptive field in the output neuron, it is necessary to \\neither use a massive number of large filters or increase the network depth prohibitively. Remember that \\nthe receptive field of a neuron in a layer is the cross-section of the previous layer from which neurons \\nprovide inputs. For this reason, pure ConvNets are not so effective in learning how to synthesize audio.\\nThe key intuition behind WaveNet is the so-called Dilated Causal Convolutions [5] (sometimes called \\natrous convolution), which simply means that some input values are skipped when the filter of a \\nconvolutional layer is applied. “Atrous” is a “bastardization” of the French expression “à trous,” meaning \\n“with holes.” So an atrous convolution is a convolution with holes. As an example, in one dimension, a \\nfilter w of size 3 with a dilation of 1 would compute the following sum: w[0] x[0] + w[1] x[2] + w[3] x[4].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b87f4e38-8d29-4853-b8b6-4ce7d89a6cb4', embedding=None, metadata={'page_label': '636', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 636\\nIn short, in D-dilated convolution, usually the stride is 1, but nothing prevents you from using other \\nstrides. An example is given in Figure 20.14 with increased dilatation (hole) sizes = 0, 1, 2:\\nFigure 20.14: Dilatation with increased sizes\\nThanks to this simple idea of introducing holes, it is possible to stack multiple dilated convolutional \\nlayers with exponentially increasing filters and learn long-range input dependencies without having \\nan excessively deep network.\\nA WaveNet is therefore a ConvNet where the convolutional layers have various dilation factors, allowing \\nthe receptive field to grow exponentially with depth and therefore efficiently cover thousands of audio \\ntimesteps.\\nWhen we train, the inputs are sounds recorded from human speakers. The waveforms are quantized \\nto a fixed integer range. A WaveNet defines an initial convolutional layer accessing only the current \\nand previous input. Then, there is a stack of dilated ConvNet layers, still accessing only current and \\nprevious inputs. At the end, there is a series of dense layers combining previous results, followed by \\na softmax activation function for categorical outputs.\\nAt each step, a value is predicted from the network and fed back into the input. At the same time, a new \\nprediction for the next step is computed. The loss function is the cross-entropy between the output \\nfor the current step and the input at the next step. Figure 20.15 shows the visualization of a WaveNet \\nstack and its receptive field as introduced in Aaron van den Oord [9]. Note that generation can be slow \\nbecause the waveform has to be synthesized in a sequential fashion, as x t must be sampled first in \\norder to obtain 𝑥𝑥>𝑡𝑡  where x is the input: \\nFigure 20.15: WaveNet internal connections', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3167e919-4afb-48d0-a7b8-722945ab1253', embedding=None, metadata={'page_label': '637', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 637\\nA method for performing a sampling in parallel has been proposed in Parallel WaveNet [10], which \\nachieves a three orders-of-magnitude speedup. This uses two networks as a WaveNet teacher network, \\nwhich is slow but ensures a correct result, and a WaveNet student network, which tries to mimic the \\nbehavior of the teacher; this can prove to be less accurate but is faster. This approach is similar to the \\none used for GANs (see Chapter 9, Generative Models) but the student does not try to fool the teacher, \\nas typically happens in GANs. In fact, the model is not just quicker but also of higher fidelity, capable \\nof creating waveforms with 24,000 samples per second:\\nFigure 20.16: Examples of WaveNet Student and Teacher\\nThis model has been deployed in production at Google, and is currently being used to serve Google \\nAssistant queries in real time to millions of users. At the annual I/O developer conference in May 2018, \\nit was announced that new Google Assistant voices were available thanks to WaveNet.\\nTwo implementations of WaveNet models for TensorFlow are currently available. One is the original \\nimplementation of DeepMind’s WaveNet, and the other is called Magenta NSynth. The original WaveNet \\nversion is available at https://github.com/ibab/tensorflow-wavenet . NSynth is an evolution of \\nWaveNet recently released by the Google Brain group, which, instead of being causal, aims at seeing the \\nentire context of the input chunk. Magenta is available at https://magenta.tensorflow.org/nsynth . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8583e57-0004-4157-b511-d8126fdd02a4', embedding=None, metadata={'page_label': '638', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 638\\nThe neural network is truly complex, as depicted in the image below, but for the sake of this introductory \\ndiscussion, it is sufficient to know that the network learns how to reproduce its input by using an \\napproach based on reducing the error during the encoding/decoding phases:\\nFigure 20.17: Magenta internal architecture\\nIf you are interested in understanding more, I would suggest having a look at the online Colab notebook \\nwhere you can play with models generated with NSynth. NSynth Colab is available at https://colab.\\nresearch.google.com/notebooks/magenta/nsynth/nsynth.ipynb .\\nMuseNet is a very recent and impressive cool audio generation tool developed by OpenAI. MuseNet \\nuses a sparse transformer to train a 72-layer network with 24 attention heads. MuseNet is available at \\nhttps://openai.com/blog/musenet/ . Transformers, discussed in Chapter 6, are very good at predicting \\nwhat comes next in a sequence – whether text, images, or sound.\\nIn transformers, every output element is connected to every input element, and the weightings between \\nthem are dynamically calculated according to a process called attention. MuseNet can produce up to \\n4-minute musical compositions with 10 different instruments, and can combine styles from country, \\nto Mozart, to the Beatles. For instance, I generated a remake of Beethoven’s “Für Elise” in the style \\nof Lady Gaga with piano, drums, guitar, and bass. You can try this for yourself at the link provided \\nunder the section Try MuseNet:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dca49298-b5c7-4df1-8ef2-f82154bec420', embedding=None, metadata={'page_label': '639', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 639\\nFigure 20.18: An example of using MuseNet\\nA summary of convolution operations\\nIn this section, we present a summary of different convolution operations. A convolutional layer has I \\ninput channels and produces O  output channels. I  x O x K parameters are used, where K is the number \\nof values in the kernel. \\nBasic CNNs\\nLet’s remind ourselves briefly what a CNN is. CNNs take in an input image (two dimensions), text \\n(two dimensions), or video (three dimensions) and apply multiple filters to the input. Each filter is \\nlike a flashlight sliding across the areas of the input, and the areas that it is shining over are called the \\nreceptive field. Each filter is a tensor of the same depth of the input (for instance, if the image has a \\ndepth of three, then the filter must also have a depth of three).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27ca4d9e-8b54-48bf-b539-7866ddc2ee24', embedding=None, metadata={'page_label': '640', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 640\\nWhen the filter is sliding, or convolving, around the input image, the values in the filter are multiplied \\nby the values of the input. The multiplications are then summarized into one single value. This process \\nis repeated for each location, producing an activation map (a.k.a. a feature map). Of course, it is \\npossible to use multiple filters where each filter will act as a feature identifier. For instance, for images, \\nthe filter can identify edges, colors, lines, and curves. The key intuition is to treat the filter values as \\nweights and fine-tune them during training via backpropagation.\\nA convolution layer can be configured by using the following config parameters:\\n• Kernel size: This is the field of view of the convolution.\\n• Stride: This is the step size of the kernel when we traverse the image.\\n• Padding: Defines how the border of our sample is handled.\\nDilated convolution\\nDilated convolutions (or atrous convolutions) introduce another config parameter:\\n• Dilation rate: This is the spacing between the values in a kernel.\\nDilated convolutions are used in many contexts including audio processing with WaveNet.\\nTransposed convolution\\nTransposed convolution is a transformation going in the opposite direction of a normal convolution. \\nFor instance, this can be useful to project feature maps into a higher dimensional space or for \\nbuilding convolutional autoencoders (see Chapter 8, Autoencoders). One way to think about transposed \\nconvolution is to compute the output shape of a normal CNN for a given input shape first. Then we \\ninvert input and output shapes with the transposed convolution. TensorFlow 2.0 supports transposed \\nconvolutions with Conv2DTranspose layers, which can be used, for instance, in GANs (see Chapter 9, \\nGenerative Models) for generating images.\\nSeparable convolution\\nSeparable convolution aims at separating the kernel in multiple steps. Let the convolution be y  = conv (x, \\nk) where y  is the output, x  is the input, and k  is the kernel. Let’s assume the kernel is separable, k  = k1.k2 \\nwhere . is the dot product – in this case, instead of doing a 2-dimension convolution with k, we can \\nget to the same result by doing two 1-dimension convolutions with k1 and k2. Separable convolutions \\nare frequently used to save on computation resources.\\nDepthwise convolution\\nLet’s consider an image with multiple channels. In the normal 2D convolution, the filter is as deep as \\nthe input, and it allows us to mix channels for generating each element of the output. In depthwise \\nconvolutions, each channel is kept separate, the filter is split into channels, each convolution is applied \\nseparately, and the results are stacked back together into one tensor.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73eb1086-9561-4ebb-9bcb-63da12b3656c', embedding=None, metadata={'page_label': '641', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 641\\nDepthwise separable convolution\\nThis convolution should not be confused with the separable convolution. After completing the depthwise \\nconvolution, an additional step is performed: a 1x1 convolution across channels. Depthwise separable \\nconvolutions are used in Xception. They are also used in MobileNet, a model particularly useful for \\nmobile and embedded vision applications because of its reduced model size and complexity.\\nIn this section, we have discussed all the major forms of convolution. The next section will discuss \\ncapsule networks, a new form of learning introduced in 2017.\\nCapsule networks\\nCapsule networks (or CapsNets) are a very recent and innovative type of deep learning network. This \\ntechnique was introduced at the end of October 2017 in a seminal paper titled Dynamic Routing Between \\nCapsules by Sara Sabour, Nicholas Frost, and Geoffrey Hinton ( https://arxiv.org/abs/1710.09829 ) \\n[14]. Hinton is the father of deep learning and, therefore, the whole deep learning community is \\nexcited to see the progress made with Capsules. Indeed, CapsNets are already beating the best CNN \\non MNIST classification, which is... well, impressive!!\\nWhat is the problem with CNNs?\\nIn CNNs, each layer “understands” an image at a progressive level of granularity. As we discussed in \\nmultiple sections, the first layer will most likely recognize straight lines or simple curves and edges, \\nwhile subsequent layers will start to understand more complex shapes such as rectangles up to complex \\nforms such as human faces. \\nNow, one critical operation used for CNNs is pooling. Pooling aims at creating positional invariance \\nand it is used after each CNN layer to make any problem computationally tractable. However, pooling \\nintroduces a significant problem because it forces us to lose all the positional data. This is not good. \\nThink about a face: it consists of two eyes, a mouth, and a nose, and what is important is that there is a \\nspatial relationship between these parts (for example, the mouth is below the nose, which is typically \\nbelow the eyes). Indeed, Hinton said: The pooling operation used in convolutional neural networks is a big \\nmistake and the fact that it works so well is a disaster. Technically, we do not need positional invariance but \\ninstead we need equivariance. Equivariance is a fancy term for indicating that we want to understand \\nthe rotation or proportion change in an image, and we want to adapt the network accordingly. In this \\nway, the spatial positioning among the different components in an image is not lost.\\nWhat is new with capsule networks?\\nAccording to Hinton et al., our brain has modules called “capsules,” and each capsule is specialized \\nin handling a particular type of information. In particular, there are capsules that work well for \\n“understanding” the concept of position, the concept of size, the concept of orientation, the concept \\nof deformation, textures, and so on. In addition to that, the authors suggest that our brain has \\nparticularly efficient mechanisms for dynamically routing each piece of information to the capsule \\nthat is considered best suited for handling a particular type of information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a2eafbf-7977-457d-a517-43b1dbfa3a3d', embedding=None, metadata={'page_label': '642', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 642\\nSo, the main difference between CNN and CapsNets is that with a CNN, we keep adding layers for \\ncreating a deep network, while with CapsNet, we nest a neural layer inside another. A capsule is a group \\nof neurons that introduces more structure to a network, and it produces a vector to signal the existence \\nof an entity in an image. In particular, Hinton uses the length of the activity vector to represent the \\nprobability that the entity exists and its orientation to represent the instantiation parameters. When \\nmultiple predictions agree, a higher-level capsule becomes active. For each possible parent, the capsule \\nproduces an additional prediction vector.\\nNow a second innovation comes in place: we will use dynamic routing across capsules and will no longer \\nuse the raw idea of pooling. A lower-level capsule prefers to send its output to higher-level capsules for \\nwhich the activity vectors have a big scalar product, with the prediction coming from the lower-level \\ncapsule. The parent with the largest scalar prediction vector product increases the capsule bond. All \\nthe other parents decrease their bond. In other words, the idea is that if a higher-level capsule agrees \\nwith a lower-level one, then it will ask to send more information of that type. If there is no agreement, \\nit will ask to send fewer of them. This dynamic routing by the agreement method is superior to the \\ncurrent mechanism like max pooling and, according to Hinton, routing is ultimately a way to parse \\nthe image. Indeed, max pooling is ignoring anything but the largest value, while dynamic routing \\nselectively propagates information according to the agreement between lower layers and upper layers.\\nA third difference is that a new nonlinear activation function has been introduced. Instead of adding a \\nsquashing function to each layer as in CNN, CapsNet adds a squashing function to a nested set of layers. \\nThe nonlinear activation function is represented in Equation 1 , and it is called the squashing function:\\n                                        v 𝑗𝑗=‖s𝑗𝑗‖2\\n1 + ‖s 𝑗𝑗‖2s𝑗𝑗\\n‖s𝑗𝑗‖                                          (1)  \\nwhere v j is the vector output of capsule j and s j is its total input.\\nMoreover, Hinton and others show that a discriminatively trained, multi-layer capsule system \\nachieves state-of-the-art performances on MNIST and is considerably better than a convolutional \\nnet at recognizing highly overlapping digits.\\nBased on the paper Dynamic Routing Between Capsules, a simple CapsNet architecture looks as follows:\\nFigure 20.19: An example of CapsNet', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfa25de7-9594-4940-bef2-c06324c90f1e', embedding=None, metadata={'page_label': '643', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20 643\\nThe architecture is shallow with only two convolutional layers and one fully connected layer. Conv1 \\nhas 256 9 x 9 convolution kernels with a stride of 1 and ReLU activation. The role of this layer is \\nto convert pixel intensities to the activities of local feature detectors that are then used as inputs \\nto the PrimaryCapsules layer. PrimaryCapsules is a convolutional capsule layer with 32 channels; \\neach primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride of 2. In total, \\nPrimaryCapsules has [32, 6, 6] capsule outputs (each output is an 8D vector) and each capsule in the [6, \\n6] grid shares its weights with each other. The final layer (DigitCaps) has one 16D capsule per digit class \\nand each one of these capsules receives input from all the other capsules in the layer below. Routing \\nhappens only between two consecutive capsule layers (for example, PrimaryCapsules and DigitCaps).\\nSummary\\nIn this chapter, we have seen many applications of CNNs across very different domains, from traditional \\nimage processing and computer vision to close-enough video processing, not-so-close audio processing, \\nand text processing. In just a few years, CNNs have taken machine learning by storm.\\nNowadays, it is not uncommon to see multimodal processing, where text, images, audio, and videos \\nare considered together to achieve better performance, frequently by means of combining CNNs \\ntogether with a bunch of other techniques such as RNNs and reinforcement learning. Of course, there \\nis much more to consider, and CNNs have recently been applied to many other domains such as genetic \\ninference [13], which are, at least at first glance, far away from the original scope of their design.\\nReferences\\n1. Yosinski, J. and Clune, Y. B. J. How transferable are features in deep neural networks. Advances in \\nNeural Information Processing Systems 27, pp. 3320–3328.\\n2. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna, Z. (2016). Rethinking the Inception \\nArchitecture for Computer Vision. 2016 IEEE Conference on Computer Vision and Pattern \\nRecognition (CVPR), pp. 2818–2826.\\n3. Sandler, M., Howard, A., Zhu, M., Zhmonginov, A., and Chen, L. C. (2019). MobileNetV2: Inverted \\nResiduals and Linear Bottlenecks. Google Inc.\\n4. Krizhevsky, A., Sutskever, I., Hinton, G. E., (2012). ImageNet classification with deep convolutional \\nneural networks.\\n5. Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (28 Jan 2018). Densely Connected \\nConvolutional Networks. http://arxiv.org/abs/1608.06993\\n6. Chollet, F. (2017). Xception: Deep Learning with Depthwise Separable Convolutions. https://arxiv.\\norg/abs/1610.02357\\n7. Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). A Neural Algorithm of Artistic Style. https://\\narxiv.org/abs/1508.06576\\n8. Mordvintsev, A., Olah, C., and Tyka, M. ( 2015). DeepDream - a code example for visualizing Neural \\nNetworks. Google Research.\\n9. van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., \\nSenior, A., and Kavukcuoglu, K. (2016). WaveNet: A generative model for raw audio. arXiv preprint.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='361a9d4a-734d-4ce1-a189-22bd1ac5403d', embedding=None, metadata={'page_label': '644', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Convolutional Neural Networks 644\\n10. van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den \\nDriessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., \\nDieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., \\nand Hassabis, D. (2017). Parallel WaveNet: Fast High-Fidelity Speech Synthesis.\\n11. He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). Mask R-CNN.\\n12. Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. (2018). Encoder-Decoder with \\nAtrous Separable Convolution for Semantic Image Segmentation.\\n13. Flagel, L., Brandvain, Y., and Schrider, D.R. (2018). The Unreasonable Effectiveness of Convolutional \\nNeural Networks in Population Genetic Inference.\\n14. Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic Routing Between Capsules https://\\narxiv.org/abs/1710.09829\\nJoin our book’s Discord space\\nJoin our Discord community to meet like-minded people and learn alongside more than 2000 members \\nat: https://packt.link/keras', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cd961ac-26d3-44bf-94fc-ed0bedbc0dae', embedding=None, metadata={'page_label': '645', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='packt.com\\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as in -\\ndustry leading tools to help you plan your personal development and advance your career. For more \\ninformation, please visit our website.\\nWhy subscribe?\\n• Spend less time learning and more time coding with practical eBooks and Videos from over \\n4,000 industry professionals\\n• Improve your learning with Skill Plans built especially for you\\n• Get a free eBook or video every month\\n• Fully searchable for easy access to vital information\\n• Copy and paste, print, and bookmark content\\nAt www.packt.com , you can also read a collection of free technical articles, sign up for a range of free \\nnewsletters, and receive exclusive discounts and offers on Packt books and eBooks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed0eef4a-14f4-47fd-941a-415e816bddac', embedding=None, metadata={'page_label': '646', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b1ec01c-f47f-451a-8c5d-fe1b8879da44', embedding=None, metadata={'page_label': '647', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Books  \\nYou May Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt: \\nMachine Learning with PyTorch and Scikit-Learn\\nSebastian Raschka\\nYuxi (Hayden) Liu\\nVahid Mirjalili\\nISBN: 9781801819312\\n• Explore frameworks, models, and techniques for machines to ‘learn’ from data\\n• Use scikit-learn for machine learning and PyTorch for deep learning\\n• Train machine learning classifiers on images, text, and more\\n• Build and train neural networks, transformers, and boosting algorithms\\n• Discover best practices for evaluating and tuning models\\n• Predict continuous target outcomes using regression analysis\\n• Dig deeper into textual and social media data using sentiment analysis', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fbe4467a-bbbc-4d2e-a64c-8fab47f89e46', embedding=None, metadata={'page_label': '648', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Books You May Enjoy 648\\nTransformers for Natural Language Processing, Second Edition\\nDenis Rothman\\nISBN: 9781803247335\\n• Find out how ViT and CLIP label images (including blurry ones!) and create images from a \\nsentence using DALL-E\\n• Discover new techniques to investigate complex language problems\\n• Compare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based transformers\\n• Carry out sentiment analysis, text summarization, casual speech analysis, machine translations, \\nand more using TensorFlow, PyTorch, and GPT-3\\n• Measure the productivity of key transformers to define their scope, potential, and limits in \\nproduction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9752e593-fe8a-42e4-8a12-aa5bffed038c', embedding=None, metadata={'page_label': '649', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Books You May Enjoy 649\\nNatural Language Processing with TensorFlow, Second Edition\\nThushan Ganegedara\\nISBN: 9781838641351\\n• Learn core concepts of NLP and techniques with TensorFlow\\n• Use state-of-the-art Transformers and how they are used to solve NLP tasks\\n• Perform sentence classification and text generation using CNNs and RNNs\\n• Utilize advanced models for machine translation and image caption generation\\n• Build end-to-end data pipelines in TensorFlow\\n• Learn interesting facts and practices related to the task at hand\\n• Create word representations of large amounts of data for deep learning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5629c33-54b4-4a96-891d-dc2816f78980', embedding=None, metadata={'page_label': '650', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Other Books You May Enjoy 650\\nPackt is searching for authors like you\\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com  and apply \\ntoday. We have worked with thousands of developers and tech professionals, just like you, to help \\nthem share their insight with the global tech community. You can make a general application, apply \\nfor a specific hot topic that we are recruiting an author for, or submit your own idea.\\nShare your thoughts\\nNow you’ve finished Deep Learning with TensorFlow and Keras, Third Edition , we’d love to hear your \\nthoughts! If you purchased the book from Amazon, please click here to go straight to the \\nAmazon review page  for this book and share your feedback or leave a review on the site that you \\npurchased it from.\\nYour review is important to us and the tech community and will help us make sure we’re delivering \\nexcellent quality content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85de58ff-b809-4ba6-ba2f-084d856823a7', embedding=None, metadata={'page_label': '651', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index\\nA\\naccuracy  13\\naction  391\\naction-value function  392\\nactivation functions  9, 478\\nReLU  479\\nsigmoid  478\\ntanh  479\\nALBERT  214\\nkey intuitions  214\\naleatory uncertainty  438\\nprobabilistic neural networks  441, 442\\nprobabilistic neural networks, using  440\\nAlexNet  95\\nAndroid Studio\\nreference link  589\\nApplication-Specific Integrated Circuit  \\n(ASIC)  501\\nArduino Nano 33 BLE Sense\\nreference link  587\\nArea Under the Curve (AUC)  458, 549\\nArea Under the Receiver Operating \\nCharacteristic Curve (AUC ROC)  458\\nArm Cortex-M\\nreference link  587\\nArtificial General Intelligence (AGI)  445\\nartificial neural networks (nets/ANNs)  3\\natrous convolution\\nusing, for audio  635\\nAttention mechanism  182-184, 195-197\\ncomputing  198, 199\\nfull, versus sparse matrices  206local attention  206\\nLSH Attention  206\\nseq2seq model, using with  184-189\\nAugmented Multiscale Deep InfoMax  \\n(AMDIM)  380\\nautoencoders  287-289\\ncontext 367, 368\\nconvolutional 301-306\\ndeonising  297-301\\nsparse  295-297\\nstacked 301\\nstacked deonising 367\\nvariational 314-319, 371\\nAutoEncoding (AE)  365\\nAutograd Module  521\\nAutoKeras  450\\narchitecture  451\\nautomatic differentiation  495\\nAutomatic Machine Learning  \\n(AutoML)  445, 446, 570\\nachieving  446\\nautomatic data preparation  447\\nautomatic feature engineering  447, 448\\nautomatic model generation  448, 449\\nH2O, using  523- 525\\npipeline, steps  446\\ntuning  38, 39\\nautoregressive (AR) generation  364\\nGPT-3  365\\nImage GPT (IPT)  364\\nPixelRNN  364\\nWaveNet  366\\nWaveRNN  366\\nXLNet  365', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b68e80f-8dce-4cb0-b0a5-885d75c4fe3c', embedding=None, metadata={'page_label': '652', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 652\\nautoregressive (AR) models  362\\nB\\nbackdrop  480\\nbackpropagation  3, 480\\nand ConvNets  491\\nand RNNs  492-494\\nbackstep  483, 484\\nforward propagation  481\\nforward step  482, 483\\noverview  39- 41\\npurpose  480\\nbackpropagation through time  \\n(BPTT)  142, 143, 494\\nbackstep  483, 484\\nneuron equation, form hidden layer to hidden \\nlayer  485-489\\nneuron equation, form hidden layer to output \\nlayer  484, 485\\nBahdanau attention (additive)  183\\nBarlow Twins model  377, 378\\nbaseline model  569\\nBatch Gradient Descent (BGD)   491\\nbatch normalization  33\\nBayesian Networks (BN)  434-436\\nfactors  436, 437\\nBayesian optimization  450\\nbeginning-of-string (BOS)  172\\nBernoulli distribution  428\\nbest practices\\nfor data  564\\nfor model  568\\nneed for  563, 564\\nbfloat16  504\\nBidirectional Encoder Representations from \\nTransformers (BERT)  133, 207, 366, 367\\nkey intuitions  208\\nusing, as feature extractor  134, 135bidirectional LSTM (biLSTM)  147\\nbidirectional RNNs  147, 148\\nBigBird  211\\nkey intuitions  212\\nBiLingual Evaluation Understudy (BLEU)  179\\nbinary_crossentropy, objective functions  13\\nBootstrap Your Own Latent (BYOL) model  378\\nbottleneck layer  289\\nByte Pair Encoding (BPE)  372\\nC\\nCaffe\\nURL  2\\ncapsule networks (CapsNets)  641\\ncatastrophic forgetting  394\\ncategorical_crossentropy, objective  \\nfunctions  13\\ncausal graphs  434\\nCentral Processing Units (CPUs)  499\\nchain rule  477\\ncharacter embedding  128, 129\\nCIFAR-10\\nperformance, improving with data \\naugmentation  84-87\\nperformance, improving with deeper  \\nnetwork  82-84\\npredicting with  87\\nCIFAR-10 images\\nrecognizing, with deep learning  78-82\\nclassification task\\nversus regression task  58\\nCLEVR dataset\\nreference link  624\\nCLIP model  381, 382\\nCNN architectures  95\\nAlexNet  95\\nDenseNets  96', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29053130-dff8-4c44-a579-5e64c139a03f', embedding=None, metadata={'page_label': '653', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 653\\nHighwaysNet  96\\nresidual networks  95\\nXception  97, 98, 99\\nCodeSearchNet model  383\\nCodex  519, 520\\nColab  33, 35\\nreference link  509, 510\\nusing, with TPU  509\\ncollaborative filtering (CF) models  288\\ncolorization  369\\nconcatenative TTS  635\\nConceptNet Numberbatch  117\\nConditional Probability Table (CPT)  434\\ncontent-based attention  183\\ncontext autoencoder  367, 368\\nContext Free Network (CFN)  370\\nContextualized Vectors (CoVe)  129\\ncontinuous backpropagation\\nhistory  473\\nContinuous Bag of Words (CBOW)  107\\nContrastive Divergence (CD)  278\\ncontrastive learning (CL)  207, 362, 373\\nDeep InfoMax (DIM)  207\\ninstance transformation  376\\nmultimodal models  381\\nmultiview coding  380\\nNext Sentence Prediction (NSP)\\nReplaced Token Detection (RTD)  207\\nSentence Order Prediction (SOP)  207\\ntraining objectives  373\\ncontrastive loss  374\\nContrastive Multiview Coding (CMC)  381\\nconvergence  19ConvNet (1D CNN)  118\\nConvNets\\nsummarizing  69\\nconvolutional autoencoder  301\\nused, for removing noise from images  301-306\\nConvolutional Neural Network  \\n(CNN)  41, 114, 533\\nclassification and localization  614\\ncomposing, for complex tasks  613\\nfeatures  641-643\\ninstance segmentation  619-621\\nissue  641\\nobject detection  616-619\\nsemantic segmentation  615\\nusing, for audio  634\\nusing, for sentiment analysis  632- 634\\nusing, with videos  630\\nconvolution layer configuration, parameters\\nkernel size  640\\npadding  640\\nstride  640\\nconvolution operations  639\\nbasic CNNs  639, 640\\ndepthwise convolution  640\\ndepthwise separable convolution  641\\ndilated convolution  640\\nseparable convolution  640\\ntransposed convolution  640\\ncost functions  13\\ncritic network  419\\ncross entropy  489\\nderivative  489, 490\\ncustom graph dataset  554\\nmultiple graphs, in datasets  557-559\\nsingle graphs, in datasets  554-556\\nCycleGAN  340-342\\nimplementing, in TensorFlow  348-356', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9f69474-b100-4251-aa25-b980091adcce', embedding=None, metadata={'page_label': '654', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 654\\nD\\nDALL-E 2  371, 372, 518, 519\\nData2Vec model  383\\ndata best practices  564\\nfeatures and data  565, 566\\nfeature selection  565\\ndata cleansing  447\\ndata generation\\ndiffusion models, using for  358, 359\\nflow-based models, using for  356-358\\ndata pipelines\\nbuilding, with TFDS  583-585\\ndata synthesis  447\\nD-dilated convolution  636\\nDeBERTa  217\\nkey intuitions  217\\ndecision boundaries  58\\ndecoder pretraining  206\\nDeep Averaging Network (DAN)  131\\ndeep belief networks (DBNs)  283\\nDeepCluster  379\\ndeep convolutional GAN (DCGAN)  329, 330\\nfor MNIST digits  330-339\\nDeep Convolutional Neural Network (DCNN)  66\\nConvNets, in TensorFlow  68\\nexample, LeNet  69\\nfor large-scale image recognition  88, 90\\nlocal receptive fields  66\\nmathematical example  67, 68\\npooling layers  68\\nshared weights and bias  67\\nDeep Deterministic Policy Gradient  \\n(DDPG)  418, 419\\nDeepDream network\\ncreating  625-629\\ndeep learning  3, 413\\nCIFAR-10 images, recognizing with  78-82importance  77\\nDeep Q-Networks (DQNs)  406, 407\\nfor CartPole  407-412\\nused, for playing Atari game  412- 415\\nvariants  415-418\\nDeep Reinforcement Learning (DRL)\\nsuccess stories  395, 396\\nDeep Reinforcement Learning (DRL)  \\nalgorithms  393\\naction selection, by agent  394\\nbalance, maintaining between exploration and \\nexploitation  394\\nhighly correlated input state space, dealing \\nwith  394, 395\\nmoving targets issues, dealing with  395\\npolicy-based methods  393\\nvalue-based methods  393\\nDeepWalk  123\\ndenoising autoencoders  297\\nused, for clearing images  298-301\\nDenseNets  96\\ndependent variable  44\\ndepthwise convolution  640\\ndepthwise separable convolution  641\\nderivatives  474\\ndifferentiation rules  477\\ndiffusion models\\nusing, for data generation  358, 359\\nDilated Causal Convolutions  635\\ndilated ConvNet  636\\ndilated convolution  640\\nDirected Acyclic Graph (DAG)  434\\ndistributed representations  105, 106\\ndouble DQN  416\\nDQN variants\\ndouble DQN  416\\ndueling DQN  416-418\\nRainbow  418', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b622c7fa-7969-4f7b-bb57-773e9c208858', embedding=None, metadata={'page_label': '655', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 655\\ndropout\\nused, for improving net in TensorFlow  19-21\\ndueling DQN  416-418\\ndynamic embeddings  129, 130\\nE\\nedge computing  597\\nEdge TPU  506, 507\\nEfficient Neural Architecture Search (ENAS)  449\\neigen decomposition  262\\nElasticNet regularization  32\\nELECTRA  216\\nkey intuitions  216\\nEmbedding Projector tool  265\\ndata panel  265\\ninspector panel  266\\nprojections panel  265\\nembeddings\\ncreating, with Gensim  110, 111\\nEmbeddings from Language Models  \\n(ELMo)  130\\nembedding space\\nexploring, with Gensim  111-113\\nencoder-decoder architecture  200\\nseq2seq model  172\\nencoder-decoder pretraining  206\\nencoder pretraining  206\\nend-of-sentence (EOS)  174\\nentity extraction  631\\nepistemic uncertainty  438\\naccounting  442, 443\\nEvolutionary Algorithm (EA)  449\\nEvolved Transformer  217\\nkey intuitions  217, 218experience replay method  394\\nexploration vs exploitation tradeoff  394\\nExponential Linear Unit (ELU)  8\\nF\\nFacebook AI Research (FAIR)  129\\nFalse Positive Rate (FPR)  458\\nFast Attention Via positive Orthogonal Random \\n(FAVOR)  196\\nfastText  117\\nfeature clustering  378\\nfeature construction  447\\nfeature map\\nproperties  273\\nfeature mapping  448\\nfeatures  4\\nfeature selection  447\\nfederated core (FC)  599\\nFederated Learning (FL)  599\\narchitecture  598\\nissues  598\\noverview  597, 598\\nTensorFlow FL APIs  599\\nFeedForward Network (FFN)  533\\nFigureQA dataset\\nreference link  624\\nFlatBuffers  586\\nreference link  586\\nFloating-Point Unit (FPU)  502\\nflow-based models\\nusing, for data generation  356-358\\nFrechet Inception Distance (FID)  358, 372\\nfunction approximator  49\\nfuzzy clustering  270', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3104fee8-f085-49f2-bad2-2aab697fd3c2', embedding=None, metadata={'page_label': '656', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 656\\nG\\nGAN architectures  339\\nCycleGAN  340-342\\nInfoGAN  342-344\\nSRGAN  339, 340\\nGated Recurrent Units (GRUs)  494\\nGaussian Mixture Model (GMM)  569\\nGazebo\\nreference link  397\\nGeneral Language Understanding Evaluation \\n(GLUE)  252\\ncomponents  252\\nreference link  252\\nGenerative Adversarial Network  \\n(GAN)  322, 323, 372\\napplications  344-348\\nbuilding, with MNIST in TensorFlow  324-329\\nGenerative Pre-Trained (GPT) model  205\\nGenerative Pre-trained Transformer (GPT-3) \\nmodel  365, 517, 518\\nkey intuitions  209\\nGenerative Pretraining (GPT)  133\\nGenetic Programming (GP)  449\\nGensim\\nembedding space, exploring with  111-113\\ninstallation link  110\\nused, for creating embeddings  110, 111\\nGlobal vectors for word representation  \\n(GloVe)  109, 110, 117\\ndownload link  110\\nGLUE  252, 253\\nGoogle Cloud AutoML  451\\nreference link  451\\ntables solution, using  451-462\\ntext solution, using  463- 466\\ntraining cost  470\\nvideo solution, using  466-470Google Colab\\nplaying with  33-35\\nURL  33\\nGPT-2\\nkey intuitions  209\\ngradient descent (GD)  22, 483\\ngradients  474-476\\nGradle\\nURL  591\\ngraph\\nbasics  532\\nconvolutions  533, 534\\ncustomizations  551\\ngraph classification  541-545\\nlink prediction  545-551\\nmachine learning  532\\nlink prediction  545-551\\nnode classification  537-541\\nGraph Attention Network (GAT)  535\\ngraph customizations  551\\ncustom layers  551\\nmessage-passing mechanism  551-554\\nGraphic Processing Units (GPUs)  134, 499-500\\nGraph Isomorphism Network (GIN)  536, 537\\ngraph layers  534\\nGraph Attention Network (GAT)  535\\nGraph Convolution Network (GCN)  535\\nGraphSAGE  536\\ngreedy search  154\\ngrid search  450\\ngRPC Remote Procedure Calls (gRPC)  509\\nH\\nH2O\\nreference link  523\\nusing, for AutoML  523, 524, 525\\nH2O.ai  522, 523', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b17a4c7-6c93-49d3-b938-89af218a252f', embedding=None, metadata={'page_label': '657', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 657\\nH2O AutoML  523\\nH2O model, explain module  526\\nmodel correlation  528\\nPartial Dependence Plots (PDP)  526\\nvariable importance heatmap  527\\nhandwritten digits\\nbaseline, establishing  15, 16\\nexperiments, summarizing  31\\nnet, improving in TensorFlow with  \\ndropout  19-21\\nnet, improving in TensorFlow with hidden \\nlayers  16-19\\nneural net, defining in TensorFlow  11-15\\nnumber of epochs, increasing  27\\nnumber of internal hidden neurons,  \\nincreasing  28-30\\none hot-encoding (OHE)  10\\noptimizer learning rate, controlling  28\\noptimizers, testing in TensorFlow  22-27\\nrecognizing  10\\nreconstructing, with vanilla  \\nautoencoders  292-295\\nsimple neural net, defining in  \\nTensorFlow  13, 14\\nsize of batch computation, increasing  30\\nTensorFlow net, running  15, 16\\nhard clustering  270\\nhard negatives  374\\nhard update  395\\nheterogeneous graphs  560\\nhidden layers\\nused, for improving net in TensorFlow  16-19\\nHighwaysNet  96\\nHugging Face  242, 515-517\\nautotokenization  244\\nfeatures  242\\nfine-tuning  248, 249\\nmodel, autoselecting with  244\\nnamed entity recognition, performing  245\\nsummarization  246, 247used, for text generation  242-244\\nusing  242\\nhybrid self-prediction models  370\\nDALL-E  371, 372\\nJukebox  371\\nVQ-GAN  372\\nVQ-VAE  371\\nhyperparameters  38\\ntuning  38, 39, 448, 449\\nI\\nidentity block  96\\nimage classification  592, 593\\nImage GPT (IPT) AR model  364\\nImportance Weight Sampling\\nreference link  566\\nInception V3\\nfor transfer learning  93-95\\nindependent and dependent variables in \\nMachine Learning\\nreference link  44\\nindependent variable  44\\nInfoGAN  342, 343\\nInfoNCE loss  375\\nInformation Retrieval (IR)  104\\ninnate relationship prediction  369\\njigsaw puzzles, solving  370\\nrelative position  369\\nrotation  370\\ninput features  4\\ninstance transformation  376\\nBarlow Twins model  377, 378\\nBootstrap Your Own Latent (BYOL) model  378\\nDeepCluster  379\\nfeature clustering  378\\nInterCLR model  379, 380\\nSimCLR model  376', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c43d37d1-9061-49c2-8a9a-da146c617faa', embedding=None, metadata={'page_label': '658', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 658\\nSWapping Assignments between multiple \\nViews (SwAV) model  379\\nInterCLR model  379, 380\\nInternet of Things (IoT)  506\\nItem2Vec embedding model  122\\nJ\\nJacobian matrix  494\\nJava Caffe\\nURL  88\\njigsaw puzzles\\nsolving  370\\nJukebox  371\\nK\\nKaggle VQA challenge\\nreference link  624\\nKeras  3\\nKeras applications  98, 621\\nKeras initializer, 5\\nKeras MNIST TPU, end-to-end training  510\\nkernel  66\\nk-means clustering  266\\nimplementing, in TensorFlow  268-270\\nvariations  270, 271\\nworking  266\\nKohonen networks  271\\nKullback-Leiber (KL) divergence  296\\nL\\nL1 regularization   32\\nL2 regularization   32\\nlabel smoothing  326\\nLaMDA  219\\nkey intuitions  219, 220\\nlanguage model-based embedding  132, 133Language Modeling (LM)  207\\nLASSO  32\\nlatent loss  315\\nLatent Semantic Analysis (LSA)  104\\nlatent space  315\\nleaderboard  523\\nLeakyReLU  9\\nlearning rate  22\\nlearning with a critic  390\\nleft singular matrix  262\\nLeNet  69\\ndefining, in TF  70-77\\nlifted structured loss  375\\nlinear regression  44\\nmultiple linear regression  48\\nmultivariate linear regression  49\\nneural networks  49\\nsimple linear regression  45-47\\nused, for prediction  44\\nlogistic regression  59\\napplying, on MNIST dataset  60-64\\nreference link  60\\nLong Short-Term Memory (LSTM)  130, 194, 494\\nloss functions  13, 373\\nreference link  13\\nloss minimization  13\\nLSTM-based autoencoder\\nbuilding, to generate sentence vectors  306-314\\nM\\nMachine Learning (ML)  532\\nMagenta  638\\nMagenta NSynth  637\\nMalmo\\nreference link  397', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25059e10-e619-4ba2-a2de-643cf7fa73cc', embedding=None, metadata={'page_label': '659', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 659\\nmany-to-many topology\\nPOS tagging  163-172\\nmany-to-one topology\\nsentiment analysis  157-163\\nMarkov Decision Process (MDP)  393\\nMarkov property  392\\nMasked Autoencoder for Distribution \\nEstimation (MADE)  358\\nmasked generation models  366\\nBidirectional Encoder Representation from \\nTransformers (BERT)  366, 367\\ncolorization  369\\ncontext autoencoder  367, 368\\nstacked denoising autoencoder (AE)  367\\nMasked Language Modeling (MLM)  207, 366\\nmathematical tools\\nchain rule  477\\nderivatives and gradients  474-476\\ndifferentiation rules  477\\ngradient descent  476\\nmatrix operations  478\\nvectors  474\\nmatrix factorization  109\\nmatrix operations  478\\nmax pooling operator  68\\nMean Opinion Score (MOS)  635\\nmean squared error (MSE)  288\\nmessage function  551\\nmessage-passing mechanism  535-554\\nMessage Passing Neural Network (MPNN)  551\\nmethod of least squares  45\\nmetrics  14\\nreference link  13, 14\\nMini-Batch Gradient Descent (MBGD)  491\\nMLOps\\nreference link  257MNIST\\nused, for building GAN in TensorFlow  324-329\\nMNIST dataset\\nlogistic regression, applying  60-64\\nPCA, implementing on  262-264\\nmobile neural architecture search (MNAS)  593\\nmobile optimized interpreter  586\\nmodel best practices\\nAutoML  570\\nbaseline model  569\\nevaluation and validation  570\\nimprovements  571, 572\\nmodel evaluation and validation\\nmodel deltas, using  570\\npatterns, searching in measured errors  571\\nunseen data, testing  571\\nuser experience techniques  570\\nutilitarian power  570\\nmodel evaluation approaches\\nfew-shot learning  210\\none-shot learning  210\\nzero-shot learning  210\\nmodel-free reinforcement learning  392\\nmodel generation  448\\nmodel improvements\\ndata drift  571\\ntraining-serving skew  571, 572\\nmodel of the environment  392\\nmse, objective functions  13\\nmulti-head (self-)attention  198\\nmulti-layer perceptron (MLP)  5, 288\\nactivation functions  9\\nexample  5\\nExponential Linear Unit (ELU)  8\\nLeakyReLU  9\\nperceptron and solution, training problems  6\\nReLU activation function  7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa91bb93-c0c0-49c3-a2bc-31c5d240ae82', embedding=None, metadata={'page_label': '660', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Index 660\\nsigmoid activation function  7\\ntanh activation function  7\\nmultimodal models  381\\nCLIP  381, 382\\nCodeSearchNet  383\\nData2Vec  383\\nmultiple linear regression  48\\nexploring, with TensorFlow Keras API  53-58\\nmultiplicative( Luong's) attention  183, 184\\nMultitask Unified Model (MUM)  216\\nreference link  216\\nmultivariate linear regression  49\\nexploring, with TensorFlow Keras API  53-58\\nmultivariate normal distribution  433, 434\\nmultiview coding  380\\nAugmented Multiscale Deep InfoMax  \\n(AMDIM)  380\\nContrastive Multiview Coding (CMC)  381\\nMuseNet  638, 639\\nreference link  638\\nMXNet\\nURL  2\\nN\\nNamed Entity Recognition (NER)  245\\nNatural Language Generation (NLG)  205\\nNatural Language Processing  \\n(NLP)  104, 150, 563\\nnegative sampling  108\\nnetwork\\ninspections, performing  629, 630\\nNeural Architecture Search (NAS)  217\\nneural embeddings  122\\nItem2Vec  122\\nnode2vec  123-128\\nNeural Machine Translation (NMT)  195neural networks  3, 10\\ndefining, in TensorFlow  11, 12\\nfor linear regression  49\\nneurons  3\\nNext Sentence Prediction (NSP)  366\\nNLP-progress  255\\nreference link  255\\nnode2vec embedding model  123-128\\nNode.js\\nTensorFlow.js, using with  610\\nnodes  532\\nNoise Contrastive Estimation (NCE) loss  375\\nNon-linear Independent Components \\nEstimation (NICE)  358\\nnormal distribution  431\\nmultivariate normal distribution  433, 434\\nunivariate normal distribution  431-433\\nnormalization\\nbatch normalization  33\\nN-pair loss  374\\nNSynth  637\\nO\\nobjective function  13\\none-dimensional Convolutional Neural Network \\n(1D CNN)  118\\none hot-encoding (OHE)  10\\none-to-many topology\\ntext generation  150-156\\nOpenAI  517\\nOpenAI Codex  519, 520\\nOpenAI DALL-E 2  518, 519\\nOpenAI GPT-3 API  517\\nexamples, reference link  36\\nreference link  517\\ntasks  518\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='015d73ce-10ee-43ff-90e6-c8346ed2d7e8', embedding=None, metadata={'page_label': '661', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 661\\nOpenAI Gym  397-401\\nrandom agent, playing Breakout game  401-403\\nsupported environments  398\\nwrappers  403-405\\nOpen Neural Network Exchange (ONNX)  522\\noptimizer learning rate\\ncontrolling  28\\noptimizers  12\\nreference link  13\\ntesting, in TensorFlow  22- 27\\nOptim Module  522\\nout of vocabulary (OOV)  308\\noutput\\npredicting  39\\noverfitting  31\\nP\\nparagraph embedding  131, 132\\nParagraph Vectors - Distributed Bag of Words \\n(PV-DBOW)  132\\nParagraph Vectors - Distributed Memory  \\n(PV-DM)  132\\nparametric TTS  635\\nparaphrase database (PPDB)  117\\nPartial Dependence Plots (PDP)  526\\nPart-of-Speech (POS)  150\\nanalysis  631\\ntagging  163-172\\nPathways Language Model (PaLM)  223\\npeephole LSTM  147\\nperceptron  3, 4\\nPermuted Language Modeling (PLM)  207\\nPixel Neural Core  506\\nPixelRNN AR model  364\\npolicy  392pooling layers  68\\naverage pooling  69\\nmax pooling  68\\nreference link  69\\npositional encoding  195\\nposterior probabilities  437\\npre-built deep learning models\\nrecycling, for feature extraction  91, 92\\nprecision  13\\nprediction  58\\nlinear regression, using with  44\\npretext tasks  384\\npretrained models  570\\npretrained models, TensorFlow Lite  591, 592\\naudio speech synthesis  591\\nimage classification  591, 592\\nlarge language models  596\\nmobile GPUs  596\\nobject detection  591, 594\\npose estimation  594\\nquestion and answer  591\\nsegmentation  594\\nsegmentations  591\\nsmart reply  594\\nstyle transfer  594\\nstyle transfers  591\\ntext classification  591, 595\\ntext embedding  591\\npretrained TPU models\\nusing  511-513\\npretraining  132, 206, 384\\ndecoder pretraining  206\\nencoder-decoder pretraining  206\\nencoder pretraining  206\\nprincipal component analysis (PCA)  261, 288\\nimplementing, on MNIST dataset  262-264', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1abbe848-b05c-489c-83d6-9f09caf22e92', embedding=None, metadata={'page_label': '662', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 662\\nprobabilistic neural networks\\nfor aleatory uncertainty  441, 442\\nusing, for aleatory uncertainty  440\\nprompt engineering  365\\nPyTorch  520\\nmodules  520\\nURL  2\\nPyTorch, modules\\nAutograd Module  521\\nNN Module  520\\nOptim Module  522\\nQ\\nquantization  585\\npost-training quantization  585\\nquantization-aware training  586\\nR\\nRainbow  418\\nrandom search  450\\nReAding Comprehension dataset from \\nExaminations (RACE)  255\\nReal-valued Non-Volume Preserving  \\n(RealNVP)  357\\nrecall  13\\nRecurrent Neural Network  \\n(RNN)  194, 362, 364 533\\nreduce function  551\\nReformer model  210\\nkey intuitions  210\\nRegion-based CNN (R-CNN)  617\\nRegion Of Interest (ROI)  617\\nRegion Proposal Network (RPN)  618\\nregression  43, 44\\nregression model\\nbuilding, with TensorFlow  439, 440regression task\\nversus classification task  58\\nregularization\\nadopting, to avoid overfitting  31, 32\\nused, in machine learning  32\\nregularizers\\nreference link  33\\nreinforcement learning (RL)  389, 390\\ngoal  389\\ninteraction, with environment  389\\nsimulation environments  396\\ntrial and error  389\\nrelative position prediction  369\\nReLU (REctified Linear Unit)  7\\nderivative  479\\nLeakyReLU  9\\nRemote Procedure Call (RPC)  509\\nresidual block  96\\nresidual networks  95\\nREST API\\nreference link  461\\nRestricted Boltzmann Machines (RBM)  278, 362\\nbackward pass operation  278\\ndeep belief networks (DBNs)  283\\nforward pass operation  278\\nhidden layer  278\\nimages, reconstructing with  279-283\\nvisible layer  278\\nRetrieval Database (DB)  222\\nRetrieval-Enhanced Transformer (RETRO)  222\\nkey intuitions  222\\nreturn  392\\nreward  391\\nRidge  32\\nright singular matrix  262\\nRNN cell  140-142\\nbackpropagation through time (BPTT)  142, 143\\ngradients, exploding  144', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05cca5fe-6355-4df5-aa61-50ce00fd2b85', embedding=None, metadata={'page_label': '663', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 663\\ngradients, vanishing  143\\nRNN cell variants  144\\ngated recurrent unit (GRU)  146\\nlong short-term memory (LSTM)  144-146\\npeephole LSTM  147\\nRNN topologies  149, 150\\nmany-to-many topology  163-171\\nmany-to-one topology  157-163\\nRNN variants  147\\nbidirectional RNNs  147, 148\\nstateful RNNs  148\\nRoBERTa  213\\nkey intuitions  213\\nRobot Operating System (ROS)  397\\nrotation\\nusing, as self-supervision signal  370\\nRotNet model  370\\nS\\nSavedModel  587\\nscaled dot-product attention  184\\nScheduled Sampling  178\\nscikit-learn  114\\nreference link  114\\nself-attention mechanism  198\\nSelf-Driving Car (SDC)  391\\nself-organizing maps (SOMs)  271, 272\\nimplementing  272, 273\\nused, for color mapping  273- 278\\nself-prediction  363\\nautoregressive (AR) generation  364\\nhybrid self-prediction models  370\\ninnate relationship prediction  369\\nmasked generation models  366\\nself-supervised learning  363\\nadvantages  363semi-supervised learning  287\\nSensibleness, Specificity, and Interestingness \\n(SSI)  220\\nsentence embedding  131, 132\\nsentiment analysis  36-38, 631\\nAutoML, tuning  38, 39\\nhyperparameters, tuning  38, 39\\nseparable convolution  640\\nseq2seq model  172\\nexample  173-182\\nusing, with Attention mechanism for machine \\ntranslation  184-189\\nSequential() model  4\\nshallow neural networks  278\\nShort Message Service (SMS)  114\\nsigmoid function  7\\nderivative  479\\nSimCLR model  376\\nsimple linear regression  45-47\\nbuilding, with TensorFlow Keras  49-53\\nsimulation environments, for RL\\nBlender learning environment  397\\nGazebo  397\\nMalmo  397\\nOpenAI Gym  397\\nUnity ML-Agents SDK  397\\nSingle Shot Detectors (SSD)  619\\nsingular value decomposition (SVD)  262\\nSkip-Gram with Negative Sampling (SGNS) \\nmodel  109\\nskip-thought vectors  131\\nsoft clustering  270\\nsoft nearest neighbors loss  376\\nsoft update  395\\nspam detection  114, 122, 631\\nSparkFun Edge\\nreference link  587', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cc722fe-905d-43ec-a2d5-68a6c3fd168d', embedding=None, metadata={'page_label': '664', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 664\\nsparse autoencoder  295-297\\nstacked autoencoder  301\\nstacked denoising autoencoder (AE)  367\\nStanford Question Answering Dataset  \\n(SQuAD)  254\\nstate  391\\nstateful RNNs  148\\nstate-value function  392\\nstatic embeddings  106\\nGloVe  109, 110\\nWord2Vec  106-109\\nSTM32F746 Discovery kit\\nreference link  587\\nStochastic Gradient Descent  \\n(SGD)  14, 23, 109, 473, 491\\nStructBERT  214\\nkey intuitions  214, 215\\nstyle transfer  99, 100\\ncontent distance  100\\nstyle distance  101\\nsubword embedding  128, 129\\nsum of squared error (SSE) distance  269\\nSuperGLUE  253, 254\\nSuper Resolution GANs (SRGANs)  339, 340\\nsupervised learning  10\\nsupport-vector machines (SVMs)\\nreference link  617\\nSWapping Assignments between multiple Views \\n(SwAV) model  379\\nSwitch Transformer  221\\nkey intuitions  222\\nsynthetic dataset\\ncreating  438, 439\\nT\\ntanh function  7\\nderivative  479taxonomy\\npretraining  207\\nTeacher Forcing  178\\ntechniques, for augmenting speech data\\nFrequency Masking  568\\nTime Masking  568\\ntime warping  568\\ntechniques, for augmenting textual data\\nback translation  567\\nsynonym replacement  567\\nTemporal Graphs  560, 561\\nTensorFlow (TF)  495\\nConvNets  68\\nCycleGAN, implementing  348-356\\nfeatures  2\\nGAN, building with MNIST  324-329\\nused, for building regression model  439, 440\\nTensorFlow Datasets (TFDS)  580\\ndata pipelines, building with  583-585\\nTensorFlow Embedding API  264-266\\nTensorFlow Federated (TTF)\\ndatasets  600\\nFederated core (FC)  599\\nFederated learning (FL)  599\\nTensorFlow FL APIs\\nbuilders  600\\nmodels  599\\nTensorFlow Hub  576, 621\\npretrained models, using for inference  577- 580\\nreference link  621\\nTensorFlow.js  600\\nmodels, converting  607\\npretrained models  607, 609\\nusing, with Node.js  610\\nVanilla TensorFlow.js  600-606\\nTensorFlow Keras\\nused, for building simple linear  \\nregression  49-53', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f66da407-af8b-4360-a33d-26e8cce0dbed', embedding=None, metadata={'page_label': '665', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 665\\nused, for exploring multiple linear  \\nregression  53-58\\nused, for exploring multivariate linear \\nregression  53-58\\nTensorFlow Keras layers \\ncustom layers, defining  290, 291\\nTensorFlow Lite  585\\narchitecture  587\\nexample  588, 589, 591\\nFlatBuffers  586\\nGPUs and accelerators, using  589\\nmobile converter  586\\nmobile optimized interpreter  586\\npretrained models  591\\nquantization  585\\nsupported platforms  587\\nusing  588\\nTensorFlow Probability (TFP)  423-427\\ndistributions  427\\nused, for handling uncertainty in  \\npredictions  437\\nTensorFlow Probability (TFP) distributions  427\\ncoin-flip example  428\\nnormal distribution  431\\nusing  428\\nTensor Processing Unit (TPU)  134, 500\\navailability, checking  509, 510\\nEdge TPU  506, 507\\nfirst generation  501-503\\nfourth generation  506\\ngenerations  501\\nperformance  507, 508\\nsecond generation  504\\nthird generation  505, 506\\nusing, with Colab  509\\nTerm Frequency-Inverse Document Frequency \\n(TF-IDF)  104\\nText-to-Speech (TTS)  635\\nText-to-Text Transfer Transformer (T5)\\nkey intuitions  215textual data\\naugmenting  567, 568\\nTFDS dataset\\nloading  581, 582, 583\\nTFHub  250\\nreference link  250\\nusing  250, 251\\ntf.Keras built-in VGG16 net module\\nutilizing  90, 91\\ntf.keras.datasets\\nreference link  11\\nTFLite Converter  587\\nTFLite FlatBuffer  587\\nTFLite interpreter  587\\nthought vector  131\\ntopic categorization  631\\ntraining objectives, CL models  373\\ncontrastive loss  374\\nInfoNCE loss  375\\nlifted structured loss  375\\nNoise Contrastive Estimation (NCE) loss  375\\nN-pair loss  374\\nsoft nearest neighbors loss  376\\ntriplet loss  374\\ntransfer learning\\nInception V3  93-95\\ntransformer categories\\ndecoder or autoregressive  205\\nencoder or autoencoding  205\\nmultimodal  205\\nretrieval  205\\nseq2seq  205\\ntransformers\\narchitecture  194-204\\narchitectures  204\\nattention mechanism  205\\nbest practices  258, 259\\ncategories  204', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11a856ef-f782-489b-a232-14b81b9b9e72', embedding=None, metadata={'page_label': '666', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 666\\ncost of serving  257\\nevaluating  252\\nfuture  259\\nimplementations  223\\nnormalization layer  200\\noptimization  257\\npitfalls  259\\nquality, measuring  252\\nreference implementation  224-242\\nresidual layers  200\\nsize, measuring  256, 257\\ntraining, via semi-supervised learning  204\\ntransformers optimization  257\\nknowledge distillation  258\\nquantization  257\\nweight pruning  257\\nTransformer-XL  212\\nkey intuitions  212\\ntransposed convolution  640\\ntriplet loss  374\\nTrue Positive Rate (TPR)  458\\nU\\nUCI ML repository\\nreference link  53\\nuncertainty, in predictions\\naleatory uncertainty  438\\nepistemic uncertainty  438\\nhandling, with TensorFlow Probability  437\\nsynthetic dataset, creating  438, 439\\nunderfitting  32\\nU-Net\\nreference link  615\\nunivariate normal distribution  431-433\\nUniversal Language Model Fine-Tuning \\n(ULMFiT) model  133\\nupdate function  551V\\nvalue function  392\\nvanilla autoencoders  289, 290\\nhandwritten digits, reconstructing  \\nwith  292-295\\nTensorFlow Keras layers  290, 291\\nVanilla TensorFlow.js  600-606\\nvariational autoencoders  314-319, 371\\nvectorization  104\\nVector Processing Unit (VPU)  504\\nVector Quantized Variational  \\nAutoEncoder (VQ-VAE)  371\\nvectors  474\\nVertex AI  451\\nVGG16 net\\ncats, recognizing with  90\\nvideos\\nclassifying, with pretrained nets  630\\nvision transformers (ViTs)  259\\nVisual Question Answering (VQA)  622-625\\nreference link  622\\nURL  622\\nvocabulary  158\\nVQ-GAN  372\\nW\\nWaveNet  366, 636, 637\\nreference link  637\\nWaveRNN  366\\nweight pruning\\nreference link  258\\nWinner-Take-All Units (WTUs)  271\\nWord2Vec  106-109, 117\\nalgorithm  384\\nCBOW architecture  107, 108', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52ffff89-5a29-4924-b1f8-4d206f8458bf', embedding=None, metadata={'page_label': '667', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 667\\nreference link  109\\nskip-gram architecture  107, 108\\nword embedding, used for spam detection  114\\ndata, obtaining  115\\ndata, processing  115, 116\\nembedding matrix, building  117, 118\\nmodel, evaluating  120\\nmodel, training  120\\nspam classifier, defining  118\\nspam detector, running  121, 122\\nwrappers  403\\nX\\nXception  97-99\\nXLNet  213, 365\\nkey intuitions  213\\nY\\nYou Only Look Once (YOLO)\\nreference link  619', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='887847f1-cb58-4b61-9aea-95c4ff93d247', embedding=None, metadata={'page_label': '668', 'file_name': 'Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Amita Kapoor, Antonio Gulli, Sujit Pal - Deep Learning with TensorFlow and Keras_ Build and deploy supervised, unsupervised, deep, and reinforcement learning models, 3rd Edition-Packt Publishing (2022.pdf', 'file_type': 'application/pdf', 'file_size': 25590612, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='850f2e10-db2a-4290-be15-cdcfbdaa97e9', embedding=None, metadata={'page_label': 'Cover', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='499ca1c8-16a8-4781-84d2-507f52036207', embedding=None, metadata={'page_label': 'FM-1', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Natural Language Processing \\nwith TensorFlow\\nSecond Edition\\nThe definitive NLP book to implement the most sought-after machine \\nlearning models and tasks\\nThushan Ganegedara\\nBIRMINGHAM—MUMBAI', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d946c1e8-7e25-4030-8a22-f36256d392e5', embedding=None, metadata={'page_label': 'FM-2', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Natural Language Processing with TensorFlow\\nSecond Edition\\nCopyright © 2022 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \\nany form or by any means, without the prior written permission of the publisher, except in the case of brief \\nquotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \\npresented. However, the information contained in this book is sold without warranty, either express or \\nimplied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any \\ndamages caused or alleged to have been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \\nthe accuracy of this information.\\nSenior Publishing Product Manager: Tushar Gupta\\nAcquisition Editor – Peer Reviews: Saby Dsilva\\nProject Editor:  Parvathy Nair\\nContent Development Editor:  Georgia Daisy van der Post\\nCopy Editor:  Safis Editing\\nTechnical Editor:  Tejas Mhasvekar\\nProofreader:  Safis Editing\\nIndexer: Subalakshmi Govindhan\\nPresentation Designer:  Rajesh Shirsath\\nFirst published: May 2018\\nSecond edition: July 2022\\nProduction reference: 1260722\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-83864-135-1\\nwww.packt.com', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='763e6e47-dbee-4072-bfb8-248111f6f1c0', embedding=None, metadata={'page_label': 'FM-3', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword\\nThis book addresses the important need for describing how Natural Language Processing (NLP) \\nproblems can be solved using TensorFlow-based NLP stacks.\\nDeep Learning revolutionized NLP recently. Many industrial and academic NLP problems that \\nrequired a large amount of work in terms of designing new features, tuning models, and finding \\nthe best modeling approach (CRF, SVM, Bayesian methods, etc.) can now be solved by NLP sci -\\nentists in a significantly smaller amount of time. Also, the new deep-learning-based methods \\ntypically produce much more accurate models than traditional NLP methods. The big problem is \\nhow to make these models work in a modern production setting with operational parameters such \\nas latency and throughput, cloud costs, and operational quality (uptime, etc.). The TensorFlow \\nenvironment is designed to solve these problems when running NLP models.\\nIn this book, the author teaches the fundamentals of TensorFlow and Keras, a Python-based \\ninterface for TensorFlow. Then, the bulk of the book, from Chapter 3, Word2vec – Learning Word \\nEmbeddings,  onward, is focused on NLP problems and solving them using TensorFlow.\\nThis book provides:\\n• A knowledge of NLP methods in good detail, from their definition to various evaluation \\nmethods\\n• Information about TensorFlow, Keras, and Hugging Face libraries, which are powerful \\ntools to build NLP solutions\\n• An understanding of neural architectures, which is important to build better models, by \\nbuilding architectures for specific tasks that the reader will encounter in their practice.\\nThe author describes the process of building embeddings and other vector representations that are \\nthe basis of most modern deep learning NLP methods. The author also describes popular Neural \\nNetwork architectures, such as Recurrent Neural Networks, Convolutional Neural Networks, Long \\nShort-Term Memory networks, and Transformer-based architectures, in detail and shows their \\napplication in solving various NLP tasks, such as sentence classification, named entity recognition, \\ntext generation, machine translation, image caption generation, and more.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b1ae5ad-fe8c-4ebf-8aa6-3450647ae658', embedding=None, metadata={'page_label': 'FM-4', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In each chapter, the author provides a deep dive into the neural network architecture, with an \\nexplanation of why this architecture works; the nature of the NLP problem and why it is an im -\\nportant NLP task; and how the solution to the problem is evaluated. Such deep dives will help \\nreaders to address industrial tasks that are reducible to these NLP problems, and to solve other \\nNLP problems through understanding how typical NLP problems are evaluated. These deep dives \\nwill also help provide the reader with the knowledge to modify and improve necessary network \\narchitectures for particular practical tasks. The author also provides a detailed, step-by-step \\ndescription of how such models are trained in a TensorFlow/Keras environment.\\nAt the end, the author writes about Transformers, the modern state-of-the-art method to solve \\nNLP problems, with a focus on BERT (a popular transformer method developed by Google). The \\nauthor provides exercises on how BERT can be used for practical tasks such as answering ques-\\ntions, but the explanations of BERT will also help to solve other tasks with BERT-based networks. \\nThe author also dives into Hugging Face, a popular software library for transformer-based NLP \\nsolutions.\\nAll of this content makes this book invaluable for practitioners who want to learn how to build \\nTensorFlow-based solutions for NLP problems.\\nAndrei Lopatenko\\nVP Engineering and Head of Search & NLP at Zillow', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7e96356-fd43-4ffe-a0fd-8dc6ba054df2', embedding=None, metadata={'page_label': 'FM-5', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contributors\\nAbout the author\\nThushan Ganegedara  is a Senior Machine Learning engineer at Canva, an Australian tech -\\nnology unicorn that’s democratizing graphic designing and visualizations.. Thushan works with \\nlarge-scale visual and text data, in order to build and deploy Machine Learning models to make \\nproducts smarter. Before this, Thushan worked as a Senior Data Scientist at QBE Insurance, help -\\ning to solve business problems and make claim processing more efficient using machine learning. \\nThushan has a PhD from the University of Sydney specializing in Deep Learning.\\nI would like to acknowledge my parents and my wife, Thushani, for all the support and encouragement \\nprovided during the development of this book.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da057768-7f8c-427c-861c-fc0e3198d715', embedding=None, metadata={'page_label': 'FM-6', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='About the reviewers\\nArman Cohan  is a Research Scientist at the Allen Institute for AI (AI2). His broad research interest \\nis in developing Natural Language Processing methods for addressing information overload. This \\nincludes language models for complex document and multi-document tasks, natural language \\ngeneration and summarization, and information discovery and filtering. His research has been \\nrecognized with multiple awards from leading conferences in the field, including a best paper \\naward at EMNLP 2017, an honorable mention at COLING 2018, and the 2019 Harold N. Glassman \\nDistinguished Doctoral Dissertation award.\\nPratik Kotian  is a Senior Conversation AI engineer with six years of experience in building \\nconversational AI agents and designing products related to conversational design. He is working \\nas a Senior Conversation Bot Engineer (specializing in conversational AI) at Quantiphi, which is \\nan AI company and recognized Google Partner. He has also worked with Packt on reviewing The \\nTensorFlow Workshop  and Conversational AI with Rasa .\\nI would like to thank my family and friends, who are always supportive and have always believed in me and \\nmy talents. It’s because of them that I am doing well in my career. And lastly, I would like to thank all the \\nreaders of this book: you are definitely going to learn a lot about recent developments in NLP and TensorFlow \\nfrom this book.\\nTaeuk Kim  is an assistant professor at the Department of Computer Science, Hanyang Univer -\\nsity. Before joining Hanyang University, he received his Bachelor of Science and PhD from Seoul \\nNational University. His expertise lies in the field of Natural Language Processing, where he has \\nbeen making contributions as an active researcher and a program committee member for related \\ntop-tier conferences including ACL and EMNLP.\\nDr Pham Quang Nhat Minh  is the head of Multimodal AI Lab of Aimesoft JSC, Vietnam. His \\nresearch field is Language and Speech Processing. Dr Minh has more than fifteen years of experi -\\nence working in the Natural Language Processing and Machine Learning fields in both academia \\nand industry. He obtained a Bachelor’s degree at VNU University of Engineering and Technology \\nin 2006. He obtained a Master’s degree in Information Science in 2010 at Japan Advanced In -\\nstitute of Science and Technology (JAIST) and a PhD in Information Science in 2013 at the same \\nschool. He has published several research papers in the NLP field. Currently, he is working in the \\nfield of law text processing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf83ea0a-164f-4897-897b-ab1338ecccf6', embedding=None, metadata={'page_label': 'i', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents\\nPreface   xix\\nChapter 1: Introduction to Natural Language Processing   1\\nWhat is Natural Language Processing?  ��������������������������������������������������������������������������������  2\\nTasks of Natural Language Processing  ��������������������������������������������������������������������������������  3\\nThe traditional approach to Natural Language Processing  ��������������������������������������������������  5\\nUnderstanding the traditional approach • 5\\nExample – generating football game summaries • 7\\nDrawbacks of the traditional approach • 10\\nThe deep learning approach to Natural Language Processing  ��������������������������������������������  11\\nHistory of deep learning • 12\\nThe current state of deep learning and NLP • 13\\nUnderstanding a simple deep model – a fully connected neural network • 16\\nIntroduction to the technical tools  �����������������������������������������������������������������������������������  18\\nDescription of the tools • 18\\nInstalling Anaconda and Python • 18\\nCreating a Conda environment • 19\\nTensorFlow (GPU) software requirements • 19\\nAccessing Jupyter Notebook • 19\\nVerifying the TensorFlow installation • 20\\nSummary  ���������������������������������������������������������������������������������������������������������������������������  21\\nChapter 2: Understanding TensorFlow 2   23\\nWhat is TensorFlow?  ���������������������������������������������������������������������������������������������������������  24\\nGetting started with TensorFlow 2 • 24\\nTensorFlow 2 architecture – What happens during graph build? • 29', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2bf31532-16b2-4522-847e-be0a993ad008', embedding=None, metadata={'page_label': 'ii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents viii\\nTensorFlow architecture – what happens when you execute the graph? • 31\\nCafé Le TensorFlow 2 – understanding TensorFlow 2 with an analogy • 33\\nFlashback: TensorFlow 1 • 35\\nInputs, variables, outputs, and operations  ������������������������������������������������������������������������  38\\nDefining inputs in TensorFlow • 39\\nFeeding data as NumPy arrays • 39\\nFeeding data as tensors • 39\\nBuilding a data pipeline using the tf.data API • 40\\nDefining variables in TensorFlow • 43\\nDefining outputs in TensorFlow • 45\\nDefining operations in TensorFlow • 45\\nComparison operations • 45\\nMathematical operations • 46\\nUpdating (scattering) values in tensors • 47\\nCollecting (gathering) values from a tensor • 49\\nNeural network-related operations • 49\\nNonlinear activations used by neural networks • 49\\nThe convolution operation • 51\\nThe pooling operation • 54\\nDefining loss • 56\\nKeras: The model building API of TensorFlow  �������������������������������������������������������������������  56\\nSequential API • 57\\nFunctional API • 58\\nSub-classing API • 58\\nImplementing our first neural network  ����������������������������������������������������������������������������  59\\nPreparing the data • 60\\nImplementing the neural network with Keras • 61\\nTraining the model • 62\\nTesting the model • 63\\nSummary  ��������������������������������������������������������������������������������������������������������������������������  64', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b375d0d3-b1fe-4f92-b10e-f97809d95a64', embedding=None, metadata={'page_label': 'iii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents ix\\nChapter 3: Word2vec – Learning Word Embeddings   67\\nWhat is a word representation or meaning?  ����������������������������������������������������������������������  69\\nClassical approaches to learning word representation  ������������������������������������������������������  70\\nOne-hot encoded representation • 70\\nThe TF-IDF method • 71\\nCo-occurrence matrix • 72\\nAn intuitive understanding of Word2vec – an approach to learning word representation   73\\nExercise: does queen = king – he + she? • 74\\nThe skip-gram algorithm  ��������������������������������������������������������������������������������������������������  78\\nFrom raw text to semi-structured text • 78\\nUnderstanding the skip-gram algorithm • 79\\nImplementing and running the skip-gram algorithm with TensorFlow • 82\\nImplementing the data generators with TensorFlow • 83\\nImplementing the skip-gram architecture with TensorFlow • 95\\nTraining and evaluating the model • 99\\nThe Continuous Bag-of-Words algorithm  �����������������������������������������������������������������������  102\\nGenerating data for the CBOW algorithm • 103\\nImplementing CBOW in TensorFlow • 104\\nTraining and evaluating the model • 106\\nSummary  ������������������������������������������������������������������������������������������������������������������������  108\\nChapter 4: Advanced Word Vector Algorithms   111\\nGloVe – Global Vectors representation  �����������������������������������������������������������������������������  112\\nUnderstanding GloVe • 114\\nImplementing GloVe • 116\\nGenerating data for GloVe • 120\\nTraining and evaluating GloVe • 123\\nELMo – Taking ambiguities out of word vectors  ��������������������������������������������������������������  126\\nDownloading ELMo from TensorFlow Hub • 128\\nPreparing inputs for ELMo • 129', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='066a0167-f3d6-4202-bb5c-7148c96a0800', embedding=None, metadata={'page_label': 'iv', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents x\\nGenerating embeddings with ELMo • 132\\nDocument classification with ELMo  ��������������������������������������������������������������������������������  134\\nDataset • 135\\nGenerating document embeddings • 138\\nClassifying documents with document embeddings • 142\\nSummary  �������������������������������������������������������������������������������������������������������������������������  145\\nChapter 5: Sentence Classification with Convolutional Neural Networks   147\\nIntroducing CNNs  �����������������������������������������������������������������������������������������������������������  148\\nCNN fundamentals • 148\\nThe power of CNNs • 151\\nUnderstanding CNNs  �������������������������������������������������������������������������������������������������������  151\\nConvolution operation • 151\\nStandard convolution operation • 152\\nConvolving with stride • 152\\nConvolving with padding • 153\\nTransposed convolution • 154\\nPooling operation • 155\\nMax pooling • 156\\nMax pooling with stride • 156\\nAverage pooling • 157\\nFully connected layers • 158\\nPutting everything together • 158\\nExercise – image classification on Fashion-MNIST with CNN  ������������������������������������������  159\\nAbout the data • 160\\nDownloading and exploring the data • 160\\nImplementing the CNN • 162\\nAnalyzing the predictions produced with a CNN • 167\\nUsing CNNs for sentence classification  ���������������������������������������������������������������������������  168\\nHow data is transformed for sentence classification • 169\\nImplementation – downloading and preparing data • 171', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4370d4c-7859-4fda-b829-d595c7b18ece', embedding=None, metadata={'page_label': 'v', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xi\\nImplementation – building a tokenizer • 176\\nThe sentence classification CNN model • 177\\nThe convolution operation • 177\\nPooling over time • 180\\nImplementation – sentence classification with CNNs • 182\\nTraining the model  • 186\\nSummary  ������������������������������������������������������������������������������������������������������������������������  188\\nChapter 6: Recurrent Neural Networks   191\\nUnderstanding RNNs  �������������������������������������������������������������������������������������������������������  192\\nThe problem with feed-forward neural networks • 193\\nModeling with RNNs • 194\\nTechnical description of an RNN • 196\\nBackpropagation Through Time  ��������������������������������������������������������������������������������������  197\\nHow backpropagation works • 197\\nWhy we cannot use BP directly for RNNs • 199\\nBackpropagation Through Time – training RNNs • 200\\nTruncated BPTT – training RNNs efficiently • 200\\nLimitations of BPTT – vanishing and exploding gradients • 201\\nApplications of RNNs  ������������������������������������������������������������������������������������������������������  203\\nOne-to-one RNNs • 203\\nOne-to-many RNNs • 203\\nMany-to-one RNNs • 204\\nMany-to-many RNNs • 205\\nNamed Entity Recognition with RNNs  ����������������������������������������������������������������������������  206\\nUnderstanding the data • 206\\nProcessing data • 212\\nDefining hyperparameters • 215\\nDefining the model • 216\\nIntroduction to the TextVectorization layer • 217\\nDefining the rest of the model • 219', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3d28ad0-f4a8-4af0-b43f-141334931325', embedding=None, metadata={'page_label': 'vi', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xii\\nEvaluation metrics and the loss function • 223\\nTraining and evaluating RNN on NER task • 226\\nVisually analyzing outputs • 229\\nNER with character and token embeddings  ���������������������������������������������������������������������  231\\nUsing convolution to generate token embeddings • 231\\nImplementing the new NER model • 234\\nDefining hyperparameters • 235\\nDefining the input layer • 235\\nDefining the token-based TextVectorization layer • 235\\nDefining the character-based TextVectorization layer • 235\\nProcessing the inputs for the char_vectorize_layer • 235\\nPerforming convolution on the character embeddings • 237\\nModel training and evaluation • 239\\nOther improvements you can make • 239\\nSummary  ������������������������������������������������������������������������������������������������������������������������  240\\nChapter 7: Understanding Long Short-Term Memory Networks   243\\nUnderstanding Long Short-Term Memory Networks  ������������������������������������������������������  244\\nWhat is an LSTM? • 245\\nLSTMs in more detail • 246\\nHow LSTMs differ from standard RNNs • 255\\nHow LSTMs solve the vanishing gradient problem  ���������������������������������������������������������  256\\nImproving LSTMs  �����������������������������������������������������������������������������������������������������������  259\\nGreedy sampling • 259\\nBeam search • 260\\nUsing word vectors • 261\\nBidirectional LSTMs (BiLSTMs) • 263\\nOther variants of LSTMs  �������������������������������������������������������������������������������������������������  265\\nPeephole connections • 265\\nGated Recurrent Units • 266\\nSummary  ������������������������������������������������������������������������������������������������������������������������  268', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a738553-b619-49ad-8ba1-4d60734e32f5', embedding=None, metadata={'page_label': 'vii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xiii\\nChapter 8: Applications of LSTM – Generating Text   271\\nOur data  ��������������������������������������������������������������������������������������������������������������������������  272\\nAbout the dataset • 272\\nGenerating training, validation, and test sets • 274\\nAnalyzing the vocabulary size • 275\\nDefining the tf.data pipeline • 276\\nImplementing the language model  ���������������������������������������������������������������������������������  282\\nDefining the TextVectorization layer • 283\\nDefining the LSTM model • 284\\nDefining metrics and compiling the model • 286\\nTraining the model • 288\\nDefining the inference model • 288\\nGenerating new text with the model • 292\\nComparing LSTMs to LSTMs with peephole connections and GRUs  �������������������������������  295\\nStandard LSTM • 295\\nReview • 295\\nGated Recurrent Units (GRUs) • 296\\nReview • 296\\nThe model • 297\\nLSTMs with peepholes • 298\\nReview • 298\\nThe code • 299\\nTraining and validation perplexities over time • 300\\nImproving sequential models – beam search  ������������������������������������������������������������������  301\\nImplementing beam search • 302\\nGenerating text with beam search • 304\\nImproving LSTMs – generating text with words instead of n-grams  �������������������������������  305\\nThe curse of dimensionality • 306\\nWord2vec to the rescue • 306\\nGenerating text with Word2vec • 306', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d3186dfc-e0a6-4b71-b0d9-2a7f9e02867b', embedding=None, metadata={'page_label': 'viii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xiv\\nSummary  ������������������������������������������������������������������������������������������������������������������������  308\\nChapter 9: Sequence-to-Sequence Learning – Neural Machine Translation   311\\nMachine translation ���������������������������������������������������������������������������������������������������������  312\\nA brief historical tour of machine translation  ������������������������������������������������������������������  313\\nRule-based translation • 313\\nStatistical Machine Translation (SMT) • 315\\nNeural Machine Translation (NMT) • 316\\nUnderstanding neural machine translation  ��������������������������������������������������������������������  320\\nIntuition behind NMT systems • 320\\nNMT architecture • 320\\nThe embedding layer • 322\\nThe encoder • 322\\nThe context vector • 323\\nThe decoder • 323\\nPreparing data for the NMT system ���������������������������������������������������������������������������������  324\\nThe dataset • 324\\nAdding special tokens • 326\\nSplitting training, validation, and testing datasets • 326\\nDefining sequence lengths for the two languages • 327\\nPadding the sentences • 328\\nDefining the model  ���������������������������������������������������������������������������������������������������������  330\\nConverting tokens to IDs • 331\\nDefining the encoder • 332\\nDefining the decoder • 333\\nAttention: Analyzing the encoder states • 336\\nComputing Attention • 338\\nImplementing Attention • 340\\nDefining the final model • 344\\nTraining the NMT  �����������������������������������������������������������������������������������������������������������  346\\nThe BLEU score – evaluating the machine translation systems  ���������������������������������������  353', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fc73a59-80a1-43d1-bb02-901252de1d53', embedding=None, metadata={'page_label': 'ix', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xv\\nModified precision • 354\\nBrevity penalty • 355\\nThe final BLEU score • 355\\nVisualizing Attention patterns  ����������������������������������������������������������������������������������������  355\\nInference with NMT  ��������������������������������������������������������������������������������������������������������  360\\nOther applications of Seq2Seq models – chatbots  ������������������������������������������������������������  361\\nTraining a chatbot • 362\\nEvaluating chatbots – the Turing test • 363\\nSummary  ������������������������������������������������������������������������������������������������������������������������  364\\nChapter 10: Transformers   365\\nTransformer architecture  ������������������������������������������������������������������������������������������������  365\\nThe encoder and the decoder • 366\\nComputing the output of the self-attention layer • 370\\nEmbedding layers in the Transformer • 372\\nResiduals and normalization • 375\\nUnderstanding BERT  �������������������������������������������������������������������������������������������������������  377\\nInput processing for BERT • 379\\nTasks solved by BERT • 379\\nHow BERT is pre-trained • 382\\nMasked Language Modeling (MLM) • 383\\nNext Sentence Prediction (NSP) • 383\\nUse case: Using BERT to answer questions  ����������������������������������������������������������������������  384\\nIntroduction to the Hugging Face transformers library • 384\\nExploring the data • 385\\nImplementing BERT • 387\\nImplementing and using the Tokenizer • 387\\nDefining a TensorFlow dataset • 391\\nBERT for answering questions • 393\\nDefining the config and the model • 394\\nTraining and evaluating the model • 397', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6b7d6ea-94ef-4657-9f29-d508c929d8e3', embedding=None, metadata={'page_label': 'x', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xvi\\nAnswering questions with Bert • 399\\nSummary  ������������������������������������������������������������������������������������������������������������������������  401\\nChapter 11: Image Captioning with Transformers   403\\nGetting to know the data  �����������������������������������������������������������������������������������������������  404\\nILSVRC ImageNet dataset • 405\\nThe MS-COCO dataset • 406\\nDownloading the data  ����������������������������������������������������������������������������������������������������  407\\nProcessing and tokenizing data  ��������������������������������������������������������������������������������������  408\\nPreprocessing data • 408\\nTokenizing data • 412\\nDefining a tf.data.Dataset  �����������������������������������������������������������������������������������������������  414\\nThe machine learning pipeline for image caption generation  �����������������������������������������  420\\nVision Transformer (ViT) • 421\\nText-based decoder Transformer • 423\\nPutting everything together • 423\\nImplementing the model with TensorFlow  ��������������������������������������������������������������������  424\\nImplementing the ViT model • 424\\nImplementing the text-based decoder • 425\\nDefining the self-attention layer • 425\\nDefining the Transformer layer • 427\\nDefining the full decoder • 429\\nTraining the model  ���������������������������������������������������������������������������������������������������������  432\\nEvaluating the results quantitatively  ������������������������������������������������������������������������������  435\\nBLEU • 436\\nROUGE • 437\\nMETEOR • 438\\nCIDEr • 439\\nEvaluating the model  ������������������������������������������������������������������������������������������������������  440\\nCaptions generated for test images  ���������������������������������������������������������������������������������  441\\nSummary  ������������������������������������������������������������������������������������������������������������������������  446', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc0878cf-51bd-44a5-b4dc-d06e3cb34b5f', embedding=None, metadata={'page_label': 'xi', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xvii\\nAppendix A: Mathematical Foundations and Advanced TensorFlow   449\\nBasic data structures  �������������������������������������������������������������������������������������������������������  449\\nScalar • 449\\nVectors • 449\\nMatrices • 450\\nIndexing of a matrix • 450\\nSpecial types of matrices  ��������������������������������������������������������������������������������������������������  451\\nIdentity matrix • 451\\nSquare diagonal matrix • 452\\nTensors • 452\\nTensor/matrix operations  �����������������������������������������������������������������������������������������������  452\\nTranspose • 452\\nMatrix multiplication • 453\\nElement-wise multiplication • 453\\nInverse • 454\\nFinding the matrix inverse – Singular Value Decomposition (SVD) • 455\\nNorms • 456\\nDeterminant • 456\\nProbability  ����������������������������������������������������������������������������������������������������������������������  457\\nRandom variables • 457\\nDiscrete random variables • 458\\nContinuous random variables • 458\\nThe probability mass/density function • 458\\nConditional probability • 460\\nJoint probability • 461\\nMarginal probability • 461\\nBayes’ rule • 461\\nVisualizing word embeddings with TensorBoard  ������������������������������������������������������������  462\\nStarting TensorBoard • 462\\nSaving word embeddings and visualizing via TensorBoard • 463', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1272ad2b-3912-4cdb-bee9-1e9b1969f388', embedding=None, metadata={'page_label': 'xii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table of Contents xviii\\nSummary  ������������������������������������������������������������������������������������������������������������������������  467\\nOther Books You May Enjoy   471\\nIndex   475', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72c1c66a-94b9-4d69-ac90-d4e547e3f50a', embedding=None, metadata={'page_label': 'xiii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface\\nTensorFlow is at the center of developing Machine Learning ( ML) solutions. It is an ecosystem \\nthat can support all of the different stages in the life cycle of an ML project, from the early proto -\\ntyping up until the productionization of the model. TensorFlow provides various reusable building \\nblocks, allowing you to build not just the simplest but also the most complex deep neural networks.\\nWho this book is for\\nThis book is aimed at novice - to intermediate-level users of TensorFlow. The reader may be from \\nacademia doing cutting-edge research on ML or an industry practioner using ML in their job. You \\nwill get the most benefit from this book if you have some basic familiarity with TensorFlow (or \\na similar framework like Pytorch) already. This will help you to grasp the concepts and use cases \\ndiscussed in the book quicker.\\nWhat this book covers\\nChapter 1, Introduction to Natural Language Processing , explains what natural language processing \\nis and the kinds of tasks it may entail. We then discuss how an NLP task is solved using traditional \\nmethods. This paves the way to discuss how deep learning is used in NLP and what the benefits \\nare. Finally, we discuss the installation and usage of the technical tools in this book.\\nChapter 2 , Understanding TensorFlow 2 , provides you with a sound guide to writing programs \\nand running them in TensorFlow 2. This chapter will first offer an in-depth explanation of how \\nTensorFlow executes a program. This will help you to understand the TensorFlow execution work -\\nflow and feel comfortable with TensorFlow terminology. Next, we will discuss various building \\nblocks in TensorFlow and useful operations that are available. We will finally discuss how all this \\nknowledge of TensorFlow can be used to implement a simple neural network to classify images \\nof handwritten digits.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42b9ea29-13b9-4a8b-8277-217a75f584d8', embedding=None, metadata={'page_label': 'xiv', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xx\\nChapter 3, Word2vec – Learning Word Embeddings, introduces Word2vec—a method to learn numer -\\nical representations of words that reflect the semantics of the words. But before diving straight into \\nWord2vec techniques, we first discuss some classical approaches used to represent words, such \\nas one-hot-encoded representations, and the Term Frequency-Inverse Document Frequency  \\n(TF-IDF) frequency method. Following this, we will move on to a modern tool forlearning word \\nvectors known as Word2vec, which uses a neural network to learn word representations. We will \\ndiscuss two popular Word2vec variants: skip-gram and the Continuous Bag-of-Words ( CBOW) \\nmodel. Finally, we will visualize the word representations learned using a dimensionality reduc -\\ntion technique to map the vectors to a more interpretable two-dimensional surface.\\nChapter 4, Advanced Word Vector Algorithms , starts with a more recent word embedding learning \\ntechnique known as GloVe, which incorporates both global and local statisticsin text data to \\nfind word vectors. Next, we will learn about one of the modern, more sophisticated techniques \\nfor generating dynamic word representations based on the context of a word, known as ELMo.\\nChapter 5, Sentence Classification with Convolutional Neural Networks , introduces you to Convolu-\\ntional Neural Networks (CNNs). CNNs are a powerful family of deep models that can leverage \\nthe spatial structure of an input to learn from data. In other words, a CNN can process images in \\ntheir two-dimensional form, whereas a multilayer perceptron needs the image to be unwrapped \\nto a one-dimensional vector. We will first discuss various operations that are undergone in CNNs, \\nsuch as the convolution and pooling operations, in detail. Then, we will see an example where we \\nwill learn to classify images of clothes with a CNN. Then, we will transition into an application of \\nCNNs in NLP. More precisely, we will be investigating how to apply a CNN to classify sentences, \\nwhere the task is to classify if a sentence is about a person, location, object, and so on.\\nChapter 6, Recurrent Neural Networks, focuses on introducing Recurrent Neural Networks  (RNNs ) \\nand using RNNs for language generation. RNNs are different from feed-forward neural networks \\n(for example, CNNs) as RNNs have memory. The memory is stored as a continuously updated \\nsystem state. We will start with a representation of a feed-forward neural network and modify \\nthat representation to learn from sequences of data instead of individual data points. This pro -\\ncess will transform the feed-forward network to an RNN. This will be followed by a technical \\ndescription of the exact equations used for computations within the RNN. Next, we will discuss \\nthe optimization process of RNNs that is used to update the RNN’s weights. Thereafter we will \\niterate through different types of RNNs such as one-to-one RNNs and one-to-many RNNs. We \\nwill then discuss a popular application of RNNs, which is to identify named entities in text (for \\nexample, Person name, Organization, and so on). Here, we’ll be using a basic RNN model to learn. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b81579d-8fbf-4636-9fcf-57536737ac8d', embedding=None, metadata={'page_label': 'xv', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxi\\nNext, we will enhance our model further by incorporating embeddings at different scales (for \\nexample, token embeddings and character embeddings). The token embeddings are generated \\nthrough an embedding layer, where the character embeddings are generated using a CNN. We \\nwill then analyze the new model’s performance on the named entity recognition task.\\nChapter 7, Understanding Long Short-Term Memory Networks , discusses Long Short-Term Memory \\nnetworks ( LSTMs) by initially providing an intuitive explanation of how these models work \\nand progressively diving into the technical details required to implement them on your own. \\nStandard RNNs suffer from the crucial limitation of the inability to persist long-term memory. \\nHowever, advanced RNN models (for example, LSTMs and Gated Recurrent Units ( GRUs )) have \\nbeen proposed, which can remember sequences for a large number of time steps. We will also \\nexamine how exactly LSTMs alleviate the problem of persisting long-term memory (this is known \\nas the vanishing gradient problem). We will then discuss several modifications that can be used \\nto improve LSTM models further, such as predicting several time steps ahead at once and reading \\nsequences both forward and backward. Finally, we will discuss several variants of LSTM models \\nsuch as GRUs and LSTMs with peephole connections.\\nChapter 8, Applications of LSTM – Generating Text, explains how to implement the LSTMs, GRUs, and \\nLSTMs with peephole connections discussed in Chapter 7, Understanding  Long Short-Term Memory \\nNetworks . Furthermore, we will compare the performance of these extensions both qualitatively \\nand quantitatively. We will also discuss how to implement some of the extensions examined in \\nChapter 7, Understanding Long Short-Term Memory Networks , such as predicting several time steps \\nahead (known as beam search) and using word vectors as inputs instead of one-hot-encoded \\nrepresentations.\\nChapter 9, Sequence-to-Sequence Learning – Neural Machine Translation , discusses machine trans -\\nlation, which has gained a lot of attention both due to the necessity of automating translation \\nand the inherent difficulty of the task. We start the chapter with a brief historical flashback ex -\\nplaining how machine translation was implemented in the early days. This discussion ends with \\nan introduction to Neural Machine Translation  (NMT ) systems. We will see how well current \\nNMT systems are doing compared to old systems (such as statistical machine translation systems), \\nwhich will motivate us to learn about NMT systems. Afterward, we will discuss the concepts \\nunderpinning the design of NMT systems and continue with the technical details. Then, we will \\ndiscuss the evaluation metric we use to evaluate our system. Following this, we will investigate \\nhow we can implement an English-to-German translator from scratch. Next, we will learn about \\nways to improve NMT systems. We will look at one of those extensions in detail, called the at -\\ntention mechanism. The attention mechanism has become essential in sequence-to-sequence \\nlearning problems. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db293003-4e37-480d-b3d3-dbb4984ec001', embedding=None, metadata={'page_label': 'xvi', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxii\\nFinally, we will compare the performance improvement obtained with the attention mechanism \\nand analyze the reasons behind the performance gain. This chapter concludes with a section on \\nhow the same concept of NMT systems can be extended to implement chatbots. Chatbots are \\nsystems that can communicate with humans and are used to fulfill various customer requests.\\nChapter 10, Transformers, discusses Transformers, the latest breakthrough in the domain of NLP \\nwhich have outperformed many other previous state-of-the-art models. In this chapter, we will \\nuse the Hugging Face Transformers library to use pre-trained models for downstream tasks with \\nease. In this chapter, we will learn about the Transformer architecture in depth. This discussion \\nwill lead into a popular Transformer model called BERT, which we will use to solve a problem \\nof question answering. We will discuss specific components found in BERT to effectively use it \\nfor the application. Next, we will train the model on a popular question-answer dataset known \\nas SQUAD. Finally, we will evaluate the model on a test dataset and use the trained model to \\ngenerate answers for unseen questions.\\nChapter 11, Image Captioning with Transformers, looks at another exciting application, where \\nTransformers are used to generate captions (that is, descriptions) for images. This application \\nis interesting because it shows us how to combine two different types of models as well as how \\nto learn with multimodal data (for example, images and text). Here, we will use a pre-trained \\nVision Transformer model that generates a rich hidden representation for a given image. This \\nrepresentation, along with caption tokens, is fed to a text-based Transformer model. The text-\\nbased Transformer predicts the next caption token, given previous caption tokens. Once the \\nmodel is trained, we will evaluate the captions generated by our model, both qualitatively and \\nquantitatively. We will also discuss some of the popular metrics used to measure the quality of \\nsequences such as image captions.\\nAppendix A: Mathematical Foundations and Advanced TensorFlow, introduces various mathematical \\ndata structures (for example, matrices) and operations (for example, a matrix inverse). We will \\nalso discuss several important concepts in probability. Finally, we will walk you through a guide \\naimed at teaching you to use TensorBoard to visualize word embeddings. TensorBoard is a handy \\nvisualization tool that is shipped with TensorFlow. This can be used to visualize and monitor \\nvarious variables in your TensorFlow client.\\nTo get the most out of this book\\nTo get the most out of this book you need a basic understanding of TensorFlow or a similar frame -\\nwork such as PyTorch. Familiarity obtained through basic TensorFlow tutorials that are freely \\navailable in the web should suffice to get started on this book.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0dc9b584-49be-4870-9c92-b81a7d1a865e', embedding=None, metadata={'page_label': 'xvii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Preface xxiii\\n A basic knowledge of mathematics, including an understanding of n-dimensional tensors, matrix \\nmultiplication, and so on, will also prove invaluable throughout this book. Finally, you need an \\nenthusiasm for learning about cutting edge machine learning that is setting the stage for modern \\nNLP solutions.\\nDownload the example code files\\nThe code bundle for the book is hosted on GitHub at https://github.com/thushv89/packt_\\nnlp_tensorflow_2 . We also have other code bundles from our rich catalog of books and videos \\navailable at https://github.com/PacktPublishing/ . Check them out!\\nDownload the color images\\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. \\nYou can download it here: https://static.packt-cdn.com/downloads/9781838641351_\\nColorImages.pdf .\\nConventions used\\nThere are a number of text conventions used throughout this book.\\nCodeInText : Indicates code words in text, database table names, folder names, filenames, file \\nextensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “After run -\\nning the pip install  command, you should have Jupyter Notebook available in the Conda \\nenvironment.”\\nA block of code is set as follows:\\ndef layer(x, W, b):\\n    # Building the graph\\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform\\n    return h\\nAny command-line input or output is written as follows:\\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\\narray([[-1., -9.],\\n       [ 3., 10.],\\n       [ 5., 11.]], dtype=float32)>\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a52dfd1c-8ce9-49f8-afe8-91aeac2f7dd6', embedding=None, metadata={'page_label': 'xviii', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxiv\\nBold: Indicates a new term or an important word. Words that you see on the screen (such as in \\nmenus or dialog boxes) also appear in the text like this , for example: “The feature that builds this \\ncomputational graph automatically in TensorFlow is known as AutoGraph .”\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback : Email feedback@packtpub.com , and mention the book’s title in the subject of \\nyour message. If you have questions about any aspect of this book, please email us at questions@\\npacktpub.com .\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do \\nhappen. If you have found a mistake in this book we would be grateful if you would report this to \\nus. Please visit http://www.packtpub.com/submit-errata , select your book, click on the Errata \\nSubmission Form link, and enter the details.\\nPiracy : If you come across any illegal copies of our works in any form on the internet, we would \\nbe grateful if you would provide us with the location address or website name. Please contact us \\nat copyright@packtpub.com  with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you \\nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\\ncom .Warnings or important notes appear like this.\\nTips and tricks appear like this.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f7107a0-6f0c-48c8-b0a2-b7d6862fd7d1', embedding=None, metadata={'page_label': 'xix', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xxv\\nShare your thoughts\\nOnce you’ve read Natural Language Processing with TensorFlow, Second Edition , we’d love to hear \\nyour thoughts! Please click here to go straight to the Amazon review page  for this book \\nand share your feedback.\\nYour review is important to us and the tech community and will help us make sure we’re deliv -\\nering excellent quality content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95d8c586-b36a-4a78-8023-f9e7b311949e', embedding=None, metadata={'page_label': 'xx', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82225397-e340-4fdf-98fd-3d1aab9d2062', embedding=None, metadata={'page_label': '1', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1\\nIntroduction to Natural \\nLanguage Processing\\nNatural Language Processing ( NLP) offers a much-needed set of tools and algorithms for under -\\nstanding and processing the large volume of unstructured data in today’s world. Recently, deep \\nlearning has been widely adopted for many NLP tasks because of the remarkable performance \\ndeep learning algorithms have shown in a plethora of challenging tasks, such as image classifi -\\ncation, speech recognition, and realistic text generation. TensorFlow is one of the most intuitive \\nand efficient deep learning frameworks currently in existence that enables such amazing feats. \\nThis book will enable aspiring deep learning developers to handle massive amounts of data using \\nNLP and TensorFlow. This chapter covers the following topics:\\n• What is Natural Language Processing?\\n• Tasks of Natural Language Processing\\n• The traditional approach to Natural Language Processing\\n• The deep learning approach to Natural Language Processing\\n• Introduction to the technical tools\\nIn this chapter, we will provide an introduction to NLP and to the rest of the book. We will an-\\nswer the question, “What is Natural Language Processing?”. Also, we’ll look at some of its most \\nimportant use cases. We will also consider the traditional approaches and the more recent deep \\nlearning-based approaches to NLP, including a Fully Connected Neural Network (FCNN). Finally, \\nwe will conclude with an overview of the rest of the book and the technical tools we will be using.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68d5a06c-5c3f-4f98-a9c0-ff5358a5d777', embedding=None, metadata={'page_label': '2', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 2\\nWhat is Natural Language Processing?\\nAccording to DOMO ( https://www.domo.com/ ), an analytics company, there  were 1.7MB for every \\nperson on earth every second by 2020, with a staggering 4.6 billion active users on the internet. \\nThis includes roughly 500,000 tweets sent and 306 billion emails circulated every day. These \\nfigures are only going in one direction as this book is being written, and that is up! Of all this data, \\na large fraction is unstructured text and speech as there are billions of emails and social media \\ncontent created and phone calls made every day.\\nThese statistics provide a good basis for us to define what NLP is. Simply put, the goal of NLP is \\nto make machines understand our spoken and written languages. Moreover, NLP is ubiquitous \\nand is already a large part of human life. Virtual Assistants (VAs ), such as Google Assistant, Cor -\\ntana, Alexa, and Apple Siri, are largely NLP systems. Numerous NLP tasks take place when one \\nasks a VA, “Can you show me a good Italian restaurant nearby?” First, the VA needs to convert the \\nutterance to text (that is, speech-to-text). Next, it must understand the semantics of the request \\n(for example, identify the most important keywords like restaurant and Italian) and formulate a \\nstructured request (for example, cuisine = Italian, rating = 3–5, distance < 10 km). Then, the VA \\nmust search for restaurants filtering by the location and cuisine, and then, rank the restaurants \\nby the ratings received. To calculate an overall rating for a restaurant, a good NLP system may \\nlook at both the rating and text description provided by each user. Finally, once the user is at the \\nrestaurant, the VA might assist the user by translating various menu items from Italian to English. \\nThis example shows that NLP has become an integral part of human life.\\nIt should be understood that NLP is an extremely challenging field of research as words and \\nsemantics have a highly complex nonlinear relationship, and it is even more difficult to capture \\nthis information as a robust numerical representation. To make matters worse, each language \\nhas its own grammar, syntax, and vocabulary. Therefore, processing textual data involves various \\ncomplex tasks such as text parsing (for example, tokenization and stemming), morphological \\nanalysis, word sense disambiguation, and understanding the underlying grammatical structure of \\na language. For example, in these two sentences, I went to the bank  and I walked along the river bank, \\nthe word bank  has two entirely different meanings, due to the context it’s used in. To distinguish \\nor (disambiguate) the word bank , we need to understand the context in which the word is being \\nused. Machine learning has become a key enabler for NLP, helping to accomplish the aforemen -\\ntioned  tasks through machines. Below we discuss some of the important tasks that fall under NLP.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8431853e-255f-4ce9-b0ee-b382883dbee9', embedding=None, metadata={'page_label': '3', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 3\\nTasks of Natural Language Processing\\nNLP has a multitude of real-world applications. A good NLP system is one that performs many \\nNLP tasks. When you search for today’s weather on Google or use Google Translate to find out \\nhow to say, “How are you? ” in French, you rely on a subset of such tasks in NLP. We will list some \\nof the most ubiquitous tasks here, and this book covers most of these tasks:\\n• Tokenization : Tokenization is the task  of separating a text corpus into atomic units (for \\nexample, words or characters). Although it may seem trivial for a language like English, \\ntokenization is an important task. For example, in the Japanese language, words are not \\ndelimited by spaces or punctuation marks.\\n• Word-Sense Disambiguation ( WSD ): WSD is the task of identifying  the correct meaning \\nof a word. For example, in the sentences, The dog barked at the mailman and Tree bark is \\nsometimes used as a medicine , the word bark has two different meanings. WSD is critical \\nfor tasks such as question answering.\\n• Named Entity Recognition  (NER ): NER attempts to extract entities (for example, person, \\nlocation, and organization) from a given body of text or a text corpus. For example, the \\nsentence, John gave Mary two apples at school on Monday will be transformed to [John]name \\ngave [Mary]name [two]number apples at [school]organization on [Monday]time. NER is an \\nimperative topic in fields such as information retrieval and knowledge representation.\\n• Part-of-Speech ( PoS ) tagging: PoS tagging is the task of assigning words to their re -\\nspective parts of speech. It can either be basic tags such as noun, verb, adjective, adverb, \\nand preposition, or it can be granular such as proper noun, common noun, phrasal verb, \\nverb, and so on. The Penn Treebank project, a popular project focusing PoS, defines a \\ncomprehensive list of PoS tags at https://www.ling.upenn.edu/courses/ling001/\\npenn_treebank_pos.html .\\n• Sentence/ synopsis classification : Sentence or synopsis (for example, movie reviews) \\nclassification  has many use cases such as spam detection, news article classification (for \\nexample, political, technology, and sport), and product review ratings (that is, positive \\nor negative). This is achieved by training a classification model with labeled data (that is, \\nreviews annotated by humans, with either a positive or negative label).\\n• Text generation : In text generation, a learning model (for example, a neural network) is \\ntrained with text corpora (a large collection of textual documents), and it then predicts \\nnew text that follows. For example, language modeling can output an entirely new science \\nfiction story by using existing science fiction stories for training. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2178612-dee8-4eef-88e2-685a7f448d99', embedding=None, metadata={'page_label': '4', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 4\\nRecently, OpenAI released a language model known as OpenAI-GPT-2, which can gen -\\nerate incredibly realistic text. Furthermore, this task plays a very important role in un -\\nderstanding language, which helps a downstream decision-support model get off the \\nground quickly.\\n• Question Answering (QA ): QA techniques possess a high commercial value, and such \\ntechniques  are found at the foundation of chatbots  and VA (for example, Google Assistant \\nand Apple Siri). Chatbots have been adopted by many companies for customer support. \\nChatbots can be used to answer and resolve straightforward customer concerns (for ex -\\nample, changing a customer’s monthly mobile plan), which can be solved without human \\nintervention. QA touches upon many other aspects of NLP such as information retrieval \\nand knowledge representation. Consequently, all this makes developing a QA system \\nvery difficult.\\n• Machine Translation ( MT): MT is the task of transforming  a sentence/phrase  from a \\nsource language (for example, German) to a target language (for example, English). This \\nis a very challenging task, as different languages have different syntactical structures, \\nwhich means that it is not a one-to-one transformation. Furthermore, word-to-word \\nrelationships  between languages can be one-to-many, one-to-one, many-to-one, or ma-\\nny-to-many. This  is known as the word alignment problem in MT literature.\\nFinally, to develop a system that can assist a human in day-to-day tasks (for example, VA or a \\nchatbot) many of these tasks need to be orchestrated in a seamless manner. As we saw in the \\nprevious example where the user asks, “Can you show me a good Italian restaurant nearby?” several \\ndifferent NLP tasks, such as speech-to-text conversion, semantic and sentiment analyses, question \\nanswering, and machine translation, need to be completed. In Figure 1.1, we provide a hierarchical \\ntaxonomy of different NLP tasks categorized into several different types. It is a difficult task to \\nattribute an NLP task to a single classification. Therefore, you can see some tasks spanning mul-\\ntiple categories. We will split the categories into two main types: language-based (light-colored \\nwith black text) and problem formulation-based (dark-colored with white text). The linguistic \\nbreakdown has two categories: syntactic (structure-based) and semantic (meaning-based). The \\nproblem formulation-based breakdown has three categories: preprocessing tasks (tasks that are \\nperformed on text data before feeding to a model), discriminative tasks (tasks where we attempt \\nto assign an input text to one or more categories from a set of predefined categories) and gen -\\nerative tasks (tasks where we attempt to generate a new textual output). Of course, this is one \\nclassification among many. But it will show how difficult it is to assign a specific NLP task to a \\nspecific category.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6de1dd0c-38b6-401e-a391-41adceccb2c4', embedding=None, metadata={'page_label': '5', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 5\\n \\nFigure 1.1: A taxonomy of the popular tasks of NLP categorized under broader \\ncategories\\nHaving understood the various tasks in NLP, let us now move on to understand how we can solve \\nthese tasks with the help of machines. We will discuss both the traditional method and the deep- \\nlearning-based approach.\\nThe traditional approach to Natural Language \\nProcessing\\nThe traditional or classical approach to solving NLP is a sequential flow of several key steps, and \\nit is a statistical approach. When we take a closer look at a traditional NLP learning model, we \\nwill be able to see a set of distinct tasks taking place, such as preprocessing data by removing \\nunwanted data, feature engineering to get good numerical representations of textual data, learn -\\ning to use machine learning algorithms with the aid of training data, and predicting outputs for \\nnovel, unseen data. Of these, feature engineering was the most time-consuming and crucial step \\nfor obtaining good performance on a given NLP task.\\nUnderstanding the traditional approach\\nThe traditional approach to solving NLP tasks involves a collection of distinct subtasks. First, \\nthe text corpora need to be preprocessed, focusing on reducing the vocabulary and distractions . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5205ca03-e530-401e-b3af-bebc5e03571e', embedding=None, metadata={'page_label': '6', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 6\\nBy distractions , I refer to the things that distract the algorithm (for example, punctuation marks \\nand stop word removal) from capturing the vital linguistic information required for the task.\\nNext come several feature engineering steps. The main objective of feature engineering is to make \\nlearning easier for the algorithms. Often the features are hand-engineered and biased toward \\nthe human understanding of a language. Feature engineering was of the utmost importance for \\nclassical NLP algorithms, and consequently, the best-performing systems often had the best-en -\\ngineered features. For example, for a sentiment classification task, you can represent a sentence \\nwith a parse tree and assign positive, negative, or neutral labels to each node/subtree in the tree \\nto classify that sentence as positive or negative. Additionally, the feature engineering phase can \\nuse external resources such as WordNet (a lexical database that can provide insights into how \\ndifferent words are related to each other – e.g. synonyms) to develop better features. We will soon \\nlook  at a simple feature engineering technique known as bag-of-words .\\nNext, the learning algorithm learns to perform well at the given  task using the obtained features \\nand, optionally, the external resources. For example, for a text summarization task, a parallel \\ncorpus containing common phrases and succinct paraphrases would be a good external resource. \\nFinally, prediction  occurs. Prediction is straightforward, where you will feed a new input and ob -\\ntain the predicted label by forwarding the input through the learning model. The entire process \\nof the traditional approach is depicted in Figure 1.2 :\\nFigure 1.2: The general approach of classical NLP', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95a15630-1336-4da6-ab67-47e09fc0e158', embedding=None, metadata={'page_label': '7', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 7\\nNext, let’s discuss a use case where we use NLP to generate football game summaries.\\nExample – generating football game summaries\\nTo gain an in-depth understanding  of the traditional approach to NLP, let’s consider a task of \\nautomatic text generation from the statistics of a game of football. We have several sets of game \\nstatistics (for example, the score, penalties, and yellow cards) and the corresponding articles gen -\\nerated for that game by a journalist, as the training data. Let’s also assume that for a given game, \\nwe have a mapping from each statistical parameter to the most relevant phrase of the summary \\nfor that parameter. Our task here is that, given a new game, we need to generate a natural-looking \\nsummary of the game. Of course, this can be as simple as finding the best-matching statistics for \\nthe new game from the training data and retrieving the corresponding summary. However, there \\nare more sophisticated and elegant ways of generating text.\\nIf we were to incorporate machine learning to generate natural language, a sequence of opera-\\ntions, such as preprocessing the text, feature engineering, learning, and prediction, is likely to \\nbe performed.\\nPreprocessing : The text involves operations, such as tokenization (for example, splitting “I went \\nhome” into “I ”, “went”, “ home”), stemming (for example, converting listened  to listen), and removing \\npunctuation (for example, ! and ;), in order to reduce the vocabulary (that is, the features), thus \\nreducing the dimensionality of the data. Tokenization might appear  trivial for a language such as \\nEnglish, as the words are isolated; however, this is not the case for certain languages such as Thai, \\nJapanese, and Chinese, as these languages are not consistently delimited. Next, it is important to \\nunderstand that stemming is not a trivial operation either. It might appear that stemming is a \\nsimple operation that relies on a simple set of rules such as removing ed from a verb (for example, \\nthe stemmed result of listened  is listen ); however, it requires more than a simple rule base to de -\\nvelop a good stemming algorithm, as stemming certain words can be tricky (for example, using \\nrule-based stemming, the stemmed result of argued is argu). In addition, the effort required for \\nproper stemming can vary in complexity for other languages.\\nFeature engineering is used to transform raw text data into an appealing numerical representation \\nso that a model can be trained on that data, for example, converting text into a bag-of-words rep -\\nresentation or using n-gram representation, which we will discuss later. However, remember that \\nstate-of-the-art classical models rely on much more sophisticated feature engineering techniques.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbf151d7-ceb3-4f97-93ba-38c5f3517caf', embedding=None, metadata={'page_label': '8', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 8\\nThe following are some of the feature engineering techniques:\\nBag-of-words: This is a feature engineering technique that creates feature representations based \\non the word occurrence frequency. For example, let’s consider the following sentences:\\n• Bob went to the market to buy some flowers\\n• Bob bought the flowers to give to Mary\\nThe vocabulary for these two sentences would be:\\n[“Bob”, “went”, “to”, “the”, “market”, “buy”, “some”, “flowers”, “bought”, “give”, “Mary”]\\nNext, we will create a feature vector of size V (vocabulary size) for each sentence, showing how \\nmany times each word in the vocabulary appears in the sentence. In this example, the feature \\nvectors for the sentences would respectively be as follows:\\n[1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0]\\n[1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1]\\nA crucial limitation of the bag-of-words method  is that it loses contextual information as the \\norder of words is no longer preserved.\\nn-gram: This is another feature engineering technique that breaks down text into smaller compo -\\nnents  consisting of n letters (or words). For example, 2-gram would break the text into two-letter \\n(or two-word) entities. For example, consider this sentence:\\nBob went to the market to buy some flowers\\nThe letter level n-gram decomposition for this sentence is as follows:\\n[“Bo”, “ob”, “b “, “ w”, “we”, “en”, ..., “me”, “e “,” f”, “fl”, “lo”, “ow”, “we”, “er”, “rs”] \\nThe word-based n-gram decomposition is this:\\n[“Bob went”, “went to”, “to the”, “the market”, ..., “to buy”, “buy some”, “some flowers”]\\nThe advantage in this representation (letter level) is that the vocabulary will be significantly \\nsmaller than if we were to use words as features for large corpora.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2942be0-4cb1-4108-819f-c54536df9428', embedding=None, metadata={'page_label': '9', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 9\\nNext, we need to structure our data to be able to feed it into a learning model. For example, we \\nwill have data tuples of the form ( a statistic, a phrase explaining the statistic ) as follows:\\nTotal goals = 4, “The game was tied with 2 goals for each team at the end of the first half”\\nTeam 1 = Manchester United, “The game was between Manchester United and Barcelona”\\nTeam 1 goals = 5, “Manchester United managed to get 5 goals”\\nThe learning process may comprise three sub-modules: a Hidden Markov Model ( HMM), a \\nsentence planner, and a discourse planner. An HMM is a recurrent model that can be used to \\nsolve time-series problems. For example, generating text is a time-series problem as the order \\nof generated words matters. In our example, an HMM might learn to model language (i.e. gen -\\nerate meaningful text) by training on a corpus of statistics and related phrases. We will train the \\nHMM so that it produces a relevant sequence of text, given the statistics as the starting input. \\nOnce trained, the HMM can be used for inference in a recursive manner, where we start with a \\nseed (e.g. a statistic) and predict the first word of the description, then use the predicted word \\nto generate the next word, and so on.\\nNext, we can have a sentence planner that corrects any syntactical or grammatical errors, that \\nmight have been introduced by the model. For example, a sentence planner might take in the \\nphrase, I go house and output I go home. For this, it can use a database of rules, which contains  \\nthe correct way of conveying meanings, such as the need for a preposition between a verb and \\nthe word house.\\nUsing the HMM and the sentence planner, we will have syntactically grammatically correct sen-\\ntences. Then, we need to collate these phrases in such a way that the essay made from the collection \\nof phrases is human readable and flows well. For example, consider the three phrases, Player 10 of \\nthe Barcelona team scored a goal in the second half, Barcelona played against Manchester United, and \\nPlayer 3 from Manchester United got a yellow card in the first half ; having these sentences in this order \\ndoes not make much sense. We like to have them in this order: Barcelona played against Manchester \\nUnited, Player 3 from Manchester United got a yellow card in the first half, and Player 10 of the Barcelona \\nteam scored a goal in the second half. To do this, we use a discourse planner; discourse planners can \\norganize a set of messages so that the meaning of them is conveyed properly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e126f9b4-45f9-4aee-87b6-0bbb46d89fc0', embedding=None, metadata={'page_label': '10', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 10\\nNow, we can get a set of arbitrary test statistics and obtain an essay explaining the statistics by \\nfollowing the preceding workflow, which is depicted in Figure 1.3 :\\nFigure 1.3: The classical approach to solving a language modeling task\\nHere, it is important to note that this is a very high-level explanation that only covers the main \\ngeneral-purpose components that are most likely to be included in traditional NLP. The details \\ncan largely vary according to the particular application we are interested in solving. For example, \\nadditional application-specific crucial components might be needed for certain tasks (a rule base \\nand an alignment model in machine translation). However, in this book, we do not stress about such \\ndetails as the main objective here is to discuss more modern ways of Natural Language Processing.\\nDrawbacks of the traditional approach\\nLet’s list several key drawbacks of the traditional approach as this would lay a good foundation \\nfor discussing the motivation for deep learning:\\n• The preprocessing steps used in traditional NLP forces a trade-off of potentially useful \\ninformation embedded in the text (for example, punctuation and tense information) in \\norder to make the learning feasible by reducing the vocabulary. Though preprocessing is \\nstill used in modern deep-learning-based solutions, it is not as crucial for them as it is for \\nthe traditional NLP workflow due to the large representational capacity of deep networks \\nand their ability to optimize high-end hardware like GPUs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e4a8193-dba5-42e9-8379-53bfdcf941e8', embedding=None, metadata={'page_label': '11', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 11\\n• Feature engineering is very labor-intensive. In order to design a reliable system, good \\nfeatures need to be devised. This process can be very tedious as different feature spaces \\nneed to be extensively explored and evaluated. Additionally, in order to effectively ex -\\nplore robust features, domain expertise is required, which can be scarce and expensive \\nfor certain NLP tasks.\\n• Various external resources are needed for it to perform well, and there are not many freely \\navailable ones. Such external resources often consist of manually created information \\nstored in large databases. Creating one for a particular task can take several years, de -\\npending on the severity of the task (for example, a machine translation rule base).\\nNow, let’s discuss how deep learning can help to solve NLP problems.\\nThe deep learning approach to Natural Language \\nProcessing\\nI think it is safe to say that deep learning  revolutionized machine learning, especially in fields \\nsuch as computer vision, speech recognition, and of course, NLP. Deep models created a wave of \\nparadigm shifts in many of the fields in machine learning, as deep models learned rich features \\nfrom raw data instead of using limited human-engineered features. This consequentially caused \\nthe pesky and expensive feature engineering to be obsolete. With this, deep models made the \\ntraditional workflow more efficient, as deep models perform feature learning and task learning, \\nsimultaneously. Moreover, due to the massive number of parameters (that is, weights) in a deep \\nmodel, it can encompass significantly more features than a human could’ve engineered. However, \\ndeep models are considered a black box due to the poor interpretability of the model. For example, \\nunderstanding the “how” and “what” features learned by deep models for a given problem is \\nstill an active area of research. But it is important to understand that there is a lot more research \\nfocusing on “model interpretability of deep learning models”. \\nA deep neural network  is essentially an artificial neural network that has an input layer, many \\ninterconnected hidden layers in the middle, and finally, an output layer (for example, a classifier or \\na regressor). As you can see, this forms an end-to-end model from raw data to predictions. These \\nhidden layers in the middle give the power to deep models as they are responsible for learning \\nthe good features from raw data, eventually succeeding at the task at hand. Let’s now understand \\nthe history of deep learning briefly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db66e50e-9828-4501-b8a2-96981b9c5567', embedding=None, metadata={'page_label': '12', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 12\\nHistory of deep learning\\nLet’s briefly discuss the roots of deep learning and how the field evolved to be a very promising \\ntechnique for machine learning. In 1960, Hubel and Weisel performed an interesting experiment \\nand discovered that a cat’s visual cortex is made of simple and complex cells, and that these cells \\nare organized in a hierarchical form. Also, these cells react differently to different stimuli. For ex -\\nample, simple cells are activated by variously oriented edges while complex cells are insensitive \\nto spatial variations (for example, the orientation of the edge). This kindled the motivation for \\nreplicating a similar behavior in machines, giving rise to the concept of artificial neural networks.\\nIn the years that followed, neural networks gained the attention of many researchers. In 1965, a \\nneural network  trained by a method known as the Group Method of Data Handling  (GMDH) \\nand based on the famous Perceptron by Rosenblatt, was introduced by Ivakhnenko and others. \\nLater, in 1979, Fukushima introduced the Neocognitron, which planted the seeds for one  of the \\nmost famous variants of deep models—Convolutional Neural Networks (CNNs). Unlike the per -\\nceptrons, which always took in a 1D input, a Neocognitron was able to process 2D inputs using \\nconvolution operations.\\nArtificial neural networks used to backpropagate the error signal to optimize the network param -\\neters by computing the gradients of the weights of a given layer with regards to the loss. Then, \\nthe weights are updated by pushing them in the opposite direction of the gradient, in order to \\nminimize the loss. For a layer further away from the output layer (i.e. where the loss is computed), \\nthe algorithm uses the chain rule to compute gradients. The chain rule used with many layers led \\nto a practical problem known as the vanishing gradients problem, strictly limiting the potential \\nnumber of layers (depth) of the neural network. The gradients of layers closer to the inputs (i.e. \\nfurther away from the output layer), being very small, cause the model training to stop prema-\\nturely, leading to an underfitted  model. This is known as the vanishing gradients phenomenon . \\nThen, in 2006, it was found that pretraining a deep neural network by minimizing the reconstruction \\nerror  (obtained by trying to compress the input to a lower dimensionality and then reconstruct -\\ning it back into the original dimensionality) for each layer of the network provides a good initial \\nstarting point for the weight of the neural network; this allows a consistent flow of gradients \\nfrom the output layer to the input layer. This essentially allowed neural network models to have \\nmore layers without the ill effects of the vanishing gradient. Also, these deeper models were able \\nto surpass traditional machine learning models in many tasks, mostly in computer vision (for \\nexample, test accuracy for the MNIST handwritten digit dataset). With this breakthrough, deep \\nlearning became the buzzword in the machine learning community.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4209bd5e-4bb5-46dd-81cb-e78d118c3b8a', embedding=None, metadata={'page_label': '13', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 13\\nThings started gaining progressive momentum when, in 2012, AlexNet (a deep convolutional \\nneural network  created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton) won the Large \\nScale Visual Recognition Challenge (LSVRC ) 2012 with an error decrease of 10% from the previ-\\nous best. During this time, advances were made in speech recognition, wherein state-of-the-art \\nspeech recognition accuracies were reported using deep neural networks. Furthermore, people \\nbegan to realize that Graphical Processing Units (GPUs) enable more parallelism, which allows \\nfor faster training of larger and deeper networks compared with Central Processing Units ( CPUs).\\nDeep models were further improved with better model initialization techniques (for example, \\nXavier initialization), making the time-consuming pretraining redundant. Also, better nonlinear \\nactivation functions, such as Rectified Linear Units (ReLUs ), were introduced, which alleviated \\nthe adversities of the vanishing gradient in deeper models. Better optimization (or learning) \\ntechniques, such as the Adam optimizer, automatically tweaked  individual learning rates of each \\nparameter among the millions of parameters that we have in the neural network model, which \\nrewrote the state-of-the-art performance in many different fields of machine learning, such as \\nobject classification and speech recognition. These advancements also allowed neural network \\nmodels to have large numbers of hidden layers. The ability to increase the number of hidden layers \\n(that is, to make the neural networks deep) is one of the primary contributors to the significantly \\nbetter performance of neural network models compared with other machine learning models. \\nFurthermore, better intermediate regularizers, such as batch normalization layers, have improved \\nthe performance of deep nets for many tasks.\\nLater, even deeper models such as ResNets, Highway Nets, and Ladder Nets were introduced, \\nwhich had hundreds of layers and billions of parameters. It was possible to have such an enor -\\nmous  number of layers with the help of various empirically and theoretically inspired techniques. \\nFor example, ResNets use shortcut connections or skip connections to connect layers that are far \\napart, which minimizes the diminishing of gradients layer to layer, as discussed earlier.\\nThe current state of deep learning and NLP\\nMany different deep models have seen the light since their inception in early 2000. Even though  \\nthey share a resemblance, such as all of them using nonlinear transformation of the inputs and \\nparameters, the details  can vary vastly. For example, a CNN  can learn from two-dimensional \\ndata (for example, RGB images) as it is, while a multilayer perceptron model requires the input \\nto be unwrapped to a one-dimensional vector, causing the loss of important spatial information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aec6dc99-eac0-42f6-a846-910c5374287d', embedding=None, metadata={'page_label': '14', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 14\\nWhen processing text, as one of the most intuitive interpretations of text is to perceive it as a \\nsequence of characters, the learning model should be able to do time-series modeling, thus re -\\nquiring the memory of the past. To understand this, think of a language modeling task; the next \\nword for the word cat should be different from the next word for the word climbed. One such pop -\\nular model  that encompasses this ability is known as a Recurrent Neural Network  (RNN). We \\nwill see in Chapter 6, Recurrent Neural Networks , how exactly RNNs achieve this by going through \\ninteractive exercises.\\nIt should be noted that memory is not a trivial operation that is inherent to a learning model. \\nConversely, ways of persisting memory should be carefully designed.\\nAlso, the term memory should not be confused with the learned weights of a non-sequential deep \\nnetwork that only looks at the current input, where a sequential model (for example, an RNN) \\nwill look at both the learned weights and the previous element of the sequence to predict the \\nnext output.\\nOne prominent drawback of RNNs is that they cannot remember more than a few (approximately \\nseven) time steps, thus lacking long-term memory. Long Short-Term Memory (LSTM ) networks \\nare an extension of RNNs  that encapsulate long-term memory. Therefore, often LSTMs are pre -\\nferred over standard RNNs, nowadays. We will peek under the hood in Chapter 7, Understanding \\nLong Short-Term Memory Networks , to understand them better.\\nFinally, a model known as a Transformer  has been introduced by Google fairly recently, which \\nhas outperformed many of the previous state-of-the-art models such as LSTMs  on a pletho -\\nra of NLP tasks. Previously, both recurrent models (e.g. LSTMs) and convolutional models (e.g. \\nCNNs) dominated the NLP domain. For example, CNNs have been used for sentence classification, \\nmachine translation, and sequence-to-sequence learning tasks. However, Transformers use an \\nentirely different approach where they use neither recurrence nor convolution, but an attention \\nmechanism. The attention mechanism allows the model to look at the entire sequence at once, \\nto produce a single output. For example, consider the sentence “The animal didn’t cross the road \\nbecause it was tired.” While generating intermediate representations for the word “it ,” it would \\nbe useful for the model to learn that “it ” refers to the “animal ”. The attention mechanism allows \\nthe Transformer model to learn such relationships. This capability cannot be replicated with \\nstandard recurrent models or convolutional models. We will investigate these models further in \\nChapter 10, Transformers and Chapter 11, Image Captioning with Transformers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe1ba766-fc92-47ed-b46a-b22654ff91bd', embedding=None, metadata={'page_label': '15', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 15\\nIn summary, we can mainly separate deep networks into three categories: the non-sequential \\nmodels that deal with only a single input at a time for both training and prediction (for example, \\nimage classification), the sequential models  that cope with sequences of inputs of arbitrary length \\n(for example, text generation where a single word is a single input), and finally, attention-based \\nmodels that look at the sequence at once such as the Transformer, BERT, and XLNet, which are \\npretrained models based on the Transformer architecture. We can categorize non-sequential \\n(also called feed-forward) models into deep (approximately less than 20 layers) and very deep \\nnetworks (can be greater than hundreds of layers). The sequential models are categorized into \\nshort-term memory models (for example, RNNs), which can only memorize short-term patterns, \\nand long-term memory models, which can memorize longer patterns. In Figure 1.4, we outline \\nthe discussed taxonomy. You don’t have to understand these different deep learning models fully \\nat this point, but it illustrates the diversity of the deep learning models:\\nFigure 1.4: A general taxonomy of the most commonly used deep learning \\nmethods, categorized into several classes\\nNow, let’s take our first steps toward understanding the inner workings of a neural network.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='af67cc9b-fd13-4797-abd6-a81b8f1839a0', embedding=None, metadata={'page_label': '16', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 16\\nUnderstanding a simple deep model – a fully connected \\nneural network\\nNow, let’s have a closer look at a deep neural network  in order to gain a better understanding. \\nAlthough there are numerous different  variants of deep models, let’s look at one of the earliest  \\nmodels (dating back to 1950–60), known as a fully connected neural network ( FCNN), sometimes \\ncalled a multilayer perceptron. Figure 1.5  depicts a standard three-layered FCNN.\\nThe goal of an FCNN is to map an input (for example, an image or a sentence) to a certain label \\nor annotation (for example, the object category for images). This is achieved by using an input \\nx to compute h – a hidden representation of x – using a transformation such as ℎ=𝜎𝜎(𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊 ) ; \\nhere, W and b are the weights and bias of the FCNN, respectively, and 𝜎𝜎  is the sigmoid activation \\nfunction. Neural networks use non-linear activation functions at every layer. Sigmoid activation \\nis one such activation. It is an element-wise transformation applied to the output of a layer, where \\nthe sigmoidal output of x is given by, 𝜎𝜎(𝑥𝑥)=1(1+𝑒𝑒𝑒𝑥𝑥) ⁄  . Next, a classifier is placed on top of the \\nFCNN that gives the ability to leverage the learned features in hidden layers to classify inputs. The \\nclassifier is a part of the FCNN and yet another hidden layer with some weights, W s and a bias, bs. \\nAlso, we can calculate the final output of the FCNN as 𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜 𝑜 𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜 (𝑊𝑊𝑠𝑠∗ℎ+𝑏𝑏 𝑠𝑠) . For example, \\na softmax classifier can be used for multi-label classification problems. It provides a normalized \\nrepresentation of the scores output by the classifier layer. That is, it will produce a valid proba-\\nbility distribution over the classes in the classifier layer. The label is considered to be the output \\nnode with the highest softmax value. Then, with this, we can define a classification loss that is \\ncalculated as the difference between the predicted output label and the actual output label. An \\nexample of such a loss function is the mean squared loss. You don’t have to worry if you don’t \\nunderstand the actual intricacies of the loss function. We will discuss quite a few of them in later \\nchapters. Next, the neural network parameters, W, b, W s, and b s, are optimized using a standard \\nstochastic optimizer (for example, the stochastic gradient descent) to reduce the classification \\nloss of all the inputs. Figure 1.5  depicts the process explained in this paragraph for a three-layer \\nFCNN. We will walk through the details on how to use such a model for NLP tasks, step by step, \\nin Chapter 3 , Word2vec – Learning Word Embeddings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1c3f2ca2-dd6e-492a-a39f-59766de80bff', embedding=None, metadata={'page_label': '17', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 17\\nFigure 1.5: An example of a fully connected neural network (FCNN)\\nLet’s look at an example of how to use a neural network for a sentiment analysis task. Consider \\nthat we have a dataset where the input is a sentence expressing a positive or negative opinion \\nabout a movie and a corresponding label saying if the sentence is actually positive ( 1) or negative \\n(0). Then, we are given a test dataset, where we have single-sentence movie reviews, and our task \\nis to classify these new sentences as positive or negative.\\nIt is possible to use a neural network (which can be deep or shallow, depending on the difficulty \\nof the task) for this task by adhering to the following workflow:\\n1. Tokenize the sentence by words.\\n2. Convert the sentences into a fixed sized numerical representation (for example, Bag-of-\\nWords representation). A fixed sized representation is needed as fully connected neural \\nnetworks require a fixed sized input.\\n3. Feed the numerical inputs to the neural network, predict the output (positive or negative), \\nand compare that with the true target.\\n4. Optimize the neural network using a desired loss function.\\nIn this section we looked at deep learning in more detail. We looked at the history and the cur -\\nrent state of NLP. Finally, we looked at a fully connected neural network (a type of deep learning \\nmodel) in more detail. \\nNow that we’ve introduced NLP, its tasks, and how approaches to it have evolved over the years, \\nlet’s take a moment to look the technical tools required for the rest of this book.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cb49730-0a00-4d2d-ab42-10071f248c46', embedding=None, metadata={'page_label': '18', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 18\\nIntroduction to the technical tools\\nIn this section, you will be introduced to the technical tools  that will be used in the exercises \\nof the following chapters. First, we will present a brief introduction to the main tools provided. \\nNext, we will present a rough guide on how to install each tool along with hyperlinks to detailed \\nguides provided by the official websites. Additionally, we will share tips on how to make sure \\nthat the tools were installed properly.\\nDescription of the tools\\nWe will use Python as the coding/scripting language. Python is a very versatile, easy-to-set-up \\ncoding language that is heavily used by the scientific and machine learning communities.\\nAdditionally, there are numerous scientific libraries built for Python, catering to areas ranging \\nfrom deep learning to probabilistic inference to data visualization. TensorFlow is one such library \\nthat is well known among the deep learning community, providing many basic and advanced \\noperations that are useful for deep learning. Next, we will use Jupyter Notebook in all our exercises \\nas it provides a rich and interactive environment for coding compared to using Python scripts. \\nWe will also use pandas, NumPy and scikit-learn — three popular — two popular libraries for \\nPython—for various miscellaneous purposes such as data preprocessing. Another library we \\nwill be using for various text-related operations is NLTK—the Python Natural Language Toolkit. \\nFinally, we will use Matplotlib for data visualization.\\nInstalling Anaconda and Python\\nPython is hassle-free to install in any of the commonly used operating systems, such as Windows, \\nmacOS, or Linux. We will use Anaconda to set up Python, as it does all the laborious work for \\nsetting up Python as well as the essential libraries.\\nTo install Anaconda, follow these steps:\\n1. Download Anaconda from https://www.continuum.io/downloads\\n2. Select the appropriate OS and download Python 3.7\\n3. Install Anaconda by following the instructions at https://docs.continuum.io/anaconda/\\ninstall/\\nTo check whether Anaconda was properly installed, open a Terminal window (Command Prompt \\nin Windows), and then run the following command:\\nconda --version', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15995f2f-9515-495a-a98c-4beae848462d', embedding=None, metadata={'page_label': '19', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 19\\nIf installed properly, the version of the current Anaconda distribution should be shown in the \\nTerminal.\\nCreating a Conda environment\\nOne of the attractive features of Anaconda is that it allows you to create multiple Conda, or virtual, \\nenvironments. Each Conda environment can have its own environment variables and Python \\nlibraries. For example, one Conda environment can be created to run TensorFlow 1.x, whereas \\nanother can run TensorFlow 2.x. This is great because it allows you to separate your development \\nenvironments from any changes taking place in the host’s Python installation. Then, you can \\nactivate or deactivate Conda environments depending on which environment you want to use.\\nTo create a Conda environment, follow these instructions:\\n1. Run Conda and create -n packt.nlp.2 python=3.7  in the terminal window using the \\ncommand conda create -n packt.nlp.2 python=3.7 .\\n2. Change directory ( cd) to the project directory.\\n3. Activate the new Conda environment by entering activate  packt.nlp.2  in the terminal. \\nIf successfully activated, you should see (packt.nlp.2)  appearing before the user prompt \\nin the terminal.\\n4. Install the required libraries using one of the following options.\\n5. If you have a GPU, use pip install -r requirements-base.txt -r requirements-\\ntf-gpu.txt\\n6. If you do not have a GPU, use pip install -r requirements-base.txt -r requirements-\\ntf.txt\\nNext, we’ll discuss some prerequisites for GPU support for TensorFlow.\\nTensorFlow (GPU) software requirements\\nIf you are using the TensorFlow GPU version, you will need to satisfy certain software require -\\nments such as installing CUDA 11.0. An exhaustive list is available at https://www.tensorflow.\\norg/install/gpu#software_requirements .\\nAccessing Jupyter Notebook\\nAfter running the pip install  command, you should have Jupyter Notebook  available in the \\nConda environment. To check whether Jupyter Notebook is properly installed and can be accessed, \\nfollow these steps:\\n1. Open a Terminal window.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbff6ff4-f831-45ed-890f-dd56cc12a396', embedding=None, metadata={'page_label': '20', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 20\\n2. Activate the packt.nlp.2  Conda environment if it is not already by running activate \\npackt.nlp.2\\n3. Run the command: jupyter notebook\\nYou should be presented  with a new browser window that looks like Figure 1.6:\\nFigure 1.6: Jupyter Notebook installed successfully\\nVerifying the TensorFlow installation\\nIn this book, we are using TensorFlow 2.7.0. It is important that you install the exact version used \\nin the book as TensorFlow can undergo many changes while migrating from one version to the \\nother. TensorFlow should be installed in the packt.nlp.2  Conda environment if everything went \\nwell. If you are having trouble installing TensorFlow,  you can find guides and troubleshooting \\ninstructions at https://www.tensorflow.org/install .\\nTo check whether TensorFlow installed properly, follow these steps:\\n1. Open Command Prompt in Windows or Terminal in Linux or macOS.\\n2. Activate the packt.nlp.2  Conda environment.\\n3. Type python  to enter the Python prompt. You should now see the Python version right \\nbelow. Make sure that you are using Python 3.\\n4. Next, enter the following commands: \\nimport tensorflow as tf \\nprint(tf. version )\\nIf all went well, you should not have any errors (there might be warnings if your computer does not \\nhave a dedicated GPU, but you can ignore them) and TensorFlow version 2.7.0 should be shown.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ed8c3ea-6d2e-46b0-bd6e-a649fdeb953a', embedding=None, metadata={'page_label': '21', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1 21\\nSeveral popular cloud-based  computational platforms are as follows:\\n• Google Colab: https://colab.research.google.com/\\n• Google Cloud Platform ( GCP ): https://cloud.google.com/\\n• Amazon Web Services ( AWS ): https://aws.amazon.com/\\nGoogle Colab is a great cloud-based platform that allows you to write TensorFlow code and ex -\\necute it on CPU/GPU hardware for free. \\nSummary\\nIn this chapter, we broadly explored NLP to get an impression of the kind of tasks involved in \\nbuilding a good NLP-based system. First, we explained why we need NLP and then discussed \\nvarious tasks of NLP to generally understand the objective of each task and how difficult it is to \\nsucceed at them.\\nAfter that, we looked at the classical approach of solving NLP and went into the details of the \\nworkflow using an example of generating sport summaries for football games. We saw that the \\ntraditional approach usually involves cumbersome and tedious feature engineering. For example, \\nin order to check the correctness of a generated phrase, we might need to generate a parse tree for \\nthat phrase. Then, we discussed the paradigm shift that transpired with deep learning and saw \\nhow deep learning made the feature engineering step obsolete. We started with a bit of time-trav -\\neling to go back to the inception of deep learning and artificial neural networks and worked our \\nway through to the massive modern networks with hundreds of hidden layers. Afterward, we \\nwalked through a simple example illustrating a deep model—a multilayer perceptron model—to \\nunderstand the mathematical wizardry taking place in such a model (on the surface of course!).Many cloud-based computational platforms are also available, where you can set \\nup your own machine with various customization (operating system, GPU card type, \\nnumber of GPU cards, and so on). Many are migrating to such cloud-based services \\ndue to the following benefits: \\n• More customization options \\n• Less maintenance effort \\n• No infrastructure requirements', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a580d38-4ecb-4ca7-8f4c-34449d0de515', embedding=None, metadata={'page_label': '22', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Natural Language Processing 22\\nWith a foundation in both the traditional and modern ways of approaching NLP, we then dis -\\ncussed the roadmap to understand the topics we will be covering in the book, from learning \\nword embeddings to mighty LSTMs, and to state-of-the-art Transformers! Finally, we set up our \\nvirtual Conda environment by installing Python, scikit-learn, Jupyter Notebook, and TensorFlow.\\nIn the next chapter, you will learn the basics of TensorFlow. By the end of the chapter, you should \\nbe comfortable with writing a simple algorithm that can take some input, transform the input \\nthrough a defined function and output the result.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b60f886-33a1-4a4e-aabb-6033679e64e0', embedding=None, metadata={'page_label': '23', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2\\nUnderstanding TensorFlow 2\\nIn this chapter, you will get an in-depth understanding of TensorFlow. This is an open source \\ndistributed numerical computation framework, and it will be the main platform on which we \\nwill be implementing all our exercises. This chapter covers the following topics:\\n• What is TensorFlow?\\n• The building blocks of TensorFlow (for example, variables and operations)\\n• Using Keras for building models\\n• Implementing our first neural network\\nWe will get started with TensorFlow by defining a simple calculation and trying to compute it \\nusing TensorFlow. After we complete this, we will investigate how TensorFlow executes this \\ncomputation. This will help us to understand how the framework creates a computational graph \\nto compute the outputs and execute this graph to obtain the desired outputs. Then we will dive \\ninto the details of how TensorFlow architecture operates by looking at how TensorFlow executes \\nthings, with the help of an analogy of how a fancy café works. We will then see how TensorFlow \\n1 used to work so that we can better appreciate the amazing features TensorFlow 2 offers. Note \\nthat when we use the word “TensorFlow” by itself, we are referring to TensorFlow 2. We will \\nspecifically mention TensorFlow 1 if we are referring to TensorFlow 1.\\nHaving gained a good conceptual and technical understanding of how TensorFlow operates, we \\nwill look at some of the important computations the framework offers. First, we will look at de -\\nfining various data structures in TensorFlow, such as variables and tensors, and we’ll also see how \\nto read inputs through data pipelines. Then we will work through some neural network-related \\noperations (for example, convolution operation, defining losses, and optimization). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4a1be96-c6a2-49f8-8616-0fb0cf0f0cab', embedding=None, metadata={'page_label': '24', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 24\\nFinally, we will apply this knowledge in an exciting exercise, where we will implement a neural net -\\nwork that can recognize images of handwritten digits. You will also see that you can implement or \\nprototype neural networks very quickly and easily by using a high-level submodule such as Keras.\\nWhat is TensorFlow?\\nIn Chapter 1, Introduction to Natural Language Processing , we briefly discussed what TensorFlow \\nis. Now let’s take a closer look at it. TensorFlow is an open source, distributed numerical com-\\nputation framework released by Google that is mainly intended to alleviate the painful details \\nof implementing a neural network (for example, computing derivatives of the weights of the \\nneural network). TensorFlow takes this a step further by providing efficient  implementations of \\nsuch numerical computations using Compute Unified Device Architecture (CUDA), which is \\na parallel computational platform introduced  by NVIDIA (for more information on CUDA, visit \\nhttps://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/ ). The Application Program-\\nming Interface (API) of TensorFlow at https://www.tensorflow.org/api_docs/python/tf/\\nall_symbols  shows that TensorFlow provides thousands of operations  that make our lives easier.\\nTensorFlow was not developed overnight. This is a result of the persistence of talented, good-heart -\\ned developers and scientists who wanted to make a difference by bringing deep learning to a wider \\naudience. If you are interested, you can take a look at the TensorFlow code at https://github.\\ncom/tensorflow/tensorflow . Currently, TensorFlow has around 3,000 contributors, and it sits \\non top of more than 115,000 commits, evolving to be better and better every day.\\nGetting started with TensorFlow 2\\nNow let’s learn about a few essential components in the TensorFlow framework by working \\nthrough a code example. Let’s write an example to perform the following computation, which is \\nvery common for neural networks:\\nℎ = 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 𝑠𝑠𝑠 𝑠𝑠𝑠𝑠  \\nThis computation encompasses what happens in a single layer of a fully connected neural net -\\nwork. Here W and x are matrices and b is a vector. Then, “ .\" denotes the dot product. sigmoid is \\na non-linear transformation given by the following equation:\\nsigmoid(𝑥𝑥𝑥 𝑥1\\n1+𝑒𝑒−𝑥𝑥 \\nWe will discuss how to do this computation through TensorFlow step by step.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8130e447-4337-4244-a1e3-6bf14c3ab1be', embedding=None, metadata={'page_label': '25', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 25\\nFirst, we will need to import TensorFlow and NumPy. NumPy is another scientific computation \\nframework that provides various mathematical and other operations to manipulate data. Import -\\ning them is essential before you run any type of TensorFlow or NumPy-related operation in Python:\\nimport tensorflow as tf \\nimport numpy as np\\nFirst, we will write a function that can take the inputs x, W, and b and perform this computation \\nfor us:\\ndef layer(x, W, b):    \\n    # Building the graph\\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform\\n    return h\\nNext, we add a Python decorator called tf.function  as follows:\\n@tf.function\\ndef layer(x, W, b):    \\n    # Building the graph\\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform\\n    return h\\nPut simply, a Python decorator is just another function. A Python decorator provides a clean way \\nto call another function whenever you call the decorated function. In other words, every time the \\nlayer()  function is called, tf.function()  is called. This can be used for various purposes, such as:\\n• Logging the content and operations in a function\\n• Validating the inputs and outputs of another function\\nWhen the layer()  function is passing through tf.function() , TensorFlow will trace the con-\\ntent (in other words, the operations and data) in the function and build a computational  graph \\nautomatically.\\nThe computational graph (also known as the dataflow graph) builds  a DAG (a directed acyclic \\ngraph) that shows what kind of inputs are required, and what sort of computations need to be \\ndone in the program. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10c676c7-e876-4eaa-ae7d-0f5da6fcd92d', embedding=None, metadata={'page_label': '26', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 26\\nIn our example, the layer()  function produces h by using inputs x, W, and b, and some transfor-\\nmations or operations such as + and tf.matmul() :\\nFigure 2.1: A computational graph of the client\\nIf we look at an analogy for a DAG, if you think of the output as a cake, then the graph would be \\nthe recipe to make that cake using ingredients (that is, inputs). \\nThe feature that builds this computational graph automatically in TensorFlow is known as  \\nAutoGraph . AutoGraph is not just looking at the operations in the passed function; it also scru-\\ntinizes the flow of operations. This means that you can have if statements, or for /while  loops \\nin your function, and AutoGraph will take care of those when building the graph. You will see \\nmore on AutoGraph in the next section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ada16ba7-b54c-416a-9b25-08563ca8357a', embedding=None, metadata={'page_label': '27', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 27\\nNext, you can use this function  right away, as follows:\\nx = np.array([[ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], \\ndtype=np.float32)\\nHere, x is a simple NumPy array:\\ninit_w = tf.initializers.RandomUniform(minval=- 0.1, maxval= 0.1)\\n(shape=[ 10,5])\\nW = tf.Variable(init_w, dtype=tf.float32, name= 'W') \\ninit_b = tf.initializers.RandomUniform()(shape=[ 5])\\nb = tf.Variable(init_b, dtype=tf.float32, name= 'b')\\nW and b are TensorFlow variables defined using the tf.Variable  object. W and b hold tensors. A \\ntensor is essentially an n-dimensional array. For example, a one-dimensional vector or a two-di-\\nmensional matrix are called tensors. A tf.Variable  is a mutable structure, which means the \\nvalues in the tensor stored in that variable can change over time. For example, variables are used \\nto store neural network weights, which change during the model optimization.In TensorFlow 1.x, the user needed to implement the computational graph explicitly. \\nThis meant the user could not write typical Python code using if-else  statements or \\nfor loops, but had to explicitly control the flow of operations using special bespoke \\nTensorFlow operations such as tf.cond()  and tf.control_dependencies() . This \\nis because, unlike TensorFlow 2.x, TensorFlow 1.x did not immediately execute op -\\nerations when you called them. Rather, after they were defined, they needed to be \\nexecuted explicitly using the context of a TensorFlow Session . For example, when \\nyou run the following in TensorFlow 1,\\n             h = tf.nn.sigmoid(tf.matmul(x,W) + b)\\nh will not have any value until h is executed in the context of a Session . Therefore, \\nh could not be treated like any other Python variable. Don’t worry if you don’t un-\\nderstand how the Session  works. It will be discussed in the coming sections.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c37f6c07-bc00-4a56-a4cb-b8dfefd3717c', embedding=None, metadata={'page_label': '28', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 28\\nAlso, note that for W and b, we provide some important arguments, such as the following:\\ninit_w = tf.initializers.RandomUniform(minval=- 0.1, maxval= 0.1)\\n(shape=[ 10,5])\\ninit_b = tf.initializers.RandomUniform()(shape=[ 5])\\nThese are called variable initializers  and are the tensors  that will be assigned to the W and b variables \\ninitially. A variable must have an initial value provided. Here, tf.initializers.RandomUniform  \\nmeans that we uniformly sample values between minval  (-0.1)  and maxval  (0.1)  to assign \\nvalues to the tensors. There are many different initializers provided in TensorFlow ( https://\\nwww.tensorflow.org/api_docs/python/tf/keras/initializers ). It is also very important to \\ndefine the shape of your initializer when you are defining the initializer itself. The shape  property \\ndefines the size of each dimension of the output tensor. For example, if shape  is [10, 5] , this \\nmeans that it will be a two-dimensional structure and will have 10 elements on axis 0 (rows) \\nand 5 elements on axis 1 (columns):\\nh = layer(x,W,b)\\nFinally, h is called a TensorFlow tensor in general. A TensorFlow tensor is an immutable structure. \\nOnce a value is assigned to a TensorFlow tensor, it cannot be changed. \\nAs you can see, the term “tensor” is used in two ways: \\n• To refer to an n-dimensional array\\n• To refer to an immutable data structure in TensorFlow\\nFor both, the underlying concept is the same as they hold an n-dimensional data structure, only \\ndiffering in the context they are used. The term will be used interchangeably to refer to these \\nstructures in our discussion.\\nFinally, you can immediately see the value of h using,\\nprint(f\"h = {h.numpy()} \")\\nwhich will give,\\nh = [[0.7027744 0.687556  0.635395  0.6193934 0.6113584]]\\nThe numpy()  function retrieves the NumPy array from the TensorFlow Tensor object. The full code is \\nas below. All the code examples in this chapter will be available in the tensorflow_introduction.\\nipynb file in the ch2  folder:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64f1a554-4bda-4fb8-970d-049dd1676631', embedding=None, metadata={'page_label': '29', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 29\\n@tf.function\\ndef layer(x, W, b):    \\n    # Building the graph\\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\\n    return h\\nx = np.array([[ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]],  \\ndtype=np.float32) \\n# Variable\\ninit_w = tf.initializers.RandomUniform(minval=- 0.1, maxval= 0.1)\\n(shape=[ 10,5])\\nW = tf.Variable(init_w, dtype=tf.float32, name= \\'W\\') \\n# Variable\\ninit_b = tf.initializers.RandomUniform()(shape=[ 5])\\nb = tf.Variable(init_b, dtype=tf.float32, name= \\'b\\') \\nh = layer(x,W,b)\\nprint(f\"h = {h.numpy()} \")\\nFor future reference, let’s call our example the sigmoid example.\\nAs you can already see, defining a TensorFlow computational graph and executing that is very \\n“Pythonic”. This is because TensorFlow executes its operations “eagerly”, or immediately after the \\nlayer()  function is called. This is a special mode  in TensorFlow known as eager execution  mode. \\nThis was an optional mode for TensorFlow 1, but has been made the default in TensorFlow 2. \\nAlso note that the next two sections will be somewhat complex and technical. However, don’t \\nworry if you don’t understand everything completely because the explanation will be supple -\\nmented with a more digestible, and thorough, real-world example that explains how an order is \\nfulfilled in our new-and-improved restaurant, Café Le TensorFlow 2 .\\nTensorFlow 2 architecture – What happens during graph \\nbuild?\\nLet’s now understand what TensorFlow does when you execute TensorFlow operations.\\nWhen you call a function decorated by tf.function() , such as the layer()  function, there is quite \\na bit happening in the background. First, TensorFlow will trace all the TensorFlow operations \\ntaking place in the function and build the computational graph automatically. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='693ef652-1957-44c7-9b44-c3516425612c', embedding=None, metadata={'page_label': '30', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 30\\nIn fact, tf.function()  will return a function that executes the built dataflow graph when invoked. \\nTherefore, tf.function()  is a multi-stage process, where it first builds the dataflow graph and \\nthen executes it. Additionally, since TensorFlow traces each line in the function, if something \\ngoes wrong, TensorFlow can point to the exact line that is causing the issue. \\nIn our sigmoid example, the computational, or dataflow, graph would look like  Figure 2.2. A single \\nelement  or vertex of the graph is called a node � There are two main types of objects in this graph: \\noperations and tensors . In the preceding example, tf.nn.sigmoid  is an operation and h is a tensor:\\nFigure 2.2: A computational graph of the client\\nThe preceding graph shows the order of operations as well as how inputs flow through them.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b5a2a17-55b9-4acd-ae21-7499ce395340', embedding=None, metadata={'page_label': '31', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 31\\nNow we know that TensorFlow is skilled at creating a nice computational graph, with all the \\ndependencies and operations so that it knows exactly how, when, and where the data flows. \\nHowever, we did not quite answer how this graph is executed. In fact, TensorFlow does quite a \\nbit behind the scenes. For example, the graph might be divided into subgraphs, and subsequently \\ninto even finer pieces, to achieve parallelization. These subgraphs or pieces will then be assigned \\nto workers that will perform the assigned task. \\nTensorFlow architecture – what happens when you execute \\nthe graph?\\nThe computational graph uses the tf.GraphDef  protocol to canonicalize the dataflow graph and \\nsend it to the distributed master. The distributed master would perform the actual operation \\nexecution and parameter updates in a single-process setting. In a distributed setting, the master \\nwould delegate these tasks to worker processes/devices and manage these worker processes. \\ntf.GraphDef  is a standardized representation of the graph specific to TensorFlow. The distributed \\nmaster sees all computations in the graph and divides the computations into different devices \\n(for example, different GPUs and CPUs). TensorFlow operations have multiple kernels. A kernel is \\na device-specific implementation of a certain operation. For example, the tf.matmul()  function \\nwill be implemented differently to run on the CPU or GPU since, on a GPU, you can achieve much \\nbetter performance due to more parallelization.\\nNext, the computational graph will be broken into subgraphs and pruned by the distributed \\nmaster. Although decomposing the computational graph in Figure 2.2  appears too trivial in our \\nexample, the computational graph can exponentially grow in real-world solutions with many \\nhidden layers. Additionally, it becomes important to break the computational graph into multiple \\npieces and shave off any redundant computations in order to get results faster (for example, in \\na multi-device setting). Keep in mind that tf.function()  or AutoGraph is not a silver bullet that turns \\nany arbitrary Python function using TensorFlow operations into a computational \\ngraph; it has its limitations. For example, the current version cannot handle recur -\\nsive calls. To see a full list of the eager mode capabilities, refer to the following link:  \\nhttps://github.com/sourcecode369/tensorflow-1/blob/master/\\ntensorflow/python/autograph/g3doc/reference/limitations.md .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bfb88718-1351-4be9-b2e5-331cffe6ac5c', embedding=None, metadata={'page_label': '32', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 32\\nExecuting the graph or a subgraph (if the graph is divided into subgraphs) is called a single task , \\nwhere each task is allocated  to a single worker (which could be a single process or an entire device). \\nThese workers can run as a single process in a multi-process device (for example, a multi-process -\\ning CPU), or run on different devices (for example, CPUs and GPUs). In a distributed  setting, we \\nwould have multiple workers executing tasks (for example, multiple workers training the model \\non different batches of data). On the contrary, we have only one set of parameters. So how do \\nmultiple workers manage to update the same set of parameters?\\nTo solve this, there is one worker that is considered the parameter server and will hold the main \\ncopy of the parameters. The workers will copy the parameters over, update them, and send them \\nback to the parameter server. Typically, the parameter server will define some resolution strat -\\negy to resolve multiple updates coming from multiple workers (for example, taking the mean). \\nThese details were provided so you can understand the complexity that has gone into Tensor -\\nFlow. However, our book will be based on using TensorFlow in a single-process/worker setting. \\nIn this setting, the organization of the distributed master, workers, and the parameter server is \\nmuch more straightforward and is absorbed mostly by a special session implementation used by  \\nTensorFlow. This general workflow of a TensorFlow client is depicted in Figure 2.3 :\\nFigure 2.3: The generic execution of a TensorFlow client. A TensorFlow client starts with a graph \\nthat gets sent to the distributed master. The master spins up worker processes to perform \\nactual tasks and parameter updates\\nOnce the calculation is done, the session brings back the updated data to the client from the \\nparameter server. The architecture of TensorFlow is shown in Figure 2.4:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17f357bd-8f45-4417-a8f6-e3e87eb594f7', embedding=None, metadata={'page_label': '33', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 33\\nFigure 2.4: TensorFlow framework architecture. This explanation is based on the official \\nTensorFlow documentation found at: https://github.com/tensorflow/docs/blob/\\nmaster/site/en/r1/guide/extend/architecture.md\\nMost of the changes introduced in TensorFlow 2 can be attributed to front-end changes. That is, \\nhow the dataflow graph is built and when the graph is executed. The way the graph is executed \\nremains more or less the same in TensorFlow 1 and 2.\\nNow we know what happens end-to-end from the moment you execute tf.function() , but this \\nwas a very technical explanation, and nothing explains something better than a good analogy. \\nTherefore, we will try to understand TensorFlow 2 with an analogy to our new and improved \\nCafé Le TensorFlow 2.\\nCafé Le TensorFlow 2 – understanding TensorFlow 2 with \\nan analogy\\nLet’s say the owners renovated  our previous Café Le TensorFlow (this is an analogy from the first \\nedition) and reopened it as Café Le TensorFlow 2. The word around the town is that it’s much \\nmore opulent than it used to be. Remembering the great experience you had before, you book a \\ntable instantly and go there to grab a seat. \\nYou want to order a chicken burger with extra cheese and no tomatoes . And you realize the café is \\nindeed fancy. There’re no waiters here, but a voice-enabled tablet for each table into which you \\nsay what you want. This will get converted to a standard format that the chefs will understand \\n(for example, table number, menu item ID, quantity, and special requirements). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b8b8c8c-b6ed-48e6-9e9f-ba349efa6f7f', embedding=None, metadata={'page_label': '34', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 34\\nHere, you represent the TensorFlow 2 program. The ability of the voice-enabled tablet that converts \\nyour voice (or TensorFlow operations) to the standard format (or GraphDef format) is analogous \\nto the AutoGraph feature.\\nNow comes the best part. As soon as you start speaking, a manager will be looking at your order \\nand assigning various tasks to chefs. The manager is responsible for making sure things happen as \\nquickly as possible. The kitchen manager makes decisions, such as how many chefs are required \\nto make the dish and which chefs are the best candidates for the job. The kitchen manager rep -\\nresents the distributed master.\\nEach chef has a cook whose responsibility it is to provide the chef with the right ingredients, \\nequipment, and so forth. So, the kitchen manager takes the order to a single chef and a cook (a \\nburger is not that hard to prepare) and asks them to prepare the dish. The chef looks at the order \\nand tells the cook what is needed. So, the cook first finds the things that will be required (for \\nexample, buns, patties, and onions) and keeps them close to fulfill the chef’s requests as soon \\nas possible. Moreover, the chef might also ask to keep the intermediate results (for example, cut \\nvegetables) of the dish temporarily until the chef needs it back again. In our example, the chef is \\nthe operation executor, and the cook is the parameter server.\\nThis café is full of surprises. As you are speaking out your order (that is, invoking Python functions \\nthat have TensorFlow operations), you see it getting prepared in real time through the tablet on \\nyour table (that is, eager execution).\\nThe best thing about this video feed is that, if you see that the chef did not put enough cheese, \\nyou know exactly why the burger wasn’t as good as expected. So, you can either order another \\none or provide specific feedback. This is a great improvement over how TensorFlow 1 did things, \\nwhere they would take your order and you would not see anything until the full burger had been \\nprepared. This process is shown in Figure 2.5 :', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='967fad9f-eed9-4975-8b6f-e85f81896f47', embedding=None, metadata={'page_label': '35', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 35\\nFigure 2.5: The restaurant analogy illustrated\\nLet’s now have a look back at how TensorFlow 1 used to work.\\nFlashback: TensorFlow 1\\nWe said numerous  times that TensorFlow 2 is very different from TensorFlow 1. But we still don’t \\nknow what it used to be like. Therefore, let’s now do a bit of time traveling to see how the same \\nsigmoid computation could have been implemented in TensorFlow 1.\\nFirst, we’ll define a graph  object, which we will populate with operations and variables later:\\ngraph = tf.Graph() # Creates a graph\\nsession = tf.InteractiveSession(graph=graph) # Creates a session\\nThe graph  object contains the computational graph that connects the various inputs and outputs \\nwe define in our program to get the final desired output. This is the same graph we discussed ear -\\nlier. Also, we’ll define a session object that takes the defined graph as the input, which executes \\nthe graph. In other words, compared to TensorFlow 2, the graph  object and the session  object \\ndo what happens when you invoke them decorated by tf.function() .Warning\\nYou will not be able to execute the following code in TensorFlow 2.x as it stands. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f3ab559-31f2-442a-98b5-f21bb6afb541', embedding=None, metadata={'page_label': '36', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 36\\nNow we’ll define a few tensors, namely x, W, b, and h. There are several different ways that you can  \\ndefine tensors in TensorFlow 1. Here, we will look at three such different approaches:\\n• First, x is a placeholder. Placeholders, as the name suggests, are not initialized with any \\nvalue. Rather, we will provide the value on the fly at the time of the graph execution. If \\nyou remember from the TensorFlow 2 sigmoid exercise, we fed x (which was a NumPy \\narray) directly to the function layer(x, w, b) . Unlike in TensorFlow 2, you cannot feed \\nNumPy arrays directly to TensorFlow 1 graphs or operations. \\n• Next, we have the variables W and b. Variables are defined similarly to TensorFlow 2 with \\nsome minor changes in the syntax. \\n• Finally, we have h, which is an immutable tensor produced by performing some opera-\\ntions on x, W, and b. Note that you will not see the value of h immediately as you needed \\nto manually execute the graph in TensorFlow 1.\\nThese tensors are defined as follows:\\nx = tf.placeholder(shape=[ 1,10],dtype=tf.float32,name= 'x')\\nW = tf.Variable(tf.random_uniform(shape=[ 10,5], minval=- 0.1, maxval= 0.1, \\ndtype=tf.float32),name= 'W')\\nb = tf.Variable(tf.zeros(shape=[ 5],dtype=tf.float32),name= 'b') h = tf.nn.\\nsigmoid(tf.matmul(x,W) + b)\\nNext, we’ll run an initialization operation that initializes the variables in the graph, W and b:\\ntf.global_variables_initializer().run()\\nNow, we will execute the graph to obtain the final output we need, h. This is done by running \\nsession.run(...) , where we provide the value to the placeholder as an argument of the session.\\nrun()  command:\\nh_eval = session.run(h,feed_dict={x: np.random.rand( 1,10)})The lifetime of variables in TensorFlow 1 was managed by the session object, mean-\\ning that variables lived in memory for as long as the session lived (even after losing \\nreferences to them in the code). However, in TensorFlow 2, variables are removed \\nsoon after the variables are not referenced in the code, just like in Python.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c81edb5-f02c-4a1a-a201-32a94cfeac77', embedding=None, metadata={'page_label': '37', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 37\\nFinally, we close the session, releasing any resources held by the session  object:\\nsession.close()\\nHere is the full code of this TensorFlow 1 example:\\nimport tensorflow as tf import numpy as np\\n# Defining the graph and session graph = tf.Graph() # Creates a graph\\nsession = tf.InteractiveSession(graph=graph) # Creates a session\\n# Building the graph\\n# A placeholder is an symbolic input\\nx = tf.placeholder(shape=[ 1,10],dtype=tf.float32,name= 'x')\\n# Variable\\nW = tf.Variable(tf.random_uniform(shape=[ 10,5], minval=- 0.1, maxval= 0.1, \\ndtype=tf.float32),name= 'W') \\nb = tf.Variable(tf.zeros(shape=[ 5],dtype=tf.float32),name= 'b')\\nh = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\\n# Executing operations and evaluating nodes in the graph\\ntf.global_variables_initializer().run()  # Initialize the variables\\n# Run the operation by providing a value to the symbolic input x h_eval = \\nsession.run(h,feed_dict={x: np.random.rand(1,10)})\\n# Closes the session to free any held resources by the session\\nsession.close()\\nAs you can see, before TensorFlow 2 the user had to:\\n• Define the computational graph using various TensorFlow data structures (for example, \\ntf.placeholder ) and operations (for example, tf.matmul() )\\n• Execute the required part of the graph using session.run()  to fetch the results by feeding \\nthe correct data into the session\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d99c300a-1063-4f2c-8256-aa3787db7ba5', embedding=None, metadata={'page_label': '38', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 38\\nIn conclusion, TensorFlow 1.x had several limitations:\\n• Coding with TensorFlow 1 did not provide the same intuitive “Pythonic” feeling as you \\nneeded to define the computational graph first and then invoke the execution of it. This \\nis known as declarative programming.\\n• The design in TensorFlow 1 made it very hard to break the code down into manageable \\nfunctions as the user needed to define the graph fully, before doing any computations. \\nThis resulted in very large functions or pieces of code containing very large computational \\ngraphs. \\n• It was very difficult to do real-time debugging of the code as TensorFlow had its own \\nruntime that used session.run() .\\n• But also, it was not without some advantages, such as the efficiency brought about by \\ndeclaring the full computational graph upfront. Knowing all the computations in advance \\nmeant TensorFlow 1 could perform all sorts of optimizations (for example, graph pruning) \\nto run the graph efficiently. \\nIn this part of the chapter, we discussed our first example in TensorFlow2 and the architecture \\nof TensorFlow. Finally, we compared and contrasted TensorFlow 1 and 2. Next, we will discuss \\nthe various building blocks of TensorFlow 2.\\nInputs, variables, outputs, and operations\\nNow we are returning from our journey into TensorFlow 1 and stepping back to TensorFlow 2. \\nLet’s proceed to the most common elements that comprise a TensorFlow 2 program. If you read \\nany of the millions of TensorFlow clients available on the internet, the TensorFlow-related code \\nall falls into one of these buckets:\\n• Inputs: Data used to train and test our algorithms\\n• Variables: Mutable tensors, mostly  defining the parameters of our algorithms\\n• Outputs: Immutable tensors storing  both terminal and intermediate outputs\\n• Operations : Various transformations for inputs to produce the desired outputs\\nIn our earlier sigmoid example, we can find instances of all these categories. We list the respective \\nTensorFlow elements and the notation used in the sigmoid example in Table 2.1 :', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a1fb3ee-cf69-44a2-af39-a585e8ab31f2', embedding=None, metadata={'page_label': '39', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 39\\nTensorFlow element Value from example client\\nInputs x\\nVariables W and b\\nOutputs h\\nOperations tf.matmul(...) , tf.nn.sigmoid(...)\\nTable 2.1: The different types of TensorFlow primitives we have encountered so far\\nThe following subsections explain each of these TensorFlow elements listed in the table in more \\ndetail.\\nDefining inputs in TensorFlow\\nThere are three different  ways you can feed data to a TensorFlow program:\\n• Feeding data as NumPy arrays\\n• Feeding data as TensorFlow tensors\\n• Using the tf.data API to create an input pipeline\\nNext, we will discuss a few different ways you can feed data to TensorFlow operations.\\nFeeding data as NumPy arrays\\nThis is the simplest way to feed data into a TensorFlow program. Here, you pass a NumPy array \\nas an input to the TensorFlow operation and the result is executed immediately. This is exactly \\nwhat we did in the sigmoid example. If you look at x, it is a NumPy array.\\nFeeding data as tensors\\nThe second method  is like the first one, but the type  of data is different. Here, we are defining x \\nas a TensorFlow tensor.\\nTo see this in action, let’s modify our sigmoid example. Remember that we defined x as:\\nx = np.array([[ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]],  \\ndtype=np.float32)\\nInstead, let’s define this as a tensor that contains specific values:\\nx = tf.constant(value=[[ 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]],\\ndtype=tf.float32,name= 'x')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62eb3397-766d-4185-86c9-b902884ed68d', embedding=None, metadata={'page_label': '40', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 40\\nAlso, the full code  would become as follows:\\nimport tensorflow as tf\\n@tf.function\\ndef layer(x, W, b):    \\n    # Building the graph\\n    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed\\n    return h\\n# A pre-loaded input\\nx = tf.constant(value=[[ 0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]], \\ndtype=tf.float32,name= \\'x\\') \\n# Variable\\ninit_w = tf.initializers.RandomUniform(minval=- 0.1, maxval= 0.1)\\n(shape=[ 10,5])\\nW = tf.Variable(init_w, dtype=tf.float32, name= \\'W\\') \\n# Variable\\ninit_b = tf.initializers.RandomUniform()(shape=[ 5])\\nb = tf.Variable(init_b, dtype=tf.float32, name= \\'b\\') \\nh = layer(x,W,b)\\nprint(f\"h = {h}\")\\nprint(f\"h is of type  {type(h)}\")\\nLet’s now discuss how we can define data pipelines in TensorFlow.\\nBuilding a data pipeline using the tf.data API\\ntf.data  provides you with a convenient way to build data pipelines in TensorFlow. Input pipelines  \\nare designed for more heavy-duty programs that need to process a lot of data. For example, if you \\nhave a small dataset (for example, the MNIST dataset) that fits into the memory, input pipelines \\nwould be excessive. However, when working with complex data or problems, where you might \\nneed to work with large datasets that do not fit in memory, augment the data (for example, for \\nadjusting image contrast/brightness), numerically transform it (for example, standardize), and \\nso on. The tf.data API provides convenient functions that can be used to easily load and trans-\\nform your data. Furthermore, it streamlines your data ingestion code with the model training. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='657cf5c9-ebe1-42c7-9823-366f374422d1', embedding=None, metadata={'page_label': '41', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 41\\nAdditionally, the tf.data API offers various options to enhance the performance of your data \\npipeline, such as multi-processing and pre-fetching data. Pre-fetching refers to bringing data \\ninto the memory before it’s required and keeping it ready. We will discuss these methods in more \\ndetail as they are used in the upcoming chapters.\\nWhen creating an input pipeline, we intend to perform the following:\\n• Source the data from a data source (for example, an in-memory NumPy array, CSV file on \\ndisk, or individual files such as images).\\n• Apply various transformations to the data (for example, cropping/resizing image data).\\n• Iterate the resulting dataset element/batch-wise. Batching is required as deep learning \\nmodels are trained on randomly sampled batches of data. As the datasets these models \\nare trained on are large, they typically do not fit in memory.\\nLet’s write an input pipeline using TensorFlow’s tf.data API. In this example, we have three text \\nfiles (iris.data.1 , iris.data.2 , and iris.data.3 ) in CSV format, each file having 50 lines and \\neach line having 4 floating-point numbers (in other words, various lengths associated with a flow -\\ner) and a string label separated by commas (an example line would be 5.6,2.9,3.6,1.3,Iris-\\nversicolor ). We will now use the tf.data API to read data from these files. We also know that \\nsome of this data is corrupted (as with any real-life machine learning project). In our case, some \\ndata points have negative lengths. So, let’s first write a pipeline to go through the data row by \\nrow and print the corrupted outputs. \\nFirst, let’s import a few important libraries as before:\\nimport tensorflow as tf\\nimport numpy as np\\nNext, we will define a list containing the filenames:\\nfilenames = [ f\"./iris.data .{i}\" for i in range (1,4)]\\nNow we will use one of the dataset readers provided in TensorFlow. The dataset reader takes in \\na list of filenames and another list that specifies the data types of each column in the dataset. As \\nwe saw previously, we have four floating numbers and one string:For more information, refer to the official TensorFlow page on importing data at \\nhttps://www.tensorflow.org/guide/data .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='81f174cd-6f04-49ae-b3a9-7fa07533aefe', embedding=None, metadata={'page_label': '42', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 42\\ndataset = tf.data.experimental.CsvDataset(filenames, [tf.float32, \\ntf.float32, tf.float32, tf.float32, tf.string])\\nNow we will organize our data into inputs and labels as follows:\\ndataset = dataset. map(lambda x1,x2,x3,x4,y: (tf.stack([x1,x2,x3,x4]), y))\\nWe are using lambda functions to separate out x1,x2,x3,x4  into one dataset and y to another \\ndataset, along with the dataset.map()  function.\\nHere, tf.stack()  stacks individual tensors (here, the individual feature) to a single tensor. When \\nusing the map function, you first need to visualize what needs to be done to a single item in the \\ndataset (a single item in our case is a single row from the dataset), and write the transformation.\\nNext, you can iterate through this dataset, examining individual data points, as you would iterate \\nthrough a normal Python list. Here, we are printing out all the corrupted items:\\nfor next_element in dataset:\\n    x, y = next_element[ 0].numpy(), next_element[ 1].numpy().\\ndecode(\\'ascii\\')\\n    if np.min(x)<0.0:\\n        print(f\"(corrupted) X =>  {x}\\\\tY => {y}\")Lambda functions are a special type of function that allow you to define some com-\\nputations succinctly. With lambda functions, you don’t need to name your function, \\nwhich can be quite handy if you are using a certain function only once in your code. \\nThe format of the lambda function looks like:\\n  lambda <arguments>: <result returned after the computation> \\nFor example, if you need to write a function that adds two numbers, simply write:\\n                        lambda x, y: x+y\\nThe map function is very simple but powerful. All it does is transform a set of given \\ninputs into a new set of values. For example, if you have a list, xx, that contains a \\nlist of numbers and want to convert them to power 2 element-wise, you can write \\nsomething like xx_pow = map(lambda x: x**2, xx) . And this can be very easily \\nparallelized as there’s no dependency between items.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1870e9ee-5509-48fc-afe3-71607c46de3d', embedding=None, metadata={'page_label': '43', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 43\\nSince you don’t want those corrupted inputs in your dataset, you can use the dataset.filter()  \\nfunction to filter out those corrupted entries as follows:\\ndataset = dataset. filter(lambda x,y: tf.reduce_min(x)> 0)\\nHere we are checking whether the minimum element in x is greater than zero; if not, those ele -\\nments will be filtered out of the dataset.\\nAnother useful function is dataset.batch() . When training deep neural networks, we often \\ntraverse the dataset in batches, not individual items. dataset.batch()  provides a convenient \\nway to do that:\\nbatch_size = 5\\ndataset = dataset.batch(batch_size=batch_size)\\nNow, if you print the shape of a single element in your dataset, you should get the following: \\nx.shape = ( 5, 4), y.shape = ( 5,)\\nNow that we have examined the three different methods you can use to define inputs in Tensor -\\nFlow, let’s see how we can define variables in TensorFlow.\\nDefining variables in TensorFlow\\nVariables play an important role in TensorFlow. A variable is essentially a tensor with a spe -\\ncific shape defining how many dimensions the variable will have and the size of each dimen -\\nsion. However, unlike a regular TensorFlow tensor, variables are mutable; meaning that the \\nvalue of the variables can change after they are defined. This is an ideal property to have to \\nimplement the parameters of a learning model (for example, neural network weights), where \\nthe weights change slightly after each step of learning. For example, if you define a variable \\nwith x = tf.Variable(0,dtype=tf.int32) , you can change the value of that variable using a \\nTensorFlow operation such as tf.assign(x,x+1) . However, if you define a tensor such as x = \\ntf.constant(0,dtype=tf.int32) , you cannot change the value of the tensor, as you could for a \\nvariable. It should stay 0 until the end of the program execution.\\nVariable creation is quite simple. In our sigmoid example, we already created two variables, W and \\nb. When creating a variable, a few things are extremely important. We will list them here and \\ndiscuss each in detail in the following paragraphs:\\n• Variable shape\\n• Initial values', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88b2c1c0-4701-4dd0-afc5-ea2511034bde', embedding=None, metadata={'page_label': '44', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 44\\n• Data type\\n• Name (optional)\\nThe variable shape is a list of the [x,y,z,...]  format. Each value in the list indicates how large \\nthe corresponding dimension or axis is. For instance, if you require a 2D tensor with 50 rows and \\n10 columns as the variable, the shape would be equal to [50,10] .\\nThe dimensionality of the variable (that is, the length of the shape  vector) is recognized as the \\nrank of the tensor in TensorFlow. Do not confuse this with the rank of a matrix.\\nNext, a variable requires an initial value to be initialized with. TensorFlow provides several dif-\\nferent initializers for our convenience, including constant initializers and normal distribution \\ninitializers. Here are a few popular TensorFlow initializers you can use to initialize variables:\\n• tf.initializers.Zeros\\n• tf.initializers.Constant\\n• tf.initializers.RandomNormal\\n• tf.initializers.GlorotUniform\\nThe shape of the variable can be provided as a part of the initializer as follows:\\ntf.initializers.RandomUniform(minval=- 0.1, maxval= 0.1)(shape=[ 10,5])\\nThe data type plays an important role in determining the size of a variable. There are many different \\ndata types, including the commonly used tf.bool , tf.uint8 , tf.float32 , and tf.int32 . Each \\ndata type has a number of bits required to represent a single value with that type. For example, \\ntf.uint8  requires 8 bits, whereas tf.float32  requires 32 bits. It is common practice to use the \\nsame data types for computations, as doing otherwise can lead to data type mismatches. So, if you \\nhave two different data types for two tensors that you need to transform, you have to explicitly \\nconvert one tensor to the other tensor’s type using the tf.cast(...)  operation.\\nThe tf.cast(...)  operation is designed to cope with such situations. For example, if you have \\nan x variable with the tf.int32  type, which needs to be converted to tf.float32 , employ \\ntf.cast(x,dtype=tf.float32)  to convert x to tf.float32 .Tensor rank in TensorFlow indicates the dimensionality of the tensor; for a two-di-\\nmensional matrix, rank  = 2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c4193ab-adaa-4f05-ae3d-dc6854266f6b', embedding=None, metadata={'page_label': '45', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 45\\nFinally, the name of the variable will be used as an ID to identify that variable in the graph. If you \\never visualize the computational graph, the variable will appear by the argument passed to the \\nname  keyword. If you do not specify a name, TensorFlow will use the default naming scheme.\\nMoving on, let’s talk about how to define TensorFlow outputs.\\nDefining outputs in TensorFlow\\nTensorFlow outputs  are usually tensors, and the result of a transformation to either an input, or \\na variable, or both. In our example, h is an output, where h = tf.nn.sigmoid(tf.matmul(x,W) \\n+ b) . It is also possible to give such outputs to other operations, forming a chained set of opera-\\ntions. Furthermore, they do not necessarily have to be TensorFlow operations. You also can use \\nstandard Python arithmetic with TensorFlow. Here is an example:\\nx = tf.matmul(w,A) \\ny = x + B\\nBelow, we explain various operations available in TensorFlow and how to use them.\\nDefining operations in TensorFlow\\nAn operation in TensorFlow takes one or more inputs and produces one or more outputs. If you \\ntake a look at the TensorFlow API at https://www.tensorflow.org/api_docs/python/tf , you \\nwill see that TensorFlow has a massive collection of operations available. Here, we will take a \\nlook at a selected few of the myriad TensorFlow operations.\\nComparison operations\\nComparison operations  are useful for comparing two tensors. The following code example in -\\ncludes a few useful comparison operations. Note that the Python variable tf.Variable  is assigned to is not known by the com-\\nputational graph, and is not a part of TensorFlow variable naming. Consider this \\nexample where you specify a TensorFlow variable as follows:\\na = tf.Variable(tf.zeros([ 5]),name= 'b')\\nHere, the TensorFlow graph will know this variable by the name b and not a.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e9c7549-5693-4b71-a773-f9af533eab38', embedding=None, metadata={'page_label': '46', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 46\\nTo understand the working of these operations, let’s consider two example tensors, x and y:\\n# Let's assume the following values for x and y \\n# x (2-D tensor) => [[1,2],[3,4]]\\n# y (2-D tensor) => [[4,3],[3,2]]\\nx = tf.constant([[ 1,2],[3,4]], dtype=tf.int32)\\ny = tf.constant([[ 4,3],[3,2]], dtype=tf.int32)\\n# Checks if two tensors are equal element-wise and returns a boolean\\n# tensor\\n# x_equal_y => [[False,False],[True,False]] \\nx_equal_y = tf.equal(x, y, name= None)\\n# Checks if x is less than y element-wise and returns a boolean tensor\\n# x_less_y => [[True,True],[False,False]]\\nx_less_y = tf.less(x, y, name= None)\\n# Checks if x is greater or equal than y element-wise and returns a\\n# boolean tensor\\n# x_great_equal_y => [[False,False],[True,True]]\\nx_great_equal_y = tf.greater_equal(x, y, name= None)\\n# Selects elements from x and y depending on whether, # the condition \\nis satisfied (select elements from x) # or the condition failed (select \\nelements from y)\\ncondition = tf.constant([[ True,False],[True,False]],dtype=tf. bool)\\n# x_cond_y => [[1,3],[3,2]]\\nx_cond_y = tf.where(condition, x, y, name= None)\\nNext, let’s look  at some  mathematical operations.\\nMathematical operations\\nTensorFlow allows you to perform math operations on tensors that range from the simple to the \\ncomplex. We will discuss a few of the mathematical operations made available in TensorFlow. \\nThe complete set of operations is available at https://www.tensorflow.org/versions/r2.0/\\napi_docs/python/tf/math :\\n# Let's assume the following values for x and y\\n# x (2-D tensor) => [[1,2],[3,4]]\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33730ec9-919b-4211-98c3-955055a60f7c', embedding=None, metadata={'page_label': '47', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 47\\n# y (2-D tensor) => [[4,3],[3,2]]\\nx = tf.constant([[ 1,2],[3,4]], dtype=tf.float32)\\ny = tf.constant([[ 4,3],[3,2]], dtype=tf.float32)\\n# Add two tensors x and y in an element-wise fashion\\n# x_add_y => [[5,5],[6,6]]\\nx_add_y = tf.add(x, y)\\n# Performs matrix multiplication (not element-wise)\\n# x_mul_y => [[10,7],[24,17]]\\nx_mul_y = tf.matmul(x, y)\\n# Compute natural logarithm of x element-wise # equivalent to computing \\nln(x)\\n# log_x => [[0,0.6931],[1.0986,1.3863]]\\nlog_x = tf.log(x)\\n# Performs reduction operation across the specified axis\\n# x_sum_1 => [3,7]\\nx_sum_1 = tf.reduce_sum(x, axis=[ 1], keepdims= False)\\n# x_sum_2 => [[4,6]]\\nx_sum_2 = tf.reduce_sum(x, axis=[ 0], keepdims= True)\\n# Segments the tensor according to segment_ids (items with same id in\\n# the same segment) and computes a segmented sum of the data\\ndata = tf.constant([ 1,2,3,4,5,6,7,8,9,10], dtype=tf.float32)\\nsegment_ids = tf.constant([ 0,0,0,1,1,2,2,2,2,2 ], dtype=tf.int32)\\n# x_seg_sum => [6,9,40]\\nx_seg_sum = tf.segment_sum(data, segment_ids)\\nNow, we  will look  at the scatter operation.\\nUpdating (scattering) values in tensors\\nA scatter operation, which refers to changing the values at certain indices of a tensor, is very \\ncommon in scientific computing problems. This functionality was originally provided through \\nan intimidating tf.scatter_nd()  function, which can be difficult to understand. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a61c7f8d-93c0-46a8-9dd1-af730e5207fa', embedding=None, metadata={'page_label': '48', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 48\\nHowever, in recent TensorFlow versions, you can perform scatter operations via array indexing \\nand slicing using NumPy-like syntax. Let’s see a few examples. Say you have the TensorFlow \\nvariable v, which is a [3,2] matrix:\\nv = tf.Variable(tf.constant([[ 1,9],[3,10],[5,11]], \\ndtype=tf.float32),name= 'ref')\\nYou can change the 0th row of this tensor with:\\nv[0].assign([- 1, -9])\\nwhich results in:\\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\\narray([[-1., -9.],\\n       [ 3., 10.],\\n       [ 5., 11.]], dtype=float32)>\\nYou can change the value at index [1,1] with:\\nv[1,1].assign(- 10)\\nwhich results in:\\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\\narray([[  1.,   9.],\\n       [  3., -10.],\\n       [  5.,  11.]], dtype=float32)>\\nYou can perform row slicing with:\\nv[1:,0].assign([- 3,-5])\\nwhich results in:\\n<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=\\narray([[ 1.,  9.],\\n       [-3., 10.],\\n       [-5., 11.]], dtype=float32)>\\nIt is important to remember that the scatter operation (performed via the assign()  \\noperation) can only be performed on tf.Variables , which are mutable structures. \\nRemember that tf.Tensor /tf.EagerTensor  are immutable objects.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='391ededa-2daa-4886-93c7-7004d830771f', embedding=None, metadata={'page_label': '49', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 49\\nCollecting (gathering) values from a tensor\\nA gather operation is very similar to a scatter operation. Remember  that scattering is about as -\\nsigning values to tensors, whereas gathering retrieves the values of a tensor. Let’s understand \\nthis through an example. Say you have a TensorFlow tensor, t:\\nt = tf.constant([[ 1,9],[3,10],[5,11]],dtype=tf.float32)\\nYou can obtain the 0th row of t with:\\nt[0].numpy()\\nwhich will return:\\n[1. 9.]\\nYou can also perform row-slicing with:\\nt[1:,0].numpy()\\n which will return:\\n[3. 5.]\\nUnlike the scatter  operation, the gather operation works both on tf.Variable  and tf.Tensor  \\nstructures.\\nNeural network-related operations\\nNow, let’s look at several useful neural network-related operations that we will use heavily in \\nthe following chapters. The operations we will discuss here range from simple element-wise \\ntransformations (that is, activations) to computing partial derivatives of a set of parameters \\nwith respect to another value. We will also implement a simple neural network as an exercise.\\nNonlinear activations used by neural networks\\nNonlinear activations  enable neural networks to perform well at numerous tasks. Typically, there \\nis a nonlinear activation transformation (that is, activation layer) after each layer output in a \\nneural network (except for the last layer). A nonlinear transformation helps a neural network \\nto learn various nonlinear patterns that are present in data. This is very useful for complex re -\\nal-world problems, where data often has more complex nonlinear patterns, in contrast to linear \\npatterns. If not for the nonlinear activations between layers, a deep neural network would be a \\nbunch of linear layers stacked on top of each other. Also, a set of linear layers can essentially be \\ncompressed to a single bigger linear layer. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfcdc740-fa14-4042-a61b-338b59eef100', embedding=None, metadata={'page_label': '50', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 50\\nIn conclusion, if not for the nonlinear activations, we cannot create a neural network with more \\nthan one layer.\\nNow we’ll list two commonly used nonlinear activations in neural networks (in other words, \\nsigmoid and ReLU) and how they can be implemented in TensorFlow:\\n# Sigmoid activation of x is given by 1 / (1 + exp(-x))\\ntf.nn.sigmoid(x,name= None)\\n# ReLU activation of x is given by max(0,x) \\ntf.nn.relu(x, name= None)\\nThe functional form of these computations is visualized in Figure 2.6:Let’s observe the importance of nonlinear activation through an example. First, re -\\ncall the computation for the neural networks we saw in the sigmoid example. If we \\ndisregard b, it will be this:\\nh = sigmoid(W*x)\\nAssume a three-layer neural network (having W1, W2, and W3 as layer weights) where \\neach layer does the preceding computation; we can summarize the full computation \\nas follows:\\nh = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))\\nHowever, if we remove the nonlinear activation (that is, sigmoid), we get this:\\nh = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x\\nSo, without the nonlinear activations, the three layers can be brought down to a \\nsingle linear layer.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc9a9957-522f-4a66-8aea-5c28c5a48539', embedding=None, metadata={'page_label': '51', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 51\\nFigure 2.6: The functional forms of sigmoid (left) and ReLU (right) activations\\nNext, we will discuss the convolution operation.\\nThe convolution operation\\nA convolution operation is a widely used signal-processing technique. For images, convolution \\nis used to produce different effects (such as blurring), or extract features (such as edges) from an \\nimage. An example of edge detection using convolution is shown in Figure 2.7. This is achieved by \\nshifting a convolution filter on top of an image to produce a different output at each location (see \\nFigure 2.8 later in this section). Specifically, at each location, we do element-wise multiplication \\nof the elements in the convolution filter with the image patch (the same size as the convolution \\nfilter) that overlaps with the convolution filter and takes the sum of the multiplication:\\nFigure 2.7: Using the convolution operation for edge detection in an image (Source: https://\\nen.wikipedia.org/wiki/Kernel_(image_processing) )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbef59b7-f99a-41ba-bfed-486b37f6226e', embedding=None, metadata={'page_label': '52', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 52\\nThe following is the implementation of the convolution operation:\\nx = tf.constant(\\n    [[\\n        [[ 1],[2],[3],[4]],\\n        [[ 4],[3],[2],[1]],\\n        [[ 5],[6],[7],[8]],\\n        [[ 8],[7],[6],[5]]\\n    ]],\\n    dtype=tf.float32)\\nx_filter = tf.constant(\\n    [ [ [[ 0.5]],[[1]] ],\\n      [ [[ 0.5]],[[1]] ] \\n    ],\\n    dtype=tf.float32)\\nx_stride = [ 1,1,1,1]\\nx_padding = 'VALID'\\nx_conv = tf.nn.conv2d(\\n    input=x, filters=x_filter, strides=x_stride, padding=x_padding\\n)\\nHere, the apparently excessive number of square brackets  used might make you think that the \\nexample can be made easy to follow by getting rid of these redundant brackets. Unfortunately, \\nthat is not the case. For the tf.nn.conv2d(...)  operation, TensorFlow requires input , filters , \\nand strides  to be of an exact format. We will now go through each argument in tf.conv2d(input, \\nfilters, strides, padding)  in more detail:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a43e86c-cc60-49aa-adbb-ffaab3d0f9f5', embedding=None, metadata={'page_label': '53', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 53\\n• input : This is typically a 4D tensor where the dimensions should be ordered as [batch_\\nsize, height, width, channels] :\\n• batch_size : This is the amount of data (for example, inputs such as images, and \\nwords) in a single batch of data. We normally process data in batches as large \\ndatasets are used for learning. At a given training step, we randomly sample a \\nsmall batch of data that approximately represents the full dataset. And doing \\nthis for many steps allows us to approximate the full dataset quite well. This \\nbatch_size  parameter is the same as the one we discussed in the TensorFlow \\ninput pipeline example.\\n• height and width : This is the height and the width of the input.\\n• channels : This is the depth of an input (for example, for an RGB image, the number \\nof channels will be 3—a channel for each color).\\n• filters: This is a 4D tensor that represents the convolution window of the convolution op -\\neration. The filter dimensions should be [height, width, in_channels, out_channels] :\\n• height and width: This is the height and the width of the filter (often smaller \\nthan that of the input)\\n• in_channels : This is the number of channels of the input to the layer\\n• out_channels: This is the number of channels to be produced in the output of \\nthe layer\\n• strides: This is a list with four elements, where the elements are [batch_stride, height_\\nstride, width_stride, channels_stride] . The strides  argument denotes how many \\nelements to skip during a single shift of the convolution window on the input. Usually, you \\ndon’t have to worry about batch_stride  and channels_stride . If you do not completely \\nunderstand what strides  is, you can use the default value of 1.\\n• padding: This can be one of ['SAME', 'VALID'] . It decides how to handle the convolution \\noperation near the boundaries of the input. The VALID  operation performs the convolution \\nwithout padding. If we were to convolve an input of n length with a convolution window \\nof size h, this will result in an output of size (n-h+1 < n). The diminishing of the output \\nsize can severely limit the depth of neural networks. SAME  pads zeros to the boundary such \\nthat the output will have the same height and width as the input.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2417bca-d6da-4968-9924-7c35bbc8f609', embedding=None, metadata={'page_label': '54', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 54\\nTo gain a better understanding  of what filter size, stride, and padding are, refer to Figure 2.8:\\nFigure 2.8: The convolution operation. Note how the kernel is moved over the input to compute \\nvalues at each position\\nNext, we will discuss the pooling operation.\\nThe pooling operation\\nA pooling operation behaves similarly to the convolution operation, but the final output is differ -\\nent. Instead of outputting the sum of the element-wise multiplication of the filter and the image \\npatch, we now take the maximum element of the image patch for that location (see Figure 2.9 ):\\nx = tf.constant(\\n    [[\\n        [[ 1],[2],[3],[4]],\\n        [[ 4],[3],[2],[1]],\\n        [[ 5],[6],[7],[8]],', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c901e253-97bf-4f1e-9708-66bed3d89f75', embedding=None, metadata={'page_label': '55', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 55\\n        [[ 8],[7],[6],[5]]\\n    ]],\\n    dtype=tf.float32)\\nx_ksize = [ 1,2,2,1]\\nx_stride = [ 1,2,2,1]\\nx_padding = 'VALID'\\nx_pool = tf.nn.max_pool2d(\\n    input=x, ksize=x_ksize,\\n    strides=x_stride, padding=x_padding\\n)\\n# Returns (out) => [[[[ 4.],[ 4.]],[[ 8.],[ 8.]]]]\\nFigure 2.9: The max-pooling operation\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fb77879-600b-40fe-b672-5ab7a5f71ca0', embedding=None, metadata={'page_label': '56', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 56\\nDefining loss\\nWe know that, for a neural network  to learn something useful, a loss needs  to be defined. The \\nloss represents how close or far away the predictions are from actual targets. There are several \\nfunctions for automatically calculating the loss in TensorFlow, two of which are shown in the \\nfollowing code. The tf.nn.l2_loss  function is the mean squared error loss, and tf.nn.softmax_\\ncross_entropy_with_logits  is another type of loss that actually gives better performance in \\nclassification tasks. And by logits here, we mean the unnormalized output of the neural network \\n(that is, the linear output of the last layer of the neural network):\\n# Returns half of L2 norm of t given by sum(t**2)/2\\nx = tf.constant([[ 2,4],[6,8]],dtype=tf.float32)\\nx_hat = tf.constant([[ 1,2],[3,4]],dtype=tf.float32)\\n# MSE = (1**2 + 2**2 + 3**2 + 4**2)/2 = 15\\nMSE = tf.nn.l2_loss(x-x_hat)\\n# A common loss function used in neural networks to optimize the network\\n# Calculating the cross_entropy with logits (unnormalized outputs of the \\nlast layer)\\n# instead of probabilsitic outputs leads to better numerical stabilities\\ny = tf.constant([[ 1,0],[0,1]],dtype=tf.float32)\\ny_hat = tf.constant([[ 3,1],[2,5]],dtype=tf.float32)\\n# This function alone doesn't average the cross entropy losses of all data \\npoints,\\n# You need to do that manually using reduce_mean function\\nCE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_\\nhat,labels=y))\\nHere, we discussed several  important operations  intertwined with neural networks, such as \\nthe convolution operation and the pooling operation. We will now discuss how a sub-library in \\nTensorFlow known as Keras can be used to build models.\\nKeras: The model building API of TensorFlow\\nKeras was developed  as a separate library that provides high-level building blocks to build mod-\\nels conveniently. It was initially platform-agnostic and supported many softwares (for example, \\nTensorFlow and Theano). \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cef20321-cc71-4740-9945-368572f28835', embedding=None, metadata={'page_label': '57', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 57\\nHowever, TensorFlow acquired Keras and now is an integral part of TensorFlow for building \\nmodels effortlessly.\\nKeras’s primary focus is model building. For that, Keras  provides several different APIs with vary -\\ning degrees of flexibility and complexity. Choosing the right API for the job will require sound \\nknowledge of the limitations of each API as well as experience. The APIs provided by Keras are:\\n• Sequential API – The most easy-to-use API. In this  API, you simply stack layers on top of \\neach other to create a model.\\n• Functional API – The functional API provides more flexibility by allowing you to define \\ncustom models that can have multiple input layers/multiple output layers.\\n• Sub-classing API – The sub-classing API enables you to define custom reusable layers/\\nmodels as Python classes. This is the most flexible API, but it requires strong familiarity \\nwith the API and raw TensorFlow operations  to use it correctly.\\nOne of the most innate concepts in Keras is that a model is composed of one or more layers con-\\nnected in a specific way. Here, we will briefly go through what the code looks like, using different \\nAPIs to develop models. You are not expected to fully understand the code below. Rather, focus \\non the code style to spot any differences between the three methods.\\nSequential API\\nWhen using the Sequential API, you simply  define your model as a list of layers. Here, the first \\nelement  in the list is the closest to the input, where the last is the output layer:\\nmodel = tf.keras.Sequential([\\n        tf.keras.layers.Dense( 500, activation= 'relu', shape=( 784, )),\\n        tf.keras.layers.Dense( 250, activation= 'relu'),\\n        tf.keras.layers.Dense( 10, activation= 'softmax' )\\n    ])Do not confuse the  Keras TensorFlow sub-module ( https://www.tensorflow.org/\\napi_docs/python/tf/keras ) with the external Keras library ( https://keras.io/ ). \\nThey share roots in terms of where they’ve come from, but they are not the same. You \\nwill run into strange issues if you treat them as the same during your development. \\nIn this book, we exclusively use tf.keras .\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef8b85c1-022d-4d93-a9c8-77c6a389e7ea', embedding=None, metadata={'page_label': '58', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 58\\nIn the preceding code, we have three layers. The first layer has 500 output nodes and takes in a \\nvector of 784 elements as the input. The second layer is automatically connected to the first one, \\nwhereas the last layer is connected  to the second layer. All of these layers are fully-connected \\nlayers, where all input nodes are connected to all output nodes.\\nFunctional API\\nIn the Functional API, we do things differently. We first define one or more input layers, and other \\nlayers that carry computations. Then we connect the inputs to outputs ourselves, as shown in \\nthe following code:\\ninp = tf.keras.layers.Input(shape=( 784,))\\nout_1 = tf.keras.layers.Dense( 500, activation= 'relu')(inp)\\nout_2 = tf.keras.layers.Dense( 250, activation= 'relu')(out_1)\\nout = tf.keras.layers.Dense( 10, activation= 'softmax' )(out_2)\\nmodel = tf.keras.models.Model(inputs=inp, outputs=out)\\nIn the code, we start with an input layer that accepts a 784 element-long vector. The input is \\npassed to a Dense layer that has 500 nodes. The output of that layer is assigned to out_1 . Then \\nout_1  is passed to another Dense layer, which outputs out_2 . Next, a Dense layer with 10 nodes \\noutputs the final output. Finally, the model is defined as a tf.keras.models.Model  object that \\ntakes two arguments: \\n• inputs – One or more input layers\\n• outputs – One or more outputs produced by any tf.keras.layers  type object\\nThe model is identical to what was defined in the previous section. One of the benefits of the \\nFunctional API is that you can create far more complex models as you’re not bounded to have \\nlayers as a list. Because of this freedom, you can have multiple inputs connecting to many layers \\nin many different ways and potentially produce many outputs as well.\\nSub-classing API\\nFinally, we will use the sub-classing API to define a model. With sub-classing, you define your mod -\\nel as a Python object that inherits from the base object, tf.keras.Model . When using sub-classing, \\nyou need to define two important functions: __init__() , which will specify any special parameters, \\nlayers, and so on required to successfully perform the computations, and call(), which defines \\nthe computations that need to happen in the model:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06d9413c-3528-4ee8-8243-5fd9c03aeb1c', embedding=None, metadata={'page_label': '59', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 2 59\\nclass MyModel (tf.keras.Model):\\n    def __init__ (self, num_classes):\\n        super().__init__()\\n        self.hidden1_layer = tf.keras.layers.Dense( 500, activation= 'relu')\\n        self.hidden2_layer = tf.keras.layers.Dense( 250, activation= 'relu')\\n        self.final_layer = tf.keras.layers.Dense(num_classes,  \\n        activation= 'softmax' )\\n    def call(self, inputs):\\n        h = self.hidden1_layer(inputs)\\n        h = self.hidden2_layer(h)\\n        y = self.final_layer(h)\\n        return y\\n    \\n    \\nmodel = MyModel(num_classes= 10)\\nHere, you can see that our model has three layers, just like all the previous models we defined. Next, \\nthe call function defines how these layers connect to produce the final output. The sub-classing \\nAPI is considered the most difficult to master, mainly due to the freedom allowed by the method. \\nHowever, the rewards are immense once you learn the API as it enables you to define very complex \\nmodels/layers as unit computations that can be reused later. Now that you understand how each \\nAPI works, let’s implement a neural network using Keras and train it on a dataset.\\nImplementing our first neural network\\nGreat! Now that you’ve learned the architecture and foundations of TensorFlow, it’s high time \\nthat we move on and implement something slightly more complex. Let’s implement a neural \\nnetwork. Specifically, we will implement a fully connected neural network model (FCNN), which \\nwe discussed in Chapter 1 , Introduction to Natural Language Processing .\\nOne of the stepping stones to the introduction of neural networks is to implement a neural net -\\nwork that is able to classify digits. For this task, we will be using  the famous MNIST dataset made \\navailable at http://yann.lecun.com/exdb/mnist/ .\\nYou might feel a bit skeptical regarding our using a computer vision task rather than an NLP task. \\nHowever, vision tasks can be implemented with less preprocessing and are easy to understand.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3e361bc-dabf-490d-b68c-3e2b2088ff64', embedding=None, metadata={'page_label': '60', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 60\\nAs this is our first encounter with neural networks, we will see how to implement this model \\nusing Keras. Keras is the high-level submodule that provides a layer of abstraction over Tensor -\\nFlow. Therefore, you can implement neural networks with much less effort with Keras than using \\nTensorFlow’s raw operations. To run the examples end to end, you can find the full exercise in \\nthe tensorflow_introduction.ipynb  file in the Ch02-Understanding-TensorFlow  folder. The \\nnext step is to prepare the data.\\nPreparing the data\\nFirst, we need to download the dataset. TensorFlow out of the box provides convenient functions \\nto download data and MNIST is one of those supported datasets. We will be performing four \\nimportant steps during the data preparation:\\n• Downloading the data and storing it as numpy.ndarray  objects. We will create a folder \\nnamed data within our ch2  directory and store the data there.\\n• Reshaping the images so that 2D grayscale images in the dataset will be converted to 1D \\nvectors.\\n• Standardizing the images to have a zero-mean and unit-variance (also known as  \\nwhitening ).\\n• One-hot encoding the integer class labels. One-hot encoding  refers to the process of rep -\\nresenting integer class labels as a vector. For example, if you have 10 classes and a class \\nlabel of 3 (where labels range from 0-9), your one-hot encoded vector will be [0, 0, 0, \\n1, 0, 0, 0, 0, 0, 0] .\\nThe following code performs these functions for us:\\nos.makedirs( 'data', exist_ok= True)\\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\\n    path=os.path.join(os.getcwd(), 'data', 'mnist.npz' )\\n)\\n# Reshaping x_train and x_test tensors so that each image is represented\\n# as a 1D vector\\nx_train = x_train.reshape(x_train.shape[ 0], -1)\\nx_test = x_test.reshape(x_test.shape[ 0], -1)\\n# Standardizing x_train and x_test tensors\\nx_train = (\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7f052048-8211-4593-83ce-0379351604fa', embedding=None, metadata={'page_label': '61', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 61\\n    x_train - np.mean(x_train, axis= 1, keepdims= True)\\n)/np.std(x_train, axis= 1, keepdims= True)\\nx_test = (\\n    x_test - np.mean(x_test, axis= 1, keepdims= True)\\n)/np.std(x_test, axis= 1, keepdims= True)\\n# One hot encoding y_train and y_test\\ny_onehot_train = np.zeros((y_train.shape[ 0], num_labels),  \\ndtype=np.float32)\\ny_onehot_train[np.arange(y_train.shape[ 0]), y_train] = 1.0\\ny_onehot_test = np.zeros((y_test.shape[ 0], num_labels), dtype=np.float32)\\ny_onehot_test[np.arange(y_test.shape[ 0]), y_test] = 1.0\\nYou can see that we are using the tf.keras.datasets.mnist.load_data()  function provided by \\nTensorFlow to download the training and testing data. It will be downloaded to a folder named \\ndata  within the Ch02-Understanding-TensorFlow  folder. This will provide four output tensors:\\n• x_train – A 60000 x 28 x 28 sized tensor where each image is 28 x 28\\n• y_train – A 60000 sized vector, where each element is a class label between 0-9\\n• x_test – A 10000 x 28 x 28 sized tensor\\n• y_test – A 10000 sized vector\\nOnce the data is downloaded, we reshape the 28 x 28 sized images into a 1D vector. This is because \\nwe will be implementing a fully connected neural network. Fully connected neural networks \\ntake a 1D vector as the input. Therefore, all the pixels in the image will be arranged as a sequence \\nof pixels in order to feed into the model. Finally, if you look at the range of values present in \\nthe x_train  and x_test  tensors, they will be in the range of 0-255 (typical grayscale range). We \\nwould bring these values to a zero mean unit-variance range by subtracting the mean of each \\nimage and dividing by the standard deviation.\\nImplementing the neural network with Keras\\nLet’s now examine how to implement the type of neural network we discussed in Chapter 1, In -\\ntroduction to Natural Language Processing, with Keras. The network is a fully connected neural \\nnetwork with 3 layers having 500, 250, and 10 nodes, respectively. The first two layers will use \\nReLU activation, whereas the last layer uses softmax. To implement this, we are going to use the \\nsimplest of the Keras APIs available to us – the Sequential API. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea51cce1-7106-4121-bedf-842029bc742c', embedding=None, metadata={'page_label': '62', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Understanding TensorFlow 2 62\\nYou can find the full exercise in the tensorflow_introduction.ipynb  file in the Ch02-\\nUnderstanding-TensorFlow  folder:\\nmodel = tf.keras.Sequential([\\n        tf.keras.layers.Dense( 500, activation= 'relu'),\\n        tf.keras.layers.Dense( 250, activation= 'relu'),\\n        tf.keras.layers.Dense( 10, activation= 'softmax' )\\n    ])\\nYou can see that all it takes is a single line in the Keras Sequential API to define the model we just \\ndefined. Keras provides various types of layers. You can see the full list of layers available to you \\nat https://www.tensorflow.org/api_docs/python/tf/keras/layers . For a fully connected \\nnetwork, we only need Dense layers that mimic the computations of a hidden layer in a fully \\nconnected network. With the model defined, you need to compile this model with an appropriate \\nloss function, an optimizer, and, optionally, performance metrics:\\noptimizer = tf.keras.optimizers.RMSprop()\\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[ 'acc'])\\nWith the model defined and compiled, we can now train our model on the prepared data.\\nTraining the model\\nTraining a model could not be easier in Keras. Once the data is prepared, all you need to do is call \\nthe model.fit()  function with the required arguments:\\nbatch_size = 100\\nnum_epochs = 10\\ntrain_history = model.fit(\\n    x=x_train, \\n    y=y_onehot_train, \\n    batch_size=batch_size, \\n    epochs= num_epochs, \\n    validation_split= 0.2\\n)\\nmodel.fit()  accepts several important arguments. We will go through them in more detail here:\\n• x – An input tensor. In our case, this is a 60000 x 784 sized tensor.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9532679a-2ac0-494f-a78d-7a0c71ca40ac', embedding=None, metadata={'page_label': '63', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 63\\n• y – The one-hot encoded label tensor. In our case, this is a 60000 x 10 sized tensor.\\n• batch_size  – Deep learning models are trained with batches of data (in other words, \\nstochastically) as opposed to feeding the full dataset at once. The batch size defines how \\nmany examples are included in a single batch. The larger the batch size, the better the \\naccuracy of your model would be generally.\\n• epochs  – Deep learning models iterate through the dataset in batches several times. The \\nnumber of times iterated through the dataset is known as the number of epochs. In our \\nexample, this is set to 10.\\n• validation_split  – When training deep learning models, a validation set is used to \\nmonitor performance, where the validation set acts as a proxy for real-world performance. \\nvalidation_split  defines how much of the full dataset is to be used as the validation \\nsubset. In our example, this is set to 20% of the total dataset size.\\nHere’s what the training loss and validation accuracy look like over the number of epochs we \\ntrained the model ( Figure 2.10 ):\\nFigure 2.10: Training loss and validation accuracy over 10 epochs as the model is trained\\nNext up is testing our model on some unseen data.\\nTesting the model\\nTesting the model is also straightforward. During testing, we measure the loss and the accuracy of \\nthe model on the test dataset. In order to evaluate the model on a dataset, Keras models provide \\na convenient function called evaluate() :\\ntest_res = model.evaluate(\\n    x=x_test, \\n    y=y_onehot_test, \\n    batch_size=batch_size\\n)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='066d9140-a1ca-4811-a61f-057c1310a268', embedding=None, metadata={'page_label': '64', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding TensorFlow 2 64\\nThe arguments expected by the model.evaluate()  function are already covered during our dis-\\ncussion of model.fit() :\\n• x – An input tensor. In our case, this is a 10000 x 784 sized tensor.\\n• y – The one-hot encoded label tensor. In our case, this is a 10000 x 10 sized tensor.\\n• batch_size  – Batch size defines how many examples are included in a single batch. The \\nlarger the batch size, the better the accuracy of your model would be generally.\\nYou will get a loss of 0.138 and an accuracy of 98%. You will not get the exact same values due to \\nvarious randomness present in the model, as well as during training.\\nIn this section, we went through an end-to-end example of training a neural network. We prepared \\nthe data, trained the model on that data, and finally tested it on some unseen data.\\nSummary\\nIn this chapter, you took your first steps to solving NLP tasks by understanding the primary \\nunderlying platform (TensorFlow) on which we will be implementing our algorithms. First, \\nwe discussed the underlying details of TensorFlow architecture. Next, we discussed the essen-\\ntial ingredients of a meaningful TensorFlow program. We got to know some new features in  \\nTensorFlow 2, such as the AutoGraph feature, in depth. We then discussed more exciting elements \\nin TensorFlow such as data pipelines and various TensorFlow operations. \\nSpecifically, we discussed the TensorFlow architecture by lining up the explanation with an \\nexample TensorFlow program; the sigmoid example. In this TensorFlow program, we used the \\nAutoGraph feature to generate a TensorFlow graph; that is, using the tf.function()  decorator \\nover the function that performs the TensorFlow operations. Then, a GraphDef  object was created \\nrepresenting the graph and sent to the distributed master. The distributed master looked at the \\ngraph, decided which components to use for the relevant computation, and divided it into several \\nsubgraphs to make the computations faster. Finally, workers executed subgraphs and returned \\nthe result immediately.\\nNext, we discussed the various elements that comprise a typical TensorFlow client: inputs, vari-\\nables, outputs, and operations. Inputs are the data we feed to the algorithm for training and testing \\npurposes. We discussed three different ways of feeding inputs: using NumPy arrays, preloading \\ndata as TensorFlow tensors, and using tf.data to define an input pipeline. Then we discussed \\nTensorFlow variables, how they differ from other tensors, and how to create and initialize them. \\nFollowing this, we discussed how variables can be used to create intermediate and terminal \\noutputs. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c53ecc9a-0eec-48d9-8cc9-2ac26d5be95f', embedding=None, metadata={'page_label': '65', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2 65\\nFinally, we discussed several available TensorFlow operations, including mathematical operations, \\nmatrix operations, and neural network-related operations that will be used later in the book.\\nLater, we discussed Keras, a sub-module in TensorFlow that supports building models. We learned \\nthat there are three different APIs for building models: the Sequential API, the Functional API, \\nand the Sub-classing API. We learned that the Sequential API is the easiest to use, whereas the \\nSub-classing API takes much more effort. However, the Sequential API is very restrictive in terms \\nof the type of models that can be implemented with it.\\nFinally, we implemented a neural network using all the concepts learned previously. We used \\na three-layer neural network to classify a MNIST digit dataset, and we used Keras (a high-level \\nsub-module in TensorFlow) to implement this model.\\nIn the next chapter, we will see how to use the fully connected neural network we implemented \\nin this chapter for learning the semantic, numerical word representation of words.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='026b7f70-1bed-452b-b1b9-07391a9b2de7', embedding=None, metadata={'page_label': '66', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed4a248a-71bd-4178-a944-698f10e0663e', embedding=None, metadata={'page_label': '67', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3\\nWord2vec – Learning Word \\nEmbeddings\\nIn this chapter, we will discuss a topic of paramount importance in NLP—Word2vec, a data-driven \\ntechnique for learning powerful numerical representations (that is, vectors) of words or tokens \\nin a language. Languages are complex. This warrants sound language understanding capabil -\\nities in the models we build to solve NLP problems. When transforming words to a numerical \\nrepresentation, a lot of methods aren’t able to sufficiently capture the semantics and contextual \\ninformation that word carries. For example, the feature representation of the word forest  should \\nbe very different from oven  as these words are rarely used in similar contexts, whereas the repre -\\nsentations of forest  and jungle should be very similar. Not being able to capture this information \\nleads to underperforming models. \\nWord2vec tries to overcome this problem by learning word representations by consuming large \\namounts of text.\\nIn this chapter, we will learn the mechanics of several Word2vec algorithms. But first, we will \\ndiscuss the classical approaches to solving this problem and their limitations. This then motivates \\nus to look at learning neural-network-based Word2vec algorithms that deliver state-of-the-art \\nperformance when finding good word representations. Word2vec is called a distributed representation , as the semantics of the word are cap -\\ntured by the activation pattern of the full representation vector, in contrast to a single \\nelement of the representation vector (for example, setting a single element in the \\nvector to 1 and rest to 0 for a single word).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='459638e2-de0d-417f-a656-5be0dc717015', embedding=None, metadata={'page_label': '68', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 68\\nWe will train a model on a dataset and analyze the representations learned by the model. We \\nvisualize (using t-SNE, a visualization technique for high-dimensional data) these learned word \\nembeddings for a set of words on a 2D canvas in Figure 3. 1. If you take a closer look, you will see that \\nsimilar things are placed close to each other (for example, numbers in the cluster in the middle):\\nFigure 3. 1: An example visualization of learned word embeddings using t-SNE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c76b45d-e8de-4697-b8b1-a74d96d22503', embedding=None, metadata={'page_label': '69', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 69\\nThis chapter covers this information through the following main topics:\\n• What is a word representation or meaning?\\n• Classical approaches to learning word representations\\n• Word2vec – a neural network-based approach to learning word representation\\n• The skip-gram algorithm\\n• The Continuous Bag-of-Words algorithm\\nBy the end of this chapter, you will have gained a thorough understanding of how the history \\nof word representations has led to Word2vec, how to utilize two different Word2vec algorithms, \\nand the vital importance of Word2vec for NLP.\\nWhat is a word representation or meaning?\\nWhat is meant by the word meaning ? This is more of a philosophical question than a technical \\none. So, we will not try to discern the best answer for this question, but accept a more modest \\nanswer, that is, meaning is the idea conveyed by or some representation associated with a word. \\nFor example, when you hear the word “cat” you conjure up a mental picture of something that \\nmeows, has four legs, has a tail, and so on; then, if you hear the word “dog,” you again formulate \\na mental image of something that barks, has a bigger body than a cat, has four legs, has a tail, \\nand so on. In this new space (that is, the mental pictures), it is easier for you to understand that \\ncats and dogs are similar than by just looking at the words. Since the primary objective of NLP is \\nto achieve human-like performance in linguistic tasks, it is sensible to explore principled ways \\nof representing words for machines. To achieve this, we will use algorithms that can analyze a \\ngiven text corpus and come up with good numerical representations of words (that is, word em-\\nbeddings) such that words that fall within similar contexts (for example, one and two, I and we) \\nwill have similar numerical representations compared to words that are unrelated (for example, \\ncat and volcano ).t-Distributed Stochastic Neighbor Embedding ( t-SNE)\\nThis is a dimensionality reduction technique that projects high-dimensional data \\nto a two-dimensional space. This allows us to imagine how high-dimensional data \\nis distributed in space, because humans are generally not so good at intuitively \\nunderstanding data in more than three dimensions. You will learn about t-SNE in \\nmore detail in the next chapter.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='145117e2-62be-482e-aba8-57a51a8669ec', embedding=None, metadata={'page_label': '70', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 70\\nFirst, we will discuss some classical approaches to achieve this and then move on to understanding \\nrecent, more sophisticated methods that use neural networks to learn feature representations \\nand deliver state-of-the-art performance.\\nClassical approaches to learning word representation\\nIn this section, we will discuss  some of the classical approaches used for numerically represent -\\ning words. It is important to have an understanding of the alternatives to word vectors, as these \\nmethods are still used in the real world, especially when limited data is available. \\nMore specifically, we will discuss common representations, such as one-hot encoding and Term \\nFrequency-Inverse Document Frequency (TF-IDF).\\nOne-hot encoded representation\\nOne of the simpler ways of representing words is to use the one-hot encoded representation. This \\nmeans that if we have a vocabulary of size V, for each ith word w i, we will represent the word w i \\nwith a V-length vector [0, 0, 0, …, 0, 1, 0, …, 0, 0, 0] where the ith element is 1 and other elements \\nare 0. As an example, consider this sentence:\\nBob and Mary are good friends.\\nThe one-hot encoded representation of each word might look like this:\\nBob: [1,0,0,0,0,0]\\nand: [0,1,0,0,0,0]\\nMary: [0,0,1,0,0,0]\\nare: [0,0,0,1,0,0]\\ngood: [0,0,0,0,1,0]\\nfriends: [0,0,0,0,0,1]\\nHowever, as you might have already figured out, this representation has many drawbacks.\\nThis representation does not encode the similarity between words in any way and completely \\nignores  the context in which the words are used. Let’s consider the dot product between the word \\nvectors as the similarity measure. The more similar two vectors are, the higher the dot product is \\nfor those two vectors. For example, the representation of the words car and automobile  will have \\na similarity distance of 0, while car and pencil  will also have the same value.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7538554-e98f-4e72-97c2-96d945697e4d', embedding=None, metadata={'page_label': '71', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 71\\nThis method becomes extremely ineffective for large vocabularies. Also, for a typical NLP task, \\nthe vocabulary easily can exceed 50,000 words. Therefore, the word representation matrix for \\n50,000 words will result in a very sparse 50,000 × 50,000 matrix.\\nHowever, one-hot encoding plays an important role even in state-of-the-art word embedding \\nlearning algorithms. We use one-hot encoding to represent words numerically and feed them \\ninto neural networks so that the neural networks can learn better and smaller numerical feature \\nrepresentations of the words.\\nWe will now discuss another technique for representing words, known as the TF-IDF method.\\nThe TF-IDF method\\nTF-IDF is a frequency-based method  that takes into account  the frequency with which a word \\nappears in a corpus. This is a word representation in the sense that it represents the importance \\nof a specific word in a given document. Intuitively, the higher the frequency of the word, the \\nmore important that word is in the document. For example, in a document about cats, the word \\ncats  will appear more often than in a document that isn’t about cats. However, just calculating \\nthe frequency would not work because words such as this and is  are very frequent in documents \\nbut do not contribute much information. TF-IDF takes this into consideration and gives values \\nof near-zero for such common words.\\nAgain, TF stands for term frequency and IDF stands for inverse document frequency:\\nTF(w i) = number of times w i appear / total number of words\\nIDF(w i) = log(total number of documents / number of documents with w i in it)\\nTF-IDF(w i) = TF(w i) x IDF(w i)\\nLet’s do a quick exercise. Consider two documents:\\n• Document 1: This is about cats. Cats are great companions .\\n• Document 2: This is about dogs. Dogs are very loyal .One-hot encoding is also known as a localist representation (the opposite to the \\ndistributed representation), as the feature representation is decided by the activation \\nof a single element in the vector.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ecc01722-20cc-48ba-bae4-05b0f786b9e1', embedding=None, metadata={'page_label': '72', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 72\\nNow let’s crunch some numbers:\\nTF-IDF (cats, doc1)  = (2/8) * log(2/1) = 0.075\\nTF-IDF (this, doc2)  = (1/8) * log(2/2) = 0.0\\nTherefore, the word cats  is informative, while this is not. This is the desired  behavior we needed  \\nin terms of measuring the importance of words.\\nCo-occurrence matrix\\nCo-occurrence matrices, unlike one-hot-encoded representations, encode the context information \\nof words, but require maintaining a V × V matrix. To understand the co-occurrence matrix, let’s \\ntake two example sentences:\\n• Jerry and Mary are friends .\\n• Jerry buys flowers for Mary.\\nThe co-occurrence matrix will look like the following matrix. We only show one half of the matrix, \\nas it is symmetrical:\\nJerry and Mary are friends buys flowers for\\nJerry 0 1 0 0 0 1 0 0\\nand 0 1 0 0 0 0 0\\nMary 0 1 0 0 0 1\\nare 0 1 0 0 0\\nfriends 0 0 0 0\\nbuys 0 1 0\\nflowers 0 1\\nfor 0\\nHowever, it is not hard to see that maintaining such a co-occurrence matrix comes at a cost as \\nthe size of the matrix grows polynomially with the size of the vocabulary. Furthermore, it is \\nnot straightforward to incorporate a context window size larger than 1. One option is to have a \\nweighted count, where the weight for a word in the context deteriorates with the distance from \\nthe word of interest.\\nAs you can see, these methods are very limited in their representational power. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='65aa1154-c4cd-4472-af57-9ba11d886176', embedding=None, metadata={'page_label': '73', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 73\\nFor example, in the one-hot encoded method, all words will have the same vector distance to each \\nother. The TF-IDF method represents a word with a single number and is unable to capture the \\nsemantics of words. Finally calculating the co-occurrence matrix is very expensive and provides \\nlimited information about a word’s context.\\nWe end our discussion about simple representations of words here. In the following section, we \\nwill first develop an intuitive understanding of word embeddings by working through an example. \\nThen we will define a loss function so that we can use machine learning to learn word embed -\\ndings. Also, we will discuss two Word2vec algorithms, namely, the skip-gram and Continuous \\nBag-of-Words (CBOW) algorithms.\\nAn intuitive understanding of Word2vec – an \\napproach to learning word representation\\nThis statement, uttered by J. R. Firth in 1957, lies at the very foundation of Word2vec, as Word2vec \\ntechniques use the context of a given word to learn its semantics.\\nWord2vec is a groundbreaking approach that allows computers to learn the meaning of words \\nwithout any human intervention. Also, Word2vec learns numerical representations of words by \\nlooking at the words surrounding a given word.\\nWe can test the correctness of the preceding quote by imagining a real-world scenario. Imagine \\nyou are sitting an exam and you find this sentence in your first question: “Mary is a very stubborn \\nchild. Her pervicacious nature always gets her in trouble.” Now, unless you are very clever, you \\nmight not know what pervicacious  means. In such a situation, you automatically will be com -\\npelled to look at the phrases surrounding the word of interest. In our example, pervicacious  is \\nsurrounded by stubborn, nature, and trouble. Looking at these three words is enough to determine \\nthat pervicacious in fact means the state of being stubborn. I think this is adequate evidence to \\nobserve the importance of context for a word’s meaning.\\nNow let’s discuss the basics of Word2vec. As already mentioned, Word2vec learns the meaning \\nof a given word by looking at its context and representing it numerically.  “You shall know a word by the company it keeps.”\\n– J.R. Firth', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1f8bc82-5576-422d-9f39-9c8b8921a0e6', embedding=None, metadata={'page_label': '74', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 74\\nBy context, we refer to a fixed number of words in front of and behind the word of interest. Let’s \\ntake a hypothetical corpus with N words. Mathematically, this can be represented by a sequence \\nof words denoted by w0, w1, …, wi, and wN, where w i is the ith word in the corpus.\\nNext, if we want to find a good algorithm that is capable of learning word meanings, given a word, \\nour algorithm should be able to predict the context words correctly.\\nThis means that the following probability should be high for any given word w i:\\n𝑃𝑃(𝑤𝑤𝑖𝑖𝑖𝑖𝑖,…,𝑤𝑤𝑖𝑖𝑖𝑖,𝑤𝑤𝑖𝑖𝑖𝑖,…,𝑤𝑤𝑖𝑖𝑖𝑖𝑖|𝑤𝑤𝑖𝑖)= ∏ 𝑃𝑃𝑃𝑤𝑤 𝑗𝑗|𝑤𝑤𝑖𝑖)𝑖𝑖𝑖𝑖𝑖\\n𝑗𝑗𝑗𝑖𝑖𝑗𝑗𝑗𝑗𝑖𝑖𝑖𝑖𝑖 \\nTo arrive at the right-hand side of the equation, we need to assume that given the target word \\n(w i), the context words are independent of each other (for example, wi-2 and w i-1 are independent). \\nThough not entirely true, this approximation makes the learning problem practical and works \\nwell in practice. Let’s go through an example to understand the computations.\\nExercise: does queen = king – he + she?\\nBefore proceeding further, let’s do a small exercise to understand how maximizing the previously \\nmentioned probability leads to finding good meaning (or representations) of words. Consider \\nthe following very small corpus:\\nThere was a very rich king. He had a beautiful queen. She was very kind .\\nTo keep the exercise simple, let’s do some manual preprocessing and remove the punctuation \\nand the uninformative words:\\nwas rich king he had beautiful queen she was kind\\nNow let’s form a set of tuples for each word with their context words in the format ( target word \\n--> context word 1 , context word 2 ). We will assume a context window size of 1 on either side:\\nwas --> rich\\nrich --> was, king\\nking --> rich, he\\nhe --> king, had\\nhad --> he, beautiful\\nbeautiful --> had, queen', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d1ff22b-20e7-44c2-b229-6a0a800774c0', embedding=None, metadata={'page_label': '75', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 75\\nqueen --> beautiful, she\\nshe --> queen, was\\nwas --> she, kind\\nkind --> was\\nRemember, our goal is to be able to predict the words on the right given the word on the left. To \\ndo this, for a given word, the words on the right-side context should share a high numerical or \\ngeometrical similarity with the words on the left-side context. In other words, the word of inter -\\nest should be conveyed by the surrounding words. Now let’s consider actual numerical vectors \\nto understand how this works. For simplicity, let’s only consider the tuples highlighted in bold. \\nLet’s begin by assuming the following for the word rich:\\nrich --> [0,0]\\nTo be able to correctly predict was  and king  from rich, was and king  should have high similarity \\nwith the word rich. The Euclidean distance will be used to measure the distance between words. \\nLet’s try the following values for the words king  and rich:\\nking --> [0,1]\\nwas --> [-1,0]\\nThis works out fine as the following:\\nDist(rich,king)  = 1.0\\nDist(rich,was) = 1.0\\nHere, Dist  is the Euclidean distance between two words. This is illustrated in Figure 3.3 :\\nFigure 3.2: The positioning of word vectors for the words “rich” , “was” and “king”\\nNow let’s consider the following tuple:\\nking --> rich, he', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ec17e89-fac0-4201-bf9d-e173b6b54edb', embedding=None, metadata={'page_label': '76', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 76\\nWe have established the relationship between king  and rich already. However, it is not done yet; \\nthe more we see a relationship, the closer these two words should be. So, let’s first adjust the \\nvector of king so that it is a bit closer to rich:\\nking --> [0,0.8]\\nNext, we will need to add the word he to the picture. The word he should be closer to king . This is \\nall the information that we have right now about the word he: he --> [0.5,0.8].\\nAt this moment, the graph with the words looks like Figure 3.4:\\nFigure 3.3: The positioning of word vectors for the words “rich” , “was” , “king, ” and “he”\\nNow let’s proceed with the next  two tuples: queen --> beautiful, she  and she --> queen, was . Note that \\nI have swapped the order of the tuples as this makes it easier for us to understand the example:\\nshe --> queen, was\\nNow, we will have to use our prior knowledge of English to proceed further.\\nIt is a reasonable decision to place the word she the same distance from was  that he is from was , \\nbecause their usage in the context of the word was  is equivalent. Therefore, let’s use this:\\nshe --> [0.5,0.6]\\nNext, we will use the word queen close to the word she: queen --> [0.0,0.6].\\nThis is illustrated in Figure 3.5 :\\nFigure 3.4: The positioning of word vectors for the words “rich, ” “was, ” “king, ” “he, ” “she, ” \\nand “queen”', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34138265-50e6-483d-bd2e-cf6af94b5d8e', embedding=None, metadata={'page_label': '77', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 77\\nNext, we only have the following tuple:\\nqueen --> beautiful, she\\nHere, the word beautiful  is found. It should be approximately the same distance from the words \\nqueen and she. Let’s use the following:\\nbeautiful --> [0.25,0]\\nNow we have the following graph depicting the relationships between words. When we observe \\nFigure 3.6, it seems to be a very intuitive representation of the meanings of words:\\nFigure 3.5: The positioning of word vectors for the words “rich, ” “was, ” “king, ” “he, ” “she, ” \\n“queen, ” and “beautiful”\\nNow, let’s look at the question that has been lurking in our minds since the beginning of this \\nexercise. Are the quantities in this equation equivalent: queen = king  - he + she? Well, we’ve got \\nall the resources that we’ll need to solve this mystery now. Let’s try the right-hand side of the \\nequation first:\\n= king  – he + she\\n= [0,0.8]  – [0.5,0.8] + [0.5,0.6]\\n= [0,0.6]\\nIt all works out in the end. If you look at the word vector we obtained for the word queen, you see \\nthat this is exactly the same as the answer we deduced earlier.\\nNote that this is a crude way to show how word embeddings are learned, and this might differ \\nfrom the exact positions of word embeddings learned using an algorithm.\\nAlso keep in mind that this is an unrealistically scaled-down exercise with regard to what a re -\\nal-world corpus might look like. So, you will not be able to work out these values by hand just by \\ncrunching a dozen numbers. Sophisticated function approximators such as neural networks do \\nthis job for us. But, to use neural networks, we need to formulate our problem in a mathematically \\nassertive way. However, this is a good exercise to show the power of word vectors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d4665ab-a821-45ce-bdd5-b93111612ff9', embedding=None, metadata={'page_label': '78', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 78\\nNow that we have a good understanding of how Word2vec enables us to learn word representa-\\ntions, let’s look at the actual algorithms Word2vec utilizes in the next two sections.\\nThe skip-gram algorithm\\nThe first algorithm we will talk about is known as the skip-gram algorithm : a type of Word2vec \\nalgorithm. As we have discussed in numerous places, the meaning of a word can be elicited from \\nthe contextual words surrounding it. However, it is not entirely straightforward to develop a \\nmodel that exploits this way of learning word meanings. The skip-gram algorithm, introduced \\nby Mikolov et al. in 2013, is an algorithm that does exploit the context of the words in a written \\ntext to learn good word embeddings.\\nLet’s go through the skip-gram algorithm step by step. First, we will discuss the data preparation \\nprocess. Understanding the format of the data puts us in a great position to understand the algorithm. \\nWe will then discuss the algorithm itself. Finally, we will implement the algorithm using TensorFlow.\\nFrom raw text to semi-structured text\\nFirst, we need to design  a mechanism to extract a dataset that can be fed to our learning model. \\nSuch a dataset should be a set of tuples of the format (target, context). Moreover, this needs to \\nbe created in an unsupervised manner. That is, a human should not have to manually engineer \\nthe labels for the data. In summary, the data preparation process should do the following:\\n• Capture the surrounding words of a given word (that is, the context)\\n• Run in an unsupervised manner\\nThe skip-gram model uses the following approach to design a dataset:\\n• For a given word w i, a context window size of m is assumed. By context window size , we \\nmean the number of words considered as context on a single side. Therefore, for w i, the \\ncontext window (including the target word w i) will be of size 2m+1 and will look like this: \\n[w i-m, …, w i-1, w i, w i+1, …, w i+m].\\n• Next, (target, context) tuples are formed as […, (w i, w i-m), …, (w i,w i-1), (w i,w i+1), …, (w i,w i+m), …] ; \\nhere, 𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚   and N is the number of words in the text. Let’s use the following \\nsentence and a context window size ( m) of 1:\\nThe dog barked at the mailman .\\nFor this example, the dataset would be as follows:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ea92281-85a3-4a82-ae5e-095e4186f461', embedding=None, metadata={'page_label': '79', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 79\\n[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the, mailman)]\\nOnce the data is in the (target, context)  format, we can use a neural network to learn the word \\nembeddings.\\nUnderstanding the skip-gram algorithm\\nFirst, let’s identify the variables  and notation we need to learn the word embeddings. To store \\nthe word embeddings, we need two V × D matrices, where V is the vocabulary size and D is the \\ndimensionality of the word embeddings (that is, the number of elements in the vector that rep -\\nresent a single word). D is a user-defined hyperparameter. The higher D is, the more expressive \\nthe word embeddings learned will be. We need two matrices, one to represent the context words \\nand one to represent the target words. These matrices will be referred to as the context embedding \\nspace (or context embedding layer) and the target embedding space (or target embedding layer), or in \\ngeneral as the embedding space (or the embedding layer).\\nEach word will be represented with a unique ID in the range [1, V+1]. These IDs are passed to the \\nembedding layer to look up corresponding vectors. To generate these IDs, we will use a special \\nobject called a Tokenizer that’s available in TensorFlow. Let’s refer to an example target-context \\ntuple (w i, w j), where the target word ID is w i, and one of the context words is w j. The corre -\\nsponding target embedding of wi is ti, and the corresponding context embedding of wj is cj. Each \\ntarget-context tuple is accompanied by a label (0 or 1), denoted by y i, where true target-context \\npairs will get a label of 1, and negative (or false) target-context candidates will get a label of 0. It \\nis easy to generate negative target-context candidates by sampling a word that does not appear \\nin the context of a given target as the context word. We will talk about this in more detail later.\\nAt this point, we have defined  the necessary variables. Next, for each input w i, we will look up the \\nembedding vectors from the context embedding layer corresponding to the input. This operation \\nprovides us with c i, which is a D-sized vector (that is, a D-long embedding vector). We do the \\nsame for the input w j, using the context embedding space to retrieve c j. Afterward, we calculate \\nthe prediction output for (w i ,w i) using the following transformation:\\nlogit(w i, w i) = ci .tj\\nŷij = sigmoid(logit(w i, w i))\\nHere, logit(w i, w i)  represents the unnormalized scores (that is, logits), ŷi is a single-valued predicted \\noutput (representing the probability of context word belonging in the context of the target word). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45ae99c2-4f78-4c07-9b21-f46736bf1ec7', embedding=None, metadata={'page_label': '80', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 80\\nWe will visualize both the conceptual (Figure 3.7) and implementation (Figure 3.8) views of the \\nskip-gram model. Here is a summary of the notation:\\n• V: This is the size of the vocabulary\\n• D: This is the dimensionality of the embedding layer\\n• wi: Target word\\n• wj: Context word\\n• ti: Target embedding of the word w i\\n• cj: Context embedding of the word w j\\n• yi: This is the one-hot-encoded output word corresponding to x i\\n• ŷi: This is the predicted output for x i\\n• logit(w i, w j): This is the unnormalized score for the input x i\\nFigure 3.6: The conceptual skip-gram model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90c0286e-5926-40d0-b39f-186936daebb2', embedding=None, metadata={'page_label': '81', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 81\\nFigure 3.7: The implementation of the skip-gram model\\nUsing both the existing  and derived entities, we can now use the cross-entropy loss function to \\ncalculate the loss for a given data point [(w i, w j), y i].\\nFor binary labels, the cross-entropy loss for a single sample (𝑥𝑥𝑖𝑖,𝑦𝑦𝑖𝑖)  is computed as:\\n𝐶𝐶𝐶𝐶(𝑦𝑦𝑦𝑖𝑖,𝑦𝑦𝑖𝑖)=−[𝑦𝑦𝑖𝑖log(yĩ)+(1−𝑦𝑦𝑖𝑖)log(1 − 𝑦𝑦𝑦𝑖𝑖)] \\nWhere 𝑦𝑦𝑖𝑖̃  is the predicted label for 𝑥𝑥𝑖𝑖 . For multi-class classification problems, we generalize the \\nloss by computing the term 𝑦𝑦𝑖𝑖log(𝑦𝑦𝑦𝑖𝑖)  for each class:\\n𝐶𝐶𝐶𝐶(𝑦𝑦𝑦𝑖𝑖,𝑦𝑦𝑖𝑖)=−∑𝑦𝑦 𝑖𝑖,𝑖𝑖l o g ( 𝑦𝑦𝑦𝑙𝑙,𝑖𝑖)𝐶𝐶\\n𝑖𝑖𝑐𝑐 \\nWhere 𝑦𝑦𝑖𝑖𝑖𝑖𝑖  represents the value of the 𝑐𝑐𝑡𝑡𝑡  index of the 𝑦𝑦𝑖𝑖 , where 𝑦𝑦𝑖𝑖  is a one hot encoded vector \\nrepresenting the label of the data point. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d914ea5-9467-438a-a95a-173700b7e958', embedding=None, metadata={'page_label': '82', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 82\\nTypically, when training neural networks, this loss is computed for each sample in a given batch, \\nthen averaged to compute the loss of the batch. Finally, the batch losses are averaged over all the \\nbatches in the dataset to compute the final loss.\\nLet’s now implement the data generation process with TensorFlow.\\nImplementing and running the skip-gram algorithm with \\nTensorFlow\\nWe are now going to get  our hands dirty with TensorFlow and implement the algorithm from \\nend to end. First, we will discuss the data we’re going to use and how TensorFlow can help us to \\nget that data in the format the model accepts. We will implement the skip-gram algorithm with \\nTensorFlow and finally train the model and evaluate it on data that was prepared.Why does the original word embeddings paper use two embedding layers?\\nThe original paper (by Mikolov et al., 2013) uses two distinct V × D embedding spaces \\nto denote words in the target space (words when used as the target) and words in \\nthe contextual space (words used as context words). One motivation to do this is \\nthat a word does not occur in its own context often. So, we want to minimize the \\nprobability of such things happening.\\nFor example, for the target word dog, it is highly unlikely that the word dog is also \\nfound in its context ( P(dog|dog) ~ 0). Intuitively, if we feed the ( wi=dog and w j=dog) \\ndata point to the neural network, we are asking the neural network to give a higher \\nloss if the neural network predicts dog as a context word of dog. \\nIn other words, we are asking the word embedding of the word dog to have a very high \\ndistance to the word embedding of the word dog. This creates a strong contradiction \\nas the distance between the embeddings of the same word will be 0. Therefore, we \\ncannot achieve this if we only have a single embedding space. \\nHowever, having two separate embedding spaces for target words and contextual \\nwords allows us to have this property because this way we have two separate embed -\\nding vectors for the same word. In practice, as long as you avoid feeding input-output \\ntuples, having the same word as input and output allows us to work with a single \\nembedding space and eliminates the need for two distinct embedding layers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c650834f-467a-4d4d-8c42-20d0746f36c9', embedding=None, metadata={'page_label': '83', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 83\\nImplementing the data generators with TensorFlow\\nFirst, we will investigate how data can be generated in the correct format for the model. For this \\nexercise, we are going to use the BBC news articles dataset available at http://mlg.ucd.ie/\\ndatasets/bbc.html . It contains 2,225 news articles belonging to 5 topics, business, entertain -\\nment, politics, sport, and tech, which were published on the BBC website between 2004-2005.\\nWe write the function download_data()  below to download the data to a given folder and extract \\nit from its compressed format:\\ndef download_data (url, data_dir):\\n    \"\"\"Download a file if not present, and make sure it\\'s the right\\n    size.\"\"\"\\n  \\n    os.makedirs(data_dir, exist_ok= True)\\n    file_path = os.path.join(data_dir, \\'bbc-fulltext.zip\\' )\\n  \\n    if not os.path.exists(file_path):\\n        print(\\'Downloading file...\\' )\\n        filename, _ = urlretrieve(url, file_path)\\n    else:\\n        print(\"File already exists\" )\\n  \\n    extract_path = os.path.join(data_dir, \\'bbc\\')\\n    if not os.path.exists(extract_path):\\n        \\n        with zipfile.ZipFile(\\n            os.path.join(data_dir, \\'bbc-fulltext.zip\\' ),\\n        \\'r\\'\\n        ) as zipf:\\n            zipf.extractall(data_dir)\\n  \\n    else:\\n        print(\"bbc-fulltext.zip has already been extracted\" )\\nThe function first creates the data_dir  if it doesn’t exist. Next, if the bbc-fulltext.zip  file does \\nnot exist, it will be downloaded from the provided URL. If bbc-fulltext.zip  has not been ex -\\ntracted yet, it will be extracted to data_dir . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d78e6bd8-fae1-4315-9094-155409ac44f3', embedding=None, metadata={'page_label': '84', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 84\\nWe can call this function as follows:\\nurl = \\'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\\'\\ndownload_data(url, \\'data\\')\\nWith that, we are going to focus on reading the data contained in the news articles (in .txt  format) \\ninto the memory. To do that, we will define the read_data()  function, which takes a data directory \\npath (data_dir ), and reads the .txt files (except for the README file) found in the data directory:\\ndef read_data (data_dir):\\n    news_stories = []\\n    print(\"Reading files\" )\\n    for root, dirs, files in os.walk(data_dir):\\n        for fi, f in enumerate (files):\\n            if \\'README\\'  in f:\\n                continue\\n            print(\".\"*fi, f, end= \\'\\\\r\\')\\n            with open(os.path.join(root, f), encoding= \\'latin-1\\' ) as f:\\n                story = []\\n                for row in f:               \\n                    story.append(row.strip())\\n                story = \\' \\'.join(story)                        \\n                news_stories.append(story)                \\n    print(f\"\\\\nDetected {len(news_stories)}  stories\" )\\n    return news_stories\\nWith the read_data()  function defined, let’s use it to read in the data and print some samples \\nas well as some statistics:\\nnews_stories = read_data(os.path.join( \\'data\\', \\'bbc\\'))\\nprint(f\"{sum([len(story.split( \\' \\')) for story in news_stories])}  words \\nfound in the total news set\" )\\nprint(\\'Example words (start): \\' ,news_stories[ 0][:50])\\nprint(\\'Example words (end): \\' ,news_stories[- 1][-50:])\\nThis will print the following:\\nReading files\\n............. 361.txt\\nDetected 2225 stories', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5853c60-724b-42ee-8507-908f72d096cf', embedding=None, metadata={'page_label': '85', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 85\\n865163 words found in the total news set\\nExample words (start):  Windows worm travels with Tetris  Users are being \\nExample words (end):  is years at Stradey as \"the best time of my life.\"\\nAs we said at the beginning  of this section, there are 2,225 stories with close to a million words. \\nIn the next step, we need to tokenize each story (in the form of a long string) to a list of tokens \\n(or words). Along with that, we will perform some preprocessing on the text:\\n• Lowercase all the characters\\n• Remove punctuation\\nAll of these can be achieved with the tensorflow.keras.preprocessing.text.Tokenizer  object. \\nWe can define a Tokenizer as follows:\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ntokenizer = Tokenizer(\\n    num_words= None,\\n    filters= \\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\' ,\\n    lower= True,\\nsplit=\\' \\' \\n)\\nHere, you can see some of the most popular keyword arguments and their default values used \\nwhen defining a Tokenizer:\\n• num_words  – Defines the size of the vocabulary. Defaults to None , meaning it will consider \\nall the words appearing in the text corpus. If set to the integer n, it will only consider the \\nn most common words appearing in the corpus.\\n• filters – Defines any characters that need to be omitted during preprocessing. By de -\\nfault, it defines a string containing most of the common punctuation marks and symbols.\\n• lower – Defines whether the text needs to be converted to lowercase.\\n• split – Defines the character that the words will be tokenized on.\\nOnce the Tokenizer is defined, you can call its fit_on_texts()  method with a list of strings \\n(where each string is a news article) so that the Tokenizer will learn the vocabulary and map the \\nwords to unique IDs:\\ntokenizer.fit_on_texts(news_stories)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49807e8f-edf4-4f78-900c-308a441d09be', embedding=None, metadata={'page_label': '86', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 86\\nLet’s take a moment to analyze what the Tokenizer has produced after it has been fitted on the \\ntext. Once it has been fitted, the Tokenizer will have two important attributes populated: word_\\nindex  and index_word . Here word_index  is a dictionary that maps each word to a unique ID. The \\nindex_word  attribute is the opposite of word_index , that is, a dictionary that maps each unique \\nword ID to the corresponding word:\\nn_vocab = len(tokenizer.word_index.items())+ 1\\nprint(f\"Vocabulary size: {n_vocab} \")\\nprint(\"\\\\nWords at the top\" )\\nprint(\\'\\\\t\\', dict(list(tokenizer.word_index.items())[: 10]))\\nprint(\"\\\\nWords at the bottom\" )\\nprint(\\'\\\\t\\', dict(list(tokenizer.word_index.items())[- 10:]))\\nNote how we are using the length of the word_index  dictionary to derive the vocabulary size. \\nWe need an additional 1 as the ID 0 is a reserved ID and will not be used for any word. This will \\noutput the following:\\nVocabulary size: 32361\\nWords at the top\\n    {\\'the\\': 1, \\'to\\': 2, \\'of\\': 3, \\'and\\': 4, \\'a\\': 5, \\'in\\': 6, \\'for\\': 7, \\n\\'is\\': 8, \\'that\\': 9, \\'on\\': 10}\\nWords at the bottom\\n    {\\'counsellor\\': 32351, \"\\'frag\\'\": 32352, \\'relasing\\': 32353, \"\\'real\\'\": \\n32354, \\'hrs\\': 32355, \\'enviroment\\': 32356, \\'trifling\\': 32357, \\'24hours\\': \\n32358, \\'ahhhh\\': 32359, \\'lol\\': 32360}\\nThe more frequent a word is in the corpus, the lower the ID will be. Words such as “the”, “to” and \\n“of” which tend to be common (and are called stop words) are in fact the most  common words. \\nAs the next step, we are going to refine our Tokenizer object to have a limited-sized vocabulary. \\nBecause we are working with a relatively small corpus, we have to make sure the vocabulary is \\nnot too large, as it can lead to poorly learned word vectors due to the lack of data:\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ntokenizer = Tokenizer(\\n    num_words= 15000,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1676cb0e-938c-462e-b2e0-b0a69f506af9', embedding=None, metadata={'page_label': '87', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 87\\n    filters= \\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\' ,\\n    lower= True, split= \\' \\', oov_token= \\'\\',    \\n)\\ntokenizer.fit_on_texts(news_stories)\\nSince we have a total vocabulary of more than 30,000 words, we’ll restrict the size of the vocab -\\nulary to 15,000. This means the Tokenizer will only keep the most common 15,000 words as the \\nvocabulary. When we restrict a vocabulary this way, a new problem arises. As the Tokenizer’s \\nvocabulary does not encompass all possible words in the true vocabulary, out-of-vocabulary words \\n(or OOV words) can rear their heads. Some solutions are to replace OOV words with a special \\ntoken (such as < UNK >) or remove them from the corpus. This is possible by passing the string you \\nwant to replace OOV tokens with to the oov_token  argument in the Tokenizer. In this case, we \\nwill remove OOV words. If we are careful when setting the size of the vocabulary, omitting some \\nof the rare words would not harm learning the context of words accurately.\\nWe can have a look at the transformation done on the text by the Tokenizer as follows. Let’s con-\\nvert a string of the first 100 characters of the first story in our corpus (stored in the news_stories \\nvariable):\\nprint(f\"Original: {news_stories[ 0][:100]}\")\\nThen we can call the tokenizer \\'s texts_to_sequences()  method to convert a list of documents \\n(where each document is a string) to a list  of list of word IDs (that is, each document is converted \\nto a list of word IDs).\\nprint(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[ 0]\\n[:100]])[0]}\")\\nThis will print out:\\nOriginal: Ad sales boost Time Warner profit  Quarterly profits at US media \\ngiant TimeWarner jumped 76% to $1.1\\nSequence IDs: [4223, 187, 716, 66, 3596, 1050, 3938, 626, 21, 49, 303, \\n717, 8263, 2972, 5321, 3, 108, 108]\\nWe now have our Tokenizer sorted. There’s nothing left to do but to convert all of our news articles \\nto sequences of word IDs with a single line of code:\\nnews_sequences = tokenizer.texts_to_sequences(news_stories)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28e56f4a-6092-440e-b1a8-cd02c9ac11db', embedding=None, metadata={'page_label': '88', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 88\\nLet’s move on to generating skip-grams using the tf.keras.preprocessing.sequence.\\nskipgrams()  function, provided by TensorFlow. We call the function on a sample phrase repre -\\nsenting the first 5 words extracted from the first article in the dataset:\\nsample_word_ids = news_sequences[ 0][:5]\\nsample_phrase = \\' \\'.join([tokenizer.index_word[wid] for wid in sample_\\nword_ids])\\nprint(f\"Sample phrase: {sample_phrase} \")\\nprint(f\"Sample word IDs: {sample_word_ids } \\\\n\")\\nThis will output:\\nSample phrase: ad sales boost time warner\\nSample word IDs: [4223, 187, 716, 66, 3596]\\nLet’s consider a window size of 1. This means, for a given target word, we define the context as \\none word from each side of the target word.\\nwindow_size = 1 # How many words to consider left and right.\\nWe have all the ingredients to define extract skip-grams from the sample phrase we chose as \\nfollows. When run, this function will output data in the exact format we need the data in, that is, \\n(target-context) tuples as inputs and corresponding labels (0 or 1) as outputs:\\ninputs, labels = tf.keras.preprocessing.sequence.skipgrams(\\n    sequence=sample_word_ids, \\n    vocabulary_size=n_vocab, \\n    window_size=window_size, \\n    negative_samples= 1.0, \\n    shuffle= False,\\n    categorical= False, \\n    sampling_table= None, \\n    seed= None\\n)\\nLet’s take a moment to reflect on some of the important arguments that have been used:\\n• sequence  (list[str]  or list[int])  – A list of words or word IDs.\\n• vocabulary_size  (int)  – Size of the vocabulary.\\n• window_size  (int)  – Size of the window to be considered for the context. window_size  \\ndefines the length on each side.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c284949-bb9f-407d-a04c-68b883b1dff0', embedding=None, metadata={'page_label': '89', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 89\\n• negative_samples  (int)  – Fraction of negative candidates to generate.  For example, a \\nvalue of 1 means there will be an equal number of positive and negative skipgram candi-\\ndates. A value of 0 means there will not be any negative candidates.\\n• shuffle  (bool) – Whether to shuffle the generated inputs or not.\\n• categorical (bool)  – Whether to produce labels as categorical (that is, one-hot encod-\\ned) or integers.\\n• sampling_table  (np.ndarray)  – An array of the same size as the vocabulary. An element \\nin a given position in the array represents the probability of sampling the word indexed \\nby that position in the Tokenizer’s word ID to word mapping. As we will see soon, this is \\na handy way to avoid common uninformative words being over-sampled much.\\n• seed  (int) – If shuffling is enabled, this is the random seed to be used for shuffling.\\nWith the inputs and labels generated, let’s print some data:\\nprint(\"Sample skip-grams\" )\\nfor inp, lbl in zip(inputs, labels):\\n    print(f\"\\\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]} ) /\\n    Label: {lbl}\")\\nThis will produce:\\nSample skip-grams\\n    Input: [4223, 187] ([\\'ad\\', \\'sales\\']) / Label: 1\\n    Input: [187, 4223] ([\\'sales\\', \\'ad\\']) / Label: 1\\n    Input: [187, 716] ([\\'sales\\', \\'boost\\']) / Label: 1\\n    Input: [716, 187] ([\\'boost\\', \\'sales\\']) / Label: 1\\n    Input: [716, 66] ([\\'boost\\', \\'time\\']) / Label: 1\\n    Input: [66, 716] ([\\'time\\', \\'boost\\']) / Label: 1\\n    Input: [66, 3596] ([\\'time\\', \\'warner\\']) / Label: 1\\n    Input: [3596, 66] ([\\'warner\\', \\'time\\']) / Label: 1\\n    Input: [716, 9685] ([\\'boost\\', \"kenya\\'s\"]) / Label: 0\\n    Input: [3596, 12251] ([\\'warner\\', \\'rear\\']) / Label: 0\\n    Input: [4223, 3325] ([\\'ad\\', \\'racing\\']) / Label: 0\\n    Input: [66, 7978] ([\\'time\\', \\'certificate\\']) / Label: 0\\n    Input: [716, 12756] ([\\'boost\\', \\'crushing\\']) / Label: 0\\n    Input: [66, 14543] ([\\'time\\', \\'touchy\\']) / Label: 0\\n    Input: [187, 3786] ([\\'sales\\', \\'9m\\']) / Label: 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f2f59e9-16b2-449b-a473-f12310dae26b', embedding=None, metadata={'page_label': '90', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Word2vec – Learning Word Embeddings 90\\n    Input: [187, 3917] (['sales', 'doherty']) / Label: 0\\nFor example, since the word “sales” appears in the context of the word “ad”, it is considered a \\npositive candidate. On the other hand, since the word “racing” (randomly sampled from the \\nvocabulary) does not appear in the context of the word “ad”, it is added as a negative candidate.\\nWhen selecting negative candidates, the skipgrams()  function selects them randomly, giving \\nuniform weights to all the words in the vocabulary. However, the original paper explains that \\nthis can lead to poor performance. A better strategy is to use the unigram distribution as a prior \\nfor selecting negative context words.\\nYou might be wondering what a unigram distribution is. It represents the frequency counts of \\nunigrams (or tokens) found in the text. Then the frequency counts are easily converted to prob -\\nabilities (or normalized frequencies) by dividing them by the sum of all frequencies. The most \\namazing thing is that you don’t have to compute this by hand for every corpus of text! It turns \\nout that if you take any sufficiently large corpus of text, compute the normalized frequencies of \\nunigrams, and order them from high to low, you’ll see that the corpus  approximately follows a \\ncertain constant distribution. For the word with rank math in a corpus of math unigrams, the \\nnormalized frequency f k is given by:\\n𝑓𝑓𝑘𝑘=𝑘𝑘−𝑠𝑠\\n∑𝑛𝑛−𝑠𝑠𝑁𝑁\\n𝑖𝑖𝑖𝑖 \\nHere, math is a hyperparameter that can be tuned to match the true distribution more closely. This \\nis known as Zipf’s law . In other words, if you have a vocabulary where words are ranked (ID-ed) \\nfrom most common to least common, you can approximate the normalized frequency of each word \\nusing Zipf’s law. We will be sampling words according to the probabilities output through Zipf’s \\nlaw instead of giving equal probabilities to the words. This means words are sampled according to \\ntheir presence (that is, the more frequent, the higher the chance of being sampled) in the corpus.\\nTo do that, we can use the tf.random.log_uniform_candidate_sampler()  function. This func-\\ntion takes a batch of positive context candidates of shape [b, num_true] , where b is the batch \\nsize and num_true  is the number of true candidates per example (1 for the skip-gram model), and \\nit outputs a [ num_sampled ] sized array, where num_sampled  is the number of negative samples \\nwe need. We will discuss the nitty-gritty of this function soon, while going through an exercise. \\nBut let’s first generate some positive candidates using the tf.keras.preprocessing.sequence.\\nskipgrams()  function:\\ninputs, labels = tf.keras.preprocessing.sequence.skipgrams(\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef86d021-25df-4b62-8ec2-a56b1415a079', embedding=None, metadata={'page_label': '91', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 91\\n    sample_phrase_word_ids, \\n    vocabulary_size= len(tokenizer.word_index.items())+ 1, \\n    window_size=window_size, \\n    negative_samples= 0, \\n    shuffle= False    \\n)\\ninputs, labels = np.array(inputs), np.array(labels)\\nNote that we’re specifying negative_samples=0 , as we will be generating negative samples with \\nthe candidate sampler. Let’s now discuss how we can use the tf.random.log_uniform_candidate_\\nsampler()  function to generate negative candidates. Here we will first use this function to generate \\nnegative candidates for a single word:\\nnegative_sampling_candidates, true_expected_count, sampled_expected_count \\n= tf.random.log_uniform_candidate_sampler(\\n    true_classes=inputs[: 1, 1:], # [b, 1] sized tensor\\n    num_true= 1, # number of true words per example\\n    num_sampled= 10,\\n    unique= True,\\n    range_max=n_vocab,            \\n    name= \"negative_sampling\"\\n)\\nThis function  takes the following arguments:\\n• true_classes  (np.ndarray  or tf.Tensor)  – A tensor containing true target words. This \\nneeds to be a [ b, num_true ] sized array, where num_true  denotes the number of true \\ncontext candidates per example. Since we have one context word per example, this is 1.\\n• num_true  (int)  – The number of true context terms per example.\\n• num_sampled  (int)  – The number of negative samples to generate.\\n• unique  (bool)  – Whether to generate unique samples or with replacement.\\n• range_max  (int)  – The size of the vocabulary.\\nIt returns:\\n• sampled_candidates  (tf.Tensor)  – A tensor of size [ num_sampled ] containing negative \\ncandidates', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9f8c80d-9935-494b-b88a-7c2016b329c4', embedding=None, metadata={'page_label': '92', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 92\\n• true_expected_count  (tf.Tensor)  – A tensor of size [ b, num_true ]; the probability of \\neach true candidate being sampled (according to Zipf’s law)\\n• sampled_expected_count  (tf.Tensor)  – A tensor of size [ num_sampled ]; the probabilities \\nof each negative sample occurring along with true candidates, if sampled from the corpus\\nWe will not worry too much about the latter two entities. The most important to us is sampled_\\ncandidates . When calling the function, we have to make sure true_classes  has the shape [b, \\nnum_true] . In our case, we will run this in a single input word ID, which will be in the shape [1, \\n1]. It returns the following:\\nPositive sample: [[187]]\\nNegative samples: [   1   10 9744 3062  139    5   14   78 1402  115]\\ntrue_expected_count: [[0.00660027]]\\nsampled_expected_count: [4.0367463e-01 1.0333969e-01 1.2804421e-04 \\n4.0727769e-04 8.8460185e-03\\n 1.7628242e-01 7.7631921e-02 1.5584969e-02 8.8879210e-04 1.0659459e-02]\\nNow, putting everything together, let’s write a data generator function that generates batches \\nof data for the model. This function, named skip_gram_data_generator() , takes the following \\narguments:\\n• sequences  (List[List[int]])  – A list of list of word IDs. This is the output generated by \\nthe Tokenizer’s texts_to_sequences()  function.\\n• window_size  (int)  – The window size for the context.\\n• batch_size  (int)  – The batch size.\\n• negative_samples  (int)  – The number of negative samples per example to generate.\\n• vocabulary_size  (int)  – The vocabulary size.\\n• seed – The random seed.\\nIt will return a batch of data containing:\\n• A batch of target word IDs\\n• A batch of corresponding context word IDs (both positive and negative)\\n• A batch of labels (0 and 1)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c817c8bc-2bf3-4298-b0c4-463d9d3caf7a', embedding=None, metadata={'page_label': '93', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 93\\nThe function signature looks as follows:\\ndef skip_gram_data_generator (sequences, window_size, batch_size, negative_\\nsamples, vocab_size, seed= None):\\nFirst, we are going to shuffle the news articles so that every time we generate data, they are fetched \\nin a different order. This helps the model to generalize better:\\n    rand_sequence_ids = np.arange( len(sequences))\\n    np.random.shuffle(rand_sequence_ids)\\nNext, for each text sequence in the corpus we generate positive skip grams. positive_skip_grams  \\ncontains tuples of (target, context) word pairs in that order:\\n    for si in rand_sequence_ids:\\n        \\n        positive_skip_grams, _ = \\n        tf.keras.preprocessing.sequence.skipgrams(\\n            sequences[si], \\n            vocabulary_size=vocab_size, \\n            window_size=window_size, \\n            negative_samples= 0.0, \\n            shuffle= False,\\n            sampling_table=sampling_table,\\n            seed=seed\\n        )\\nNote that we are passing a sampling_table  argument. This is another strategy to enhance the \\nperformance of Word2vec models. sampling_table is simply  an array that is the same size as your \\nvocabulary and specifies a probability at each index of the array with which the word indexed by \\nthat index will be sampled during skip gram  generation. This technique is known as subsampling. \\nEach word w i is sampled with the probability given by the following equation:\\n𝑝𝑝(𝑤𝑤𝑖𝑖)=1−√𝑡𝑡\\n𝑓𝑓(𝑤𝑤𝑖𝑖) \\nHere, t is a tunable parameter. It defaults to 0.00001 for a large enough corpus. In TensorFlow, \\nyou can generate this table easily as follows. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ad344c5-5c3d-4496-9284-acc4706aa9f3', embedding=None, metadata={'page_label': '94', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 94\\nYou don’t need the exact frequencies to compute the sampling table, as we can leverage Zipf’s \\nlaw to approximate those frequencies:\\nsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\\n    n_vocab, sampling_factor= 1e-05\\n)\\nFor each tuple contained in positive_skip_grams , we generate negative_samples  number of \\nnegative candidates. We then populate targets, contexts, and label lists with both positive and \\nnegative candidates:\\n        targets, contexts, labels = [], [], []\\n        \\n        for target_word, context_word in positive_skip_grams:\\n            context_class = tf.expand_dims(tf.constant([context_word], \\n            dtype= \"int64\"), 1)\\n            \\n            negative_sampling_candidates, _, _ = \\n            tf.random.log_uniform_candidate_sampler(\\n              true_classes=context_class,\\n              num_true= 1,\\n              num_sampled=negative_samples,\\n              unique= True,\\n              range_max=vocab_size,\\n              name= \"negative_sampling\" )\\n            # Build context and label vectors (for one target word)\\n            context = tf.concat(\\n                [tf.constant([context_word], dtype= \\'int64\\'), \\n                negative_sampling_candidates],\\n                axis= 0\\n            )\\n            label = tf.constant([ 1] + [0]*negative_samples, \\n            dtype= \"int64\")\\n            # Append each element from the training example to global\\n            # lists.\\n            targets.extend([target_word]*(negative_samples+ 1))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='981ed72b-938e-4422-8cdc-782e1ea26adb', embedding=None, metadata={'page_label': '95', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 95\\n            contexts.append(context)\\n            labels.append(label)\\nWe will then convert  these to arrays as follows and randomly shuffle the data. When shuffling, \\nyou have to make sure all the arrays are consistently shuffled. Otherwise, you will corrupt the \\nlabels associated with the inputs:\\n        contexts, targets, labels = np.concatenate(contexts), \\n        np.array(targets), np.concatenate(labels)\\n        \\n        # If seed is not provided generate a random one\\n        if not seed:\\n            seed = random.randint( 0, 10e6)\\n        np.random.seed(seed)\\n        np.random.shuffle(contexts)\\n        np.random.seed(seed)\\n        np.random.shuffle(targets)\\n        np.random.seed(seed)\\n        np.random.shuffle(labels)\\nFinally, batches of data are generated as follows:\\n        for eg_id_start in range (0, contexts.shape[ 0], batch_size): \\n            yield (\\n                targets[eg_id_start: min(eg_id_start+batch_size, \\n                inputs.shape[ 0])], \\n                contexts[eg_id_start: min(eg_id_start+batch_size, \\n                inputs.shape[ 0])]\\n            ), labels[eg_id_start: min(eg_id_start+batch_size, \\n               inputs.shape[ 0])]\\nNext, we will  look at the specifics  of the model we’re going to use.\\nImplementing the skip-gram architecture with TensorFlow\\nWe will now walk through an implementation of the skip-gram algorithm that uses the  \\nTensorFlow library. The full exercise is available in ch3_word2vec.ipynb  in the Ch03-Word-Vectors  \\nexercise directory.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79303e20-3f66-49b9-8f02-db48270d6553', embedding=None, metadata={'page_label': '96', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 96\\nFirst, let’s define the hyperparameters of the model. You are free to change these hyperparame -\\nters to see how they affect final performance (for example, batch_size = 1024  or batch_size \\n= 2048 ). However, since this is a simpler problem than the more complex real-world problems, \\nyou might not see any significant differences (unless you change them to extremes, for example, \\nbatch_size = 1  or num_sampled = 1 ):\\nbatch_size = 4096 # Data points in a single batch\\nembedding_size = 128 # Dimension of the embedding vector.\\nwindow_size= 1 # We use a window size of 1 on either side of target word\\nnegative_samples = 4 # Number of negative samples generated per example\\nepochs = 5 # Number of epochs to train for\\n# We pick a random validation set to sample nearest neighbors\\nvalid_size = 16 # Random set of words to evaluate similarity on.\\n# We sample valid datapoints randomly from a large window without always\\n# being deterministic\\nvalid_window = 250\\n# When selecting valid examples, we select some of the most frequent words \\n# as well as  some moderately rare words as well\\nnp.random.seed( 54321)\\nrandom.seed( 54321)\\nvalid_term_ids = np.array(random.sample( range(valid_window), valid_size))\\nvalid_term_ids = np.append(\\n    valid_term_ids, random.sample( range(1000, 1000+valid_window), \\n    valid_size),\\n    axis= 0\\n)\\nNext, we define  the model. To do this, we will be relying on the Functional API of Keras. We need \\nto go beyond the simplest API, that is, the Sequential API, as this model requires two input streams \\n(one for the context and one for the target).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0fc0d083-d602-48cc-859d-c20f97f0cf59', embedding=None, metadata={'page_label': '97', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 97\\nWe will start off with an import. Then we will clear any current running sessions, to make sure \\nthere aren’t any other models occupying the hardware:\\nimport tensorflow.keras.backend as K\\nK.clear_session()\\nWe will define two input layers:\\n# Inputs - skipgrams() function outputs target, context in that order\\ninput_1 = tf.keras.layers.Input(shape=(), name= 'target' )\\ninput_2 = tf.keras.layers.Input(shape=(), name= 'context' )\\nNote how the shape is defined as (). When defining the shape  argument, the actual output shape \\nwill have a new undefined dimension (i.e. None sized) added. In other words, the final output \\nshape will be [None] .\\nNext, we define two embedding layers: a target embedding layer and a context embedding layer. \\nThese layers will be used to look up the embeddings for target and context word IDs that will be \\ngenerated by the input generation function.\\n# Two embeddings layers are used one for the context and one for the\\n# target\\ntarget_embedding_layer = tf.keras.layers.Embedding(\\n    input_dim=n_vocab, output_dim=embedding_size, \\n    name= 'target_embedding'\\n)\\ncontext_embedding_layer = tf.keras.layers.Embedding(\\n    input_dim=n_vocab, output_dim=embedding_size, \\n    name= 'context_embedding'\\n)\\nWith the embedding layers defined, let’s look up the embeddings for the word IDs that will be \\nfed to the input layers:\\n# Lookup outputs of the embedding layers\\ntarget_out = target_embedding_layer(input_1)\\ncontext_out = context_embedding_layer(input_2)\\nWe now need to compute the dot product of target_out  and context_out . \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0939b0d-28ef-46e8-9c9c-fc5a1127c24c', embedding=None, metadata={'page_label': '98', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 98\\nTo do that, we are going to use the tf.keras.layers.Dot  layer:\\n# Computing the dot product between the two \\nout = tf.keras.layers.Dot(axes=- 1)([context_out, target_out])\\nFinally, we define our model as a tf.keras.models.Model  object, where we specify inputs  and \\noutputs  arguments. inputs  need to be one or more input layers, and outputs  can be one or more \\noutputs produced by a series of tf.keras.layers  objects:\\n# Defining the model\\nskip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], \\noutputs=out, name= \\'skip_gram_model\\' )\\nWe compile the model using a loss function and an optimizer:\\n# Compiling the model\\nskip_gram_model. compile(loss=tf.keras.losses.BinaryCrossentropy(from_\\nlogits=True), optimizer= \\'adam\\', metrics=[ \\'accuracy\\' ])\\nLet’s see a summary of our model by calling the following:\\nskip_gram_model.summary()\\nThis will output:\\nModel: \"skip_gram_model\"\\n________________________________________________________________________\\nLayer (type)                    Output Shape    Param    #  Connected to\\n========================================================================\\ncontext (InputLayer)            [(None,)]       0         \\n_______________________________________________________________________\\ntarget (InputLayer)              [(None,)]      0         \\n_______________________________________________________________________\\ncontext_embedding (Embedding)   (None, 128)     1920128    context[0][0]\\n_______________________________________________________________________\\ntarget_embedding (Embedding)    (None, 128)     1920128    target[0][0]\\n_______________________________________________________________________\\ndot (Dot)                        (None, 1)      0  context_embedding[0][0]\\n                                                   target_embedding[0][0]\\n=======================================================================\\nTotal params: 3,840,256', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4972f4f5-5042-4938-9757-d150d75c0432', embedding=None, metadata={'page_label': '99', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 99\\nTrainable params: 3,840,256\\nNon-trainable params: 0\\n________________________________________________________________________\\nTraining and evaluating the model will be the next item on our agenda.\\nTraining and evaluating the model\\nOur training process is going to be very simple as we have defined a function to generate batches \\nof data in the exact format the model needs them in. But before we go ahead with training the \\nmodel, we need to think about how we evaluate word vector models. The idea of word vectors \\nis that words sharing semantic similarity will have a smaller distance between them, whereas \\nwords with no similarity will be far apart. To compute the similarities between words, we can \\nuse the cosine distance. We picked a set of random word IDs and stored them in valid_term_ids  \\nduring our hyperparameter discussion. We will implement a way to compute the closest k words \\nto each of those terms at the end of every epoch.\\nFor this, we utilize Keras callbacks. Keras callbacks give you a way to execute some important \\noperation(s) at the end of every training iteration, epoch, prediction step, and so on. You can see a \\nfull list of the available callbacks at https://www.tensorflow.org/api_docs/python/tf/keras/\\ncallbacks . Since we need a bespoke evaluation mechanism designed for word vectors, we will \\nneed to implement our own callback. Our callback will take a list of word IDs intended as the \\nvalidation words, a model containing the embedding matrix, and a Tokenizer to decode word IDs:\\nclass ValidationCallback (tf.keras.callbacks.Callback):\\n    \\n    def __init__ (self, valid_term_ids, model_with_embeddings, tokenizer):\\n        \\n        self.valid_term_ids = valid_term_ids\\n        self.model_with_embeddings = model_with_embeddings\\n        self.tokenizer = tokenizer\\n        \\n        super().__init__()\\n        \\n    def on_epoch_end (self, epoch, logs= None):\\n        \"\"\" Validation logic \"\"\"\\n        # We will use context embeddings to get the most similar words', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82fbd42e-189a-48be-b068-7733528c267d', embedding=None, metadata={'page_label': '100', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 100\\n        # Other strategies include: using target embeddings, mean\\n        # embeddings after avaraging context/target\\n        embedding_weights = \\n        self.model_with_embeddings.get_layer(\\n            \"context_embedding\"\\n        ).get_weights()[ 0]\\n        normalized_embeddings = embedding_weights / \\n        np.sqrt(np. sum(embedding_weights** 2, axis= 1, keepdims= True))\\n        \\n        # Get the embeddings corresponding to valid_term_ids\\n        valid_embeddings = normalized_embeddings[self.valid_term_ids, \\n        :]\\n        \\n        # Compute the similarity between valid_term_ids and all the\\n        # embeddings\\n        # V x d (d x D) => V x D\\n        top_k = 5 # Top k items will be displayed\\n        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\\n        \\n        # Invert similarity matrix to negative\\n        # Ignore the first one because that would be the same word as the\\n        # probe word\\n        similarity_top_k = np.argsort(-similarity, axis= 1)[:, 1: \\n        top_k+ 1]\\n                \\n        # Print the output\\n        for i, term_id in enumerate (valid_term_ids):\\n            similar_word_str = \\', \\'.join([self.tokenizer.index_word[j] \\n            for j in similarity_top_k[i, :] if j > 1])\\n            print(f\"{self.tokenizer.index_word[term_id]} : \\n            {similar_word_str } \")\\n        print(\\'\\\\n\\')\\nThe evaluation will be done at the end of a training epoch, therefore we will override the on_\\nepoch_end() function. The function extracts the embeddings from the context embedding layer. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0e8b9c3d-1965-42f2-89e7-5a68248d6dbe', embedding=None, metadata={'page_label': '101', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 101\\nThen the embeddings are normalized to have a unit length. Afterward, embeddings corresponding \\nto validation words are extracted to a separate matrix called valid_embeddings . Then the cosine \\ndistance is computed between the validation embeddings and all word embeddings, which re -\\nsults in a [valid_size, vocabulary size] sized matrix. From this, we extract the top k similar \\nwords and display them through print  statements.\\nFinally, the model can be trained as follows:\\nskipgram_validation_callback = ValidationCallback(valid_term_ids, skip_\\ngram_model, tokenizer)\\nfor ei in range (epochs):\\n    \\n    print(f\"Epoch: {ei+1}/{epochs}  started\" )\\n    \\n    news_skip_gram_gen = skip_gram_data_generator(\\n        news_sequences, window_size, batch_size, negative_samples, \\n        n_vocab\\n    )\\n    \\n    skip_gram_model.fit(\\n        news_skip_gram_gen, epochs= 1, \\n        callbacks=skipgram_validation_callback,\\n    )\\nWe are simply defining  an instance of the callback first. Next, we train the model for several  \\nepochs. In each, we generate skip gram data (while shuffling the order of the articles) and call \\nskip_gram_model.fit()  on the data. Here’s the result after five epochs of training:\\nEpoch: 5/5 ended\\n2233/2233 [==============================] - 146s 65ms/step - loss: 0.4842 \\n- accuracy: 0.8056\\nmonths: days, weeks, years, detained, meaning\\nwere: are, was, now, davidson, widened\\nmr: resignation, scott, tony, stead, article\\nchampions: premier, pottage, kampala, danielli, dominique\\nbusinesses: medium, port, 2002\\'s, tackling, doug\\npositive: electorate, proposal, bolz, visitors\\', strengthen\\npop: \\'me\\', style, lacks, tourism, tuesdays', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='898feb50-58b4-4d6d-ab2f-98b7be6c443e', embedding=None, metadata={'page_label': '102', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 102\\nHere, we denote some of the most sensible word vectors learned. For example, we can see that \\ntwo of the most similar words to the word “months” are “days” and “weeks”. The title “mr” is \\naccompanied by male names such as “scott” and “tony”. The word “premier” appears as a similar \\nword to “champion”. You can further experiment with:\\n• Different  negative candidate sampling methods available at https://www.tensorflow.\\norg/api_docs/python/tf/random\\n• Different hyperparameter choices (such as the embedding size and the number of neg -\\native samples)\\nIn this section, we discussed the skip-gram algorithm from end to end. We saw how we can use \\nfunctions in TensorFlow to transform data. Then we implemented the skip-gram architecture \\nusing layers in Keras and the Functional API. Finally, we trained the model and visually inspected \\nits performance on some test data. We will now discuss another popular Word2vec algorithm \\nknown as the Continuous Bag-of-Words ( CBOW) model.\\nThe Continuous Bag-of-Words algorithm\\nThe CBOW model works  in a similar way to the skip-gram algorithm, with one significant change \\nin the problem formulation. In the skip-gram model, we predict the context words from the target \\nword. However, in the CBOW model, we predict the target word from contextual words. Let’s \\ncompare what data looks like for the skip-gram algorithm and the CBOW model by taking the \\nprevious example sentence:\\nThe dog barked at the mailman.\\nFor the skip-gram algorithm, the data tuples— (input word, output word) —might look like this:\\n(dog, the), (dog, barked) , (barked, dog) , and so on\\nFor CBOW, the data tuples would look like the following:\\n([the, barked], dog) , ([dog, at], barked) , and so on\\nConsequently, the input of the CBOW has a dimensionality of 2 × m × D, where m is the context \\nwindow size and D is the dimensionality of the embeddings. The conceptual model of CBOW is \\nshown in Figure 3.13 :', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4570eb31-172a-4e0a-b413-817926c2e397', embedding=None, metadata={'page_label': '103', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 103\\nFigure 3.8: The CBOW model\\nWe will not go into great detail about the intricacies of CBOW as it is quite similar to skip-gram. \\nFor example, once the embeddings are aggregated (that is, concatenated or summed), they flow \\nthrough a softmax layer to finally compute the same loss as we did with the skip-gram algorithm. \\nHowever, we will discuss the algorithm’s implementation (though not in depth) to get a clear \\nunderstanding of how to properly implement CBOW. The full implementation of CBOW is avail -\\nable at ch3_word2vec.ipynb  in the Ch03-Word-Vectors  exercise folder.\\nGenerating data for the CBOW algorithm\\nUnfortunately, unlike for the skip-gram algorithm, we do not have a handy function to generate \\ndata for the CBOW algorithm at our disposal. Therefore, we will need to implement this function \\nourselves. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a027331d-d654-42ff-956e-64df33e8ebb8', embedding=None, metadata={'page_label': '104', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 104\\nYou can find the implementation of this function (named cbow_grams() ) in ch3_word2vec.ipynb  \\nin the Ch03-Word-Vectors  folder. The procedure will be quite similar to the one we used for skip-\\ngrams. However, the format of the data will be slightly different. Therefore, we will discuss the \\nformat of the data returned by this function.\\nThe function takes the same arguments as the skip_gram_data_generator()  function we dis-\\ncussed earlier:\\n• sequences  (List[List[int]])  – A list of list of word IDs. This is the output generated by \\nTokenizer’s texts_to_sequences()  function.\\n• window_size  (int)  – The window size for the context.\\n• batch_size  (int)  – The batch size.\\n• negative_samples  (int)  – The number of negative samples per example to generate.\\n• vocabulary_size  (int)  – The vocabulary size.\\n• seed – The random seed.\\nThe data returned also has a slightly different format. It will return a batch of data containing:\\n• A batch of target word IDs, these target words are both positive and negative.\\n• A batch of corresponding context word IDs. Unlike skip-grams, for CBOW, we need all the \\nwords in the context, not just one. For example, if we define a batch size of b and window \\nsize of w, this will be a [b, 2w]  sized tensor.\\n• A batch or labels (0 and 1).\\nWe will now learn about the specifics  of the algorithm.\\nImplementing CBOW in TensorFlow\\nWe will use  the same hyperparameters as before:\\nbatch_size = 4096 # Data points in a single batch\\nembedding_size = 128 # Dimension of the embedding vector.\\nwindow_size= 1 # We use a window size of 1 on either side of target word\\nepochs = 5 # Number of epochs to train for\\nnegative_samples = 4 # Number of negative samples generated per example\\n# We pick a random validation set to sample nearest neighbors', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='321481bb-9061-446b-b6a5-8f7aee5d181a', embedding=None, metadata={'page_label': '105', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 3 105\\nvalid_size = 16 # Random set of words to evaluate similarity on.\\n# We sample valid datapoints randomly from a large window without always\\n# being deterministic\\nvalid_window = 250\\n# When selecting valid examples, we select some of the most frequent words\\n# as well as  some moderately rare words as well\\nnp.random.seed( 54321)\\nrandom.seed( 54321)\\nvalid_term_ids = np.array(random.sample( range(valid_window), valid_size))\\nvalid_term_ids = np.append(\\n    valid_term_ids, random.sample( range(1000, 1000+valid_window), \\n    valid_size),\\n    axis= 0\\n)\\nJust as before, let’s first clear out any remaining sessions, if there are any:\\nimport tensorflow.keras.backend as K\\nK.clear_session()\\nWe define two input layers. Note how the second input layer is defined to have 2 x window_size  \\ndimensions. This means the final shape of that layer will be  [None, 2 x window_size] :\\n# Inputs\\ninput_1 = tf.keras.layers.Input(shape=())\\ninput_2 = tf.keras.layers.Input(shape=(window_size* 2,))\\nLet’s now define two embedding layers: one  for the context words and one for the target words. \\nWe will feed the inputs from the input layers and produce context_out  and target_out :\\ncontext_embedding_layer = tf.keras.layers.Embedding(\\n    input_dim=n_vocab+ 1, output_dim=embedding_size, \\n    name= 'context_embedding'\\n)\\ntarget_embedding_layer = tf.keras.layers.Embedding(\\n    input_dim=n_vocab+ 1, output_dim=embedding_size, \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='368c9903-ec6c-4271-b0a6-cdf0a50f1c69', embedding=None, metadata={'page_label': '106', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Word2vec – Learning Word Embeddings 106\\n    name= 'target_embedding'\\n)\\ncontext_out = context_embedding_layer(input_2)\\ntarget_out = target_embedding_layer(input_1)\\nIf you look at the shape of context_out , you will see that it has the shape [None, 2, 128] , where \\n2 is 2 x window_size , due to taking the whole context around a word. This needs to be reduced to \\n[None, 128]  by taking the average of all the context words. This is done by using a Lambda layer:\\nmean_context_out = tf.keras.layers.Lambda( lambda x: tf.reduce_mean(x, \\naxis=1))(context_out)\\nWe pass a Lambda  function to the tf.keras.layers.Lambda  layer to reduce the context_out  tensor \\non the second dimension to produce a [None, 128]  sized tensor. With both the target_out  and \\nmean_context_out  tensors having the shape [None, 128] , we can compute the dot product of \\nthe two to produce an output tensor [None, 1] :\\nout = tf.keras.layers.Dot(axes=- 1)([context_out, target_out])\\nWith that, we can define the final model as follows:\\ncbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, \\nname='cbow_model' )\\nSimilar to skip_gram_model , we will compile cbow_model  as follows:\\ncbow_model. compile(\\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits= True), \\n    optimizer= 'adam', \\n    metrics=[ 'accuracy' ]\\n)\\nAgain, if you would like to see the summary of the model, you can run cbow_model.summary() .\\nTraining and evaluating the model\\nThe model training is identical to how we trained the skip-gram model. First, let’s define a callback \\nto find the top k words similar to the words defined in the valid_term_ids  set:\\ncbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, \\ntokenizer)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19286a69-6a21-4630-98e5-596058ec0927', embedding=None, metadata={'page_label': '107', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 107\\nNext, we train cbow_model  for several epochs:\\nfor ei in range (epochs):\\n    print(f\"Epoch: {ei+1}/{epochs}  started\" )\\n    news_cbow_gen = cbow_data_generator(\\n        news_sequences, \\n        window_size, \\n        batch_size, \\n        negative_samples\\n    )\\n    cbow_model.fit(\\n        news_cbow_gen, \\n        epochs= 1, \\n        callbacks=cbow_validation_callback,\\n    )\\nThe output should look like the following. We have cherry-picked some of the most sensible \\nword vectors learned:\\nmonths: years, days, weeks, minutes, seasons\\nyou: we, they, i, don\\'t, we\\'ll\\nwere: are, aren\\'t, have, because, need\\nmusic: terrestrial, cameras, casual, divide, camera\\nalso: already, previously, recently, rarely, reportedly\\nbest: supporting, actress, category, fiction, contenders\\nhim: them, me, themselves, won\\'t, censors\\nmr: tony, gordon, resignation, cherie, jack\\n5bn: 5m, 7bn, 4bn, 8bn, 8m\\nchampions: premier, rugby, appearances, irish, midfielder\\ndeutsche: austria, austria\\'s, butcher, violence, 1989\\nfiles: movies, collections, vast, habit, ballad\\npop: fiction, veteran, scrubs, wars, commonwealth\\nFrom visual inspection, it seems CBOW has learned some effective word vectors. Similar to the \\nskip-gram model, it has picked  words like “years” and “days” as similar  to “months”. Numerical \\nvalues such as “5bn” have “5m” and “7bn” around them. But it’s important to remember that \\nvisual inspection is just a quick and dirty way to evaluate word vectors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e3af8b0-bbcb-4e7e-87d2-ee350bad0668', embedding=None, metadata={'page_label': '108', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Word2vec – Learning Word Embeddings 108\\nTypically, word vectors are evaluated on some downstream tasks. One of the popular tasks is the \\nword analogical reasoning task. It focuses on answering questions like:\\nAthens is to Greece as Baghdad to ____\\nThe answer is Iraq . How is the answer computed? If the word vectors are sensible, then:\\nWord2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)\\nor\\nWord2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)\\nThe answer is computed as the vector given by Word2vec(Baghdad) - Word2vec(Athens) + \\nWord2vec(Greece) . The next step for this analogy task would be to see if the most similar vec-\\ntor to the resulting vector is given by the word Iraq. This way, accuracy can be computed for an \\nanalogy reasoning task. However, we will not utilize this task in this chapter, as our dataset is \\nnot big enough to perform well in this task.\\nHere, we conclude  our discussion on the CBOW algorithm. Though CBOW shares similarities \\nwith the skip-gram algorithm, it had some architectural differences as well as differences in data.\\nSummary\\nWord embeddings have become an integral part of many NLP tasks and are widely used for tasks \\nsuch as machine translation, chatbots, image caption generation, and language modeling. Not \\nonly do word embeddings act as a dimensionality reduction technique (compared to one-hot \\nencoding), they also give a richer feature representation than other techniques. In this chapter, \\nwe discussed two popular neural-network-based methods for learning word representations, \\nnamely the skip-gram model and the CBOW model.\\nFirst, we discussed the classical approaches to this problem to develop an understanding of how \\nword representations were learned in the past. We discussed various methods, such as using \\nWordNet, building a co-occurrence matrix of the words, and calculating TF-IDF.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='973bc823-5cf5-4ff7-8647-04e436d1371a', embedding=None, metadata={'page_label': '109', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3 109\\nNext, we explored neural-network-based word representation learning methods. First, we worked \\nout an example by hand to understand how word embeddings or word vectors can be calculated \\nto help us understand the computations involved.\\nNext, we discussed the first word-embedding learning algorithm—the skip-gram model. We \\nthen learned how to prepare the data to be used for learning. Later, we examined how to design \\na loss function that allows us to use word embeddings using the context words of a given word. \\nFinally, we discussed how to implement the skip-gram algorithm using TensorFlow.\\nThen we reviewed the next choice for learning word embeddings—the CBOW model. We also \\ndiscussed how CBOW differs from the skip-gram model. Finally, we discussed a TensorFlow \\nimplementation of CBOW as well.\\nIn the next chapter, we will learn several other word embedding learning techniques known as \\nGlobal Vectors, or GloVe, and Embeddings from Language Models, or ELMo.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d4e7a5a3-d164-4303-8050-9f0e6c75acdd', embedding=None, metadata={'page_label': '110', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62ace5bd-3fb5-4cc6-b2ba-27c245fb7637', embedding=None, metadata={'page_label': '111', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4\\nAdvanced Word Vector \\nAlgorithms\\nIn Chapter 3, Word2vec – Learning Word Embeddings , we introduced you to Word2vec, the basics \\nof learning word embeddings, and the two common Word2vec algorithms: skip-gram and CBOW. \\nIn this chapter, we will discuss several other word vector algorithms:\\n• GloVe – Global Vectors\\n• ELMo – Embeddings from Language Models\\n• Document classification with ELMo\\nFirst, you will learn a word embedding learning technique known as Global Vectors ( GloVe) and \\nthe specific advantages that GloVe has over skip-gram and CBOW.\\nYou will also look at a recent approach for representing language called Embeddings from  \\nLanguage Models ( ELMo ). ELMo has an edge over other algorithms as it is able to disambiguate \\nwords, as well as capture semantics. Specifically, ELMo generates “contextualized” word repre -\\nsentations, by using a given word along with its surrounding words, as opposed to treating word \\nrepresentations independently, as in skip-gram or CBOW. \\nFinally, we will solve an exciting use-case of document classification using our newly founded \\nELMo vectors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b7cb0c6-4830-4c1f-a480-694e441651d7', embedding=None, metadata={'page_label': '112', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 112\\nGloVe – Global Vectors representation\\nOne of the main limitations of skip-gram and CBOW algorithms is that they can only capture \\nlocal contextual information, as they only look at a fixed-length window around a word. There’s \\nan important part of the puzzle missing here as these algorithms do not look at global statistics \\n(by global statistics we mean a way for us to see all the occurrences of words in the context of \\nanother word in a text corpus). \\nHowever, we have already studied a structure that could contain this information in Chapter 3, \\nWord2vec – Learning Word Embeddings: the co-occurrence matrix. Let’s refresh our memory on \\nthe co-occurrence matrix, as GloVe uses the statistics captured in the co-occurrence matrix to \\ncompute vectors.\\nCo-occurrence matrices encode the context information of words, but they require maintaining \\na V × V matrix, where V is the size of the vocabulary. To understand the co-occurrence matrix, \\nlet’s take two example sentences:\\n• Jerry and Mary are friends .\\n• Jerry buys flowers for Mary.\\nIf we assume a context window of size 1, on each side of a chosen word, the co-occurrence ma-\\ntrix will look like the following (we only show the upper triangle of the matrix, as the matrix is \\nsymmetric):\\nJerry and Mary are friends buys flowers for\\nJerry 0 1 0 0 0 1 0 0\\nand 0 1 0 0 0 0 0\\nMary 0 1 0 0 0 1\\nare 0 1 0 0 0\\nfriends 0 0 0 0\\nbuys 0 1 0\\nflowers 0 1\\nfor 0\\nWe can see that this matrix shows us how a word in a corpus is related to any other word, hence it \\ncontains global statistics about the corpus. That said, what are some of the advantages of having \\na co-occurrence matrix, as opposed to seeing just the local context?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ea9c1fa-bd6b-4c90-9a51-8550b7a26ce2', embedding=None, metadata={'page_label': '113', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 113\\n• It provides you with additional information about the characteristics of the words. For \\nexample, if you consider the sentence “the cat sat on the mat,” it is difficult to say if “the” \\nis a special word that appears in the context of words such as “cat” or “mat.” However, \\nif you have a large-enough corpus and a co-occurrence matrix, it’s very easy to see that \\n“the” is a frequently occurring stop word.\\n• The co-occurrence matrix recognizes the repeating usages of contexts or phrases, whereas \\nin the local context this information is ignored. For example, in a large enough corpus, \\n“New York” will be a clear winner, showing that the two words appear in the same context \\nmany times.\\nIt is important to keep in mind that Word2vec algorithms use various techniques to approxi -\\nmately inject some word co-occurrence patterns, while learning word vectors. For example, the \\nsub-sampling technique we used in the previous chapter (i.e. sampling lower-frequency words \\nmore) helps to detect and avoid stop words. But they introduce additional hyperparameters and \\nare not as informative as the co-occurrence matrix.\\nGloVe, a new technique for learning word embeddings was introduced in the paper “GloVe: Global \\nVectors for Word Representation” by Pennington et al. ( https://nlp.stanford.edu/pubs/glove.\\npdf). GloVe attempts to bridge the gap of missing global co-occurrence information in Word2vec \\nalgorithms. The main contribution of GloVe is a new cost function (or an objective function) \\nthat uses the valuable statistics available in the co-occurrence matrix. Let’s first understand the \\nmotivation behind the GloVe method.Using global statistics to come up with word representations is not a new concept. \\nAn algorithm known as Latent Semantic Analysis  (LSA ) has been using global sta-\\ntistics in its approach.\\nLSA is used as a document analysis technique that maps words in the documents \\nto something known as a concept, a common pattern of words that appears in a \\ndocument. Global matrix factorization-based methods efficiently exploit the global \\nstatistics of a corpus (for example, co-occurrence of words in a global scope), but have \\nbeen shown to perform poorly at word analogy tasks. On the other hand, context \\nwindow-based methods have been shown to perform well at word analogy tasks, but \\ndo not utilize global statistics of the corpus, leaving space for improvement. GloVe \\nattempts to get the best of both worlds—an approach that efficiently leverages global \\ncorpus statistics while optimizing the learning model in a context window-based \\nmanner similar to skip-gram or CBOW.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76e9e5ba-dc34-43f1-9173-1c63c59028c5', embedding=None, metadata={'page_label': '114', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 114\\nUnderstanding GloVe\\nBefore looking at the implementation details of GloVe, let’s take time to understand the concepts \\ngoverning the computations in GloVe. To do so, let’s consider an example:\\n• Consider word i =Ice and j=Steam\\n• Define an arbitrary probe word k\\n• Define 𝑃𝑃𝑖𝑖𝑖𝑖  to be the probability of words i and k  occurring close to each other, and 𝑃𝑃𝑗𝑗𝑗𝑗  to \\nbe the words j and k occurring together\\nNow let’s look at how the 𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄  entity behaves with different values for k .\\nFor k = “Solid” , it is highly likely to appear with i, thus, 𝑃𝑃𝑖𝑖𝑖𝑖  will be high. However, k would not \\noften appear along with j causing a low 𝑃𝑃𝑗𝑗𝑗𝑗 . Therefore, we get the following expression:\\n𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄≫1 \\nNext, for k  = “gas”, it is unlikely to appear in the close proximity of i and therefore will have a low \\n 𝑃𝑃𝑖𝑖𝑖𝑖 ; however, since k  highly correlates with j, the value of 𝑃𝑃𝑗𝑗𝑗𝑗  will be high. This leads to the fol-\\nlowing:\\n𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄≈0 \\nNow, for words such as k = “water”, which has a strong relationship with both i and j, or k = \\n“Fashion”, which i  and j both have minimal relevance to, we get this:\\n𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄≈1 \\nIf you assume we have learned sensible word embeddings for these words, these relationships can \\nbe visualized in a vectors space to understand why the ratio 𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄  behaves this way (see Figure 4. \\n1). In the figure below, the solid arrow shows the distance between the words (i, j), whereas the \\ndashed lines express the distance between the words, ( i, k) and ( j, k). These distances can then be \\nassociated with the probability values we discussed. For example, when i  = “ice” and k = “solid”, \\nwe expect their vectors to have a shorter distance between them (i.e. more frequently co-occur -\\nring). Therefore, we can associate distance between (i, k) as the inverse of 𝑃𝑃𝑖𝑖𝑖𝑖  (i.e. 1𝑃𝑃𝑖𝑖𝑖𝑖⁄ ) due to \\nthe definition of 𝑃𝑃𝑖𝑖𝑖𝑖 . This diagram shows how these distances vary as the probe word k  changes:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d4bc246-e416-40e7-b651-f18637e81e00', embedding=None, metadata={'page_label': '115', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 115\\nFigure 4.1: How the entities P_ik and P_jk behave as the probe word changes in proximity to \\nthe words i and j\\nIt can be seen that the 𝑃𝑃𝑖𝑖𝑖𝑖𝑃𝑃𝑗𝑗𝑖𝑖⁄  entity, which is calculated by measuring the frequency of two words \\nappearing close to each other, behaves in different ways as the relationship between the three \\nwords changes. As a result, it becomes a good candidate for learning word vectors. Therefore, a \\ngood starting point for defining the loss function will be as shown here:\\n𝐹𝐹𝐹𝐹𝐹𝑖𝑖,𝐹𝐹𝑗𝑗,𝐹𝐹𝑤𝑘𝑘)=𝑃𝑃𝑖𝑖𝑘𝑘𝑃𝑃𝑗𝑗𝑘𝑘⁄ \\nHere, F is some function and w and 𝑤𝑤𝑤  are two different embedding spaces we’ll be using. In other \\nwords, the words 𝑖𝑖  and 𝑗𝑗  are looked up from one embedding space, whereas the probe word 𝑘𝑘  is \\nlooked up from another. From this point, the original paper goes through the derivation metic-\\nulously to reach the following loss function:\\n𝐽𝐽 𝐽 𝐽 𝐽𝐽𝐽𝐽𝐽 𝑖𝑖𝑖𝑖)𝐽𝑤𝑤𝑖𝑖𝑇𝑇𝑤𝑤𝑤𝑖𝑖+𝑏𝑏𝑖𝑖+𝑏𝑏̃𝑖𝑖−log𝐽𝐽𝐽𝑖𝑖𝑖𝑖))2𝑉𝑉\\n𝑖𝑖𝑖𝑖𝑖𝑖𝑖 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='586ca146-f4c0-425c-ba09-14e11920b91c', embedding=None, metadata={'page_label': '116', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 116\\nWe will not go through the derivation here, as that’s out of scope for this book. Rather we will \\nuse the derived loss function and implement the algorithm with TensorFlow. If you need a less  \\nmathematically dense explanation of how we can derive this cost function, please refer to the \\nauthor-written article at https://towardsdatascience.com/light-on-math-ml-intuitive-\\nguide-to-understanding-glove-embeddings-b13b4f19c010 .\\nHere, 𝑓𝑓(𝑥𝑥)  is defined as 𝑓𝑓(𝑥𝑥)=(𝑥𝑥𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚⁄)(34⁄) , if 𝑥𝑥𝑥𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚 , else 1, where 𝑋𝑋𝑖𝑖𝑖𝑖  is the frequency with which \\nthe word j appeared in the context of the word i. 𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚  is a hyperparameter we set. Remember that we \\ndefined two embedding spaces 𝑤𝑤  and 𝑤𝑤𝑤  in our loss function. 𝑤𝑤𝑖𝑖  and 𝑏𝑏𝑖𝑖  represent the word embedding \\nand the bias embedding for the word i obtained from embedding space 𝑤𝑤 , respectively. And, 𝑤𝑤𝑤𝑗𝑗  and \\n𝑏𝑏̃𝑗𝑗  represent the word embedding and bias embedding for word j obtained from embedding space  \\n𝑤𝑤𝑤 , respectively.  Both these embeddings behave similarly except for the randomization at the \\ninitialization. At the evaluation phase, these two embeddings are added together, leading to \\nimproved performance.\\nImplementing GloVe\\nIn this  subsection, we will discuss the steps for implementing GloVe. The full code is available in \\nthe ch4_glove.ipynb  exercise file located in the ch4  folder.\\nFirst, we’ll define the hyperparameters as we did in the previous chapter: \\nbatch_size = 4096 # Data points in a single batch\\nembedding_size = 128 # Dimension of the embedding vector.\\nwindow_size= 1 # We use a window size of 1 on either side of target word\\nepochs = 5 # Number of epochs to train for\\n# We pick a random validation set to sample nearest neighbors\\nvalid_size = 16 # Random set of words to evaluate similarity on.\\n# We sample valid datapoints randomly from a large window without always \\n# being deterministic\\nvalid_window = 250\\n# When selecting valid examples, we select some of the most frequent words \\n# as well as  some moderately rare words as well', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78fa0c1e-980e-4f1e-80cf-efe13ae81029', embedding=None, metadata={'page_label': '117', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 117\\nnp.random.seed( 54321)\\nrandom.seed( 54321)\\nvalid_term_ids = np.array(random.sample( range(valid_window), valid_size))\\nvalid_term_ids = np.append(\\n    valid_term_ids, random.sample( range(1000, 1000+valid_window), valid_ \\n    size),\\n    axis= 0\\n)\\nThe hyperparameters you define here are the same hyperparameters we defined in the previous \\nchapter. We have a batch size, embedding size, window size, the number of epochs, and, finally, \\na set of held-out validation word IDs that we will print the most similar words to.\\nWe will then define the model. First, we will import a few things we will need down the line: \\nimport tensorflow.keras.backend as K\\nfrom tensorflow.keras.layers import Input, Embedding, Dot, Add\\nfrom tensorflow.keras.models import Model\\nK.clear_session()\\nThe model is going to have two input layers: word_i and word_j . They represent a batch of context \\nwords and a batch of target words (or a batch of positive skip-grams): \\n# Define two input layers for context and target words\\nword_i = Input(shape=())\\nword_j = Input(shape=())\\nNote how the shape is defined. The shape is defined as an empty tuple. This means the final shape \\nof word_i  and word_j  would be [None] , meaning it will take a vector of an arbitrary number of \\nelements as the input. \\nNext, we are going to define the embedding layers. There will be four embedding layers:\\n• embeddings_i  – The context embedding layer\\n• embeddings_j  – The target embedding layer\\n• b_i – The context embedding bias\\n• b_j – The target embedding bias', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba57e7c4-f76e-4eb1-bd23-3b5d936b92a8', embedding=None, metadata={'page_label': '118', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 118\\nThe following code defines these:\\n# Each context and target has their own embeddings (weights and biases)\\n# Embedding weights\\nembeddings_i = Embedding(n_vocab, embedding_size, name= 'target_embedding' )\\n(word_i)\\nembeddings_j = Embedding(n_vocab, embedding_size,  \\nname='context_embedding' )(word_j)\\n# Embedding biases\\nb_i = Embedding(n_vocab, 1, name= 'target_embedding_bias' )(word_i)\\nb_j = Embedding(n_vocab, 1, name= 'context_embedding_bias' )(word_j)\\nNext, we are going to compute the output. The output of this model will be:\\n𝑤𝑤𝑖𝑖𝑇𝑇𝑤𝑤𝑗𝑗̃+𝑏𝑏 𝑖𝑖+𝑏𝑏𝑗𝑗̃ \\nAs you can see, that’s a portion of our final loss function. We have all the right ingredients to \\ncompute this result: \\n# Compute the dot product between embedding vectors (i.e. w_i.w_j)\\nij_dot = Dot(axes=- 1)([embeddings_i,embeddings_j])\\n# Add the biases (i.e. w_i.w_j + b_i + b_j )\\npred = Add()([ij_dot, b_i, b_j])\\nFirst we will use the tensorflow.keras.layers.Dot  layer to compute the dot product batch-\\nwise between the context embedding lookup ( embeddings_i ) and the target embedding look -\\nup (embeddings_j ). For example, the two inputs to the Dot  layer will be of size [batch size, \\nembedding size] . After the dot product, the output ij_dot  will be [batch size, 1] , where \\nij_dot[k]  will be the dot product between embeddings_i[k, :]  and embeddings_j[k, :] . Then \\nwe simply add b_i  and b_j  (which has shape [None, 1] ) element-wise to ij_dot .\\nFinally, the model is defined as taking word_i  and word_j  as inputs and outputting pred :\\n# The final model\\nglove_model = Model(\\n    inputs=[word_i, word_j],outputs=pred,\\nname='glove_model'\\n)\\nNext, we are going to do something quite important. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fac18c46-2db0-4dbc-8814-2640dbfbd46c', embedding=None, metadata={'page_label': '119', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 119\\nWe have to devise a way to compute the complex loss function defined above, using various \\ncomponents/functionality available in a model. First let’s revisit the loss function.\\n𝐽𝐽 𝐽 𝐽 𝐽𝐽𝐽𝐽𝐽 𝑖𝑖𝑖𝑖)𝐽𝑤𝑤𝑖𝑖𝑇𝑇𝑤𝑤𝑤𝑖𝑖+𝑏𝑏𝑖𝑖+𝑏𝑏̃𝑖𝑖−log𝐽𝐽𝐽𝑖𝑖𝑖𝑖))2𝑉𝑉\\n𝑖𝑖𝑖𝑖𝑖𝑖𝑖 \\nwhere,\\n𝑓𝑓(𝑥𝑥)=(𝑥𝑥𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚⁄)(34⁄) , if 𝑥𝑥𝑥𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚 , else 1.\\nAlthough it looks complex, we can use already existing loss functions and other functionality to \\nimplement the GloVe loss. You can abstract this loss function into three components as shown \\nin the image below:\\nFigure 4.2: The breakdown of the GloVe loss function showing how predictions, targets, and \\nweights interact with each other to compute the final loss\\nTherefore, if sample weights are denoted by 𝑊𝑊 , predictions are denoted by 𝑌𝑌̂ , and true targets are \\ndenoted by 𝑌𝑌 , then we can write the loss as:\\n𝐽𝐽 𝐽 𝐽𝐽𝐽𝐽𝐽̂–𝐽𝐽𝑌2 \\nThis is simply a weighted mean squared loss. Therefore, we will use \"mse\"  as the loss for our model:\\n# Glove has a specific loss function with a sound mathematical\\n# underpinning\\n# It is a form of mean squared error\\nglove_model. compile(loss=\"mse\", optimizer = \\'adam\\')\\nWe will later see how we can feed in sample weights to the model to complete the loss function. \\nSo far, we have defined different components of the GloVe algorithm and compiled the model. \\nNext, we are going to have a look at how data can be generated to train the GloVe model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0219992-086a-4fbb-8c76-e6ae48dff812', embedding=None, metadata={'page_label': '120', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 120\\nGenerating data for GloVe\\nThe dataset we will be using is the same as the dataset from the previous chapter. To recap, we \\nwill be using the BBC news articles dataset available at http://mlg.ucd.ie/datasets/bbc.html . \\nIt contains 2225 news articles belonging to 5 topics, business, entertainment, politics, sport, and \\ntech, which were published on the BBC website between 2004 and 2005.\\nLet’s now generate the data. We will be encapsulating the data generation in a function called \\nglove_data_generator() . As the first step, let us write a function signature: \\ndef glove_data_generator (\\n    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix,\\n    x_max= 100.0, alpha= 0.75, seed=None\\n):\\nThe function takes several arguments:\\n• sequences  (List[List[int]] ) – a list of a list of word IDs. This is the output generated \\nby tokenizer’s texts_to_sequences()  function. \\n• window_size  (int ) – Window size for the context.\\n• batch_size  (int ) – Batch size.\\n• vocab_size  (int ) – Vocabulary size. \\n• cooccurrence_matrix  (scipy.sparse.lil_matrix ) – A sparse matrix containing co-oc-\\ncurrences of words.\\n• x_max  (int ) – Hyperparameter used by GloVe to compute sample weights.\\n• alpha  (float ) – Hyperparameter used by GloVe to compute sample weights.\\n• seed  – The random seed.\\nIt also has several outputs:\\n• A batch of (target, context) word ID tuples\\n• The corresponding 𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑖𝑖𝑖𝑖)  values for the (target, context) tuples\\n• Sample weights (i.e. 𝑓𝑓𝑓𝑓𝑓𝑖𝑖𝑖𝑖) ) values for the (target, context) tuples\\nFirst we will shuffle the order of news articles:\\n    # Shuffle the data so that, every epoch, the order of data is\\n    # different\\n    rand_sequence_ids = np.arange( len(sequences))\\n    np.random.shuffle(rand_sequence_ids)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76d1286c-d567-42d1-a97e-1cd38c232aef', embedding=None, metadata={'page_label': '121', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 121\\nNext, we will create the  sampling table, so that we can use sub-sampling to avoid over-sampling \\ncommon words (e.g. stop words):\\n    sampling_table = \\n    tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\\nWith that, for every sequence (i.e. list of word IDs) representing an article, we generate positive \\nskip-grams. Note how we are keeping negative_samples=0.0  as, unlike skip-gram or CBOW \\nalgorithms, GloVe does not rely on negative candidates: \\n    # For each story/article\\n    for si in rand_sequence_ids:\\n        \\n        # Generate positive skip-grams while using sub-sampling \\n        positive_skip_grams, _ = tf.keras.preprocessing.sequence.\\n        skipgrams(\\n            sequences[si], \\n            vocabulary_size=vocab_size, \\n            window_size=window_size, \\n            negative_samples= 0.0, \\n            shuffle= False,   \\n            sampling_table=sampling_table,\\n            seed=seed\\n        )\\nWith that, we first break down the skip-gram tuples into two lists, one containing targets and the \\nother containing context words, and convert them to NumPy arrays subsequently:\\n        # Take targets and context words separately\\n        targets, context = zip(*positive_skip_grams)\\n        targets, context = np.array(targets).ravel(),\\n        np.array(context).ravel()\\nWe then index the positions given by the (target, context) word pairs, from the co-occurrence \\nmatrix to retrieve the corresponding 𝑋𝑋𝑖𝑖𝑖𝑖  values, where ( i,j) represents a (target, context) pair:\\n        x_ij = np.array(cooccurrence_matrix[targets, \\n        context].toarray()).ravel()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ecf4de6-9152-486c-b72e-95edffc885c7', embedding=None, metadata={'page_label': '122', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 122\\nThen we compute a corresponding 𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑖𝑖𝑖𝑖)  (denoted by log_x_ij ) and 𝑓𝑓𝑓𝑓𝑓𝑖𝑖𝑖𝑖)  (denoted by sample_\\nweights ):\\n        # Compute log - Introducing an additive shift to make sure we\\n        # don't compute log(0)\\n        log_x_ij = np.log(x_ij + 1)\\n        \\n        # Sample weights \\n        # if x < x_max => (x/x_max)**alpha / else => 1        \\n        sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1)\\nIf a code is not chosen, a random seed is set. Afterward, all of context , targets , log_x_ij , and \\nsample_weights  are shuffled while maintaining the correspondence of elements between the \\narrays:\\n        # If seed is not provided generate a random one\\n        if not seed:\\n            seed = random.randint( 0, 10e6)\\n        \\n        # Shuffle data\\n        np.random.seed(seed)\\n        np.random.shuffle(context)\\n        np.random.seed(seed)\\n        np.random.shuffle(targets)\\n        np.random.seed(seed)\\n        np.random.shuffle(log_x_ij)\\n        np.random.seed(seed)\\n        np.random.shuffle(sample_weights)\\nFinally, we iterate through batches of the data we created above. Each batch will consist of\\n• A batch of (target, context) word ID tuples\\n• The corresponding 𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 𝑖𝑖𝑖𝑖)  values for the (target, context) tuples\\n• Sample weights (i.e. 𝑓𝑓𝑓𝑓𝑓𝑖𝑖𝑖𝑖) ) values for the (target, context) tuples\\nin that order.\\n        # Generate a batch or data in the format \\n        # ((target words, context words), log(X_ij) <- true targets,\\n        # f(X_ij) <- sample weights)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cff8e030-0aa4-44e2-8afc-7320cb29f29d', embedding=None, metadata={'page_label': '123', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 123\\n        for eg_id_start in range (0, context.shape[ 0], batch_size):            \\n            yield (\\n                targets[eg_id_start: min(eg_id_start+batch_size, \\n                targets.shape[ 0])], \\n                context[eg_id_start: min(eg_id_start+batch_size, \\n                context.shape[ 0])]\\n            ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, \\n            log_x_ij.shape[ 0])], \\\\\\n            sample_weights[eg_id_start: min(eg_id_start+batch_size, \\n            sample_weights.shape[ 0])]\\nNow that the data is ready to be pumped in, let’s discuss the final piece of the puzzle: training \\nthe model.\\nTraining and evaluating GloVe\\nTraining the model is effortless, as we have all the components to train the model. As the first \\nstep, we will reuse the ValidationCallback  we created in Chapter 3, Word2vec – Learning Word \\nEmbeddings. To recap, ValidationCallback  is a Keras callback. Keras callbacks give you a way \\nto execute some important operation(s) at the end of every training iteration, epoch, predic-\\ntion step, etc. Here we are using the callback to perform a validation step at the end of every \\nepoch. Our callback would take a list of word IDs intended as the validation words (held out in \\nvalid_term_ids ), the model containing the embedding matrix, and a tokenizer to decode word \\nIDs. Then it will compute the most similar top-k words for every word in the validation word set \\nand print that as the output:\\nglove_validation_callback = ValidationCallback(valid_term_ids, glove_\\nmodel, tokenizer)\\n# Train the model for several epochs\\nfor ei in range (epochs):\\n    \\n    print(\"Epoch: {}/{} started\" .format(ei+1, epochs))\\n    \\n    news_glove_data_gen = glove_data_generator(\\n        news_sequences, window_size, batch_size, n_vocab\\n    )\\n    ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7e83354f-51c4-4382-81fd-e9b6f43eff3a', embedding=None, metadata={'page_label': '124', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 124\\n    glove_model.fit(\\n        news_glove_data_gen, epochs= 1, \\n        callbacks=glove_validation_callback,\\n    )\\nYou should get a sensible-looking output once the model has finished training. Here are some of \\nthe cherry-picked results:\\nelection: attorney, posters, forthcoming, november's, month's\\nmonths: weeks, years, nations, rbs, thirds\\nyou: afford, we, they, goodness, asked\\nmusic: cameras, mp3, hp's, refuseniks, divide\\nbest: supporting, category, asante, counterparts, actor\\nmr: ron, tony, bernie, jack, 63\\nleave: pay, need, unsubstantiated, suited, return\\n5bn: 8bn, 2bn, 1bn, 3bn, 7bn\\ndebut: solo, speakerboxxx, youngster, nasty, toshack\\nimages: 117, pattern, recorder, lennon, unexpectedly\\nchampions: premier, celtic, football, representatives, neighbour\\nindividual: extra, attempt, average, improvement, survived\\nbusinesses: medium, sell, redder, abusive, handedly\\ndeutsche: central, austria's, donald, ecb, austria\\nmachine: unforced, wireless, rapid, vehicle, workplace\\nYou can see that words like “months,” “weeks,” and “years” are grouped together. Numbers like \\n“5bn,” “8bn,” and “2bn” are grouped together as well. “Deutsche” is surrounded by “Austria’s” and \\n“Austria.” Finally, we will save the embeddings to the disk. We will combine weights and the bias \\nof each context and target vector space to a single array, where the last column of the array will \\nrepresent the bias  and save it to the disk:\\ndef save_embeddings (model, tokenizer, vocab_size, save_dir):\\n    \\n    os.makedirs(save_dir, exist_ok= True)\\n    \\n    _, words_sorted = zip(*sorted (list(tokenizer.index_word.items()),\\n    key= lambda x: x[0])[:vocab_size- 1])\\n        \\n    words_sorted = [ None] + list(words_sorted)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aca88ac5-1b6c-4376-929d-95449b77eff2', embedding=None, metadata={'page_label': '125', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 125\\n    \\n    context_embedding_weights = model.get_layer( \"context_embedding\" ).get_\\n    weights()[ 0]\\n    context_embedding_bias = model.get_layer( \"context_embedding_bias\" ).\\n    get_weights()[ 0]\\n    context_embedding = np.concatenate([context_embedding_weights,\\n    context_embedding_bias], axis= 1)\\n    \\n    target_embedding_weights = model.get_layer( \"target_embedding\" ).get_\\n    weights()[ 0]\\n    target_embedding_bias = model.get_layer( \"target_embedding_bias\" ).get_\\n    weights()[ 0]\\n    target_embedding = np.concatenate([target_embedding_weights, target_\\n    embedding_bias], axis= 1)\\n    \\n    pd.DataFrame(\\n        context_embedding, \\n        index = words_sorted\\n    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\" ))\\n    \\n    pd.DataFrame(\\n        target_embedding, \\n        index = words_sorted\\n    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\" ))\\n    \\nsave_embeddings(glove_model, tokenizer, n_vocab, save_dir= \\'glove_\\nembeddings\\' )\\nWe will save embeddings as pandas DataFrames. First we get all the words sorted by their IDs. \\nWe subtract 1 to discount the reserved word ID 0 as we’ll add that manually, in the following \\nline. Note that, word ID 0 will not show up in tokenizer.index_word . Next we get the required \\nlayers by name (namely, context_embedding , target_embedding , context_embedding_bias  \\nand target_embedding_bias ). Once we have the layers we can use the get_weights()  function \\nto retrieve weights. \\nIn this section, we looked at GloVe, another word embedding learning technique. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d516622f-2257-4a54-a7a8-bcc7ea7856f8', embedding=None, metadata={'page_label': '126', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 126\\nThe main advantage of GloVe over the Word2vec techniques discussed in Chapter 3, Word2vec \\n– Learning Word Embeddings , is that it pays attention to both global and local statistics of the \\ncorpus to learn embeddings. As GloVe is able to capture the global information about words, it \\ntends to give better performance, especially when the corpus size increases. Another advantage is \\nthat, unlike in Word2vec techniques, GloVe does not approximate the cost function (for example, \\nWord2vec using negative sampling), but calculates the true cost. This leads to better and easier \\noptimization of the loss.\\nIn the next section, we are going to look at one more word vector algorithm known as Embeddings \\nfrom Language Models  (ELMo).\\nELMo – Taking ambiguities out of word vectors\\nSo far, we’ve looked at word embedding algorithms that can give only a unique representation of \\nthe words in the vocabulary. However, they will give a constant representation for a given word, \\nno matter how many times you query. Why would this be a problem? Consider the following \\ntwo phrases:\\nI went to the bank to deposit some money\\nand\\nI walked along the river bank\\nClearly, the word “bank” is used in two totally different contexts. If you use a vanilla word vector \\nalgorithm (e.g. skip-gram), you can only have one representation for the word “bank”, and it is \\nprobably going to be muddled between the concept of a financial institution and the concept of \\nwalkable edges along a river, depending on the references to this word found in the corpus it’s \\ntrained on. Therefore, it is more sensible to provide embeddings for a word while preserving and \\nleveraging the context around it. This is exactly what ELMo is striving for.\\nSpecifically, ELMo takes in a sequence, as opposed to a single token, and provides contextualized \\nrepresentations for each token in the sequence. Figure 4.3  depicts various components encompass -\\ning the model. The first thing to understand is that ELMo is a complicated beast! There are lots of \\nneural network models orchestrating in ELMo to produce the output. Particularly, the model uses: \\n• A character embedding layer (an embedding vector for each character).\\n• A convolutional neural network  (CNN ) – a CNN consists of many convolutional layers \\nfollowed by an optional fully connected classification layer. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e4d90ba-99c6-4852-b772-1e0ebc90412e', embedding=None, metadata={'page_label': '127', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 127\\nA convolution layer takes in a sequence of inputs (e.g. sequence of characters in a word) \\nand moves a window of weights over the input to generate a latent representation. We \\nwill discuss CNNs in detail in the coming chapters.\\n• Two bi-directional LSTM layers – an LSTM is a type of model that is used to process \\ntime-series data. Given a sequence of inputs (e.g. sequence of word vectors), an LSTM \\ngoes from one input to the other, on the time dimension, and produces an output at each \\nposition. Unlike fully connected networks, LSTMs have memory, meaning the output \\nat the current position will be affected by what the LSTM has seen in the past. We will \\ndiscuss LSTMs in detail in the coming chapters.\\nThe specifics of these different components are outside the scope of this chapter. They will be \\ndiscussed in detail in the coming chapters. Therefore, do not worry if you do not understand the \\nexact mechanisms of the sub-components shown here ( Figure 4.3 ).\\nFigure 4.3: Different components of the ELMo model. Token embeddings are generated using \\na type of neural network known as a CNN. These token embeddings are fed to an LSTM model \\n(that can process time-series data). The output of the first LSTM model is fed to a second LSTM \\nmodel to generate a latent contextualized representation for each token\\nWe can download a pretrained ELMo model from TensorFlow Hub ( https://tfhub.dev ). TF Hub \\nis a repository for various pretrained models. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d22cb21-c2d6-4505-a1e7-0d080a2b5aa7', embedding=None, metadata={'page_label': '128', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 128\\nIt hosts models for tasks such as image classification, text classification, text generation, etc. You \\ncan go to the site and browse various available models.\\nDownloading ELMo from TensorFlow Hub\\nThe ELMo model we will be using is found at https://tfhub.dev/google/elmo/3 . It has been \\ntrained on a very large corpus of text to solve a task known as language modeling. In language \\nmodeling, we try to predict the next word given the previous sequence of tokens. We will learn \\nmore about language modeling in the coming chapters.\\nBefore downloading the model, let’s set the following environment variables:\\n# Not allocating full GPU memory upfront\\n%env TF_FORCE_GPU_ALLOW_GROWTH=true\\n# Making sure we cache the models and are not downloaded all the time\\n%env TFHUB_CACHE_DIR=./tfhub_modules\\nTF_FORCE_GPU_ALLOW_GROWTH  allows TensorFlow to allocate GPU memory on-demand as opposed \\nto allocating all GPU memory at once. TFHUB_CACHE_DIR  sets the directory where the models will \\nbe downloaded. We will first import TensorFlow Hub:\\nimport tensorflow_hub as hub\\nNext, as usual, we will clear any running TensorFlow sessions by running the following code:\\nimport tensorflow as tf\\nimport tensorflow.keras.backend as K\\nK.clear_session()\\nFinally, we will download the ELMo model. You can employ two ways to download pretrained \\nmodels from TF Hub and use them in our code:\\n• hub.load(<url>, **kwargs)  – Recommended way for downloading and using Tensor -\\nFlow 2-compatible models\\n• hub.KerasLayer(<url>, **kwargs)  – This is a workaround for using TensorFlow 1-based \\nmodels in TensorFlow 2\\nUnfortunately, ELMo has not been ported to TensorFlow 2 yet. Therefore, we will use the  \\nhub.KerasLayer()  as the workaround to load ELMo in TensorFlow 2:\\nelmo_layer = hub.KerasLayer(\\n    \"https://tfhub.dev/google/elmo/3\" , \\n    signature= \"tokens\" ,signature_outputs_as_dict= True\\n)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='08aaf823-0b99-4d76-af87-68faf1d25956', embedding=None, metadata={'page_label': '129', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 129\\nNote that we are providing two arguments, signature  and signature_outputs_as_dict : \\n• signature  (str) – Can be default  or tokens . The default signature accepts a list of strings, \\nwhere each string will be converted to a list of tokens internally. The tokens signature \\ntakes in inputs as dictionary having two keys. Namely, tokens  (a list of list of tokens. Each \\nlist of tokens is a single phrase/sentence and includes padding tokens to bring them to \\na fixed length) and “ sequence_len \" (the length of each list of tokens, to determine the \\npadding length).\\n• signature_outputs_as_dict  (bool ) – When set to true , it will return all the outputs \\ndefined in the provided signature.  \\nNow that we have understood the  components of ELMo and downloaded it from TensorFlow \\nHub, let’s see how we can process input data for ELMo.\\nPreparing inputs for ELMo\\nHere we will define a function that will convert a given list of strings to the format ELMo expects \\nthe inputs to be in. Remember that we set the signature of ELMo to be tokens. An example input \\nto the signature \"tokens\"  would look as follows.\\n{\\n    \\'tokens\\': [\\n        [\\'the\\', \\'cat\\', \\'sat\\', \\'on\\', \\'the\\', \\'mat\\'],\\n        [\\'the\\', \\'mat\\', \\'sat\\', \\'\\', \\'\\', \\'\\']\\n    ], \\n    \\'sequence_len\\': [6, 3]\\n}\\nLet’s take a moment to process what the input comprises. First it has the key tokens , which has \\na list of tokens. Each list of tokens can be thought of as a sentence. Note how padding is added \\nto the end of the short sentence to match the length. This is important as, otherwise, the mod-\\nel will throw an error as it can’t convert arbitrary-length sequences to a tensor. Next we have \\nsequence_len , which is a list of integers. Each integer specifies the true length of each sequence. \\nNote how the second element says 3, to match the actual tokens present in the second sequence.\\nGiven a list of strings, we can write a function to do this transformation for us. That’s what the \\nformat_text_for_elmo()  function will do for us. Let’s sink our teeth into the specifics:\\ndef format_text_for_elmo (texts, lower= True, split= \" \", max_len= None):\\n    \\n    \"\"\" Formats a given text for the ELMo model (takes in a list of\\n    strings) \"\"\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='defcbece-2822-46d8-8b4c-11f82d04a969', embedding=None, metadata={'page_label': '130', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 130\\n        \\n    token_inputs = [] # Maintains individual tokens\\n    token_lengths = [] # Maintains the length of each sequence\\n    \\n    max_len_inferred = 0 \\n    # We keep a variable to maintain the max length of the input\\n    # Go through each text (string)\\n    for text in texts:    \\n        \\n        # Process the text and get a list of tokens\\n        tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, \\n        lower=lower, split=split)\\n        \\n        # Add the tokens \\n        token_inputs.append(tokens)                   \\n        \\n        # Compute the max length for the collection of sequences\\n        if len(tokens)>max_len_inferred:\\n            max_len_inferred = len(tokens)\\n    \\n    # It's important to make sure the maximum token length is only as\\n    # large as the longest input in the sequence\\n    # Here we make sure max_len is only as large as the longest input\\n    if max_len and max_len_inferred < max_len:\\n        max_len = max_len_inferred\\n    if not max_len:\\n        max_len = max_len_inferred\\n    \\n    # Go through each token sequence and modify sequences to have same\\n    # length\\n    for i, token_seq in enumerate (token_inputs):\\n        \\n        token_lengths.append( min(len(token_seq), max_len))\\n        \\n        # If the maximum length is less than input length, truncate\\n        if max_len < len(token_seq):\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a20225b4-3fca-4ccc-b8b4-71b1a6468eb8', embedding=None, metadata={'page_label': '131', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 131\\n            token_seq = token_seq[:max_len]            \\n        # If the maximum length is greater than or equal to input length,\\n        # add padding as needed\\n        else:            \\n            token_seq = token_seq+[ \"\"]*(max_len- len(token_seq))\\n                \\n        assert len(token_seq)==max_len\\n        \\n        token_inputs[i] = token_seq\\n    \\n    # Return the final output\\n    return {\\n        \"tokens\" : tf.constant(token_inputs), \\n        \"sequence_len\" : tf.constant(token_lengths)\\n    }\\nWe first create two lists, token_inputs  and token_lengths , to contain individual tokens and \\ntheir respective lengths. Next we go through each string in texts , and get the individual tokens \\nusing the tf.keras.preprocessing.text.text_to_word_sequence()  function. While doing so, \\nwe will calculate the maximum token length we have observed so far. After iterating through the \\nsequences, we check if the maximum length inferred from the inputs is different to max_len  (if \\nspecified). If so, we will use max_len_inferred  as the maximum length. This is important, be -\\ncause if you do otherwise, you may unnecessarily lengthen the inputs by defining a large value \\nfor max_len . Not only that, the model will raise an error like the one below if you do so.\\n    #InvalidArgumentError:  Incompatible shapes: [2,6,1] vs. [2,10,1024]\\n    #    [[node mul (defined at .../python3.6/site-packages/tensorflow_\\n    hub/module_v2.py:106) ]] [Op:__inference_pruned_3391]\\nOnce the proper maximum length is found, we will go through the sequences and\\n• If it is longer than max_len , truncate the sequence\\n• If it is shorter than max_len , add tokens until it reaches max_len\\nFinally, we will convert them to tf.Tensor  objects using the tf.constant  construct. For example, \\nyou can call this function with:\\nprint(format_text_for_elmo([ \"the cat sat on the mat\" , \"the mat sat\" ], max_\\nlen=10))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fba187b0-e62b-4648-8c8b-21450ef18167', embedding=None, metadata={'page_label': '132', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 132\\nThis will output:\\n{\\'tokens\\': <tf.Tensor: shape=(2, 6), dtype=string, numpy=\\narray([[b\\'the\\', b\\'cat\\', b\\'sat\\', b\\'on\\', b\\'the\\', b\\'mat\\'],\\n       [b\\'the\\', b\\'mat\\', b\\'sat\\', b\\'\\', b\\'\\', b\\'\\']], dtype=object)>, \\n\\'sequence_len\\': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], \\ndtype=int32)>}\\nWe will now see how ELMo can be used to generate embeddings for the prepared inputs.\\nGenerating embeddings with ELMo\\nOnce the input is prepared, generating embeddings is quite easy. First we will transform the \\ninputs to the stipulated  format of the ELMo layer. Here we are using some example titles from \\nthe BBC dataset:\\n# Titles of 001.txt - 005.txt in bbc/business\\nelmo_inputs = format_text_for_elmo([\\n    \"Ad sales boost Time Warner profit\" ,\\n    \"Dollar gains on Greenspan speech\" ,\\n    \"Yukos unit buyer faces loan claim\" ,\\n    \"High fuel prices hit BA\\'s profits\" ,\\n    \"Pernod takeover talk lifts Domecq\"\\n])\\nNext, simply pass the elmo_inputs  to the elmo_layer  as the input and get the result:\\n# Get the result from ELMo\\nelmo_result = elmo_layer(elmo_inputs)\\nLet’s now print the results and their shapes with the following line:\\n# Print the result\\nfor k,v in elmo_result.items():    \\n    print(\"Tensor under key={} is a {} shaped Tensor\" .format(k, v.shape))\\nThis will print out:\\nTensor under key=sequence_len is a (5,) shaped Tensor\\nTensor under key=elmo is a (5, 6, 1024) shaped Tensor\\nTensor under key=default is a (5, 1024) shaped Tensor\\nTensor under key=lstm_outputs1 is a (5, 6, 1024) shaped Tensor\\nTensor under key=lstm_outputs2 is a (5, 6, 1024) shaped Tensor\\nTensor under key=word_emb is a (5, 6, 512) shaped Tensor', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3e3be09-b673-46a1-87ec-d33adf94eb3b', embedding=None, metadata={'page_label': '133', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 4 133\\nAs you can see, the model returns 6 different outputs. Let’s go through them one by one:\\n• sequence_len  – The same input we provided containing the lengths of the sequences in \\nthe input\\n• word_emb  – The token embeddings obtained via the CNN layer in the ELMo model. We got \\na vector of size 512 for all sequence positions (i.e. 6) and for all rows in the batch (i.e. 5).\\n• lstm_output1  – The contextualized representations of tokens obtained via the first LSTM \\nlayer\\n• lstm_output2  – The contextualized representations of tokens obtained via the second \\nLSTM layer\\n• default  – The mean embedding vector obtained by averaging all of the lstm_output1  \\nand lstm_output2  embeddings\\n• elmo  – The weighted sum of all of word_emb , lstm_output1 , and lstm_output2 , where \\nweights are a set of task-specific trainable parameters that will be jointly trained during \\nthe task-specific training\\nWhat we are interested in here is the default  output. That would give us a very good represen -\\ntation of what’s contained in the document.\\nOther word embedding techniques\\nApart from the word embedding techniques we discussed here, there are a few no -\\ntable widely used word embedding techniques. We will discuss a few of those here. \\nFastText\\nFastText ( https://fasttext.cc/ ), introduced in the paper “Enriching Word \\nVectors with Subword Information” by Bojanowski et al. ( https://arxiv.org/\\npdf/1607.04606.pdf ), introduces a technique where word embeddings are com-\\nputed by considering the sub-components of a word. Specifically, they compute the \\nword embedding as a summation of embeddings of n-grams of the word for several \\nvalues of n. In the paper, they use 3 <= n <=6 . For example, for the word “banana,” the \\ntri-grams ( n=3) would be ['ban', 'ana', 'nan', 'ana'] . This leads to robust \\nembeddings that can withstand common problems of text, such as spelling mistakes. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52966ac6-d121-4247-81d0-54a63a88ccaf', embedding=None, metadata={'page_label': '134', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 134\\nWe have discussed all  the bells and whistles required to confidently use the ELMo model. Next \\nwe will classify documents using ELMo, in which ELMo will generate document embeddings as \\ninputs to a classification model.\\nDocument classification with ELMo\\nAlthough Word2vec gives a very elegant way of learning numerical representations of words, \\nlearning word representations alone is not convincing enough to realize the power of word vec-\\ntors in real-world applications. \\nWord embeddings are used as the feature representation of words for many tasks, such as image \\ncaption generation and machine translation. However, these tasks involve combining different \\nlearning models such as Convolutional Neural Networks ( CNNs) and Long Short-Term Memory  \\n(LSTM ) models or two LSTM models (the CNN and LSTM models will be discussed in more detail \\nin later chapters). To understand a real-world usage of word embeddings let’s stick to a simpler \\ntask—document classification.Swivel embeddings\\nSwivel embeddings, introduced by the paper “Swivel: Improving Embeddings by No -\\nticing What’s Missing” by Shazeer et al. ( https://arxiv.org/pdf/1602.02215.\\npdf ), tries to blend GloVe and skip-grams with negative sampling. One of the critical \\nlimitations of GloVe is that it only uses information about positive contexts. There -\\nfore, the method is not penalized for trying to create similar vectors of words that \\nhave not been observed together. But the negative sampling used in skip-grams \\ndirectly tackles this problem. The biggest innovation of Swivel is a loss function that \\nincorporates unobserved word pairs. As an added benefit, it can also be trained in \\na distributed environment. \\nTransformer models\\nTransformers are a type of model that has reimagined the way we think about NLP \\nproblems. The Transformer model was initially introduced in the paper “Attention is \\nall you need” by Vaswani ( https://arxiv.org/pdf/1706.03762.pdf ). This model \\nhas many different embeddings within it and, like ELMo, can generate an embedding \\nper token by processing a sequence of text. We will talk about Transformer models \\nin detail in later chapters.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e3bceaa-36d8-497f-ba14-add9fd340006', embedding=None, metadata={'page_label': '135', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 135\\nDocument classification is one of the most popular tasks in NLP. Document classification is \\nextremely useful for anyone who is handling massive collections of data such as those for news \\nwebsites, publishers, and universities. Therefore, it is interesting to see how learning word vec-\\ntors can be adapted to a real-world task such as document classification by means of embedding \\nentire documents instead of words.\\nThis exercise is available in the Ch04-Advance-Word-Vectors  folder ( ch4_document_\\nclassification.ipynb ).\\nDataset\\nFor this task, we will use an already-organized set of text files. These are news articles from the \\nBBC. Every document in this collection belongs to one of the following categories: Business , En-\\ntertainment , Politics , Sports , or Technology. \\nHere are a couple of brief snippets from the actual data:\\nBusiness\\nJapan narrowly escapes recession\\nJapan’s economy teetered on the brink of a technical recession in the three months to September, figures \\nshow.\\nRevised figures indicated growth of just 0.1% - and a similar-sized contraction in the previous quarter. \\nOn an annual basis, the data suggests annual growth of just 0.2%,...\\nFirst, we will download the data and load the data into memory. We will use the same  \\ndownload_data() function to download the data. Then we will slightly modify the read_data()  \\nfunction to not only return a list of articles, where each article is a string, but also to return a list \\nof filenames, where each filename corresponds to the file the article was stored in. The filenames \\nwill subsequently help us to create the labels for our classification model.\\ndef read_data (data_dir):\\n    \\n    # This will contain the full list of stories\\n    news_stories = []    \\n    filenames = []\\n    print(\"Reading files\" )\\n    \\n    i = 0 # Just used for printing progress', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e085a96-7926-4b27-83cf-629f45544a78', embedding=None, metadata={'page_label': '136', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 136\\n    for root, dirs, files in os.walk(data_dir):\\n        \\n        for fi, f in enumerate (files):\\n            \\n            # We don\\'t read the readme file\\n            if \\'README\\'  in f:\\n                continue\\n            \\n            # Printing progress\\n            i += 1\\n            print(\".\"*i, f, end= \\'\\\\r\\')\\n            \\n            # Open the file\\n            with open(os.path.join(root, f), encoding= \\'latin-1\\' ) as text_\\n            file:\\n                story = []\\n                # Read all the lines\\n                for row in text_file:\\n                    story.append(row.strip())\\n                    \\n                # Create a single string with all the rows in the doc\\n                story = \\' \\'.join(story)                        \\n                # Add that to the list\\n                news_stories.append(story)  \\n                filenames.append(os.path.join(root, f))\\n                \\n        print(\\'\\', end= \\'\\\\r\\')\\n        \\n    print(\"\\\\nDetected {} stories\" .format(len(news_stories)))\\n    return news_stories, filenames\\nnews_stories, filenames = read_data(os.path.join( \\'data\\', \\'bbc\\'))\\nWe will then create and fit a tokenizer on the data, as we have done before.\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nn_vocab = 15000 + 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='149790c6-7bc5-4beb-ab06-df419c36a2d2', embedding=None, metadata={'page_label': '137', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 137\\ntokenizer = Tokenizer(\\n    num_words=n_vocab - 1,\\n    filters= \\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\' ,\\n    lower= True, split= \\' \\', oov_token= \\'\\'\\n)\\ntokenizer.fit_on_texts(news_stories)\\nAs the next step, we will create labels. Since we are training a classification model, we need both \\ninputs and labels. Our inputs will be document embeddings (we will see how to compute them \\nsoon), and the targets will be a label ID between 0 and 4. Each class we mentioned above (e.g. \\nbusiness, tech, etc.) will be assigned to a separate category. Since the filename includes the cat -\\negory as a folder, we can leverage the filename to generate a label ID.\\nWe will use the pandas library to create the labels. First we will convert the list of filenames to a \\npandas Series object using:\\nlabels_ser = pd.Series(filenames, index=filenames)\\nAn example entry in this series could look like data/bbc/tech/127.txt . Next, we will split each \\nitem on the “ /” character, which will return a list [\\'data\\', \\'bbc\\', \\'tech\\', \\'127.txt\\'] . We \\nwill also set expand=True . expand=True  will transform our Series object to a DataFrame by turn -\\ning each item in the list of tokens into a separate column of a DataFrame. In other words, our \\npd.Series  object will become an [N, 4] -sized pd.DataFrame  with one token in each column, \\nwhere N is the number of files:\\nlabels_ser = labels_ser. str.split(os.path.sep, expand= True)\\nIn the resulting data, we only care about the third column, which has the category of a given article \\n(e.g. tech ). Therefore, we will discard the rest of the data and only keep that column:\\nlabels_ser = labels_ser.iloc[:, - 2]\\nFinally, we will map the string label to an integer ID using the pandas map()  function as follows:\\nlabels_ser = labels_ser. map({\\'business\\' : 0, \\'entertainment\\' : 1, \\n\\'politics\\' : 2, \\'sport\\' : 3, \\'tech\\' : 4})\\nThis will result in something like:\\ndata/bbc/tech/272.txt    4\\ndata/bbc/tech/127.txt    4\\ndata/bbc/tech/370.txt    4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ec33666-2d6c-4f49-ab6a-50294fe138b8', embedding=None, metadata={'page_label': '138', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 138\\ndata/bbc/tech/329.txt    4\\ndata/bbc/tech/240.txt    4\\nName: 2, dtype: int64\\nWhat we did here can be written as just one line by chaining the sequence of commands to a \\nsingle line:\\nlabels_ser = pd.Series(filenames, index=filenames). str.split(os.path.sep, \\nexpand=True).iloc[:, - 2].map(\\n    {'business' : 0, 'entertainment' : 1, 'politics' : 2, 'sport' : 3,\\n    'tech': 4}\\n)\\nWith that, we move on to the next important step, i.e. splitting the data into train/test subsets. \\nWhen training a supervised model, we generally need three datasets:\\n• A training set – This is the dataset the model will be trained on.\\n• A validation set – This will be used during the training to monitor model performance \\n(e.g. signs of overfitting).\\n• A testing set – This will be not exposed to the model at any time during the model training. \\nIt will only be used after the model training to evaluate the model on unseen data.\\nIn this exercise, we will only use the training set and the testing set. This will help us to keep \\nour conversation more focused on embeddings and keep the discussion about the downstream \\nclassification model simple. Here we will use 67% of the data as training data and use 33% of data \\nas testing data. Data will be split randomly:\\nfrom sklearn.model_selection import train_test_split\\ntrain_labels, test_labels = train_test_split(labels_ser, test_size= 0.33)\\nNow we have a training dataset to train the model and a test dataset to test it on unseen data. \\nWe will now see how we can generate document embeddings from token or word embeddings.\\nGenerating document embeddings\\nLet’s first remind ourselves how we stored embeddings for skip-gram, CBOW, and GloVe algo -\\nrithms. Figure 4.4 depicts how these look in a pd.DataFrame  object.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd606b78-5456-481f-b279-8e4370c6486d', embedding=None, metadata={'page_label': '139', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 139\\nFigure 4.4: A snapshot of the context embeddings of the skip-gram algorithm we saved to the \\ndisk. You can see below it says that it has 128 columns (i.e. the embedding size)\\nELMo embeddings are an exception to this. Since ELMo generates contextualized representations \\nfor all tokens in a sequence, we have stored the mean embedding vectors resulting from averaging \\nall the generated vectors:\\n \\nFigure 4.5: A snapshot of ELMo vectors. ELMo vectors have 1024 elements\\nTo compute the document embeddings from skip-gram, CBOW, and GloVe embeddings, let us \\nwrite the following function:\\ndef generate_document_embeddings (texts, filenames, tokenizer, embeddings):\\n    \\n    \"\"\" This function takes a sequence of tokens and compute the mean\\n    embedding vector from the word vectors of all the tokens in the\\n    document \"\"\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e1e677f-6455-4601-a3cf-a14d5528321c', embedding=None, metadata={'page_label': '140', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 140\\n    \\n    doc_embedding_df = []\\n    # Contains document embeddings for all the articles\\n    assert isinstance (embeddings, pd.DataFrame), 'embeddings must be a \\n    pd.DataFrame'\\n    \\n    # This is a trick we use to quickly get the text preprocessed by the\\n    # tokenizer\\n    # We first convert text to a sequences, and then back to text, which\\n    # will give the preprocessed tokens\\n    sequences = tokenizer.texts_to_sequences(texts)    \\n    preprocessed_texts = tokenizer.sequences_to_texts(sequences)\\n    \\n    # For each text,\\n    for text in preprocessed_texts:\\n        # Make sure we had matches for tokens in the embedding matrx\\n        assert embeddings.loc[text.split( ' '), :].shape[ 0]>0\\n        # Compute mean of all the embeddings associated with words\\n        mean_embedding = embeddings.loc[text.split( ' '), :].mean(axis= 0)\\n        # Add that to list\\n        doc_embedding_df.append(mean_embedding)\\n        \\n    # Save the doc embeddings in a dataframe\\n    doc_embedding_df = pd.DataFrame(doc_embedding_df, index=filenames)\\n    \\n    return doc_embedding_df\\nThe generate_document_embeddings() function takes the following arguments: \\n• texts – A list of strings, where each string represents an article\\n• filenames  – A list of filenames corresponding to the articles in texts\\n• tokenizer  – A tokenizer that can process texts\\n• embeddings  – The embeddings as a pd.DataFrame , where each row represents a word \\nvector, indexed by the corresponding token\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba7149cd-373b-44b0-bf41-b84c6c038d33', embedding=None, metadata={'page_label': '141', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 4 141\\nThe function first preprocesses the texts by converting the strings to sequences, and then back \\nto a list of strings. This helps us to use the built-in preprocessing functionalities of the  tokenizer \\nto clean the text. Next, each preprocessed string is split by the space character to return a list of \\ntokens. Then we index all the positions in the embeddings matrix that corresponds to all the to -\\nkens in the text. Finally, the mean vector is computed for the document by computing the mean \\nof all the chosen embedding vectors.\\nWith that, we can load the embeddings from different algorithms (skip-gram, CBOW, and GloVe), \\nand compute the document embeddings. Here we will only show the process for the skip-gram \\nalgorithm. But you can easily extend it to the other algorithms, as they have similar inputs and \\noutputs:\\n# Load the skip-gram embeddings context and target\\nskipgram_context_embeddings = pd.read_pickle(\\n    os.path.join( '../Ch03-Word-Vectors/skipgram_embeddings' ,\\n    'context_embedding.pkl' )\\n)\\nskipgram_target_embeddings = pd.read_pickle(\\n    os.path.join( '../Ch03-Word-Vectors/skipgram_embeddings' ,\\n    'target_embedding.pkl' )\\n)\\n# Compute the mean of context & target embeddings for better embeddings\\nskipgram_embeddings = (skipgram_context_embeddings + skipgram_target_\\nembeddings)/ 2\\n# Generate the document embeddings with the average context target\\n# embeddings\\nskipgram_doc_embeddings = generate_document_embeddings(news_stories, \\nfilenames, tokenizer, skipgram_embeddings)\\nNow we will see how we can leverage the generated document  embedding to train a classifier.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1e17ae4-8596-4af1-8e8c-7d7966c3f0d7', embedding=None, metadata={'page_label': '142', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 142\\nClassifying documents with document embeddings\\nWe will be training a simple multi-class (or a multinomial) logistic regression classifier on this \\ndata. The logistic regression model will look as follows:\\nFigure 4.6: This diagram depicts the multinomial logistic regression model. The model takes \\nin an embedding vector and outputs a probability distribution over different available classes\\nIt’s a very simple model with a single layer, where the input is the embedding vector (e.g. a 128- \\nelement-long vector), and the output is a 5-node softmax layer that will output the likelihood of \\nthe input belonging to each category, as a probability distribution.\\nWe will be training several models, as opposed to a single run. This will give us a more consistent \\nresult on the performance of the model. To implement the model, we’ll be using a popular gener -\\nal-purpose  machine learning library called scikit-learn ( https://scikit-learn.org/stable/ ). \\nIn each run, a multi-class logistic regression classifier is created with the sklearn.linear_model.\\nLogisticRegression  object. Additionally, in each run:\\n1. The model is trained on the training inputs and targets\\n2. The model predicts the class (a value from 0 to 4) for each test input, where the class of \\nan input is the one that has the maximum probability from all classes\\n3. The model computes the test accuracy using the predicted classes and true classes of the \\ntest set', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33aff8dc-2814-42d3-ac9a-b35aef2f2fab', embedding=None, metadata={'page_label': '143', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 143\\nThe code looks like the following:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\ndef get_classification_accuracy (doc_embeddings, train_labels, test_labels, \\nn_trials):\\n    \"\"\" Train a simple MLP model for several trials and measure test \\n    accuracy\"\"\"\\n    \\n    accuracies = [] # Store accuracies across trials\\n    \\n    # For each trial\\n    for trial in range (n_trials):\\n        # Create a MLP classifier\\n        lr_classifier = LogisticRegression(multi_class= \\'multinomial\\' , \\n        max_iter= 500)\\n        \\n        # Fit the model on training data\\n        lr_classifier.fit(doc_embeddings.loc[train_labels.index],\\n        train_labels)\\n        \\n        # Get the predictions for test data\\n        predictions = lr_classifier.predict(doc_embeddings.loc[test_\\n        labels.index])\\n    \\n        # Compute accuracy\\n        accuracies.append(accuracy_score(predictions, test_labels))\\n    \\n    return accuracies\\n# Get classification accuracy for skip-gram models\\nskipgram_accuracies = get_classification_accuracy(\\n    skipgram_doc_embeddings, train_labels, test_labels, n_trials= 5\\n)\\nprint(\"Skip-gram accuracies: {}\" .format(skipgram_accuracies))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a65bcc98-7411-46cf-8afb-99a9aeeb8de8', embedding=None, metadata={'page_label': '144', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Advanced Word Vector Algorithms 144\\nBy setting multi_class='multinomial' , we are making sure it’s a multi-class logistic regression \\nmodel (or a softmax classifier). This will output:\\nSkip-gram accuracies: [0.882…, 0.882…, 0.881…, 0.882…, 0.884…]\\nWhen you follow the procedure for all the skip-gram, CBOW, GloVe, and ELMo algorithms, you \\nwill see a result similar to the  following. This is a box plot diagram. However, as performance is \\nquite similar between trials, you won’t see much variation present in the diagram:\\nFigure 4.7: Box plot interpreting performance on document classification for different models. \\nWe can see that ELMo is a clear-cut winner, where GloVe performs the worst\\nWe can see that skip-gram achieves around 86% accuracy, followed closely by CBOW, which \\nachieves on-par performance. Surprisingly GloVe achieves performance far below the skip-gram \\nand CBOW, around 66% accuracy. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c78874d0-4956-4d2d-a27c-c13ad061af78', embedding=None, metadata={'page_label': '145', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4 145\\nThis could be pointing to a limitation of the GloVe loss function. Unlike, skip-gram and CBOW, \\nwhich are considered both positive (observed) and negative (unobserved) target and context \\npairs, GloVe only focuses on observed pairs. \\nThis  could be hurting GloVe’s ability to generate effective representations of words. Finally, ELMo \\nachieves the best, which is around 98% accuracy. But it is important to keep in mind that ELMo \\nhas been trained on a much larger dataset than the BBC dataset, thus it is not fair to compare \\nELMo with other models just on this number.\\nIn this section, you learned how we can extend word embeddings turned to document embeddings \\nand how these can be used in a downstream classifier model to classify documents. First, you \\nlearned about word embeddings using a selected algorithm (e.g. skip-gram, CBOW, and GloVe). \\nThen we created document embeddings by averaging the word embeddings of all the words \\nfound in that document. This was the case for the skip-gram, CBOW, and GloVe algorithms. In \\nthe case of the ELMo algorithm, we were able to infer document embeddings straight from the \\nmodel. Later we used these document embeddings to classify some BBC news articles that fall \\ninto these categories: entertainment, tech, politics, business, and sports. \\nSummary\\nIn this chapter, we discussed GloVe—another word embedding learning technique. GloVe takes \\nthe current Word2vec algorithms a step further by incorporating global statistics into the opti-\\nmization, thus increasing the performance. \\nNext, we learned about a much more advanced algorithm known as ELMo (which stands for \\nEmbeddings from Language Models). ELMo provides contextualized representations of words \\nby looking at a word within a sentence or a phrase, not by itself. \\nFinally, we discussed a real-world application of using word embeddings—document classifica -\\ntion. We showed that word embeddings are very powerful and allow us to classify related docu -\\nments with a simple multi-class logistic regression model reasonably well. ELMo performed the \\nbest out of skip-gram, CBOW, and GloVe, due to the vast amount of data it has been trained on.\\nIn the next chapter, we will move on to discussing a different family of deep networks that are \\nmore powerful in exploiting spatial information present in data, known as Convolutional Neural \\nNetworks  (CNNs). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='418b4aee-b4ca-4cf9-8cda-54a48af4da4e', embedding=None, metadata={'page_label': '146', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Word Vector Algorithms 146\\nPrecisely, we will see how CNNs can be used to exploit the spatial structure of sentences to classify \\nthem into different classes.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1effc58-911e-4a4a-8c52-117a91bca1cc', embedding=None, metadata={'page_label': '147', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5\\nSentence Classification with \\nConvolutional Neural Networks\\nIn this chapter, we will discuss a type of neural network known as Convolutional Neural  \\nNetworks  (CNNs). CNNs are quite different from fully connected neural networks and have \\nachieved state-of-the-art performance in numerous tasks. These tasks include image classifica -\\ntion, object detection, speech recognition, and of course, sentence classification. One of the main \\nadvantages of CNNs is that, compared to a fully connected layer, a convolution layer in a CNN has \\na much smaller number of parameters. This allows us to build deeper models without worrying \\nabout memory overflow. Also, deeper models usually lead to better performance.\\nWe will introduce you to what a CNN is in detail by discussing different components found in \\na CNN and what makes CNNs different from their fully connected counterparts. Then we will \\ndiscuss the various operations used in CNNs, such as the convolution and pooling operations, \\nand certain hyperparameters related to these operations, such as filter size, padding, and stride. \\nWe will also look at some of the mathematics behind the actual operations. After establishing \\na good understanding of CNNs, we will look at the practical side of implementing a CNN with \\nTensorFlow. First, we will implement a CNN to classify images and then use a CNN for sentence \\nclassification. Specifically, we’ll go through the following topics:\\n• Learning the fundamentals of CNNs\\n• Classifying images with CNNs\\n• Classifying sentences with CNNs', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a4407b0-31da-4c1e-b676-e5e6d17fc1ba', embedding=None, metadata={'page_label': '148', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 148\\nIntroducing CNNs\\nIn this section, you will learn about CNNs. Specifically, you will first get an understanding of the \\nsort of operations present in a CNN, such as convolution layers, pooling layers, and fully connect -\\ned layers. Next, we will briefly see how all of these are connected to form an end-to-end model. \\nIt is important to note that the first use case we’ll be solving with CNNs is an image classification \\ntask. CNNs were originally used to solve computer vision tasks and were adopted for NLP much \\nlater. Furthermore, CNNs have a stronger presence in the computer vision domain than the NLP \\ndomain, making it easier to explain the underlying concepts in a vision context. For this reason, \\nwe will first learn how CNNs are used in computer vision and then move on to NLP.\\nCNN fundamentals\\nNow, let’s explore the fundamental ideas behind a CNN without delving into too much technical \\ndetail. A CNN is a stack of layers, such as convolution layers, pooling layers, and fully connected \\nlayers. We will discuss each of these to understand their role in the CNN.\\nInitially, the input is connected to a set of convolution layers. These convolution layers slide a patch \\nof weights (sometimes called the convolution window or filter) over the input and produce an \\noutput by means of the convolution operation. Convolution layers use a small number of weights, \\norganized to cover only a small patch of input in each layer, unlike fully connected neural networks, \\nand these weights are shared across certain dimensions (for example, the width and height di -\\nmensions of an image). Also, CNNs use the convolution operations to share the weights from the \\noutput by sliding this small set of weights along the desired dimension. What we ultimately get \\nfrom this convolution operation is illustrated in Figure 5.1. If the pattern present in a convolution \\nfilter is present in a patch of image, the convolution will output a high value for that location; if not, \\nit will output a low value. Also, by convolving the full image, we get a matrix indicating whether a \\npattern was present or not in a given location. Finally, we will get a matrix as the convolution output:\\nFigure 5.1: What the convolution operation does to an image', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fea431ce-708d-4efd-bfd4-039f98807a5c', embedding=None, metadata={'page_label': '149', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 149\\nAlso, these convolution layers are optionally interleaved with pooling/subsampling layers, which \\nreduces the dimensionality of the input. While reducing the dimensionality, we make the  trans-\\nlation of CNNs invariant, as well as force the CNN to learn with less information, leading to better \\ngeneralization and regularization of the model. The dimensionality is reduced by dividing the \\ninput into several patches and transforming each patch into a single element. For example, such \\ntransformations include picking the maximum element of a patch or averaging all the values in \\na patch. We will illustrate how pooling can make the translation of CNNs invariant in Figure 5.2 :\\nFigure 5.2: How the pooling operation helps to make data translation invariant\\nHere, we have the original image and an image slightly translated on the y axis. We have con -\\nvolution output for both images, and you can see that the value 10 appears at slightly different \\nplaces in the convolution output. However, using max pooling (which takes the maximum value \\nof each thick square), we can get the same output at the end. We will discuss these operations \\nin detail later.\\nFinally, the output is fed to a set of fully connected layers, which then forward the output to the \\nfinal classification/regression layer (for example, sentence/image classification). Fully connected \\nlayers contain a significant fraction of the total number of weights of the CNN, as convolution \\nlayers have a small number of weights. However, it has been found that CNNs perform better with \\nfully connected layers than without them. This could be because convolution layers learn more \\nlocalized features due to their small size, whereas fully connected layers provide a global picture \\nof how these localized features should be connected together to produce a desirable final output. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='783a36c0-125c-4646-a8e1-9ab833725335', embedding=None, metadata={'page_label': '150', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 150\\nFigure 5.3 shows a typical CNN used to classify images:\\nFigure 5.3: A typical CNN architecture\\nAs is evident from the figure, CNNs, by design, preserve the spatial structure of the inputs during \\nlearning. In other words, for a two-dimensional input, a CNN will mostly have two-dimensional \\nlayers, whereas it will only have fully connected layers close to the output layer. Preserving the \\nspatial structure allows CNNs to exploit valuable spatial information of the inputs and learn \\nabout inputs with fewer parameters. The value of spatial information is illustrated in Figure 5.4:\\nFigure 5.4: Unwrapping an image into a one-dimensional vector loses some of the important \\nspatial information\\nAs you can see, when a two-dimensional image of a cat is unwrapped to be a one-dimensional \\nvector, ears are no longer close to the eyes, and the nose is far away from the eyes as well. This \\nmeans we have destroyed some of the useful spatial information during the unwrapping. This is \\nwhy preserving the two-dimensional nature of the inputs is so important.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='827430fd-570c-46ad-aedb-8678841af982', embedding=None, metadata={'page_label': '151', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 151\\nThe power of CNNs\\nCNNs are a very versatile family of models and have shown a remarkable performance in many \\ntypes of tasks. Such versatility is attributed to the ability of CNNs to perform feature extraction \\nand learning simultaneously, leading to greater efficiency and generalizability. Let’s discuss a \\nfew examples of the utility of CNNs.\\nIn the ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) 2020, which involved clas -\\nsifying images, detecting objects, and localizing objects in an image, CNNs were used to achieve \\nincredible test accuracies. For example, for image-classification tasks, its top-1 test accuracy was \\napproximately 90% for 1,000 different object classes, which means that the CNN was able to \\ncorrectly identify around 900 different objects correctly.\\nCNNs also have been used for image segmentation. Image segmentation involves segmenting an \\nimage into different areas. For example, in an urbanscape image that includes buildings, a road, \\nvehicles, and passengers, isolating the road from the buildings is a segmentation task. Moreover, \\nCNNs have made incredible strides, demonstrating their performance in NLP tasks such as sen-\\ntence classification, text generation, and machine translation.\\nUnderstanding CNNs\\nNow that we understand the high level concepts governing CNNs, let’s walk through the technical \\ndetails of a CNN. First, we will discuss the convolution operation and introduce some terminol-\\nogy, such as filter size, stride, and padding. In brief, filter size  refers to the  window size of the \\nconvolution operation, stride refers to the distance between two movements of the convolution \\nwindow, and padding refers to the way you handle the boundaries of the input. We will also \\ndiscuss an operation that is known as deconvolution or transposed convolution. Then we will \\ndiscuss the details of the pooling operation. Finally, we will discuss how to add fully connected \\nlayers, which produce the classification or regression output.\\nConvolution operation\\nIn this section, we will discuss the convolution operation in detail. First, we will discuss the \\nconvolution operation without stride and padding, then we will describe the convolution oper -\\nation with stride, and then we will discuss the convolution operation with padding. Finally, we \\nwill discuss something called transposed convolution. For all  the operations in this chapter, we \\nconsider the index starting from one, and not from zero.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4cc96485-fef2-442a-a706-167a072f8a42', embedding=None, metadata={'page_label': '152', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 152\\nStandard convolution operation\\nThe convolution operation is a central  part of CNNs. For an input of size 𝑛𝑛𝑛𝑛𝑛   and a weight patch \\n(also known as a filter or a kernel ) of 𝑚𝑚𝑚𝑚𝑚  , where 𝑛𝑛𝑛𝑛𝑛  , the convolution operation slides the \\npatch of weights over the input. Let’s denote the input by X, the patch of weights by W, and the \\noutput by H. Also, at each location i, j, the output is calculated as follows: \\nℎ𝑖𝑖𝑖𝑖𝑖=∑∑𝑤𝑤 𝑘𝑘𝑖𝑘𝑘𝑥𝑥𝑖𝑖𝑖𝑘𝑘𝑖𝑖𝑖𝑖𝑖𝑖𝑘𝑘𝑖𝑖𝑚𝑚\\n𝑘𝑘𝑙𝑖𝑚𝑚\\n𝑘𝑘𝑙𝑖𝑖𝑤𝑤ℎ𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑤 𝑤𝑤 𝑖𝑤𝑤 𝑤 𝑤𝑤 𝑤𝑤𝑤𝑤𝑤  \\nHere, x i,j, w i,j, and h i,j denote the value at the (i,j)th location of X , W, and H , respectively. As al -\\nready shown by the equation, though the input size is 𝑛𝑛𝑛𝑛𝑛  , the output in this case will be \\n𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛  . Also, m is known as the filter size. This means the width and height of \\nthe output will be slightly less than of the original. Let’s look at this through a visualization (see \\nFigure 5.5 ):\\nFigure 5.5: The convolution operation with a filter size (m) = 3, stride = 1, and no padding\\nNext let’s discuss the stride parameter in convolution.\\nConvolving with stride\\nIn the preceding example, we shifted the filter by a single step. However, this is not mandatory; \\nwe can take large steps or strides while convolving the input. Therefore, the size of the step is \\nknown as the stride. Note\\nThe output produced by the  convolution operation (the rectangle at the top in Figure \\n5.5) is sometimes called a features map .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bae2825a-1167-4ab8-b8b8-a075c458f185', embedding=None, metadata={'page_label': '153', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 153\\nLet’s modify the previous equation to include the s i and s j strides: \\nℎ𝑖𝑖𝑖𝑖𝑖=∑∑𝑤𝑤 𝑘𝑘𝑖𝑘𝑘𝑥𝑥(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑘𝑘𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑘𝑘𝑚𝑚\\n𝑘𝑘𝑙𝑖𝑚𝑚\\n𝑘𝑘𝑙𝑖 \\n𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑤 𝑤 𝑤𝑤 𝑤 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 [(𝑛𝑛𝑛𝑛𝑛 )𝑠𝑠𝑖𝑖⁄] +𝑤 𝑎𝑎𝑛𝑛𝑎𝑎  𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤[ (𝑛𝑛𝑛𝑛𝑛 )𝑠𝑠𝑗𝑗⁄+𝑤] \\nIn this case, the output will be smaller as the size of s i and s j increases. Comparing Figure 5.5 ( stride \\n= 1) and Figure 5.6 ( stride = 2 ) illustrates the effect of different strides:\\nFigure 5.6: The convolution operation with a filter size (m) = 2, stride = 2, and no padding\\nAs you can see, doing convolution with stride helps to reduce the dimensionality of the input \\nsimilar to a pooling layer. Therefore, sometimes convolution with stride is used instead of pool-\\ning in the CNNs as it reduces the computational complexity. Also note that the dimensionality \\nreduction achieved by stride can be tuned or controlled as opposed to the inherent dimensional -\\nity reduction from the standard convolution operation. We will now discuss another important \\nconcept in convolution known as padding.\\nConvolving with padding\\nThe inevitable output size reduction  resulting from each convolution (without stride) is an un-\\ndesirable property. This greatly limits the number of layers we can have in a network. Also, it is \\nknown that deeper networks perform better than shallow networks. This should not be confused \\nwith the dimensionality reduction achieved by stride, as this is a design choice and we can de -\\ncide to have a stride of 1 if necessary. Therefore, padding is used to circumvent this issue. This is \\nachieved by padding zeros to the boundary of the input so that the output size and the input size \\nare equal. Let’s assume a stride of 1: \\nℎ𝑖𝑖𝑖𝑖𝑖=∑∑𝑤𝑤 𝑘𝑘𝑖𝑘𝑘𝑥𝑥𝑖𝑖𝑖𝑘𝑘𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 𝑘𝑘𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖\\n𝑘𝑘𝑙𝑖𝑖𝑖\\n𝑘𝑘𝑙𝑖𝑖𝑤𝑤ℎ𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑤 𝑤𝑤 𝑖𝑤𝑤 𝑤 𝑤𝑤  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d91b26e7-b992-46cc-8756-321a2f0ebe02', embedding=None, metadata={'page_label': '154', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 154\\nHere: \\n𝑥𝑥𝑖𝑖𝑖𝑖𝑖= 0 𝑖𝑖𝑖𝑖 𝑖𝑖𝑖 𝑖𝑖 𝑖 𝑖  𝑜𝑜𝑜𝑜 𝑖𝑖𝑖 𝑖𝑖 𝑖 𝑖𝑖  \\nFigure 5.7 depicts the result of the padding:\\nFigure 5.7: The convolution operation with a filter size (m=3), stride (s=1), and zero padding\\nWe will now discuss the transposed convolution operation.\\nTransposed convolution\\nThough the convolution operation looks complicated in terms of mathematics, it can be simpli -\\nfied to a matrix multiplication. For this  reason, we can define the transpose of the convolution \\noperation or deconvolution , as it is sometimes called. However, we will use the term transposed \\nconvolution  as it sounds more natural. In addition, deconvolution refers to a different mathe -\\nmatical concept. The transposed convolution operation plays an important role in CNNs for the \\nreverse accumulation of the gradients during backpropagation. Let’s go through an example.\\nFor an input of size 𝑛𝑛𝑛𝑛𝑛   and a weight patch, or filter, of 𝑚𝑚𝑚𝑚𝑚  , where 𝑛𝑛𝑛𝑛𝑛  , the convolution op -\\neration slides the patch of weights over the input. Let’s denote the input by X , the patch of weights \\nby W, and the output by H. The output H can be calculated as a matrix multiplication as follows.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef5efecb-caf6-4325-b056-9e2e450d5aff', embedding=None, metadata={'page_label': '155', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 155\\nLet’s assume 𝑛𝑛𝑛𝑛   and 𝑚𝑚𝑚𝑚   for clarity and unwrap the input X from left to right, top to bottom, \\nresulting in this: \\n𝑥𝑥(16,1)=𝑥𝑥1,1,𝑥𝑥1,2,𝑥𝑥1,3,𝑥𝑥1,4,𝑥𝑥2,1,𝑥𝑥2,2,𝑥𝑥2,3,𝑥𝑥2,4,…,𝑥𝑥4,1,𝑥𝑥4,2,𝑥𝑥4,3,𝑥𝑥4,4 \\nLet’s define a new matrix A from W: \\n𝐴𝐴(4,16)=[𝑤𝑤1,1\\n0\\n0\\n0𝑤𝑤1,2\\n𝑤𝑤1,1\\n0\\n0𝑤𝑤1,3\\n𝑤𝑤1,2\\n0\\n00\\n𝑤𝑤1,3\\n0\\n0𝑤𝑤2,1\\n0𝑤𝑤1,1\\n0𝑤𝑤2,2\\n𝑤𝑤2,1\\n𝑤𝑤1,2\\n𝑤𝑤1,1𝑤𝑤2,3\\n𝑤𝑤2,2\\n𝑤𝑤1,3\\n𝑤𝑤1,20\\n𝑤𝑤2,3\\n0\\n𝑤𝑤1,3𝑤𝑤3,1\\n0𝑤𝑤2,1\\n0𝑤𝑤3,2\\n𝑤𝑤3,1\\n𝑤𝑤2,2\\n𝑤𝑤2,1𝑤𝑤3,3\\n𝑤𝑤3,2\\n𝑤𝑤2,3\\n𝑤𝑤2,20\\n𝑤𝑤3,3\\n0\\n𝑤𝑤2,30\\n0𝑤𝑤3,1\\n00\\n0𝑤𝑤3,2\\n𝑤𝑤3,10\\n0𝑤𝑤3,3\\n𝑤𝑤3,20\\n0\\n0\\n𝑤𝑤3,3] \\nThen, if we perform the following matrix multiplication, we obtain H: \\n𝐻𝐻(4,1)=𝐴𝐴(4,16)𝑋𝑋(16,1) \\nNow, by reshaping the output 𝐻𝐻(4,1)  to 𝐻𝐻(2,2)  we obtain the convolved output. Now let’s project \\nthis result back to n and m.\\nBy unwrapping the input 𝑋𝑋(𝑛𝑛𝑛𝑛𝑛𝑛  to 𝑋𝑋(𝑛𝑛2,1)  and by creating a matrix 𝐴𝐴((𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛2,𝑛𝑛2𝑛  from w, as we \\nshowed earlier, we obtain 𝐻𝐻((𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛2,𝑛𝑛 , which will then be reshaped to 𝐻𝐻(𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛 .\\nNext, to obtain the transposed convolution, we simply transpose A and arrive at the following: \\n𝑋𝑋̂(𝑛𝑛2,1)=(𝐴𝐴𝑇𝑇)(𝑛𝑛2,(𝑛𝑛𝑛𝑛𝑛𝑛1)2)𝐻𝐻((𝑛𝑛𝑛𝑛𝑛𝑛 1)2,1) \\nHere, 𝑋𝑋̂  is the resultant output of the transposed convolution.\\nWe end our discussion about the convolution operation here. We discussed the convolution \\noperation, convolution operation with stride, convolution operation with padding, and how to \\ncalculate the transposed convolution. Next, we will discuss the pooling operation in more detail.\\nPooling operation\\nThe pooling operation, which is sometimes known as the subsampling  operation, was introduced \\nto CNNs mainly for reducing the size of the intermediate outputs as well as for making the trans-\\nlation of CNNs invariant. This is preferred over the natural dimensionality reduction caused by \\nconvolution without padding, as we can decide where to reduce the size of the output with the \\npooling layer, in contrast to forcing it to happen every time. Forcing the dimensionality to de -\\ncrease without padding would strictly limit the number of layers we can have in our CNN models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9791ad08-b39b-43df-aced-4c11fcbbab77', embedding=None, metadata={'page_label': '156', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 156\\nWe define the pooling operation mathematically in the following sections. More precisely, we will \\ndiscuss two types of pooling: max pooling and average pooling. First, however, we will define the \\nnotation. For an input of size 𝑛𝑛𝑛𝑛𝑛   and a kernel (analogous to the filter of a convolution layer) of \\nsize 𝑚𝑚𝑚𝑚𝑚  , where 𝑛𝑛𝑛𝑛𝑛  , the convolution operation slides the patch of weights over the input. \\nLet’s denote the input by X, the patch of weights by W, and the output by H. Then let us use, x i,j, \\nwi,j, and h i,j to denote the value at the (i ,j)th location of X, W, and H, respectively. We will now look \\nat specific implementations of pooling commonly used.\\nMax pooling\\nThe max pooling operation picks the maximum element within the defined kernel of an input to \\nproduce the output. The max pooling operation shifts are windows over the input (the middle \\nsquares in Figure 5.8) and take the maximum at each time. Mathematically, we define the pooling \\nequation as follows: \\nℎ𝑖𝑖𝑖𝑖𝑖= 𝑚𝑚𝑚𝑚𝑚𝑚({𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 𝑖…𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑚𝑚𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖 }) \\n 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑤 𝑤 𝑤𝑤𝑤 𝑤𝑤 𝑤 𝑤𝑤 𝑤 𝑤𝑤 𝑤 𝑤 \\nFigure 5.8 shows this operation:\\nFigure 5.8: The max pooling operation with a filter size of 3, stride of 1, and no padding\\nNext, let’s discuss how to perform max pooling with stride.\\nMax pooling with stride\\nMax pooling with stride is similar to convolution with stride. Here is the equation: \\nℎ𝑖𝑖𝑖𝑖𝑖= 𝑚𝑚𝑚𝑚𝑚𝑚𝑚{𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑖𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑖𝑖𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑖𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+2𝑖…𝑖𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑖𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑚𝑚𝑖𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+2𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑖𝑖…𝑖\\n𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+2𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑚𝑚𝑖…𝑖𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑚𝑚𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑖𝑖…𝑖𝑚𝑚(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑖𝑖+𝑚𝑚𝑖(𝑖𝑖𝑖𝑖)×𝑠𝑠𝑗𝑗+𝑚𝑚}) \\n𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑤 𝑤 𝑤𝑤 𝑤 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 [(𝑛𝑛𝑛𝑛𝑛 )𝑠𝑠𝑖𝑖⁄] +𝑤 𝑎𝑎𝑛𝑛𝑎𝑎  𝑤 𝑤 𝑎𝑎𝑤 𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 [(𝑛𝑛𝑛𝑛𝑛 )𝑠𝑠𝑗𝑗⁄]+𝑤 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a38706e8-e68d-40c1-b042-ee0b599b407f', embedding=None, metadata={'page_label': '157', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 157\\nFigure 5.9  shows the result:\\nFigure 5.9: The max pooling operation for an input of size (n=4) with a filter size of (m=2), \\nstride (s=2), and no padding\\nWe will discuss another variant of pooling known as average pooling, below.\\nAverage pooling\\nAverage pooling works similar to max pooling, except that instead of only taking the maximum, \\nthe average of all the inputs falling within the kernel is taken. Consider the following equation: \\nℎ𝑖𝑖𝑖𝑖𝑖=𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖…𝑖𝑥𝑥𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖\\n𝑚𝑚𝑚𝑚𝑚∀𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑚𝑚 𝑖𝑖 \\nThe average pooling operation is shown in Figure 5.10:\\nFigure 5.10: The average pooling operation for an input of size (n=4) with a filter size of (m=2), \\nstride (s=1), and no padding', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad0186fd-28f6-4746-a44e-facbfc5d784c', embedding=None, metadata={'page_label': '158', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 158\\nWe have so far discussed the operations directly performed on the two-dimensional inputs like \\nimages. Next we will discuss how they are connected to one-dimensional fully connected layers.\\nFully connected layers\\nFully connected layers are a fully connected set of weights from the input to the output. These \\nfully connected weights are able to learn global information as they are connected from each \\ninput to each output. Also, having such layers of full connectedness allows us to combine fea -\\ntures learned by the convolution layers preceding the fully connected layers, globally, to produce \\nmeaningful outputs.\\nLet’s define the output of the last convolution or pooling layer to be of size 𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝  , where p is \\nthe height of the input, o is the width of the input, and d is the depth of the input. As an example, \\nthink of an RGB image, which will have a fixed height, fixed width, and a depth of 3 (one depth \\nchannel for each RGB component).\\nThen, for the initial fully connected layer found immediately after the last convolution or pool-\\ning layer, the weight matrix will be 𝑤𝑤(𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 , where height x width x depth  of the layer output \\nis the number of output units produced by that last layer and m is the number of hidden units \\nin the fully connected layer. Then, during inference (or prediction), we reshape the output of \\nthe last convolution/pooling layer to be of size (𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝  and perform the following matrix \\nmultiplication to obtain h: \\nℎ(𝑚𝑚𝑚𝑚𝑚=𝑤𝑤(𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑥𝑥(𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 \\nThe resultant fully connected layers will behave as in a fully connected neural network, where \\nyou have several fully connected layers and an output layer. The output layer can be a softmax \\nclassification layer for a classification problem or a linear layer for a regression problem.\\nPutting everything together\\nNow we will discuss how the convolutional, pooling, and fully connected layers come together \\nto form a complete CNN.\\nAs shown in Figure 5.11, the convolution, pooling, and fully connected layers come together to \\nform an end-to-end learning model that takes raw data, which can be high-dimensional (for \\nexample, RGB images) and produce meaningful output (for example, the class of the object). First, \\nthe convolution layers learn the spatial features of the images. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9bd84c5-9280-4ef7-9e30-8e144cb791e1', embedding=None, metadata={'page_label': '159', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 159\\nThe lower convolution layers learn low-level features such as differently oriented edges present \\nin the images, and the higher layers learn more high-level features such as shapes present in the \\nimages (for example, circles and triangles) or bigger parts of an object (for example, the face of a \\ndog, tail of a dog, and front section of a car). The pooling layers in the middle make each of these \\nlearned features slightly translation invariant. This means that, in a new image, even if the fea-\\nture appears a bit offset compared to the location in which the feature appeared in the learned \\nimages, the CNN will still recognize that feature. Finally, the fully connected layers combine the \\nhigh-level features learned by the CNN to produce global representations that will be used by \\nthe final output layer to determine the class the object belongs to:\\nFigure 5.11: Combining convolution layers, pooling layers, and fully connected layers to form \\na CNN\\nWith a strong conceptual understanding of a CNN, we will now get started on our first use case: \\nclassifying images with a CNN model.\\nExercise – image classification on Fashion-MNIST \\nwith CNN\\nThis will be our first example of using a CNN for a real-world machine learning task. We will \\nclassify images using a CNN. The reason for not starting with an NLP task is that applying CNNs \\nto NLP tasks (for example, sentence classification) is not very straightforward. There are several \\ntricks involved in using CNNs for such a task. However, originally, CNNs were designed to cope \\nwith image data. Therefore, let’s start there, and then find our way through to see how CNNs \\napply to NLP tasks in the Using CNNs for sentence classification  section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a1fbe2e-65d9-4205-9923-2cdfd048a2e1', embedding=None, metadata={'page_label': '160', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 160\\nAbout the data\\nIn this exercise, we will use a dataset well-known in the computer vision community: the Fash-\\nion-MNIST dataset. Fashion-MNIST was  inspired by the famous MNIST dataset ( http://yann.\\nlecun.com/exdb/mnist/ ). MNIST is a database of labeled images of handwritten digits from 0 \\nto 9 (i.e. 10 digits). However, due to the simplicity of the MNIST image classification task, test \\naccuracy on MNIST is just shy of 100%. At the time of writing, the popular research benchmarking \\nsite paperswithcode.com has published a test accuracy of 99.87% ( https://paperswithcode.com/\\nsota/image-classification-on-mnist ). Because of this, Fashion-MNIST came to life.\\nFashion-MNIST consists of images of clothing garments. Our task is to classify each garment into \\na category (e.g. dress, t-shirt). The dataset contains two sets: the training set, and the test set. \\nWe will train on the training set and evaluate the performance of our model on the unseen test \\ndataset. We will further split the training set into two sets: training and validation sets. We will \\nuse the validation dataset as a continuous performance monitoring mechanism for our model. \\nWe will discuss the details later, but we will see that we can reach up to approximately 88% test \\naccuracy without any special regularization or tricks.\\nDownloading and exploring the data\\nThe very first task will be to download and explore the data. To download the data, we will sim -\\nply tap into the tf.keras.datasets  module, as it provides several datasets to be downloaded \\nconveniently through TensorFlow. To see what other datasets are available, visit https://www.\\ntensorflow.org/api_docs/python/tf/keras/datasets . The full code for this chapter is available \\nin ch5_image_classification_fashion_mnist.ipynb  in the Ch05-Sentence-Classification  \\nfolder. Simply call the following function to download the data:\\n(train_images, train_labels), (test_images, test_labels) = tf.keras.\\ndatasets.fashion_mnist.load_data()\\nThe data will be downloaded to a default cache directory specified by TensorFlow (for example: \\n~/.keras/dataset/fasion_minst ).\\nWe will then see the sizes of the data by printing their shapes:\\nprint(\"train_images is of shape: {}\" .format(train_images.shape))\\nprint(\"train_labels is of shape: {}\" .format(train_labels.shape))\\nprint(\"test_images is of shape: {}\" .format(test_images.shape))\\nprint(\"test_labels is of shape: {}\" .format(test_labels.shape))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='928b8dc4-0a47-43f6-a5da-c4bd81b10ce6', embedding=None, metadata={'page_label': '161', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 161\\nThis will produce:\\ntrain_images is of shape: (60000, 28, 28)\\ntrain_labels is of shape: (60000,)\\ntest_images is of shape: (10000, 28, 28)\\ntest_labels is of shape: (10000,)\\nWe can see that we have 60,000 training images, each of size 28x28, and 10,000 testing images \\nof the same dimensions. The labels are simple class IDs ranging from 0 to 9. We will also create \\na variable to contain the class ID to class name mapping, which will help us during explorations \\nand post-training analysis:\\n# Available at: https://www.tensorflow.org/api_docs/python/tf/keras/\\n# datasets/fashion_mnist/load_data\\nlabel_map = {\\n    0: \"T-shirt/top\" , 1: \"Trouser\" , 2: \"Pullover\" , 3: \"Dress\" , 4: \"Coat\" ,\\n    5: \"Sandal\" , 6: \"Shirt\" , 7: \"Sneaker\" ,  8: \"Bag\" , 9: \"Ankle boot\"\\n}\\nWe can also plot the images, which will give the following plot of images ( Figure 5.12 ):\\nFigure 5.12: An overview of the images found in the Fashion-MNIST dataset\\nFinally, we are going to extend train_images  and test_images  by adding a new dimension \\n(of size 1) to the end of each tensor. Standard implementation of the convolution operation in  \\nTensorFlow is  designed to work on a four-dimensional input (i.e. batch, height, width, and chan-\\nnel dimensions). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a467c7f4-1bd3-4d57-971f-bd43e0d716a4', embedding=None, metadata={'page_label': '162', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 162\\nHere, the channel dimension is omitted in the images as they are black and white images. There -\\nfore, to comply with the dimensional requirement of TensorFlow’s convolution operation, we add \\nthis additional dimension to the images. This is a necessity for using the convolution operation \\nin CNNs. You can do this as follows: \\ntrain_images = train_images[:, : , :, None]\\ntest_images = test_images[:, : ,: , None]\\nUsing the indexing and slicing capabilities available in NumPy, you can simply add a None  dimen -\\nsion to the tensor when indexing as above. Let’s now check the shapes of the tensors:\\nprint(\"train_images is of shape: {}\" .format(train_images.shape))\\nprint(\"test_images is of shape: {}\" .format(test_images.shape))\\nThis gives:\\ntrain_images is of shape: ( 60000, 28, 28, 1)\\ntest_images is of shape: ( 10000, 28, 28, 1)\\nLet’s have a crack at  implementing a CNN model that can learn from this data.\\nImplementing the CNN\\nIn this subsection, we will look at some important code snippets from the TensorFlow implemen -\\ntation of the CNN. The full code is available in ch5_image_classification_mnist.ipynb  in the \\nCh05-Sentence-Classification  folder. First, we will define several important hyperparameters. \\nThe code comments are self-explanatory for the purpose of these hyperparameters:\\nbatch_size = 100 # This is the typical batch size we\\'ve been using\\nimage_size = 28 # This is the width/height of a single image\\n# Number of color channels in an image. These are black and white images \\nn_channels = 1 \\n# Number of different digits we have images for (i.e. classes)\\nn_classes = 10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef9c052d-5e51-4a8a-b233-c5f299c99649', embedding=None, metadata={'page_label': '163', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 163\\nWith that, we can start to implement the model. We will find inspiration from one of the earliest \\nCNN models, known as LeNet, introduced in the paper Gradient-Based Learning Applied to Document \\nRecognition  by LeCun et al. ( http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf ). This \\nmodel will be a great start as it is a simple model yet gives a reasonably good performance on the \\ndataset. We will introduce some slight modifications to the original model, because the original \\nmodel operated on a 32x32-sized image, whereas in our case, the image is a 28x28-sized image. \\nLet’s go through some quick details of the model. It has the following sequence of layers:\\n• A convolutional layer with a 5x5 kernel, 1x1 stride, and valid padding\\n• A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling\\n• A convolutional layer with a 5x5 kernel, 1x1 stride, and valid pooling\\n• A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling\\n• A convolutional layer with a 4x4 kernel, 1x1 stride, and valid pooling\\n• A layer that flattens the 2D output to a 1D vector\\n• A Dense layer with 84 nodes\\n• A final softmax prediction layer with 10 nodes\\nHere, all the layers except the last have ReLU (Rectified Linear Unit) activation. A convolutional lay -\\ner in a CNN model generalizes the convolution operation we discussed, to work on multi-channel \\ninputs and produce multi-channel outputs. Let’s understand what we meant by that. The original \\nconvolution operation we saw operated on a simple 2D plane with a height h and width w. Next, \\nthe kernel moves over the plane while producing a single value at each position. This process \\nproduces another 2D plane. But in practice, CNN models operate on four-dimensional inputs, i.e. \\nan input of size [batch size, height, width, in channels] , and produce an output that is a \\nfour-dimensional, i.e. an output of size [batch size, height, width, out channels] . To pro -\\nduce this output, the kernel would need to be a four-dimensional tensor having the dimensions \\n[kernel height, kernel width, in channels, out channels] . ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da4fa32a-d31f-45f8-962b-2917308bd974', embedding=None, metadata={'page_label': '164', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 164\\nIt might not be entirely clear why inputs, outputs, and kernels would be in this format. Figure \\n5.13 clarifies this.\\nFigure 5.13: How input and output shapes look for a two-dimensional convolution layer\\nBelow, we will outline the full model. Don’t worry if you don’t understand it at first glance. We \\nwill go through line by line to understand how the model comes to be:\\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\\nfrom tensorflow.keras.models import Sequential\\nimport tensorflow.keras.backend as K\\nK.clear_session()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49c4f424-7720-463d-9740-ff0541043662', embedding=None, metadata={'page_label': '165', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 165\\nlenet_like_model = Sequential([\\n    # 1st convolutional layer\\n    Conv2D(\\n        filters= 16, kernel_size=( 5,5), strides=( 1,1), padding= 'valid', \\n        activation= 'relu', \\n        input_shape=(image_size,image_size,n_channels)\\n    ), # in 28x28 / out 24x24\\n    # 1st max pooling layer\\n    MaxPool2D(pool_size=( 2,2), strides=( 2,2), padding= 'valid'), \\n    # in 24x24 / out 12x12\\n    # 2nd convolutional layer\\n    Conv2D(filters= 16, kernel_size=( 5,5), strides=( 1,1), \\n    padding= 'valid', activation= 'relu'), # in 12x12 / out 8x8\\n    # 2nd max pooling layer\\n    MaxPool2D(pool_size=( 2,2), strides=( 2,2), padding= 'valid'), \\n    # in 8x8 / out 4x4\\n    # 3rd convolutional layer\\n    Conv2D(filters= 120, kernel_size=( 4,4), strides=( 1,1), \\n    padding= 'valid', activation= 'relu'), # in 4x4 / out 1x1\\n    # flatten the output of the last layer to suit a fully connected layer\\n    Flatten(),\\n    # First dense (fully-connected) layer\\n    Dense( 84, activation= 'relu'),\\n    # Final prediction layer\\n    Dense(n_classes, activation= 'softmax' )\\n])\\nThe very first thing to notice is that we are using the Keras Sequential API. The CNN we are im -\\nplementing here has a series of layers connected one after the other. Therefore, we will use the \\nsimplest API possible. We then have our first convolutional layer. We have already discussed the \\nconvolution operation. Let’s take the first line:\\nConv2D(\\n        filters= 16, kernel_size=( 5,5), strides=( 1,1), padding= 'valid', \\n        activation= 'relu', \\n        input_shape=(image_size,image_size,n_channels)\\n    )\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9f7e895-182f-4baa-80bd-dfb785981e39', embedding=None, metadata={'page_label': '166', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Sentence Classification with Convolutional Neural Networks 166\\nThe tensorflow.keras.layers.Conv2D  layer takes the following argument values in that order:\\n• filters  (int): This is the number of output filters (i.e. the number of out channels).\\n• kernel_size  (Tuple[int] ): This is the (height, width) of the convolution kernel.\\n• strides  (Tuple[int] ): This denotes the stride on the height and width dimension of \\nthe input.\\n• padding  (str ): This denotes the type of padding (can be 'SAME'  or 'VALID' ).\\n• activation  (str ): The non-linear activation used.\\n• input_shape  (Tuple[int] ): The shape of the input. When defining input_shape , we do \\nnot specify the batch dimension as it’s automatically added.\\nNext, we have the first max-pooling layer, which looks as follows:\\nMaxPool2D(pool_size=( 2,2), strides=( 2,2), padding= 'valid')\\nThe arguments are quite similar to the ones in tf.keras.layers.Conv2D . The pool_size  argument \\ncorresponds to the kernel_size  argument that specifies the (height, width) of the pool window. \\nFollowing a similar pattern, the following convolutional and pooling layers are defined. The final \\nconvolution layer produces a [batch size, 1, 1, 120] -sized output. The height and width \\ndimensions are equal to 1, because LeNet is designed in a way that the last convolutional kernel \\nhas the same height and width as the output. Before this input is fed to a fully connected layer, \\nwe need to flatten this output, such that it has the shape [batch size, 120] . This is because \\na standard Dense layer takes a two-dimensional input. For that, we use the tf.keras.layers.\\nFlatten()  layer:\\nFlatten(),\\nFinally, we define two Dense layers as follows. \\nDense(84, activation= 'relu'),\\nDense(n_classes, activation= 'softmax' )\\nAs the final step, we will compile the model using the sparse categorical cross-entropy loss and \\nthe Adam optimizer. We will also track the accuracy on the data:\\nlenet_like_model. compile(loss='sparse_categorical_crossentropy' , \\noptimizer= 'adam', metrics=[ 'accuracy' ])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='553a8955-b941-46ca-b138-2d3d682eafaf', embedding=None, metadata={'page_label': '167', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 167\\nWith the data prepared and the model defined fully, we are good to train our model. Model training \\nis as simple as calling one function:\\nlenet_like_model.fit(train_images, train_labels, validation_split= 0.2, \\nbatch_size=batch_size, epochs= 5)\\nThe tf.keras.layers.Model.fit()  takes many arguments. But let’s only discuss the ones we \\nhave used here:\\n• x (np.ndarray  / tf.Tensor  / other): Takes in a tensor that will act as input to the model \\n(implemented as a NumPy array or a TensorFlow tensor). But the accepted values are not \\nlimited just to tensors. To see the full list, please refer to https://www.tensorflow.org/\\napi_docs/python/tf/keras/Model#fit .\\n• y (np.ndarray  / tf.Tensor ): Takes in a tensor that will act as the labels (targets) for the \\nmodel.\\n• validation_split  (float ): Setting this argument means a fraction of training data (e.g. \\n0.2 translates to 20%) will be used as validation data.\\n• epochs  (int ): The number of epochs to train the model for.\\nYou can evaluate the trained model on the test data by calling:\\nlenet_like_model.evaluate(test_images, test_labels)\\nOnce run, you’ll see an output as follows:\\n313/313 [==============================] - 1s 2ms/step - loss: 0.3368 - \\naccuracy: 0.8806\\nThe model should get up to around 88% accuracy when trained.\\nYou just finished learning about the functions that we used to create our first CNN. You learned \\nto use the functions to implement the CNN structure as well as define the loss, minimize the \\nloss, and get predictions for unseen data. We used a simple CNN to see if it could learn to classify \\nclothing items. Also, we were able to achieve an accuracy above 88% with a reasonably simple \\nCNN. Next, we will analyze some of the results produced by the CNN. We will see why the CNN \\ncouldn’t recognize some of the images correctly.\\nAnalyzing the predictions produced with a CNN\\nHere, we can randomly pick some correctly and incorrectly classified samples from the test set \\nto evaluate the learning power of CNNs (see Figure 5.14 ). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20a257dc-e9ed-416c-9d03-698859518bed', embedding=None, metadata={'page_label': '168', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 168\\nWe can see that for the correctly classified instances, the CNN is very confident about the out -\\nput, most of the time. This is a good sign that the model is making very confident and accurate \\ndecisions. However, when we evaluate the incorrectly classified examples, we can see that some \\nof them are in fact difficult, and even a human can get some of them wrong. For example, for an \\nankle boot that’s classified as a sandal, there is a large black patch that can indicate the presence \\nof straps, which makes it more likely to be a sandal (the third image from the right in the third \\nrow). Also, in the fifth image from the right in the third row, it’s difficult to say whether it’s a \\nshirt or a collared t-shirt:\\nFigure 5.14: Fashion-MNIST correctly classified and misclassified instances\\nUsing CNNs for sentence classification\\nThough CNNs have mostly been  used for computer vision tasks, nothing stops them from being \\nused in NLP applications. But as we highlighted earlier, CNNs were originally designed for visual \\ncontent. Therefore, using CNNs for NLP tasks requires somewhat more effort. This is why we \\nstarted out learning about CNNs with a simple computer vision problem. CNNs are an attractive \\nchoice for machine learning problems due to the low parameter count of convolution layers. One \\nsuch NLP application for which CNNs have been used effectively is sentence classification.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b64b8fb-b550-4a41-97ec-233c90119ef2', embedding=None, metadata={'page_label': '169', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 169\\nIn sentence classification, a given sentence should be classified with a class. We will use a question \\ndatabase, where each question is labeled by what the question is about. For example, the question \\n“Who was Abraham Lincoln?” will be a question and its label will be Person . For this we will use \\na sentence classification dataset available at http://cogcomp.org/Data/QA/QC/ ; here you will \\nfind several datasets. We are using the set with ~5,500 training questions and their respective \\nlabels and 500 testing sentences.\\nWe will use the CNN network introduced in a paper by Yoon Kim, Convolutional Neural Networks \\nfor Sentence Classification , to understand the value of CNNs for NLP tasks. However, using CNNs \\nfor sentence classification is somewhat different from the Fashion-MNIST example we discussed, \\nbecause operations (for example, convolution and pooling) now happen in one dimension (length) \\nrather than two dimensions (height and width). Furthermore, the pooling operations will also \\nhave a different flavor to the normal pooling operation, as we will see soon. You can find the code \\nfor this exercise in the ch5_cnn_sentence_classification.ipynb  file in the Ch5-Sentence-\\nClassification  folder. As the first step, we will understand the data.\\nHow data is transformed for sentence classification\\nLet’s assume a sentence of p words. First, we will pad the sentence with some special words (if \\nthe length of the sentence is < n) to set the sentence length to n words, where 𝑛𝑛𝑛𝑛𝑛  . Next, we \\nwill represent each word in the sentence by a vector of size k, where this vector can either be a \\none-hot-encoded representation, or Word2vec word vectors learned using skip-gram, CBOW, or \\nGloVe. Then a batch of sentences of size b can be represented by a 𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏   matrix.\\nLet’s walk through an example. Let’s consider the following three sentences:\\n• Bob and Mary are friends.\\n• Bob plays soccer.\\n• Mary likes to sing in the choir.\\nIn this example, the third sentence has the most words, so let’s set n = 7, which is the number of \\nwords in the  third sentence. Next, let’s look at the one-hot-encoded representation for each word. \\nIn this case, there are 13 distinct words. Therefore, we get this:\\nBob: 1,0,0,0,0,0,0,0,0,0,0,0,0\\nand : 0,1,0,0,0,0,0,0,0,0,0,0,0\\nMary: 0,0,1,0,0,0,0,0,0,0,0,0,0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6e0181b2-13aa-4226-a26e-e8363a529aff', embedding=None, metadata={'page_label': '170', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 170\\nAlso, k = 13 for the same reason. With this representation, we can represent the three sentences \\nas a three-dimensional matrix of size 3 x 7 x 13 , as shown in Figure 5.15 :\\nFigure 5.15: A batch of sentences represented as a sentence matrix\\nYou could also utilize word embeddings instead of one-hot encoding here. Representing each word \\nas a one-hot-encoded feature introduces sparsity and wastes computational memory. By using \\nembeddings, we are enabling the model to learn more compact and powerful word representations \\nthan one-hot-encoded representations. This also means that 𝑘𝑘  becomes a hyperparameter (i.e. \\nthe embedding size), as opposed to being driven by the size of the vocabulary. This means that, in \\nFigure 5.15 , each column will be a distributed continuous vector, not a combination of 0s and 1s.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7eb535f-45fc-4bf5-ac92-29f0cedaf00e', embedding=None, metadata={'page_label': '171', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 171\\nImplementation – downloading and preparing data\\nFirst we will download the  data from the web. The data download functions are provided in the \\nnotebook and are simply downloading two files: training and testing data (the paths to the files \\nare retained in train_filename  and test_filename ). \\nIf you open these files you will see that they contain a collection of lines of text. Each line has \\nthe format:\\n<Category>: <sub-category> <question>\\nThere are two pieces of meta information for each question: a category and a sub-category. A \\ncategory is a macro-level classification, where sub-category is a finer grain identification of the \\ntype of the question. There are six categories available: DESC  (description-related), ENTY  (enti -\\nty-related), HUM (human-related), ABBR  (abbreviation related), NUM (numerical), and LOC (location \\nrelated). Each category has several sub-categories associated with them. For example, the ENTY  \\ncategory is further broken down to animal, currency, events, food, etc. For our problem, we will \\nbe focusing on high-level classification (i.e. six classes), but you could also leverage the same \\nmodel with minimal changes to classify on the sub-category level.We know that one-hot vectors lead to high-dimensional and highly sparse repre -\\nsentations that are sub-optimal. On the other hand, word vectors give richer rep -\\nresentations of words. However, learning word vectors is computationally costly. \\nThere is another alternative called the hashing trick. The beauty of the hashing trick \\nis that it is extremely simple but gives a powerful and economical alternative that \\nsits between one-hot vectors and word vectors. The idea behind the hashing trick \\nis to use a hash function that converts a given token to an integer. \\nf(<token>)-->hash value\\nHere f is a chosen hash function. Some example popular hash functions are SHA \\n(https://brilliant.org/wiki/secure-hashing-algorithms/ ) and MD5 \\n(https://searchsecurity.techtarget.com/definition/MD5 ). There’s also \\nmore advanced hashing such as locality-sensitive hashing ( https://www.pinecone.\\nio/learn/locality-sensitive-hashing/ ) to give out similar IDs for morpholog -\\nically similar words. You can easily use the hashing trick via TensorFlow ( https://  \\nwww.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/\\nhashing_trick ).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6053d9a7-e85f-4b54-82d7-fe05d1359946', embedding=None, metadata={'page_label': '172', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 172\\nOnce the files are downloaded, we’ll read the data into the memory. For that, we will implement \\nthe read_data()  function:\\ndef read_data (filename):\\n    \\'\\'\\'\\n    Read data from a file with given filename\\n    Returns a list of strings where each string is a lower case word\\n    \\'\\'\\'\\n    # Holds question strings, categories and sub categories\\n    # category/sub_cateory definitions: https://cogcomp.seas.upenn.edu/\\n    # Data/QA/QC/definition.html\\n    questions, categories, sub_categories = [], [], []     \\n    \\n    with open(filename, \\'r\\',encoding= \\'latin-1\\' ) as f:        \\n        # Read each line\\n        for row in f:   \\n            # Each string has format <cat>:<sub cat> <question>\\n            # Split by : to separate cat and (sub_cat + question)\\n            row_str = row.split( \":\")        \\n            cat, sub_cat_and_question = row_str[ 0], row_str[ 1]\\n            tokens = sub_cat_and_question.split( \\' \\')\\n            # The first word in sub_cat_and_question is the sub \\n            # category rest is the question\\n            sub_cat, question = tokens[ 0], \\' \\'.join(tokens[ 1:])        \\n            \\n            questions.append(question.lower().strip())\\n            categories.append(cat)\\n            sub_categories.append(sub_cat)\\n            \\n    return questions, categories, sub_categories', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9b70d2e-564a-40da-b1c7-dc4397e7c812', embedding=None, metadata={'page_label': '173', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 173\\ntrain_questions, train_categories, train_sub_categories = read_data(train_\\nfilename)\\ntest_questions, test_categories, test_sub_categories = read_data(test_\\nfilename)\\nThis function simply goes through each line in the file and separates the question, category, and \\nsub-category, using the format of each line elucidated above. After that, each question, category, \\nand sub-category is written to the lists questions , categories , and sub_categories  respectively. \\nFinally, the function returns these lists. With the questions , categories , and sub_categories  \\navailable for both training and testing data, we will create pandas  DataFrames for training and \\ntesting data. \\npandas  DataFrames are an expressive data structure for storing multi-dimensional data. A Data-\\nFrame can have indices, columns, and values. Each value has a specific index and a column. It is \\nquite simple to create a DataFrame: \\n# Define training and testing\\ntrain_df = pd.DataFrame(\\n    {'question' : train_questions, 'category' : train_categories, \\n    'sub_category' : train_sub_categories}\\n)\\ntest_df = pd.DataFrame(\\n    {'question' : test_questions, 'category' : test_categories,\\n    'sub_category' : test_sub_categories}\\n)\\nWe call the pd.DataFrame  construct with a dictionary. The keys of the dictionary represent col-\\numns of the DataFrame, and the values represent the elements in each column. Here we create \\nthree columns: question , category , and sub_category . \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cbb84854-4e74-4e4c-960d-4b70a706b560', embedding=None, metadata={'page_label': '174', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 174\\nFigure 5.16  depicts what the train_df  looks like.\\nFigure 5.16: A sample of data captured in the pandas DataFrame\\nWe will do a simple shuffle of rows in the  training set, to make sure we are not introducing any \\nunintentional ordering in the data:\\n# Shuffle the data for better randomization\\ntrain_df = train_df.sample(frac= 1.0, random_state=seed)\\nThis process will sample 100% of the data from the DataFrame randomly. In other words, it will \\nshuffle the order of the rows. From this point onward, we will not consider the sub_category  \\ncolumn. We will first map each class label to a class ID:\\n# Generate the label to ID mapping\\nunique_cats = train_df[ \"category\" ].unique()\\nlabels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[ 0])))\\nprint(\"Label->ID mapping: {}\" .format(labels_map))\\nn_classes = len(labels_map)\\n# Convert all string labels to IDs\\ntrain_df[ \"category\" ] = train_df[ \"category\" ].map(labels_map)\\ntest_df[ \"category\" ] = test_df[ \"category\" ].map(labels_map)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57fa4b3b-b355-49f1-9c23-0e7d049f6fd8', embedding=None, metadata={'page_label': '175', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 175\\nWe first identify the unique values present in the train_df[\"category\"] . Then we will create a \\ndictionary by mapping from the unique values to a list of numerical IDs (0 to 5). The np.arange() \\nfunction can be used to generate a series of integers in a specified range (here, the range is from \\n0 to the length of unique_cats ). This process will give us the following labels_map .\\nLabel->ID mapping: {0: 0, 1: 1, 2: 2, 4: 3, 3: 4, 5: 5}\\nThen we simply apply this mapping to the category column of both the train and test DataFrames \\nto convert string labels to numerical labels. The data would look as follows, after the transfor -\\nmation ( Figure 5.17 ).\\nFigure 5.17: A sample of data in the DataFrame after mapping categories to integers\\nWe create a validation set, stemming from the original training set, to monitor model performance \\nwhile it trains. We will use the train_test_split()  function from the scikit-learn library. 10% \\nof the data will be separated as validation data, while 90% is kept as training data.\\nfrom sklearn.model_selection import train_test_split\\ntrain_df, valid_df = train_test_split(train_df, test_size= 0.1)\\nprint(\"Train size: {}\" .format(train_df.shape))\\nprint(\"Valid size: {}\" .format(valid_df.shape))\\nThis outputs:\\nTrain size: (4906, 3)\\nValid size: (546, 3)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cd385e7-7506-4895-9005-65fe6cf35404', embedding=None, metadata={'page_label': '176', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 176\\nWe can see that approximately 4,900 examples are used as training and the rest as validation. In \\nthe next section, we will build a tokenizer to tokenize the questions and assign individual tokens \\nnumerical IDs.\\nImplementation – building a tokenizer\\nMoving on, now it’s time to build a tokenizer that can map words to numerical IDs:\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\n# Define a tokenizer and fit on train data\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(train_df[ \"question\" ].tolist())\\nHere we simply create a Tokenizer  object and use the fit_on_texts()  function to train it on \\nthe training corpus. In this process, the tokenizer will map words in the vocabulary to IDs. We \\nwill convert all of the train, validation, and test inputs to sequences of word IDs. Simply call the \\ntokenizer.texts_to_sequences()  function with a list of strings, where each string represents \\na question:\\n# Convert each list of tokens to a list of IDs, using tokenizer\\'s mapping\\ntrain_sequences = tokenizer.texts_to_sequences(train_df[ \"question\" ].\\ntolist())\\nvalid_sequences = tokenizer.texts_to_sequences(valid_df[ \"question\" ].\\ntolist())\\ntest_sequences = tokenizer.texts_to_sequences(test_df[ \"question\" ].\\ntolist())\\nIt’s important to understand that we are feeding our model a batch of questions at a given time. \\nIt is very unlikely that all of the questions have the same number of tokens. If all questions do \\nnot have the same number of tokens, we cannot form a tensor due to the uneven lengths of dif-\\nferent questions. To solve this, we have to pad shorter sequences with special tokens and trun -\\ncate sequences longer than a specified length. To achieve this we can easily use the tf.keras.\\npreprocessing.sequence.pad_sequences()  function. It would be worthwhile going through \\nthe arguments accepted by this function:\\n• sequences (List[List[int]])  – List of list integers; each list of integers is a sequence\\n• maxlen (int)  – The maximum padding length\\n• padding (string) – Whether to pad at the beginning (pre)  or end (post)\\n• truncating (string)  – Whether to truncate at the beginning (pre)  or end (post)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99af5222-abfa-4e3e-935d-346967b06b21', embedding=None, metadata={'page_label': '177', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 177\\n• value (int)  – What value is to be used for padding (defaults to 0)\\nBelow we use this function to create sequence matrices for training, validation, and testing data:\\nmax_seq_length = 22\\n# Pad shorter sentences and truncate longer ones (maximum length: max_seq_\\n# length)\\npreprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_\\nsequences(\\n    train_sequences, maxlen=max_seq_length, padding= 'post',\\n    truncating= 'post'\\n)\\npreprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_\\nsequences(\\n    valid_sequences, maxlen=max_seq_length, padding= 'post', \\n    truncating= 'post'\\n)\\npreprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_\\nsequences(\\n    test_sequences, maxlen=max_seq_length, padding= 'post', \\n    truncating= 'post'\\n)\\nThe reason we picked 22 as the sequence length is through a simple analysis. The 99% percen -\\ntile of the sequence lengths of the training corpus is equal to 22. Therefore, we have picked that. \\nAnother important statistic is that the vocabulary size will be approximately 7,880 words. Now \\nwe will discuss the model.\\nThe sentence classification CNN model\\nNow we will discuss the technical details of the CNN used for sentence classification. First, we \\nwill discuss how data or sentences are transformed into a preferred format that can easily be dealt \\nwith by CNNs. Next, we will discuss how the convolution and pooling operations are adapted \\nfor sentence classification, and finally, we will discuss how all these components are connected.\\nThe convolution operation\\nIf we ignore the batch size, that is, if we assume that we are only processing a single sentence at a \\ntime, our data is a 𝑛𝑛𝑛𝑛𝑛   matrix, where n is the number of words per sentence after padding, and \\nk is the dimension of a single word vector. In our example, this would be 7 x 13 .\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f54849fa-c05e-4f0c-82fe-9e458165667c', embedding=None, metadata={'page_label': '178', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 178\\nNow we will define our convolution weight matrix to be of size 𝑚𝑚𝑚𝑚𝑚  , where m is the filter size for \\na one-dimensional convolution operation. By convolving the input x of size 𝑛𝑛𝑛𝑛𝑛   with a weight \\nmatrix W of size 𝑚𝑚𝑚𝑚𝑚  , we will produce an output of h of size 1×𝑛𝑛   as follows:\\nℎ𝑖𝑖𝑖𝑖=∑∑𝑤𝑤 𝑗𝑗𝑖𝑗𝑗𝑘𝑘\\n𝑗𝑗𝑙𝑖𝑚𝑚\\n𝑗𝑗𝑙𝑖𝑥𝑥𝑖𝑖𝑖𝑗𝑗𝑖𝑖𝑖𝑗𝑗 \\nHere, wi,j is the (i,j)th element of W and we will pad x with zeros so that h is of size 1×𝑛𝑛  . Also, we \\nwill define this operation more simply, as shown here:\\nℎ=𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝑊  \\nHere, * defines the convolution operation (with padding) and we will add an additional scalar \\nbias b. Figure 5.18 illustrates this operation:\\nFigure 5.18: A convolution operation for sentence classification. Convolution layers with dif -\\nferent kernel widths are used to convolve over the sentence (i.e. sequence of tokens)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99206723-99a4-4a9c-a60b-7f22525a1fe2', embedding=None, metadata={'page_label': '179', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 179\\nThen, to learn a rich set of features, we have parallel layers with different convolution filter siz -\\nes. Each convolution layer  outputs a hidden vector of size 1×𝑛𝑛  , and we will concatenate these \\noutputs to form the input to the next layer of size 𝑞𝑞𝑞𝑞𝑞  , where q is the number of parallel layers \\nwe will use. The larger q is, the better the performance of the model.\\nThe value of convolving can be understood in the following manner. Think about the movie rating \\nlearning problem (with two classes, positive or negative), and we have the following sentences:\\n• I like the movie, not too bad\\n• I did not like the movie, bad\\nNow imagine a convolution window of size 5. Let’s bin the words according to the movement of \\nthe convolution window.\\nThe sentence I like the movie, not too bad  gives:\\n[I, like, the, movie, ‘,’]\\n[like, the, movie, ‘,’, not]\\n[the, movie, ‘,’, not, too]\\n[movie, ‘,’, not, too, bad]\\nThe sentence I did not like the movie, bad  gives the following:\\n[I, did, not, like, the]\\n[did, not ,like, the, movie]\\n[not, like, the, movie, ‘,’]\\n[like, the, movie, ‘,’, bad]\\nFor the first sentence, windows such as the following convey that the rating is positive:\\n[I, like, the, movie, ‘,’]\\n[movie, ‘,’, not, too, bad]\\nHowever, for the second sentence, windows such as the following convey negativity in the rating:\\n[did, not, like, the, movie]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91454cad-c391-4cb1-90e8-7ca29e112624', embedding=None, metadata={'page_label': '180', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 180\\nWe are able to see such patterns that help to classify ratings thanks to the preserved spatiality. \\nFor example, if you use a technique such as bag-of-words  to calculate sentence representations \\nthat lose spatial information, the sentence representations of the above two sentences would \\nbe highly similar. The convolution operation plays an important role in preserving the spatial \\ninformation of the sentences.\\nHaving q different layers with different filter sizes, the network learns to extract the rating with \\ndifferent size phrases, leading to an improved performance.\\nPooling over time\\nThe pooling operation is designed to subsample the outputs produced by the previously discussed \\nparallel convolution layers. This is achieved as follows.\\nLet’s assume the output of the last layer h is of size 𝑞𝑞𝑞𝑞𝑞  . The pooling over time layer would \\nproduce an output h’  of size 𝑞𝑞𝑞𝑞   output. The precise calculation would be as follows:\\nℎ′𝑖𝑖𝑖𝑖={max\\u2061(ℎ(𝑖𝑖𝑖𝑖}𝑖𝑤𝑤ℎ𝑤𝑤𝑤𝑤𝑤𝑤\\u2061𝑤 𝑤 𝑤𝑤 𝑤 𝑤𝑤  \\nHere, ℎ(𝑖𝑖𝑖=𝑊𝑊(𝑖𝑖𝑖∗𝑥𝑥𝑥𝑥𝑥   and h(i) is the output produced by the ith convolution layer and 𝑊𝑊(𝑖𝑖𝑖  is \\nthe set of weights belonging to that layer. Simply put, the pooling over time operation creates a \\nvector by concatenating the maximum element of each convolution layer. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c10ab15-a0db-40c9-a79c-c212e884e760', embedding=None, metadata={'page_label': '181', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 181\\nWe will illustrate this operation in Figure 5.19 :\\nFigure 5.19: The pooling over time operation for sentence classification', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bcc4c233-d5a9-46f1-bf48-0fae8506d89c', embedding=None, metadata={'page_label': '182', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 182\\nBy combining these operations, we finally arrive at the architecture shown in Figure 5.20 :\\nFigure 5. 20: A sentence classification CNN architecture. The pool of convolution layers having \\ndifferent kernel widths produces a set of output sequences. They are fed into the Pooling Over \\nTime Layer that produces a compact representation of that input. This is finally connected to \\na classification layer with softmax activation\\nImplementation – sentence classification with CNNs\\nWe are off implementing the model in TensorFlow 2. As a prerequisite, let’s import several nec -\\nessary modules from TensorFlow:\\nimport tensorflow.keras.backend as K\\nimport tensorflow.keras.layers as layers\\nimport tensorflow.keras.regularizers as regularizers\\nfrom tensorflow.keras.models import Model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6594d4a7-70f7-43f4-921b-74017c7b1de9', embedding=None, metadata={'page_label': '183', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 5 183\\nClear the running session to make sure previous runs are not interfering with the current run: \\nK.clear_session()\\nBefore we start, we will be using the Functional API from Keras. The reason for this is that the \\nmodel we will be building here cannot be built with the Sequential API, due to intricate pathways \\npresent in the model. Let’s start off by creating an input layer:\\nInput layer takes word IDs as inputs\\nword_id_inputs = layers.Input(shape=(max_seq_length,), dtype= 'int32')\\nThe input layer simply takes a batch of max_seq_length  word IDs. That is, a batch of sequences, \\nwhere each sequence is padded/truncated to a max length. We specify the dtype  as int32 , since \\nthey are word IDs. Next, we define an embedding layer, from which we will look up embeddings \\ncorresponding to the word IDs coming through the word_id_inputs  layer: \\n# Get the embeddings of the inputs / out [batch_size, sent_length, \\n# output_dim]\\nembedding_out = layers.Embedding(input_dim=n_vocab, output_dim= 64)(word_\\nid_inputs)\\nThis is a randomly initialized embedding layer. It contains a large matrix of size [n_vocab, 64] , \\nwhere each row represents the word vector of the word indexed by that row number. The embed -\\ndings will be jointly learned with the model, while the model is trained on the supervised task. \\nFor the next part, we will define three different one-dimensional convolution layers with three \\ndifferent kernel (filter) sizes of 3, 4, and 5, having 100 feature maps each:\\n# For all layers: in [batch_size, sent_length, emb_size] / out [batch_\\n# size, sent_length, 100]\\nconv1_1 = layers.Conv1D(\\n    100, kernel_size= 3, strides= 1, padding= 'same', \\n    activation= 'relu'\\n)(embedding_out)\\nconv1_2 = layers.Conv1D(\\n    100, kernel_size= 4, strides= 1, padding= 'same', \\n    activation= 'relu'\\n)(embedding_out)\\nconv1_3 = layers.Conv1D(\\n    100, kernel_size= 5, strides= 1, padding= 'same', \\n    activation= 'relu'\\n)(embedding_out)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4eb2c087-77e3-4bd8-aa58-a9393339035a', embedding=None, metadata={'page_label': '184', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Sentence Classification with Convolutional Neural Networks 184\\nAn important distinction to make here is that we are using one-dimensional convolution as op -\\nposed to the two-dimensional convolution we used in the earlier exercise. However, most of the \\nconcepts remain the same. The main difference is that, unlike tf.keras.layers.Conv2D,  which \\nworks on four-dimensional inputs, tf.keras.layers.Conv1D  operates on three-dimensional \\ninputs (i.e. inputs with shape [batch size, width, in channels] ). In other words, the convo -\\nlution kernel moves only in one direction over the inputs. Each of these layers produces a [batch \\nsize, sentence length, 100] -sized output. Afterward, these outputs are concatenated on the \\nlast axis to produce a single tensor:\\n# in previous conv outputs / out [batch_size, sent_length, 300]\\nconv_out = layers.Concatenate(axis=- 1)([conv1_1, conv1_2, conv1_3])\\nSubsequently, the new tensor of size [batch size, sentence length, 300]  will be used to \\nperform the pooling over time operation. We can implement the pooling over time operation \\nby defining a one-dimensional max-pooling layer (i.e. tf.keras.layers.MaxPool1D ) with a \\nwindow as wide as the sequence length. This will produce a single value as the output, for each \\nfeature map in conv_out :\\n# Pooling over time operation. \\n# This is doing the max pooling over sequence length\\n# in other words, each feature map results in a single output\\n# in [batch_size, sent_length, 300] / out [batch_size, 1, 300]\\npool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, \\npadding= 'valid')(conv_out)\\nHere we get a [batch_size, 1, 300] -sized output after performing the operation. Next, we \\nwill convert this output to a [batch_size, 300] -sized output, by using the tf.keras.layers.\\nFlatten  layer. The Flatten layer simply collapses all the dimensions (except the batch dimension) \\nto a single dimension:\\n# Flatten the unit length dimension\\nflatten_out = layers.Flatten()(pool_over_time_out)\\nFinally, flatten_out is passed to a Dense layer that has n_classes  (i.e. six) nodes as the output \\nand has a softmax activation:\\n# Compute the final output\\nout = layers.Dense(\\n    n_classes, activation= 'softmax' ,\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f1e5f76-5175-41d2-8e24-e62cd70adcdb', embedding=None, metadata={'page_label': '185', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 185\\n    kernel_regularizer=regularizers.l2( 0.001)\\n)(flatten_out)\\nNote the use of the kernel_regularizer  argument. We can use this argument to add any special \\nregularization (e.g. L1 or L2 regularization) to a given layer. Finally, we define a model as,\\n# Define the model\\ncnn_model = Model(inputs=word_id_inputs, outputs=out)\\nand compile the model with the desired loss function, an  optimizer, and metrics:\\n# Compile the model with loss/optimzier/metrics\\ncnn_model. compile(\\n    loss= \\'sparse_categorical_crossentropy\\' , \\n    optimizer= \\'adam\\', \\n    metrics=[ \\'accuracy\\' ]\\n)\\nYou can view the model by running the following line:\\ncnn_model.summary()\\nwhich gives,\\nModel: \"model\"\\n______________________________________________________________________\\nLayer (type)            Output Shape         Param #     Connected to \\n======================================================================\\ninput_1 (InputLayer)    [(None, 22)]         0                        \\n______________________________________________________________________\\nembedding (Embedding)   (None, 22, 64)       504320      input_1[0][0]\\n______________________________________________________________________\\nconv1d (Conv1D)         (None, 22, 100)      19300     embedding[0][0]\\n______________________________________________________________________\\nconv1d_1 (Conv1D)       (None, 22, 100)      25700     embedding[0][0]\\n______________________________________________________________________\\nconv1d_2 (Conv1D)       (None, 22, 100)      32100     embedding[0][0]\\n______________________________________________________________________\\nconcatenate (Concatenate) (None, 22, 300)    0            conv1d[0][0]\\n                                                        conv1d_1[0][0]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fcd8bb8d-0267-4563-8f30-4dfc94499aa6', embedding=None, metadata={'page_label': '186', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Sentence Classification with Convolutional Neural Networks 186\\n                                                        conv1d_2[0][0]\\n______________________________________________________________________\\nmax_pooling1d (MaxPooling1D) (None, 1, 300)    0     concatenate[0][0]\\n______________________________________________________________________\\nflatten (Flatten)          (None, 300)         0   max_pooling1d[0][0]\\n______________________________________________________________________\\ndense (Dense)              (None, 6)           1806    flatten[0][0] \\n======================================================================\\nTotal params: 583,226\\nTrainable params: 583,226\\nNon-trainable params: 0\\n______________________________________________________________________\\nNext, we will train the model on  the data we already prepared.\\nTraining the model \\nSince we have done the hard yard at the beginning, by making sure the data is transformed, train -\\ning the model is simple. All we need to do is call the tf.keras.layers.Model.fit()  function. \\nHowever, let’s leverage a few techniques to improve model performance. This will be done by \\nleveraging a built-in callback of TensorFlow. The technique we’ll be using is known as “decaying \\nthe learning rate.” The idea is to reduce the learning rate (by some fraction) whenever the model \\nhas stopped to improve performance. The following callback assists us to do this:\\n# Call backs\\nlr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\\n    monitor= 'val_loss' , factor= 0.1, patience= 3, verbose= 1,\\n    mode= 'auto', min_delta= 0.0001, min_lr= 0.000001\\n)\\nThe parameters can be set as you wish, to control the learning rate reduction. Let’s understand \\nthe arguments above:\\n• monitor (str)  – Which metric to monitor in order to decay the learning rate. We will \\nmonitor the validation loss\\n• factor (float)  – By how much to reduce the learning rate. For example, a factor of 0.1 \\nmeans that the learning rate will be reduced by 10 times (e.g. 0.01 will be stepped down \\nto 0.001)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c59b5a4-9a3f-4d92-ae85-ac2666adf066', embedding=None, metadata={'page_label': '187', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 187\\n• patience (int)  – How many epochs to wait without an improvement, before reducing \\nthe learning rate\\n• mode (string)  – Whether to look for an increase or decrease of the metric; ‘auto’ means \\nthat the direction will be determined by looking at the metric name\\n• min_delta (float)  – How much of an increase/decrease to consider as an improvement\\n• min_lr (float)  – Minimum learning rate (floor)\\nLet’s train the model:\\n# Train the model\\ncnn_model.fit(\\n    preprocessed_train_sequences, train_labels, \\n    validation_data=(preprocessed_valid_sequences, valid_labels),\\n    batch_size= 128, \\n    epochs= 25,\\n    callbacks=[lr_reduce_callback]\\n)\\nWe will see the accuracy quickly going up and the validation accuracy plateauing around 88%. \\nHere’s a snippet of the output produced:\\nEpoch 1/50\\n39/39 [==============================] - 1s 9ms/step - loss: 1.7147 - \\naccuracy: 0.3063 - val_loss: 1.3912 - val_accuracy: 0.5696\\nEpoch 2/50\\n39/39 [==============================] - 0s 6ms/step - loss: 1.2268 - \\naccuracy: 0.6052 - val_loss: 0.7832 - val_accuracy: 0.7509\\n...\\nEpoch 00015: ReduceLROnPlateau reducing learning rate to \\n1.0000000656873453e-06.\\nEpoch 16/50\\n39/39 [==============================] - 0s 6ms/step - loss: 0.0487 - \\naccuracy: 0.9999 - val_loss: 0.3639 - val_accuracy: 0.8846\\nRestoring model weights from the end of the best epoch.\\nEpoch 00016: early stopping', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ecfd481-a1e9-4162-8d86-5e28e826dd1e', embedding=None, metadata={'page_label': '188', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sentence Classification with Convolutional Neural Networks 188\\nNext, let’s test the model on the testing dataset:\\ncnn_model.evaluate(preprocessed_test_sequences, test_labels, return_\\ndict=True)\\nEvaluating the test data as given in the exercise gives us a test accuracy of close to 88% (for 500 \\ntest sentences) in this sentence classification task.\\nHere we end our discussion about using CNNs for sentence classification. We first discussed how \\none-dimensional convolution operations combined with a special pooling operation called pooling \\nover time can be used to implement a sentence classifier based on the CNN architecture. Finally, \\nwe discussed how to use TensorFlow to implement such a CNN and saw that it in fact performs \\nwell in sentence classification.\\nIt can be useful to know how the problem we just solved can be useful in the real world. Assume \\nthat you have a large document about the history of Rome in your hand, and you want to find \\nout about Julius Caesar without reading the whole document. In this situation, the sentence \\nclassifier we just implemented can be used as a handy tool to summarize the sentences that only \\ncorrespond to a person, so you don’t have to read the whole document.\\nSentence classification can be used for many other tasks as well; one common use of this is clas -\\nsifying movie reviews as positive or negative, which is useful for automating the computation of \\nmovie ratings. Another important application of sentence classification can be seen in the medical \\ndomain, where it is used to extract clinically useful sentences from large documents containing \\nlarge amounts of text.\\nSummary\\nIn this chapter, we discussed CNNs and their various applications. First, we went through a \\ndetailed explanation of what CNNs are and their ability to excel at machine learning tasks. Next \\nwe decomposed the CNN into several components, such as convolution and pooling layers, and \\ndiscussed in detail how these operators work. Furthermore, we discussed several hyperparameters \\nthat are related to these operators such as filter size, stride, and padding.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25465779-d9c8-4d59-b496-84613538716d', embedding=None, metadata={'page_label': '189', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5 189\\nThen, to illustrate the functionality of CNNs, we walked through a simple example of classifying \\nimages of garments. We also did a bit of analysis to see why the CNN fails to recognize some \\nimages correctly.\\nFinally, we started talking about how CNNs are applied for NLP tasks. Concretely, we discussed \\nan altered architecture of CNNs that can be used to classify sentences. We then implemented this \\nparticular CNN architecture and tested it on an actual sentence classification task.\\nIn the next chapter, we will move on to one of the most popular types of neural networks used \\nfor many NLP tasks – Recurrent Neural Networks ( RNNs).\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15556847-0c50-41c5-9f87-77406d75c1dc', embedding=None, metadata={'page_label': '190', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80e9bc0a-503b-4a5c-b053-925afc2cdc60', embedding=None, metadata={'page_label': '191', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6\\nRecurrent Neural Networks\\nRecurrent Neural Networks (RNNs) are a special family of neural networks that are designed to \\ncope  with sequential data (that is, time-series data), such as stock market prices or a sequence \\nof texts (for example, variable-length sentences). RNNs maintain a state variable that captures \\nthe various patterns present in sequential data; therefore, they are able to model sequential data. \\nIn comparison, conventional feed-forward neural networks do not have this ability unless the \\ndata is represented with a feature representation that captures the important patterns present \\nin the sequence. However, coming up with such feature representations is extremely difficult. \\nAnother alternative for feed-forward models to model sequential data is to have a separate set \\nof parameters for each position in time/sequence so that the set of parameters assigned to a cer -\\ntain position learns about the patterns that occur at that position. This will greatly increase the \\nmemory requirement for your model.\\nHowever, as opposed to having a separate set of parameters for each position like feed-forward \\nnetworks, RNNs share the same set of parameters over time. Sharing parameters over time is an \\nimportant part of RNNs and in fact is one of the main enablers for learning temporal patterns. \\nThen the state variable is updated over time for each input we observe in the sequence. These \\nparameters shared over time, combined with the state vector, are able to predict the next value of \\na sequence, given the previously observed values of the sequence. Furthermore, since we process \\na single element of a sequence at a time (for example, one word in a document at a time), RNNs \\ncan process data of arbitrary lengths without padding data with special tokens.\\nIn this chapter, we will dive into the details of RNNs. First, we will discuss how an RNN can be \\nformed by starting with a simple feed-forward model. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fd49b365-8b0c-4ddd-aabe-60f30e03de96', embedding=None, metadata={'page_label': '192', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 192\\nAfter this we will discuss the basic functionality of an RNN. We will also delve into the under -\\nlying equations, such as output calculation and parameter update rules of RNNs, and discuss \\nseveral variants of applications of RNNs: one-to-one, one-to-many, and many-to-many RNNs. \\nWe will walk through an example of using RNNs to identify named entities (e.g. person names, \\norganization, etc.), which has valuable downstream use cases like building knowledge bases. \\nWe will discuss a more complex RNN model that can read text both forward and backward, and \\nuses convolutional layers to increase the model accuracy. This chapter will cover this through \\nthe following main topics:\\n• Understanding RNNs\\n• Backpropagation Through Time\\n• Applications of RNNs\\n• Named Entity Recognition (NER) with RNNs\\n• NER with character and token embeddings\\nUnderstanding RNNs\\nIn this section, we will discuss  what an RNN is by starting with a gentle introduction, and then \\nmove on to more in-depth technical details. We mentioned earlier that RNNs maintain a state \\nvariable that evolves over time as the RNN sees more data, thus giving it the power to model \\nsequential data. In particular, this state variable is updated over time by a set of recurrent connec-\\ntions. The existence of recurrent connections is the main structural difference between an RNN \\nand a feed-forward network. The recurrent connections can be understood as links between a \\nseries of memories that the RNN learned in the past, connecting to the current state variable of \\nthe RNN. In other words, the recurrent connections update the current state variable with respect \\nto the past memory the RNN has, enabling the RNN to make a prediction based on the current \\ninput as well as the previous inputs. \\nIn the upcoming section, we will discuss the following topics. First, we will discuss how we can \\nstart by representing a feed-forward network as a computational graph. The term RNN is sometimes used to refer to the family of recurrent models, which has \\nmany different models. In other words, it is sometimes used as a generalization of a \\nspecific RNN variant. Here, we are using the term RNN to refer to one of the earliest  \\nimplementations of an RNN model known as the Elman network.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9473865-cbaf-4855-9de6-cf9a204c9ef6', embedding=None, metadata={'page_label': '193', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 193\\nThen we will see through an example why a feed-forward network might fail at a sequential task. \\nThen we will adapt that feed-forward graph to model sequential data, which will give us the basic \\ncomputational graph of an RNN. We will also discuss the technical details (for example, update \\nrules) of an RNN. Finally, we will discuss the details of how we can train RNN models.\\nThe problem with feed-forward neural networks\\nTo understand the limits of feed-forward neural networks and how RNNs address them, let’s \\nimagine a sequence of data: \\n𝑥𝑥𝑥{𝑥𝑥1,𝑥𝑥2,…,𝑥𝑥𝑇𝑇},𝑦𝑦𝑥 {𝑦𝑦1,𝑦𝑦2,…,𝑦𝑦𝑇𝑇} \\nNext, let’s assume that, in the real world, x and y are linked in the following relationship:\\nℎ𝑡𝑡=𝑔𝑔1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1) \\n𝑦𝑦𝑡𝑡=𝑔𝑔2(ℎ𝑡𝑡) \\nHere, g1 and g2 are transformations (e.g. multiplying with a weight matrix followed by a non-linear \\ntransformation). This means that the current output y t depends on the current state h t, where h t is \\ncalculated with the current input x t and previous state h t-1. The state encodes information about \\nprevious inputs observed historically by the model.\\nNow, let’s imagine a simple feed-forward neural network, which we will represent with the \\nfollowing: \\n𝑦𝑦𝑡𝑡= 𝑓𝑓𝑓𝑓𝑓𝑡𝑡;𝜃𝜃𝜃 \\nHere, yt is the predicted output for some input xt.\\nIf we use a feed-forward neural network to solve this task, the network will have to produce \\n{𝑦𝑦1,𝑦𝑦2,…,𝑦𝑦𝑇𝑇}  one at a time, by taking {𝑥𝑥1,𝑥𝑥2,…,𝑥𝑥𝑇𝑇}  as inputs, one at a time. Now, let’s consider \\nthe problem we face in this solution for a time-series problem.\\nThe predicted output y t at time t of a feed-forward neural network depends only on the current \\ninput xt. In other words, it does not have any knowledge about the inputs that led to xt (that is, \\n{𝑥𝑥1,𝑥𝑥2,…,𝑥𝑥𝑡𝑡𝑡1} ). For this reason, a feed-forward neural network will fail at a task where the current \\noutput not only depends on the current input but also on the previous inputs. Let’s understand \\nthis through an example.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fad4f72c-6ab0-426b-81ff-8a53aec8cb4d', embedding=None, metadata={'page_label': '194', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 194\\nSay we need to train a neural network  to fill in missing words. We have the following phrase, and \\nwe would like to predict the next word:\\nJames has a cat and it likes to drink ____.\\nIf we are to process one word at a time and use a feed-forward neural network, we will only have \\nthe input drink and this is not enough at all to understand the phrase or even to understand \\nthe context (the word drink  can appear in many different contexts). One can argue that we can \\nachieve good results by processing the full sentence in a single go. Even though this is true, such \\nan approach has limitations such as processing very long sentences. However, there is a new \\nfamily of models known as Transformers that are processing the full sequences of data with \\nfully-connected layers, and have been surpassing the performance of sequential models. We will \\nhave a separate chapter on these models later.\\nModeling with RNNs\\nOn the other hand, we can use an RNN to find a solution to this problem. We will start with the \\ndata we have: \\n𝑥𝑥𝑥{𝑥𝑥1,𝑥𝑥2,…,𝑥𝑥𝑇𝑇},𝑦𝑦𝑥 {𝑦𝑦1,𝑦𝑦2,…,𝑦𝑦𝑇𝑇} \\nAssume that we have the following relationship:\\nℎ𝑡𝑡=𝑔𝑔1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1) \\n𝑦𝑦𝑡𝑡=𝑔𝑔2(ℎ𝑡𝑡) \\nNow, let’s replace g1 with a function approximator 𝑓𝑓1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1;𝜃𝜃𝜃   parametrized by 𝜃𝜃  that takes the \\ncurrent input x t and the previous state of the system h t-1 as the input and produces the current \\nstate ht. Then, we will replace g 2 with 𝑓𝑓2(ℎ𝑡𝑡;𝜑𝜑𝜑  , which takes the current state of the system ht to \\nproduce y t. This gives us the following:\\nℎ𝑡𝑡=𝑓𝑓1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1;𝜃𝜃𝜃 \\n𝑦𝑦𝑡𝑡=𝑓𝑓2(ℎ𝑡𝑡;𝜑𝜑𝜑 \\nWe can think of 𝑓𝑓1∘𝑓𝑓2  as an approximation of the true model that generates x and y. To understand \\nthis more clearly, let’s now expand the equation as follows: \\n𝑦𝑦𝑡𝑡=𝑓𝑓2(𝑓𝑓1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1;𝜃𝜃);𝜑𝜑) \\nFor example, we can represent y 4 as follows: ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d83f45e-5ef0-497f-bacd-bf8d738c923b', embedding=None, metadata={'page_label': '195', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 195\\n𝑦𝑦4=𝑓𝑓2(𝑓𝑓1(𝑥𝑥4,ℎ3;𝜃𝜃);𝜑𝜑) \\nAlso, by expansion we get the following (omitting 𝜃𝜃  and 𝜑𝜑  for clarity): \\n𝑦𝑦4=𝑓𝑓2(𝑓𝑓1(𝑥𝑥4,𝑓𝑓2(𝑓𝑓1(𝑥𝑥3,𝑓𝑓2(𝑓𝑓1(𝑥𝑥2,𝑓𝑓2(𝑓𝑓1(𝑥𝑥1,ℎ0))))))))  \\nThis can be illustrated in a graph, as shown in Figure 6.1:\\nFigure 6.1: The relationship between x t and y t expanded\\nWe can generally summarize the diagram, for any given time step t, as shown in Figure 6.2 :\\nFigure 6.2: A single-step calculation of an RNN structure\\nHowever, it should be understood  that ht-1 in fact is what ht was before receiving xt. In other words, \\nht-1 is ht before one time step. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79363397-7c38-4a1b-9ea1-6f470e892b22', embedding=None, metadata={'page_label': '196', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 196\\nTherefore, we can represent the calculation of ht with a recurrent connection, as shown in Figure \\n6.3:\\nFigure 6.3: A single-step calculation of an RNN with the recurrent connection\\nThe ability to summarize a chain of equations mapping {𝑥𝑥1,𝑥𝑥2,…,𝑥𝑥𝑇𝑇}  to {𝑦𝑦1,𝑦𝑦2,…,𝑦𝑦𝑇𝑇}  as in \\nFigure 6.3  allows us to write any yt in terms of xt, ht-1, and ht. This is the key idea behind an RNN.\\nTechnical description of an RNN\\nLet’s now have an even closer look at what makes an RNN and define the mathematical equations \\nfor the calculations taking place within an RNN. Let’s start with the two functions we derived as \\nfunction approximators for learning y t from xt:\\nℎ𝑡𝑡=𝑓𝑓1(𝑥𝑥𝑡𝑡,ℎ𝑡𝑡𝑡1;𝜃𝜃𝜃 \\n𝑦𝑦𝑡𝑡=𝑓𝑓2(ℎ𝑡𝑡;𝜑𝜑𝜑 \\nAs we have seen, a neural network  is composed of a set of weights and biases and some nonlinear \\nactivation function. Therefore, we can write the preceding relation as shown here: \\nℎ𝑡𝑡= tanh(𝑈𝑈𝑈𝑈 𝑡𝑡+𝑊𝑊ℎ𝑡𝑡𝑡𝑡) \\nHere, tanh is the tanh activation function, and U is a weight matrix of size 𝑚𝑚𝑚𝑚𝑚  , where m is the \\nnumber of hidden units and d is the dimensionality of the input. Also, W is a weight matrix of \\nsize 𝑚𝑚𝑚𝑚𝑚   that creates the recurrent link from ht-1 to ht. The y t relation is given by the following \\nequation: ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64b8b4ef-6157-4a75-8ed1-414bf1b61f8d', embedding=None, metadata={'page_label': '197', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 197\\n𝑦𝑦𝑡𝑡= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 𝑡𝑡) \\nHere, V is a weight matrix of size 𝑐𝑐𝑐𝑐𝑐   and c is the dimensionality of the output (this can be \\nthe number of output classes). In Figure 6.4, we illustrate how these weights form an RNN. The \\narrows represent the direction that the data flows in the network:\\nFigure 6.4: The structure of an RNN\\nSo far, we have seen how we can represent  an RNN with a graph of computational nodes, with \\nedges denoting computations. Also, we looked at the actual mathematics behind an RNN. Let’s \\nnow look at how to optimize (or train) the weights of an RNN to learn from sequential data.\\nBackpropagation Through Time\\nFor training RNNs, a special form  of backpropagation , known as Backpropagation Through \\nTime  (BPTT), is used. To understand BPTT, however, first we need to understand how BP works. \\nThen we will discuss why BP cannot be directly applied to RNNs, but how BP can be adapted for \\nRNNs, resulting in BPTT. Finally, we will discuss two major  problems present in BPTT.\\nHow backpropagation works\\nBackpropagation is the technique that is used to train a feed-forward neural network. In back -\\npropagation, you do the following:\\n• Calculate a prediction for a given input', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0e9cc18b-fdee-425b-955c-13a11f14d457', embedding=None, metadata={'page_label': '198', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 198\\n• Calculate an error, E, of the prediction by comparing it to the actual label of the input (for \\nexample, mean squared error and cross-entropy loss)\\n• Update the weights of the feed-forward network to minimize the loss calculated in step 2, \\nby taking a small step in the opposite direction of the gradient 𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝑖𝑖𝑖𝑖⁄   for all wij, where \\nwij is the jth weight of the ith layer\\nTo understand the above computations more clearly, consider the feed-forward network depicted \\nin Figure 6.5 . This has two single weights, w1 and w2, and calculates two outputs, h and y, as shown \\nin the following figure. We assume no nonlinearities in the model for simplicity:\\nFigure 6.5: Computations of a feed-forward network\\nWe can calculate  𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕1   using the chain rule as follows: \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕1=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕1 \\nThis simplifies to the following: \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕1=𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕2\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕2ℎ𝜕\\n𝜕𝜕ℎ𝜕𝜕𝜕𝜕𝜕1𝑥𝑥𝜕\\n𝜕𝜕𝜕𝜕1 \\nHere, l is the correct label for the data point x. Also, we are assuming the mean squared error as \\nthe loss function. Everything here is defined, and it is quite straightforward to calculate 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕1 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d61ceff-e3e7-4a92-ba15-5728abffad66', embedding=None, metadata={'page_label': '199', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 199\\nWhy we cannot use BP directly for RNNs\\nNow, let’s try the same for the RNN in Figure 6.6. Now we have an additional recurrent weight \\nw3. We have omitted the time components of inputs and outputs for the clarity of the problem \\nwe are trying to emphasize:\\nFigure 6.6: Computations of an RNN\\nLet’s see what happens if we apply the chain rule to calculate 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3 :\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3 \\nThis becomes the following: \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3=𝜕𝜕(𝑦𝑦𝑦𝑦𝑦)2\\n𝜕𝜕𝑦𝑦𝜕𝜕(𝜕𝜕2ℎ)\\n𝜕𝜕ℎ(𝜕𝜕(𝜕𝜕1𝑥𝑥)\\n𝜕𝜕𝜕𝜕3+𝜕𝜕(𝜕𝜕3ℎ)\\n𝜕𝜕𝜕𝜕3) \\nThe term  𝜕𝜕𝜕𝜕𝜕3ℎ)\\n𝜕𝜕𝜕𝜕3   here creates problems because it is a recursive term. You end up with an infinite \\nnumber of derivative terms, as h is recursive (that is, calculating h includes h itself) and h is not a \\nconstant and dependent on w3. This is solved by unrolling the input sequence x over time, creat -\\ning a copy of the RNN for each input x t and calculating derivatives for each copy separately, and \\ncollapsing those updates into one, by summing up the gradients, to calculate the weight update. \\nWe will discuss the details of this process next.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='487c085c-1fea-4acd-b4d6-3a104f5e4628', embedding=None, metadata={'page_label': '200', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 200\\nBackpropagation Through Time – training RNNs\\nThe trick to calculating backpropagation for RNNs is to consider not a single input, but the full \\ninput sequence. Then, if we calculate 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3  at time step 4, we will get the following: \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3=∑𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕4𝜕𝜕𝜕𝜕4\\n𝜕𝜕𝜕4𝜕𝜕𝜕4\\n𝜕𝜕𝜕𝑗𝑗𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕33\\n𝑗𝑗𝑗𝑗 \\nThis means that we need to calculate the sum of gradients for all the time steps up to the fourth \\ntime step. In other words, we will first unroll the sequence so that we can calculate 𝜕𝜕𝜕4\\n𝜕𝜕𝜕𝑗𝑗  and 𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕3  for \\neach time step j. This is done by creating four copies of the RNN. So, to calculate 𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝑗𝑗 , we need t-j+1 \\ncopies of the RNN. Then we will roll up the copies to a single RNN by summing up gradients with \\nrespect to all previous time steps to get the gradient, and update the RNN with the gradient 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3 .\\nHowever, this becomes costly as the number of time steps increases. For more computational \\nefficiency, we can use Truncated Backpropagation Through Time ( TBPTT ) to optimize recurrent \\nmodels, which is an approximation of BPTT.\\nTruncated BPTT – training RNNs efficiently\\nIn TBPTT, we only calculate the gradients for a fixed number of T time steps (in contrast to cal-\\nculating it up to the very beginning of the sequence as in BPTT). More specifically, when calcu-\\nlating  𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3 ,  for time step t, we only calculate derivatives down to t-T (that is, we do not compute \\nderivatives up to the very beginning): \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3=∑𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡\\n𝜕𝜕𝜕𝑗𝑗𝜕𝜕𝜕𝑗𝑗\\n𝜕𝜕𝜕𝜕3𝑡𝑡𝑡𝑡\\n𝑗𝑗𝑗𝑡𝑡𝑡𝑗𝑗 \\nThis is much more computationally efficient than standard BPTT. In standard BPTT, for each time \\nstep t, we calculate derivatives up to the very beginning of the sequence. But this gets compu-\\ntationally infeasible as the sequence length becomes larger and larger (for example, this could \\noccur when processing a long text document word by word). However, in truncated BPTT, we \\nonly calculate the derivatives for a fixed number of steps backward, and as you can imagine, the \\ncomputational cost does not change as the sequence becomes larger.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a259d53-f860-4ab0-9c7e-1712354945d3', embedding=None, metadata={'page_label': '201', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 201\\nLimitations of BPTT – vanishing and exploding gradients\\nHaving a way to calculate gradients  for recurrent weights and having a computationally efficient \\napproximation such as TBPTT does not enable us to train RNNs without trouble. Something else \\ncan go wrong with the calculations.\\nTo see why, let’s expand a single term in 𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕3 , which is as follows: \\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕4𝜕𝜕𝜕𝜕4\\n𝜕𝜕𝜕4𝜕𝜕𝜕4\\n𝜕𝜕𝜕1𝜕𝜕𝜕1\\n𝜕𝜕𝜕𝜕3=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕4𝜕𝜕𝜕𝜕4\\n𝜕𝜕𝜕4𝜕𝜕𝜕𝜕𝜕1𝑥𝑥𝑥𝜕𝜕3𝜕3)\\n𝜕𝜕𝜕1𝜕𝜕𝜕𝜕𝜕1𝑥𝑥𝑥𝜕𝜕3𝜕0)\\n𝜕𝜕𝜕𝜕3 \\nSince we know that the issues of backpropagation arise from the recurrent connections, let’s \\nignore the w 1x terms and consider the following:\\n𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕4𝜕𝜕𝜕𝜕4\\n𝜕𝜕𝜕4𝜕𝜕𝜕𝜕𝜕3𝜕3)\\n𝜕𝜕𝜕1𝜕𝜕𝜕𝜕𝜕3𝜕0)\\n𝜕𝜕𝜕𝜕3 \\nBy simply expanding h 3 and doing simple arithmetic operations we can show this: \\n=𝜕𝜕𝜕𝜕\\n𝜕𝜕𝜕𝜕4𝜕𝜕𝜕𝜕4\\n𝜕𝜕𝜕4𝜕0𝑤𝑤33 \\nWe see that for just four time steps we have a term 𝑤𝑤33 . So at the nth time step, it would become \\n𝑤𝑤3𝑛𝑛𝑛𝑛 . Say we initialized w 3 to be very small (say 0.00001) at n =100 time step; the gradient \\nwould be infinitesimally small (of scale 10-500). Also, since computers have limited precision in \\nrepresenting a number, this update would be ignored (that is, arithmetic underflow). This is \\ncalled the vanishing gradient . \\nSolving the vanishing gradient is not very straightforward. There are no easy ways of rescaling the \\ngradients so that they will properly propagate through time. A few techniques used in practice to \\nsolve the problem of vanishing gradients are to use careful initialization of weights (for example, \\nthe Xavier initialization), or to use momentum-based optimization methods (that is, in addition \\nto the current gradient update, we add an additional term, which is the accumulation of all the \\npast gradients known as the velocity term ). However, more principled approaches to solving the \\nvanishing gradient problem, such as different structural modifications to the standard RNN, have \\nbeen introduced, as we will see in Chapter 7, Understanding Long  Short-Term Memory Networks .\\nOn the other hand, say that we initialized w3 to be very large (say 1000.00). Then at the n=100  \\ntime step, the gradients would be massive (of scale 10300). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b101fbd7-05f8-4931-bc3e-5d75002d8358', embedding=None, metadata={'page_label': '202', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 202\\nThis leads to numerical instabilities and you will  get values such as Inf  or NaN  (that is, not a \\nnumber) in Python. This is called the exploding gradient .\\nGradient explosion can also take place due to the complexity of the loss surface of a problem. \\nComplex nonconvex loss surfaces are very common in deep neural networks due to both the di-\\nmensionality of inputs as well as the large number of parameters (weights) present in the models.\\nFigure 6.7 illustrates the loss surface of an RNN and highlights the presence of walls with very high \\ncurvature. If the optimization method comes in contact with such a wall, then the gradients will \\nexplode or overshoot, as shown by the solid line in the image. This can either lead to very poor \\nloss minimization, numerical instabilities, or both. A simple solution to avoid gradient explosion \\nin such situations is to clip the gradients to a reasonably small value when it is larger than some \\nthreshold. The dashed line in the figure shows what happens when we clip the gradient at some \\nsmall value. (Gradient clipping is covered in the paper On the difficulty of training recurrent neural \\nnetworks , Pascanu , Mikolov, and  Bengio , International Conference on Machine Learning (2013): 1310-\\n1318.)\\nFigure 6.7: The gradient explosion phenomenon. Source: This figure is from the paper ‘On the \\ndifficulty of training recurrent neural networks’ by Pascanu, Mikolov, and Bengio\\nHere we conclude our discussion about BPTT, which adapts backpropagation for RNNs. Next \\nwe will discuss various ways that RNNs can be used to solve applications. These applications \\ninclude sentence classification, image captioning, and machine translation. We will categorize \\nthe RNNs into several different categories such as one-to-one, one-to-many, many-to-one, and \\nmany-to-many.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e3ea487-9ab7-4dfc-9b86-c95183a6ed57', embedding=None, metadata={'page_label': '203', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 203\\nApplications of RNNs\\nSo far, we have only talked about one-to-one-mapped RNNs, where the current output depends \\non the current input as well as the previously observed history of inputs. This means that there \\nexists an output for the sequence of previously observed inputs and the current input. However, \\nin the real word, there can be situations  where there is only one output for a sequence of inputs, a \\nsequence of outputs for a single input, and a sequence of outputs for a sequence of inputs where \\nthe sequence sizes are different. In this section, we will look at several different settings of RNN \\nmodels and the applications they would be used in.\\nOne-to-one RNNs\\nIn one-to-one RNNs, the current input depends on the previously  observed inputs (see Figure \\n6.8). Such RNNs are appropriate for problems where each input has an output, but the output \\ndepends both on the current input and the history of inputs that led to the current input. An \\nexample of such a task is stock market prediction, where we output a value for the current input, \\nand this output also depends on how the previous inputs have behaved. Another example would \\nbe scene classification, where each pixel in an image is labeled (for example, labels such as car, \\nroad, and person). Sometimes xt+1 can be the same as yt for some problems. For example, in text \\ngeneration problems, the previously predicted word becomes an input to predict the next word. \\nThe following figure depicts a one-to-one RNN:\\nFigure 6.8: One-to-one RNNs having temporal dependencies\\nOne-to-many RNNs\\nA one-to-many RNN would take a single input  and output a sequence (see Figure 6.9 ). Here, we \\nassume the inputs to be independent of each other. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30e20f04-2143-427b-ade2-76651d57ca31', embedding=None, metadata={'page_label': '204', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 204\\nThat is, we do not need information about previous inputs to make a prediction about the current \\ninput. However, the recurrent connections are needed because, although we process a single in-\\nput, the output is a sequence of values that depends on the previous output values. An example \\ntask where such an RNN would be used is an image captioning task. For example, for a given \\ninput image, the text caption can consist of five or ten words. In other words, the RNN will keep \\npredicting words until it outputs a meaningful phrase describing the image. The following figure \\ndepicts a one-to-many RNN:\\nFigure 6.9: A one-to-many RNN\\nMany-to-one RNNs\\nMany-to-one RNNs take an input of arbitrary length and produce  a single output for the sequence \\nof inputs (see Figure 6.10). Sentence classification is one such task that can benefit from a many-\\nto-one RNN. A sentence is represented to the model as a sequence of words of arbitrary length. \\nThe model takes it as the input and produces an output, classifying the sentence into one of a set \\nof predefined classes. Some specific examples of sentence classification are as follows:\\n• Classifying movie reviews as positive or negative statements (that is, sentiment analysis)\\n• Classifying a sentence depending on what the sentence describes (for example, person, \\nobject, or location)\\nAnother application of many-to-one RNNs is classifying large-scale images by processing only a \\npatch of images at a time and moving the window over the whole image.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3bb83f65-fdbc-42a0-9bb7-19b18c64c65d', embedding=None, metadata={'page_label': '205', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 205\\nThe following figure depicts a many-to-one RNN:\\nFigure 6.10: A many-to-one RNN\\nMany-to-many RNNs\\nMany-to-many RNNs (or Sequences-to-Sequence, seq2seq for short) often produce arbi -\\ntrary-length  outputs from arbitrary-length inputs (see Figure 6.11). In other words, inputs and \\noutputs do not have to be of the same length. This is particularly useful in machine translation, \\nwhere we translate a sentence from one language to another. As you can imagine, one sentence \\nin a certain language does not always align with a sentence from another language. Another such \\nexample is chatbots, where the chatbot reads a sequence of words (that is, a user request) and out -\\nputs a sequence of words (that is, the answer). The following figure depicts a many-to-many RNN:\\nFigure 6.11: A many-to-many RNN', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb8ff795-8c68-4395-aee6-b8f3132b010b', embedding=None, metadata={'page_label': '206', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 206\\nWe can summarize the different types of applications of feed-forward networks and RNNs as \\nfollows:\\nAlgorithm Description Applications\\nOne-to-one \\nRNNsThese take a single input and give a single \\noutput. Current input depends on the \\npreviously observed input(s).Stock market prediction, \\nscene classification, and \\ntext generation\\nOne-to-many \\nRNNsThese take a single input and give an output \\nconsisting of an arbitrary number of elementsImage captioning\\nMany-to-one \\nRNNsThese take a sequence of inputs and give a \\nsingle output.Sentence classification \\n(considering a single word \\nas a single input)\\nMany-to-many \\nRNNsThese take a sequence of arbitrary length as \\ninputs and output a sequence of arbitrary \\nlength.Machine translation, \\nchatbots\\nNext, we will learn how to use RNNs to identify various entities mentioned in a text corpus.\\nNamed Entity Recognition with RNNs\\nNow let’s look at our first task: using an RNN to identify named entities in a text corpus. This \\ntask  is known as Named Entity Recognition  (NER). We will be using a modified version of the \\nwell-known CoNLL 2003 (which stands for Conference on Computational Natural Language \\nLearning - 2003) dataset for NER. \\nCoNLL 2003 is available for multiple languages, and the English data was generated from a Re -\\nuters Corpus that contains news stories published between August 1996 and August 1997. The \\ndatabase we’ll be using is found at https://github.com/ZihanWangKi/CrossWeigh  and is called \\nCoNLLPP . It is a more closely curated version  than the original CoNLL, which contains errors \\nin the dataset induced by incorrectly understanding the context of a word. For example,  in the \\nphrase “Chicago won …” Chicago was identified as a location, whereas it is in fact an organization. \\nThis exercise is available in ch06_rnns_for_named_entity_recognition.ipynb  in the Ch06-\\nRecurrent-Neural-Networks  folder.\\nUnderstanding the data\\nWe have defined a function called download_data() , which can be used to download the data. \\nWe will not go into the details of it as it simply downloads several files and places them in a data \\nfolder. Once the download finishes, you’ll have three files:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7c925fb-3fe2-4af4-a588-76ac6e87a07c', embedding=None, metadata={'page_label': '207', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 207\\n• data\\\\conllpp_train.txt  – Training set, contains 14041 sentences\\n• data\\\\conllpp_dev.txt  – Validation set, contains 3250 sentences\\n• data\\\\conllpp_test.txt  – Test set, contains 3452 sentences\\nNext up, we will read the data and convert it into a specific format that suits our model. But before \\nthat, we need to see what our data looks like originally:\\n-DOCSTART- -X- -X- O\\nEU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto TO B-VP O\\nboycott VB I-VP O\\nBritish JJ B-NP B-MISC\\nlamb NN I-NP O\\n. . O O\\nThe DT B-NP O\\nEuropean NNP I-NP B-ORG\\nCommission NNP I-NP I-ORG\\nsaid VBD B-VP O\\n...\\nto TO B-PP O\\nsheep NN B-NP O\\n. . O O\\nAs you can see, the document has a single word in each line along with the associated tags of that \\nword. These tags are in the following order:\\n1. The Part-of-speech (POS) tag (e.g. noun - NN,  verb - VB, determinant - DT, etc.)\\n2. Chunk tag – A chunk is a segment of text made of one or more tokens (for example, NP \\nrepresents a noun phrase such as “The European Commission”)\\n3. Named entity tag (e.g. Location, Organization, Person, etc.)\\nBoth chunk tags and named entity tags have a B- and I- prefix (e.g. B-ORG  or I-ORG ). These pre -\\nfixes are there to differentiate the starting token of an entity/chunk from the continuing token \\nof an entity/chunk. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6553347a-95e2-424f-bb1d-ab9e486ccfdb', embedding=None, metadata={'page_label': '208', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 208\\nThere are also five types of entities in the dataset:\\n• Location-based entities ( LOC )\\n• Person-based entities ( PER )\\n• Organization-based entities ( ORG )\\n• Miscellaneous entities ( MISC )\\n• Non-entities ( O) \\nFinally, there’s an empty line between separate sentences. \\nNow let’s look at the code that loads the data we downloaded into memory, so that we can start \\nusing it:\\ndef read_data (filename):\\n    \\'\\'\\'\\n    Read data from a file with given filename\\n    Returns a list of sentences (each sentence a string), \\n    and list of ner labels for each string\\n    \\'\\'\\'\\n    print(\"Reading data ...\" )\\n    # master lists - Holds sentences (list of tokens),\\n    # ner_labels (for each token an NER label)\\n    sentences, ner_labels = [], [] \\n    \\n    # Open the file\\n    with open(filename, \\'r\\',encoding= \\'latin-1\\' ) as f:        \\n        # Read each line\\n        is_sos = True \\n        # We record at each line if we are seeing the beginning of a \\n        # sentence\\n        \\n        # Tokens and labels of a single sentence, flushed when encountered\\n        # a new one\\n        sentence_tokens = []\\n        sentence_labels = []\\n        i = 0\\n        for row in f:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e524a89-6e9e-4302-9481-8eeb38fde952', embedding=None, metadata={'page_label': '209', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 209\\n        # If we are seeing an empty line or -DOCSTART- that's a new line\\n            if len(row.strip()) == 0 or row.split( ' ')[0] == '-\\n            DOCSTART-' :\\n                is_sos = False\\n            # Otherwise keep capturing tokens and labels\\n            else:\\n                is_sos = True\\n                token, _, _, ner_label = row.split( ' ')\\n                sentence_tokens.append(token)\\n                sentence_labels.append(ner_label.strip())\\n            \\n            # When we reach the end / or reach the beginning of next\\n            # add the data to the master lists, flush the temporary one\\n            if not is_sos and len(sentence_tokens)> 0:\\n                sentences.append( ' '.join(sentence_tokens))\\n                ner_labels.append(sentence_labels)\\n                sentence_tokens, sentence_labels = [], []\\n    \\n    print('\\\\tDone' )\\n    return sentences, ner_labels\\nHere, we will store all the  sentences (as a list of strings in sentences ) and all the labels associ -\\nated with each token in the sentences (as a list of lists in ner_labels ). We will read the file line \\nby line. We will maintain a Boolean called is_sos  that indicates whether we are at the start of a \\nsentence. We will also have two temporary lists ( sentence_tokens  and sentence_labels ) that \\nwill accumulate the tokens and the NER labels of the current sentence. When we are at the start of \\na sentence, we reset these temporary lists. Otherwise, we keep writing each token and NER label \\nwe see in the file to these temporary lists. We can now run this function on the train, validation, \\nand test corpora we have:\\n# Train data\\ntrain_sentences, train_labels = read_data(train_filepath) \\n# Validation data\\nvalid_sentences, valid_labels = read_data(dev_filepath) \\n# Test data\\ntest_sentences, test_labels = read_data(test_filepath)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bed082a-5a52-4c22-961e-a32edf50c51b', embedding=None, metadata={'page_label': '210', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 210\\nWe will print a few samples and see what we have with:\\n# Print some data\\nprint(\\'\\\\nSample data\\\\n\\' )\\nfor v_sent, v_labels in zip(valid_sentences[: 5], valid_labels[: 5]):\\n    print(\"Sentence: {}\" .format(v_sent))\\n    print(\"Labels: {}\" .format(v_labels))\\n    print(\\'\\\\n\\')\\nThis produces:\\nSentence: West Indian all-rounder Phil Simmons took four for 38 on Friday \\nas Leicestershire beat Somerset by an innings and 39 runs in two days to \\ntake over at the head of the county championship .\\nLabels: [\\'B-MISC\\', \\'I-MISC\\', \\'O\\', \\'B-PER\\', \\'I-PER\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'O\\', \\'O\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\']\\nSentence: Their stay on top , though , may be short-lived as title rivals \\nEssex , Derbyshire and Surrey all closed in on victory while Kent made up \\nfor lost time in their rain-affected match against Nottinghamshire .\\nLabels: [\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'O\\', \\'B-ORG\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-ORG\\', \\'O\\']\\nSentence: After bowling Somerset out for 83 on the opening morning at \\nGrace Road , Leicestershire extended their first innings by 94 runs before \\nbeing bowled out for 296 with England discard Andy Caddick taking three \\nfor 83 .\\nLabels: [\\'O\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'B-LOC\\', \\'I-LOC\\', \\'O\\', \\'B-ORG\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\n\\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'O\\', \\'B-LOC\\', \\'O\\', \\'B-PER\\', \\'I-PER\\', \\'O\\', \\'O\\', \\n\\'O\\', \\'O\\', \\'O\\']\\nOne of the unique characteristics of NER tasks is the class imbalance. That is, not all classes will \\nhave a roughly equal number of samples. As you can probably guess, in a corpus, there are more \\nnon-named entities than named entities. This leads to a significant class imbalance among labels. \\nTherefore, let’s have a look at the distribution of samples among different classes:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04fed023-3635-4c1b-8975-d869a08455d3', embedding=None, metadata={'page_label': '211', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 211\\nfrom itertools import chain\\n# Print the value count for each label\\nprint(\"Training data label counts\" )\\nprint(pd.Series(chain(*train_labels)).value_counts())\\nTo analyze the data, we will first convert the NER labels into a pandas Series  object. This can \\nbe done by simply calling the pd.Series()  construct on train_labels , valid_labels , and  \\ntest_labels . But remember that these were lists of lists, where each inner list represents the \\nNER tags for all the tokens in a sentence. To create a flat list, we can use the chain()  function \\nfrom the built-in Python library itertools . It will chain several lists together to form a single \\nlist. After that, we call the value_counts()  function on that pandas Series . This will return a \\nnew list, where the indices are unique labels found in the original Series  and the values are the \\ncounts of occurrences of each label. This gives us:\\nTraining data label counts\\nO         169578\\nB-LOC       7140\\nB-PER       6600\\nB-ORG       6321\\nI-PER       4528\\nI-ORG       3704\\nB-MISC      3438\\nI-LOC       1157\\nI-MISC      1155\\ndtype: int64\\nAs you can see, O labels are several magnitudes higher than the volume of other labels. We need \\nto keep this in mind  when training the model. Subsequently, we will analyze the sequence length \\n(i.e. number of tokens) of each sentence. We need this information later to pad our sentences to \\na fixed length.\\npd.Series(train_sentences). str.split(). str.len().\\ndescribe(percentiles=[ 0.05, 0.95])\\nHere, we create a pandas Series , where each item has the length of a sentence after splitting \\neach sentence into a list of tokens. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1467abc-01a0-49be-90b7-93866246a37f', embedding=None, metadata={'page_label': '212', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 212\\nThen we will look at the 5% and 95% percentiles of those lengths. This produces:\\ncount    14041.000000\\nmean        14.501887\\nstd         11.602756\\nmin          1.000000\\n5%           2.000000\\n50%         10.000000\\n95%         37.000000\\nmax        113.000000\\ndtype: float64\\nWe can see that 95% of our sentences have 37 tokens or less.\\nProcessing data\\nNow it’s time to process  the data. We will keep the sentences in the same format, i.e. a list of \\nstrings where each string represents a sentence. This is because we will integrate text processing \\nright into our model (as opposed to doing it externally). For labels, we have to do several changes. \\nRemember labels are a list of lists, where the inner lists represent labels for all the tokens in each \\nsentence. Specifically we will do the following:\\n• Convert the class labels to class IDs\\n• Pad the sequences of labels to a specified maximum length\\n• Generate a mask that indicates the padded labels, so that we can use this information to \\ndisregard the padded labels during model training\\nFirst let’s write a function to get a class label to class ID mapping. This function leverages pandas’ \\nunique()  function to get the unique labels in the training set and generate a mapping of integers \\nto unique labels found.\\ndef get_label_id_map (train_labels):\\n    # Get the unique list of labels\\n    unique_train_labels = pd.Series(chain(*train_labels)).unique()\\n    # Create a class label -> class ID mapping\\n    labels_map = dict(\\n        zip(unique_train_labels, \\n    np.arange(unique_train_labels.shape[ 0])))\\n    print(\"labels_map: {}\" .format(labels_map))\\n    return labels_map', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b8bb48e7-4d71-4ed5-8ca5-5d61ea48b350', embedding=None, metadata={'page_label': '213', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 213\\nIf you run this with:\\nlabels_map = get_label_id_map(train_labels)\\nThen you will get:\\nlabels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, \\n'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\\nWe write a function called get_padded_int_labels()  that will take sequences of class labels and \\nreturn sequences of padded class IDs, with the option to return a mask indicating padded labels. \\nThis function takes the following arguments:\\n• labels  (List[List[str]] ) – A list of lists of strings, where each string is a class label of \\nthe string type\\n• labels_map  (Dict[str, int] ) – A dictionary mapping a string label to a class ID of type \\ninteger\\n• max_seq_length  (int) – A maximum length to be padded to (longer sequences will be \\ntruncated at this length)\\n• return_mask  (bool ) – Whether to return the mask showing padded labels or not\\nLet’s now look at the code that performs the aforementioned operations:\\ndef get_padded_int_labels (labels, labels_map, max_seq_length,\\nreturn_mask= True):\\n    # Convert string labels to integers \\n    int_labels = [[labels_map[x] for x in one_seq] for one_seq in \\n    labels]\\n    \\n    # Pad sequences\\n    if return_mask:\\n        # If we return mask, we first pad with a special value (-1) and \\n        # use that to create the mask and later replace -1 with 'O'\\n        padded_labels = np.array(\\n            tf.keras.preprocessing.sequence.pad_sequences(\\n                int_labels, maxlen=max_seq_length, padding= 'post', \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7b64cfcc-b135-492a-a8d5-08d19f30792b', embedding=None, metadata={'page_label': '214', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 214\\n                truncating= 'post', value=- 1\\n            )\\n        )\\n        \\n        # mask filter\\n        mask_filter = (padded_labels != - 1)\\n        # replace -1 with 'O' s ID\\n        padded_labels[~mask_filter] = labels_map[ 'O']        \\n        return padded_labels, mask_filter.astype( 'int')\\n    \\n    else:\\n        padded_labels = np.array(ner_pad_sequence_func(int_labels, \\n        value=labels_map[ 'O']))\\n        return padded_labels\\nYou can see the first step in the function converts all the string labels in labels  to integer labels \\nusing the labels_map . Next we get the padded sequences with the tf.keras.preprocessing.\\nsequence.pad_sequences()  function. We discussed this function in detail in the previous chapter. \\nEssentially, it will pad (with a specified value) and truncate arbitrary-length sequences, to return \\nfixed-length sequences. We are instructing the function to do both padding and truncating at the \\nend of sequences, and to pad with a special value of -1. Then we can simply generate the mask as \\na boolean filter where padded_labels  is not equal to -1. Thus, the positions where original labels \\nexist will have a value of 1 and the rest will have 0. However, we have to convert the -1 values \\nto a class ID found in the labels_map . We will give them the class ID of the label O (i.e. others).\\nFrom our findings in the previous chapter, we will set the maximum sequence length to 40. Re -\\nmember that the 95% percentile fell at the length of 37 words:\\nmax_seq_length = 40\\nAnd now we will generate processed labels and masks for all of the training, validation, and \\ntesting data:\\n# Convert string labels to integers for all train/validation/test data\\n# Pad train/validation/test data\\npadded_train_labels, train_mask = get_padded_int_labels(\\n    train_labels, labels_map, max_seq_length, return_mask= True\\n)\\npadded_valid_labels, valid_mask = get_padded_int_labels(\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e1b8547-1512-4485-84fd-67191a576139', embedding=None, metadata={'page_label': '215', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 215\\n    valid_labels, labels_map, max_seq_length, return_mask= True\\n)\\npadded_test_labels, test_mask  = get_padded_int_labels(\\n    test_labels, labels_map, max_seq_length, return_mask= True\\n)\\nFinally, we will print the processed labels and masks of the first two sequences:\\n# Print some labels IDs\\nprint(padded_train_labels[: 2])\\nprint(train_mask[: 2])\\nWhich returns:\\n[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n  1 1 1 1]\\n [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n  1 1 1 1]]\\n[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n  0 0 0 0]\\n [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n  0 0 0 0]]\\nYou can see that the mask  is indicating the true labels and padded ones clearly. Next, we will \\ndefine some hyperparameters of the model.\\nDefining hyperparameters\\nNow let’s define  several hyperparameters needed for our RNN, as shown here:\\n• max_seq_length  – Denotes the maximum length for a sequence. We infer this from our \\ntraining data during data exploration. It is important to have a reasonable length for \\nsequences, as otherwise, memory can explode, due to the unrolling of the RNN.\\n• emedding_size  – The dimensionality of token embeddings. Since we have a small corpus, \\na value < 100 will suffice.\\n• rnn_hidden_size – The dimensionality of hidden layers in the RNN. Increasing dimen-\\nsionality of the hidden layer usually leads to better performance. However, note that \\nincreasing the size of the hidden layer causes all three sets of internal weights (that is, U, \\nW, and V) to increase as well, thus resulting in a high computational footprint.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e83efb08-e5b1-4d24-a9ec-be41bcc6c5d4', embedding=None, metadata={'page_label': '216', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 216\\n• n_classes  – Number of unique output classes present.\\n• batch_size  – The batch size for training data, validation data, and test data. A higher batch \\nsize often leads to better results as we are seeing more data during each optimization step, \\nbut just like unrolling, this causes a higher memory requirement.\\n• epochs – The number of epochs to train the model for.\\nThese are defined below:\\n# The maximum length of sequences\\nmax_seq_length = 40\\n# Size of token embeddings\\nembedding_size = 64\\n# Number of hidden units in the RNN layer\\nrnn_hidden_size = 64\\n# Number of output nodes in the last layer\\nn_classes = 9\\n# Number of samples in a batch\\nbatch_size = 64\\n# Number of epochs to train\\nepochs = 3\\nNow we will define the model.\\nDefining the model\\nWe will define the model here. Our model will have an embedding layer, followed by a simple \\nRNN layer, and finally a dense prediction layer. One thing to note in the work we have done so \\nfar is that, unlike in previous chapters, we haven’t yet defined a Tokenizer  object. Although the \\nTokenizer  has been an important part of our NLP pipeline to convert each token (or word) into \\nan ID, there’s a big downside to using an external tokenizer. After training the model, if you forget \\nto save the tokenizer along with the model, your machine learning model becomes useless: to \\ncombat this, during inference, you would need to map each word to the exact ID it was mapped \\nto during training. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ca468a7b-8999-4297-a280-f6c3021d485b', embedding=None, metadata={'page_label': '217', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 217\\nThis is a significant risk the tokenizer poses. In this chapter, we will seek an alternative, where we \\nwill integrate the tokenization mechanism right into our model, so that we don’t need to worry \\nabout it later. Figure 6.12  depicts the overall architecture of the model:\\nFigure 6.12: Overall architecture of the model. The text vectorization layer tokenizes the text \\nand converts it into word IDs. Next, each token is fed as an input at each timestep of the RNN. \\nFinally, the RNN predicts a label for each token at every time step\\nIntroduction to the TextVectorization layer\\nThe TextVectorization  layer  can be thought of as a modernized tokenizer that can be plugged \\ninto the model. Here, we will play around just with the TextVectorization  layer, without the over -\\nhead of the complexity from the rest of the model. First, we will import the TextVectorization  \\nlayer:\\nfrom tensorflow.keras.layers.experimental.preprocessing import \\nTextVectorization\\nNow we will define a simple text corpus: \\ntoy_corpus = [ \"I went to the market on Sunday\" , \"The Market was empty.\" ]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0d68ec6-8b16-4138-abdb-468f21e75854', embedding=None, metadata={'page_label': '218', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 218\\nWe can instantiate a text vectorization layer as follows:\\ntoy_vectorization_layer = TextVectorization()\\nAfter instantiating, you need to fit this layer on some data. This way, just like the tokenizer we \\nused previously, it can learn a word-to-numerical ID mapping. For this, we invoke the adapt()  \\nmethod of the layer, by passing the corpus of text as an input:\\n# Fit it on a corpus of data\\ntoy_vectorization_layer.adapt(toy_corpus)\\nWe can generate the tokenized output as follows:\\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus)\\nWhich will have:\\n[[ 9  4  6  2  3  8  7]\\n [ 2  3  5 10  0  0  0]]\\nWe can also see the vocabulary the layer has learned:\\nVocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', \\n'on', 'i', 'empty']\\nWe can see that the layer has done some pre-processing (e.g. turned words to lowercase and \\nremoved punctuation). Next let’s see how we can limit the size of the vocabulary. We can do this \\nwith the max_tokens  argument:\\ntoy_vectorization_layer = TextVectorization(max_tokens= 5)\\ntoy_vectorization_layer.adapt(toy_corpus)\\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus)\\nIf you convert the toy_corpus  to word IDs, you will see:\\n[[1 4 1 2 3 1 1]\\n [2 3 1 1 0 0 0]]\\nThe vocabulary will be as follows:\\nVocabulary: ['', '[UNK]', 'the', 'market', 'went']\\nWe can now see that there are only five elements in the vocabulary, just like we specified. Now if \\nyou need to skip the text pre-processing that happens within the layer, you can do so by setting \\nthe standardize  argument  to None  in the layer:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8c6a7dc0-bab4-4228-8bc1-b6fe30680fbf', embedding=None, metadata={'page_label': '219', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 219\\ntoy_vectorization_layer = TextVectorization(standardize= None)\\ntoy_vectorization_layer.adapt(toy_corpus)\\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus)\\nThis will produce:\\n[[12  2  4  5  7  6 10]\\n [ 9 11  3  8  0  0  0]]\\nThe vocabulary will look as follows:\\nVocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', \\n'empty.', 'The', 'Sunday', 'Market', 'I']\\nFinally, we can also control the padding/truncation of sequences with the output_sequence_\\nlength  command. For example, the following command will pad/truncate sequences at length 4:\\ntoy_vectorization_layer = TextVectorization(output_sequence_length= 4)\\ntoy_vectorization_layer.adapt(toy_corpus)\\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus)\\nThis will produce:\\n[[ 9  4  6  2]\\n [ 2  3  5 10]]\\nHere the vocabulary is: \\nVocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', \\n'on', 'i', 'empty']\\nNow you have a good understanding of the arguments and what they do in the TextVectorization  \\nlayer. Let’s now discuss the model.\\nDefining the rest of the model\\nFirst we will import the necessary modules:\\nimport tensorflow.keras.layers as layers\\nimport tensorflow.keras.backend as K\\nfrom tensorflow.keras.layers.experimental.preprocessing import \\nTextVectorization\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8241a5d-805d-46a2-a22a-5e1e160ca76b', embedding=None, metadata={'page_label': '220', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 220\\nWe will define an input layer  that has a single column (i.e. each sentence represented as a single \\nunit) and has dtype=tf.string :\\n# Input layer\\nword_input = tf.keras.layers.Input(shape=( 1,), dtype=tf.string)\\nNext, we will define a function that takes a corpus, a maximum sequence length, and a vocabulary \\nsize, and returns the trained TextVectorization  layer and the vocabulary size:\\ndef get_fitted_token_vectorization_layer (corpus, max_seq_length, \\nvocabulary_size= None):\\n    \"\"\" Fit a TextVectorization layer on given data \"\"\"\\n    \\n    # Define a text vectorization layer\\n    vectorization_layer = TextVectorization(\\n        max_tokens=vocabulary_size, standardize= None,        \\n        output_sequence_length=max_seq_length, \\n    )\\n    # Fit it on a corpus of data\\n    vectorization_layer.adapt(corpus)\\n    \\n    # Get the vocabulary size\\n    n_vocab = len(vectorization_layer.get_vocabulary())\\n    return vectorization_layer, n_vocab\\nThe function does what we have already described. However, pay attention to the various argu -\\nments we have set for the vectorization layer. We are passing the vocabulary size as max_tokens ; \\nwe are setting the standardize  to None . This is an important setting. When performing NER, \\nkeeping the case of characters is very important. Typically, an entity starts with an uppercase \\nletter (e.g. the name of a person or organization). Therefore, we should preserve the case in the text. \\nFinally, we also set the output_sequence_length  to the sequence length we found during the \\nanalysis. With that, we create the text vectorization layer as follows: \\n# Text vectorization layer\\nvectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_\\nsentences, max_seq_length)\\nThen pass the word_input  to the vectorize_layer  and get the output:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ae6447b-a5c2-4ea4-95e1-9eb1e3941013', embedding=None, metadata={'page_label': '221', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 221\\n# Vectorized output (each word mapped to an int ID)\\nvectorized_out = vectorize_layer(word_input)\\nThe output from the vectorize_layer  (i.e vectorized_out ) will be sent to an embedding layer. \\nThis embedding  layer is a randomly initialized embedding layer, which will have an output di-\\nmensionality of embedding_size :\\n# Look up embeddings for the returned IDs\\nembedding_layer = layers.Embedding(\\n    input_dim=n_vocab,\\noutput_dim=embedding_size,\\nmask_zero= True\\n)(vectorized_out)\\nUntil now, we dealt with feed-forward networks. Outputs of feed-forward networks did not have a \\ntime dimension. But if you look at the output from the TextVectorization  layer, it will be a [batch \\nsize, sequence length]  - sized output. When this output goes through an embedding layer, \\nthe output would be a [batch size, sequence length, embedding size] -shaped tensor. In \\nother words, there is an additional time dimension included in the output of the embedding layer. \\nAnother difference is the introduction of the mask_true  argument. Masking is used to mask un-\\ninformative words added to sequences (e.g. the padding token added to make sentences a fixed \\nlength), as they do not contribute to the final outcome. Masking is a commonly used technique \\nin sequence learning. To learn more about masking, please read the information box below.\\nIt would be cumbersome to manually perform masking when training a model. But in Tensor -\\nFlow, most layers support masking. For example, in the embedding layer, to ignore padded values \\n(which will be zeros), all you need to do is set mask_true=True . Masking in sequence learning\\nNaturally, text has arbitrary lengths. For example, sentences in a corpus would have \\na wide variety of token lengths. But deep networks process tensors with fixed dimen -\\nsions. To bring arbitrary-length sentences to constant length, we pad these sequences \\nwith some special value (e.g. 0). However, these padded values are synthetic, and \\nonly serve as a way to ensure the correct input shape. They should not contribute \\nto the final loss or evaluation metrics. To ignore them during loss calculation and \\nevaluation, “masking” is used. The idea is to multiply the loss resulting from padded \\ntimesteps with a zero, essentially cutting them off from the final loss.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e7b8643e-1ad5-4eb4-bab4-134ae5b1c1d5', embedding=None, metadata={'page_label': '222', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 222\\nWhen you enable masking  in a layer, it will propagate the mask to the downstream layers, flowing \\ndown until the loss computations. In other words, you only need to enable masking at the start of \\nthe model (as we have done at the embedding layer) and the rest is taken care of by TensorFlow. \\nFollowing this, we will define the core layer of our model, the RNN:\\n# Define a simple RNN layer, it returns an output at each position\\nrnn_layer = layers.SimpleRNN(\\n    units=rnn_hidden_size, return_sequences= True\\n)\\nrnn_out = rnn_layer(embedding_layer)\\nYou can implement a vanilla RNN by simply calling tf.keras.layers.SimpleRNN . Here we pass \\ntwo important arguments. There are other useful arguments besides the two discussed here, \\nhowever, they will be covered in later chapters with more complex variants of RNNs:\\n• units  (int) – This defines the hidden output size of the RNN model. The larger this is, the \\nmore representational power the model will have.\\n• return_sequences  (bool ) – Whether to return outputs from all the timesteps, or to return \\nonly the last output. For NER tasks, we need to label every single token. Therefore we need \\nto return outputs for all the time steps.\\nThe rnn_layer  takes a [batch size, sequence length, embedding size] -sized tensor and \\nreturns a [batch size, sequence length, rnn hidden size] -sized tensor. Finally, the \\ntime-distributed output from the RNN will go to a Dense layer with n_classes  output nodes \\nand a softmax  activation:\\ndense_layer = layers.Dense(n_classes, activation= 'softmax' )\\ndense_out = dense_layer(rnn_out)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ca9b8ef-5f4d-47f7-9d64-8953db245c00', embedding=None, metadata={'page_label': '223', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 223\\nFinally, we can define the final model as follows. It takes a batch of string sentences as the input, \\nand returns a batch of sequences of labels as the output:\\nmodel = tf.keras.Model(inputs=word_input, outputs=dense_out)\\nWe have now finished building the model. Next, we will discuss the loss function and the eval-\\nuation metrics.\\nEvaluation metrics and the loss function\\nDuring our previous discussion, we alluded to the fact that NER tasks carry a high class imbalance. \\nIt is quite normal for text to have more non-entity-related tokens than entity-related tokens. This \\nleads to large amounts of other ( O) type labels and fewer of the remaining types. We need to take \\nthis into consideration when training the model and evaluating the model. We will address the \\nclass imbalance in two ways:\\n• We will create a new evaluation metric that is resilient to class imbalance\\n• We will use sample weights to penalize more frequent classes and boost the importance \\nof rare classes\\nIn this section, we will only address the former. The latter will be addressed in the next section. \\nWe will define a modified version of the accuracy. This is called a macro-averaged accuracy. In \\nmacro averaging, we compute accuracies for each class separately, and then average it. Therefore, \\nthe class imbalance is ignored when computing the accuracy. When computing standard metrics \\nlike accuracy precision or recall, there are different types of averaging available. To learn more \\nabout these, read the information box below.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a6bbeea-de3e-41b0-9e9d-36567ee2db14', embedding=None, metadata={'page_label': '224', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 224\\nBelow we define the function to compute macro accuracy using a batch of true targets ( y_true ) \\nand predictions ( y_pred ). y_true  will have the shape [batch_size, sequence length]  and \\ny_pred  will have the shape [batch size, sequence length, n_classes] :Different types of metric averaging\\nThere are different types of averaging available for metrics. You can read one \\nsuch example of these averaging available in scikit-learn explained at https://\\nscikit-learn.org/stable/modules/generated/sklearn.metrics.average_  \\nprecision_score.html . Consider a simple binary classification example with the \\nfollowing confusion matrix results:\\nFigure 6.13: Example confusion matrix results\\n• micro – Computes a global metric, ignoring  the differences in class distri-\\nbution. e.g. 35/65 = ~54%\\n• macro – Computes the metric for each class separately and computes the \\nmean. e.g. (35/40 + 0/25)/2 = ~43.7%\\n• weighted – Computes the metric for each class separately and weighs it by \\nsupport (i.e. number of true labels for each class). e.g. (35/40)* 40 + (0/25) \\n* 25 / 65 = ~54%\\nHere you can see the micro and weighted return the same result. This is because the \\ndenominator of the accuracy computation is the same as the support. Therefore, they \\ncancel out in the weighted averaging. However for other metrics such as precision \\nand recall you will get different values.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='413ba34d-7cac-45df-9765-e132e118234a', embedding=None, metadata={'page_label': '225', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 225\\ndef macro_accuracy (y_true, y_pred):\\n    \\n    # [batch size, time] => [batch size * time]\\n    y_true = tf.cast(tf.reshape(y_true, [- 1]), 'int32')\\n    # [batch size, sequence length, n_classes] => [batch size * time]\\n    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=- 1), [-1]), \\n    'int32')\\n    \\n    sorted_y_true = tf.sort(y_true)\\n    sorted_inds = tf.argsort(y_true)\\n    \\n    sorted_y_pred = tf.gather(y_pred, sorted_inds)\\n    \\n    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, \\n    sorted_y_pred), 'int32')\\n    \\n    # We are adding one to make sure there are no division by zero\\n    correct_for_each_label = \\n    tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), \\n    'float32' ) + 1\\n    all_for_each_label = \\n    tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), \\n    sorted_y_true), 'float32' ) + 1\\n    \\n    mean_accuracy = \\n    tf.reduce_mean(correct_for_each_label/all_for_each_label)\\n    \\n    return mean_accuracy\\nIt is important to note that we have to write this function using TensorFlow operations, so that they \\nare executed as a graph. Even though TensorFlow 2 has migrated toward more imperative style \\nexecution operations, there still are remnants of the declarative style introduced by TensorFlow 1.\\nFirst we flatten y_true  so that it’s a vector. Next we get the predicted label from y_pred  using the \\ntf.argmax()  function and flatten the predicted labels to a vector. The two flattened structures \\nwill have the same number of elements. Then we sort y_true , so that same-labeled elements \\nare close together. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13a93b85-9c67-430b-bc6e-4b14c92adc77', embedding=None, metadata={'page_label': '226', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 226\\nWe take the indices of the original data after sorting and then use the tf.gather()  function to \\norder y_pred  in the same order as y_true . In other words, sorted_y_true  and sorted_y_pred  still \\nhave the same correspondence with each other. The tf.gather()  function takes a tensor and a \\nset of indices and orders the passed tensor in the order of the indices. For more information about \\ntf.gather()  refer to https://www.tensorflow.org/api_docs/python/tf/gather .\\nThen we compute sorted_correct , which is a simple indicator function that switches on if the \\ncorresponding element in sorted_y_true  and sorted_y_pred  are the same, and if not stays off. \\nThen we use the tf.math.segment_sum()  function to compute a segmented sum of correctly \\npredicted samples. Samples belonging to each class are considered a single segment ( correct_\\nfor_each_label ). The segment_sum() function takes two arguments: data  and segment_ids . \\nFor example, if the data  is [0, 1, 2, 3, 4, 5, 6, 7]  and segment_ids  are [0, 0, 0, 1, 1, \\n2, 3, 3] , then the segment sum would be [0+1+2, 3+4, 5, 6+7] = [3, 7, 5, 13] .\\nThen we do the same for a vector of 1s. In this case, we get the number of true samples present \\nfor each class in the batch of data ( all_for_each_label ). Note that we are adding a 1 at the end. \\nThis is to avoid division by 0 in the next step. Finally, we divide correct_for_each_label  by \\nall_for_each_label , which gives us a vector containing the accuracy of each class. With that \\nwe compute the mean accuracy, which is the macro-averaged accuracy.\\nFinally we wrap this function in a MeanMetricWrapper  that will produce a tf.keras.metrics.\\nMetric  object that we can pass to the model.compile()  function:\\nmean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_\\naccuracy, name= 'macro_accuracy' )\\nCompile the model by calling:\\nmodel.compile(loss='sparse_categorical_crossentropy' , optimizer= 'adam', \\nmetrics=[mean_accuracy_metric])\\nNext, we will train the model with the data prepared.\\nTraining and evaluating RNN on NER task\\nLet’s train our model  on the data we have prepared. But first, we need to define a function to tackle \\nthe class imbalance in our dataset. We will pass sample weights to the model.fit()  function. \\nTo compute sample weights, we will first define a function called get_class_weights()  that \\ncomputes class_weights  for each class. Next we will pass the class weights to another function, \\nget_sample_weights_from_class_weights() , which will generate sample weights:\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6c74a03-0c31-467f-9654-3f797823f61a', embedding=None, metadata={'page_label': '227', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 227\\ndef get_class_weights (train_labels):\\n    \\n    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\\n    label_count_ser = label_count_ser. min()/label_count_ser\\n    \\n    label_id_map = get_label_id_map(train_labels)\\n    label_count_ser.index = label_count_ser.index. map(label_id_map)\\n    return label_count_ser.to_dict()\\nThe first function, get_class_weights() , takes a train_labels  (a list of list of class IDs). Then \\nwe create a pandas Series  object with train_labels . Note that we are using a function called \\nchain  from the built-in itertools  library, which will flatten train_labels  to a list of class IDs. \\nThe Series  object contains frequency counts of each class label that appears in the train dataset. \\nNext to compute weights, we divide the minimum frequency element-wise from other frequencies. \\nIn other words, if the frequency for class label 𝑐𝑐  is denoted by 𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓  , and the total label set is \\ndenoted by 𝐶𝐶 , the weight for class 𝑐𝑐  is computed as:\\n𝑤𝑤𝑐𝑐=min{𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓 (𝑙𝑙)∀𝑙𝑙 𝑙 𝑙𝑙𝑙\\n𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓(𝑐𝑐) \\nFinally, the output is converted into a dictionary that has class IDs as keys and class weights as \\nvalues. Next we need to convert the class_weights  to sample_weights . We simply perform a \\ndictionary lookup element-wise on each label to generate a sample weight from class_weights . \\nThe sample_weights  will be the same shape as the train_labels  as there’s one weight for each \\nsample:\\ndef get_sample_weights_from_class_weights (labels, class_weights):\\n    \"\"\" From the class weights generate sample weights \"\"\"\\n    return np.vectorize(class_weights.get)(labels)\\nWe can use NumPy’s np.vectorize()  function to achieve this. np.vectorize()  takes in a function \\n(e.g. class_weights.get()  is the key lookup function provided by Python) and applies that on \\nall elements, which gives us the sample weights. Call the functions we defined above to generate \\nthe actual weights:\\ntrain_class_weights = get_class_weights(train_labels)\\nprint(\"Class weights: {}\" .format(train_class_weights))\\n# Get sample weights (we cannot use class_weight with TextVectorization\\n# layer)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='175f9f38-20b9-4135-9ced-2b5854097c91', embedding=None, metadata={'page_label': '228', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 228\\ntrain_sample_weights = get_sample_weights_from_class_weights(padded_train_\\nlabels, train_class_weights)\\nAfter we have the sample weights at our disposal, we can train our model. You can view the \\nclass_weights  by printing them out. This will give:\\nlabels_map: {\\n    'B-ORG': 0, \\n    'O': 1, \\n    'B-MISC': 2, \\n    'B-PER': 3, \\n    'I-PER': 4, \\n    'B-LOC': 5, \\n    'I-ORG': 6, \\n    'I-MISC': 7, \\n    'I-LOC': 8\\n}\\nClass weights: {\\n    1: 0.006811025015037328, \\n    5: 0.16176470588235295, \\n    3: 0.17500000000000002, \\n    0: 0.18272425249169436, \\n    4: 0.25507950530035334, \\n    6: 0.31182505399568033, \\n    2: 0.33595113438045376, \\n    8: 0.9982713915298186, \\n    7: 1.0\\n}\\nYou can see the class Other  has the lowest weight (because it’s the most frequent), and the  class \\nI-MISC  has the highest as it’s the least frequent. Now we will train our model using the prepared \\ndata:\\n# Make train_sequences an array\\ntrain_sentences = np.array(train_sentences)\\n# Training the model\\nmodel.fit(\\n        train_sentences, padded_train_labels, \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef84863f-40c9-4c34-8ae5-e934177cb12f', embedding=None, metadata={'page_label': '229', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 229\\n        sample_weight=train_sample_weights,\\n        batch_size=batch_size,\\n        epochs=epochs, \\n        validation_data=(np.array(valid_sentences), \\n        padded_valid_labels)\\n)\\nYou should get an accuracy of around 78-79% without any special performance optimization \\ntricks. Next you can evaluate the model on test data with:\\nmodel.evaluate(np.array(test_sentences), padded_test_labels)\\nThis will give a test accuracy of around 77%. Since the validation accuracy and test accuracy are \\non par, we can say that the model has generalized well. But to make sure, let’s visually inspect a \\nfew samples from the test set.\\nVisually analyzing outputs\\nTo analyze the output, we will  use the first five sentences in the test set:\\nn_samples = 5\\nvisual_test_sentences = test_sentences[:n_samples]\\nvisual_test_labels = padded_test_labels[:n_samples]\\nNext predict using the model and convert those predictions to predicted class IDs:\\nvisual_test_predictions = model.predict(np.array(visual_test_sentences))\\nvisual_test_pred_labels = np.argmax(visual_test_predictions, axis=- 1)\\nWe will create a reversed labels_map  that has a mapping from label ID to label string:\\nrev_labels_map = dict(zip(labels_map.values(), labels_map.keys()))\\nFinally, we will print out the results:\\nfor i, (sentence, sent_labels, sent_preds) in enumerate (zip(visual_test_\\nsentences, visual_test_labels, visual_test_pred_labels)):    \\n    n_tokens = len(sentence.split())\\n    print(\"Sample:\\\\t\" ,\"\\\\t\".join(sentence.split()))\\n    print(\"True:\\\\t\" ,\"\\\\t\".join([rev_labels_map[i] for i in \\n    sent_labels[:n_tokens]]))\\n    print(\"Pred:\\\\t\" ,\"\\\\t\".join([rev_labels_map[i] for i in ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ccd36e7-7903-479a-8800-a1cc7df3e7df', embedding=None, metadata={'page_label': '230', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 230\\n    sent_preds[:n_tokens]]))\\n    print(\"\\\\n\")\\nThis will print out:\\nSample:     SOCCER    -    JAPAN    GET    LUCKY    WIN    ,    CHINA    \\nIN    SURPRISE    DEFEAT    .\\nTrue:         O    O    B-LOC    O    O    O    O    B-LOC    O    O    O    \\nO\\nPred:         O    O    B-MISC    O    O    O    O    B-PER    O    B-LOC    \\nO    O\\nSample:     Nadim    Ladki\\nTrue:         B-PER    I-PER\\nPred:         B-LOC    O\\nSample:     AL-AIN    ,    United    Arab    Emirates    1996-12-06\\nTrue:         B-LOC    O    B-LOC    I-LOC    I-LOC    O\\nPred:         B-LOC    O    B-LOC    I-LOC    I-LOC    I-ORG\\nSample:     Japan    began    the    defence    of    their    Asian    \\nCup    title    with    a    lucky    2-1    win    against    Syria    in    \\na    Group    C    championship    match    on    Friday    .\\nTrue:         B-LOC    O    O    O    O    O    B-MISC    I-MISC    O    O    \\nO    O    O    O    O    B-LOC    O    O    O    O    O    O    O    O    \\nO\\nPred:         B-LOC    I-LOC    O    O    O    O    B-MISC    I-MISC    \\nI-MISC    O    O    O    O    O    O    B-LOC    O    O    O    O    O    \\nO    O    O    O\\nIt can be seen that our model is doing a decent job. It is good at identifying locations but is \\nstruggling at identifying the names of people. Here we end our discussion about the basic RNN \\nsolution that performs NER. In the next section, we will make the model more complex, giving \\nit the ability to understand text better by providing more fine-grained details. Let’s understand \\nhow we can improve our model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60dffebf-f32d-4526-abd4-b9c5c6cd0c5f', embedding=None, metadata={'page_label': '231', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 231\\nNER with character and token embeddings\\nNowadays, recurrent models used  to solve  the NER task are much more  sophisticated than having \\njust a single embedding layer and an RNN model. They involve using more advanced recurrent \\nmodels like Long Short-Term Memory ( LSTM ), Gated Recurrent Units ( GRUs ), etc. We will \\nset aside the discussion about these advanced models for several upcoming chapters. Here we \\nwill focus our discussion on a technique that provides the model embeddings at multiple scales, \\nenabling it to understand language better. That is, instead of relying only on token embeddings, \\nalso use character embeddings. Then a token embedding is generated with the character em -\\nbeddings by shifting a convolutional window over the characters in the token. Don’t worry if \\nyou don’t understand the details yet. The following sections will go into specific details of the \\nsolution. This exercise is available in ch06_rnns_for_named_entity_recognition.ipynb  in the \\nCh06-Recurrent-Neural-Networks  folder.\\nUsing convolution to generate token embeddings\\nA combination of character embeddings  and a convolutional kernel  can be used to generate token \\nembeddings ( Figure 6.14 ). The method will be as follows:\\n• Pad each token (e.g. word) to a predefined length\\n• Look up the character embeddings for the characters in the token from an embedding layer\\n• Shift a convolutional kernel over the sequence of character embeddings to generate a \\ntoken embedding\\nFigure 6.14: How token embeddings are generated using character embeddings and \\nthe convolution operation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c45450cb-01fe-406e-add6-fcc994cdbadf', embedding=None, metadata={'page_label': '232', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 232\\nThe very first thing we need to do is analyze the statistics around how many characters there \\nare for a token in our corpus. Similar to how we did it previously, we can do this with pandas:\\nvocab_ser = pd.Series(\\n    pd.Series(train_sentences). str.split().explode().unique()\\n)\\nvocab_ser. str.len().describe(percentiles=[ 0.05, 0.95])\\nIn computing vocab_ser , the first part (i.e. pd.Series(train_sentences).str.split() ) will \\nresult in a pandas Series , whose elements are a list of tokens (each token in the sentence is an \\nitem of that list). Next, explode()  will convert the Series  of a list of tokens into a Series  of to -\\nkens, by converting each token into a separate item in the Series . Finally we take only the unique \\ntokens in that Series . Here we end up with a pandas Series  where each item is a unique token.\\nWe will now use the str.len() function to get the length of each token (i.e. the number of char-\\nacters) and look at the 95% percentile in that. We will get the following:\\ncount    23623.000000\\nmean         6.832705\\nstd          2.749288\\nmin          1.000000\\n5%           3.000000\\n50%          7.000000\\n95%         12.000000\\nmax         61.000000\\ndtype: float64\\nWe can see around 95% of our words have less than or equal to 12 characters. Next, we will  write \\na function to pad shorter tokens:\\ndef prepare_corpus_for_char_embeddings (tokenized_sentences, max_seq_\\nlength):\\n    \"\"\" Pads each sequence to a maximum length \"\"\"\\n    proc_sentences = []\\n    for tokens in tokenized_sentences:\\n        if len(tokens) >= max_seq_length:\\n            proc_sentences.append([[t] for t in \\n            tokens[:max_seq_length]])\\n        else:\\n            proc_sentences.append([[t] for t in ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b6171ad-fcf4-48f7-b444-d7fbd51274e5', embedding=None, metadata={'page_label': '233', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 233\\n            tokens+[ \\'\\']*(max_seq_length- len(tokens))])\\n            \\n    return proc_sentences\\nThe function takes a set of tokenized sentences (i.e. each sentence as a list of tokens, not a string) \\nand a maximum sequence length. Note that this is the maximum sequence length we used pre -\\nviously, not the new token length we discussed. This function would then do the following:\\n• For longer sentences, only return the max_seq_length  tokens\\n• For shorter sentences, append ‘‘ as a token until max_seq_length  is reached\\nLet’s run this function on a small toy dataset:\\n# Define sample data\\ndata = [ \\'aaaa bb c\\' , \\'d eee\\' ]\\n# Pad sequences\\ntokenized_sentences = prepare_corpus_for_char_embeddings([d.split() for d \\nin data], 3)\\nThis will return:\\nPadded sequence: [[[\\'aaaa\\'], [\\'bb\\'], [\\'c\\']], [[\\'d\\'], [\\'eee\\'], [\\'\\']]]\\nWe will now define a new TextVectorization  layer that can cope with the changes we introduced \\nto the data. Instead  of tokenizing on the token level, the new TextVectorization  layer must \\ntokenize  on the character level. For this we need to make a few changes. We will again write a \\nfunction to contain this vectorization layer:\\ndef get_fitted_char_vectorization_layer (corpus, max_seq_length, max_token_\\nlength, vocabulary_size= None):\\n    \"\"\" Fit a TextVectorization layer on given data \"\"\"\\n    def _split_char (token):\\n        return tf.strings.bytes_split(token)\\n    # Define a text vectorization layer\\n    vectorization_layer = TextVectorization(\\n        standardize= None,      \\n        split=_split_char,\\n        output_sequence_length=max_token_length, \\n    )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9d7b5b8-d043-4deb-9fa4-f40dbaf1ed79', embedding=None, metadata={'page_label': '234', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 234\\n    tokenized_sentences = [sent.split() for sent in corpus]\\n    padded_tokenized_sentences = \\n    prepare_corpus_for_char_embeddings(tokenized_sentences, \\n    max_seq_length)\\n    \\n    # Fit it on a corpus of data\\n    vectorization_layer.adapt(padded_tokenized_sentences)\\n    \\n    # Get the vocabulary size\\n    n_vocab = len(vectorization_layer.get_vocabulary())\\n    return vectorization_layer, n_vocab\\nWe first define a function called _split_char()  that takes a token (as a tf.Tensor ) and returns \\na char-tokenized tensor. For example, _split_char(tf.constant(['abcd']))  would return \\n<tf.RaggedTensor [[b'a', b'b', b'c', b'd']]> . Then we define a TextVectorization  layer \\nthat will use this newly defined function as the way to split the data it gets. We will also define \\noutput_sequence_length  as max_token_length . Then we create tokenized_sentences , a list of \\nlist of strings, and pad it using the prepare_corpus_for_char_embeddings()  function we defined \\nearlier. Finally we use the TextVectorization  layer’s adapt() function to fit it with the data we \\nprepared. Two key differences between  the previous token-based text vectorizer  and this char-\\nbased text vectorizer are in the input dimensions and the final output dimensions:\\n• Token-based vectorizer – Takes in a [batch size, 1] -sized input and produces a [batch \\nsize, sequence length] -sized output\\n• Char-based vectorizer – Takes in a [batch size, sequence length, 1] -sized input and \\nproduces a [batch size, sequence length, token length] -sized output\\nNow we are equipped with the ingredients to implement our new and improved NER classifier.\\nImplementing the new NER model\\nWith a good conceptual understanding  of the model, let’s implement the new NER model. We \\nwill first define some hyperparameters, followed by defining a text vectorizer as before. However, \\nour TextVectorization  will be more complex in this section, as we have several different levels \\nof tokenization taking place (e.g. char-level and token-level). Finally we define the RNN-based \\nmodel that produces the output.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cf82f4d5-2d22-4eae-a3bc-fd924219b8a7', embedding=None, metadata={'page_label': '235', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 235\\nDefining hyperparameters\\nFirst, we will define the two hyperparameters  as follows:\\nmax_seq_length = 40\\nmax_token_length = 12\\nDefining the input layer\\nWe then define an input layer  with the data type tf.strings  as before:\\n# Input layer (tokens)\\nword_input = tf.keras.layers.Input(shape=( 1,), dtype=tf.string)\\nThe inputs to this layer  would be a batch of sentences, where each sentence is a string. \\nDefining the token-based TextVectorization layer\\nThen we define  the token-level TextVectorization  layer just like we did above:\\n# Text vectorize layer (token)\\ntoken_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_\\nlayer(train_sentences, max_seq_length)\\n# Vectorized output (each word mapped to an int ID)\\ntoken_vectorized_out = token_vectorize_layer(word_input)\\nDefining the character-based TextVectorization layer\\nFor the character-level  vectorization layer we will employ the get_fitted_char_vectorization_\\nlayer() function we defined above:\\n# Text vectorize layer (char)\\nchar_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_\\nlayer(train_sentences, max_seq_length, max_token_length)\\nNext, we will discuss the inputs for this layer.\\nProcessing the inputs for the char_vectorize_layer\\nWe will use the same word_input  for this new  vectorization layer as well. However, using the same \\ninput means we need to introduce some interim pre-processing to get the input to the correct \\nformat intended for this layer. Remember that the input to this layer needs to be a [batch size, \\nsequence length, 1] -sized tensor. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72322a39-949d-4d4b-bf4b-0cbbcec14198', embedding=None, metadata={'page_label': '236', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 236\\nThis means the sentences need to be tokenized to a list of tokens. For that we will use the tf.keras.\\nlayers.Lambda()  layer and the tf.strings.split() function:\\ntokenized_word_input = layers.Lambda(\\n    lambda x: tf.strings.split(x).to_tensor(default_value= '', \\n    shape=[None, max_seq_length, 1])\\n)(word_input)\\nchar_vectorized_out = char_vectorize_layer(tokenized_word_input)\\nThe Lambda  layer is used as a way to create a layer from a custom TensorFlow/Keras  function, which \\nmay not be available as a standard layer in Keras. Here we are using a Lambda layer to define a \\nlayer that will tokenize a passed input to a list of tokens. Furthermore, the tf.strings.split()  \\nfunction returns a ragged tensor. In a typical tensor, all the dimensions need to have a constant \\nsize. A ragged tensor is a special tensor whose dimensions are not fixed. For example, since a list \\nof sentences is highly unlikely to have the same number of tokens, this results in a ragged tensor. \\nBut TensorFlow will complain if you try to go forward with a tf.RaggedTensor  as most layers \\ndo not support these tensors. Therefore, we need to convert this to a standard tensor using the \\nto_tensor()  function. We can pass a shape to this function and it will make sure the shape of \\nthe resulting tensor will be the defined shape (by means of padding and truncations). \\nA key thing to pay attention to is how the shapes of the input-output tensors are transformed at \\neach layer. For example, we started off with a [batch size, 1] -sized tensor that went into the \\nLambda  layer to be transformed to a [batch size, sequence length, 1] -sized layer. Finally, \\nthe char_vectorize_layer  transforms this into a [batch size, sequence length, token \\nlength] -sized tensor.\\nWe will then define an embedding layer, with which we will look up embeddings for the resulting \\nchar IDs coming from the char_vectorize_layer :\\n# Produces a [batch size, seq length, token_length, emb size]\\nchar_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_\\ndim=32, mask_zero= True)(char_vectorized_out)\\nThis layer produces a [batch size, sequence length, token length, 32] -sized tensor, with \\na char embedding vector for each character in the tensor. Now it’s time to perform convolution \\non top of this output.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9f847a8-2ba2-4f72-9040-b910ff78a5c6', embedding=None, metadata={'page_label': '237', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 237\\nPerforming convolution on the character embeddings\\nWe will define  a 1D convolution layer with a kernel size of 5 (i.e. convolutional window size), a \\nstride of 1, 'same'  padding, and a ReLU activation. We then feed the output from the previous \\nsection to this:\\n# A 1D convolutional layer that will generate token embeddings by shifting \\n# a convolutional kernel over the sequence of chars in each token (padded)\\nchar_token_output = layers.Conv1D(filters= 1, kernel_size= 5, strides= 1, \\npadding= 'same', activation= 'relu')(char_embedding_layer)\\nThis layer typically  takes a [batch size, width, in channels] -sized tensor. However, in our \\ncase, we have a four-dimensional input. This means, our Conv1D layer is going to behave in a \\ntime-distributed fashion. Put in another way, it will take an input with a temporal dimension (i.e. \\nsequence length dimension) and produce an output with that dimension intact. In other words, it \\ntakes our input of shape [batch size, sequence length, token length, 32 (in channels)]  \\nand produces a [batch size, sequence length, token length, 1 (out channels)] -sized \\noutput. You can see that the convolution only operates on the last two dimensions, while keeping \\nthe first two as they are.\\nAnother way to think about this is, ignore the batch and sequence dimensions and visualize how \\nconvolution would work on the width and in channel dimensions. Then apply the same operation \\nelement-wise to other dimensions, while considering the operation on 2D [width, in channel] \\ntensors as a single unit of computation.\\nRemember that we have a [batch size, sequence length, token length, 1] -sized output. \\nThis has an extra dimension of 1 at the end. We will write a simple Lambda  layer to get rid of this \\ndimension:\\n# There is an additional dimension of size 1 (out channel dimension) that\\n# we need to remove\\nchar_token_output = layers.Lambda( lambda x: x[:, :, :, 0])(char_token_\\noutput)\\nTo get the final output embedding (i.e. a combination of token- and character-based embeddings), \\nwe concatenate the two embeddings on the last axis. This would result in a 48 element-long \\nvector (i.e. 32 element-long token embedding + 12 element-long char-based token embedding):\\n# Concatenate the token and char embeddings\\nconcat_embedding_out = layers.Concatenate()([token_embedding_out, char_\\ntoken_output])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f4b369f-8c84-4e8a-b750-f2aed7b7ae2e', embedding=None, metadata={'page_label': '238', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 238\\nThe rest of the model, we will keep it the same. First define an RNN layer and pass the concat_\\nembedding_out  as an input:\\n# Define a simple bidirectional RNN layer, it returns an output at each\\n# position\\nrnn_layer_1 = layers.SimpleRNN(\\n    units= 64, activation= 'tanh', use_bias= True, return_sequences= True\\n)\\nrnn_out_1 = rnn_layer_1(concat_embedding_out)\\nRemember that we have set return_sequences=True , which means it will produce an output \\nat each time step, as opposed to only at the last  time step. Next, we define the final Dense layer, \\nwhich has n_classes  output nodes (i.e. 9) and a softmax  activation:\\n# Defines the final prediction layer\\ndense_layer = layers.Dense(n_classes, activation= 'softmax' )\\ndense_out = dense_layer(rnn_out_1)\\nWe define the model and compile it like before:\\n# Defines the model\\nchar_token_embedding_rnn = tf.keras.Model(inputs=word_input, \\noutputs=dense_out)\\n \\n# Define a macro accuracy measure\\nmean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_\\naccuracy, name= 'macro_accuracy' )\\n# Compile the model with a loss optimizer and metrics\\nchar_token_embedding_rnn. compile(loss='sparse_categorical_crossentropy' , \\noptimizer= 'adam', metrics=[mean_accuracy_metric])\\nThis is our final model. The key difference in this model compared to the previous solution is that \\nit used two different embedding types. A standard token-based embedding layer and a complex, \\nchar-based embedding that was leveraged to generate token embeddings using the convolution \\noperation. Now let’s train the model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f145a6d2-8ce7-48db-8dee-6bcfd8a19b3b', embedding=None, metadata={'page_label': '239', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 6 239\\nModel training and evaluation\\nModel training is identical to the training we did for the standard RNN model, so we will not \\ndiscuss it further:\\n# Make train_sequences an array\\ntrain_sentences = np.array(train_sentences)\\n# Get sample weights (we cannot use class_weight with TextVectorization\\n# layer)\\ntrain_sample_weights = get_sample_weights_from_class_weights(padded_train_\\nlabels, train_class_weights)\\n# Training the model\\nchar_token_embedding_rnn.fit(\\n    train_sentences, padded_train_labels,\\n    sample_weight=train_sample_weights,\\n    batch_size= 64,\\n    epochs= 3, \\n    validation_data=(np.array(valid_sentences), padded_valid_labels)\\n)\\nYou should get around a ~2% validation accuracy and a ~1% test accuracy boost  after these mod-\\nifications.\\nOther improvements you can make\\nHere we will discuss several improvements you can make to uplift the model performance even \\nfurther.\\n• More RNN layers – Adding more stacked  RNN layers. By adding more hidden RNN layers, \\nwe can allow the model to learn more refined latent representations, leading to better \\nperformance. An example usage is shown below:\\nrnn_layer_1 = layers.SimpleRNN(\\n    units= 64, activation= 'tanh', use_bias= True, return_\\nsequences= True\\n)\\nrnn_out_1 = rnn_layer_1(concat_embedding_out)\\nrnn_layer_2 = layers.SimpleRNN(\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f1fd336-5c98-4529-ac8d-291550405fd0', embedding=None, metadata={'page_label': '240', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Recurrent Neural Networks 240\\n    units= 32, activation= 'tanh', use_bias= True, return_\\nsequences= True\\n)\\nrnn_out_1 = rnn_layer_1(rnn_out_1)\\n• Make the RNN layer bidirectional  – The RNN models we discussed so far are uni-direc-\\ntional, i.e. looks at the sequence of text from forward to backward. However a different \\nvariant known as bi-directional RNNs looks at the sequence in both directions, i.e. forward \\nto backward and backward to forward. This leads to better language understanding in \\nmodels and inevitably better performance. We will discuss this variant in more detail in \\nthe upcoming chapters. An example usage is shown below:\\nrnn_layer_1 = layers.Bidreictional(layers.SimpleRNN(\\n    units= 64, activation= 'tanh', use_bias= True, return_\\nsequences= True\\n))\\n• Incorporate regularization techniques – You can leverage L2 regularization and dropout \\ntechniques to avoid overfitting and improve generalization of the model.\\n• Use early stopping and learning rate reduction to reduce overfitting  – During model \\ntraining, use early stopping (i.e. training the model only until the validation accuracy is \\nimproving) and learning rate reduction (i.e. gradually reducing the learning rate over \\nthe epochs).\\nWe recommend experimenting  with some of these techniques yourself to see how they can max -\\nimize the performance of your RNNs.\\nSummary\\nIn this chapter, we looked at RNNs, which are different from conventional feed-forward neural \\nnetworks and more powerful in terms of solving temporal tasks. \\nSpecifically, we discussed how to arrive at an RNN from a feed-forward neural network type \\nstructure. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='453037ae-739f-4f75-92bb-efbb4044de06', embedding=None, metadata={'page_label': '241', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6 241\\nWe assumed a sequence of inputs and outputs, and designed a computational graph that can \\nrepresent the sequence of inputs and outputs. \\nThis computational graph resulted in a series of copies of functions that we applied to each indi-\\nvidual input-output tuple in the sequence. Then, by generalizing this model to any given single \\ntime step t  in the sequence, we were able to arrive at the basic computational graph of an RNN. We \\ndiscussed the exact equations and update rules used to calculate the hidden state and the output.\\nNext we discussed how RNNs are trained with data using BPTT. We examined how we can arrive \\nat BPTT with standard backpropagation as well as why we can’t use standard backpropagation \\nfor RNNs. We also discussed two important practical issues that arise with BPTT—vanishing \\ngradient and exploding gradient—and how these can be solved on the surface level.\\nThen we moved on to the practical applications of RNNs. We discussed four main categories of \\nRNNs. One-to-one architectures are used for tasks such as text generation, scene classification, \\nand video frame labeling. Many-to-one architectures are used for sentiment analysis, where we \\nprocess the sentences/phrases word by word (compared to processing a full sentence in a single go, \\nas we saw in the previous chapter). One-to-many architectures are common in image captioning \\ntasks, where we map a single image to an arbitrarily long sentence phrase describing the image. \\nMany-to-many architectures are leveraged for machine translation tasks.\\nWe solved the task of NER with RNNs. In NER, the problem is to, given a sequence of tokens, \\npredict a label for each token. The label represents an entity (e.g. organization, location, person, \\netc.). For this we used embeddings as well as an RNN to process each token while considering \\nthe sequence of tokens as a time-series input. We also used a text vectorization layer to convert \\ntokens into word IDs. A key benefit of the text vectorization layer is that it is built as a part of the \\nmodel, unlike the tokenizer we used before. \\nFinally, we looked at how we can adopt character embeddings and the convolution operation to \\ngenerate token embeddings. We used these new token embeddings along with standard word \\nembeddings to improve model accuracy. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58a76bf8-b661-4417-a43f-373b0b88795c', embedding=None, metadata={'page_label': '242', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks 242\\nIn the next chapter, we will discuss a more powerful RNN model known as Long Short-Term \\nMemory (LSTM ) networks that further reduces the adverse effect of the vanishing gradient, and \\nthus produces much better results.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52adafb1-53d1-41ee-99df-e75a66115da8', embedding=None, metadata={'page_label': '243', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7\\nUnderstanding Long Short-Term \\nMemory Networks\\nIn this chapter, we will discuss the fundamentals behind a more advanced RNN variant known \\nas Long Short-Term Memory Networks ( LSTMs). Here, we will focus on understanding the \\ntheory behind LSTMs, so we can discuss their implementation in the next chapter. LSTMs are \\nwidely used in many sequential tasks (including stock market prediction, language modeling, \\nand machine translation) and have proven to perform better than older sequential models (for \\nexample, standard RNNs), especially given the availability of large amounts of data. LSTMs are \\ndesigned to avoid the problem of the vanishing gradient that we discussed in the previous chapter.\\nThe main practical limitation posed by the vanishing gradient is that it prevents the model from \\nlearning long-term dependencies. However, by avoiding the vanishing gradient problem, LSTMs \\nhave the ability to store memory for longer than ordinary RNNs (for hundreds of time steps). In \\ncontrast to RNNs, which only maintain a single hidden state, LSTMs have many more parameters \\nas well as better control over what memory to store and what to discard at a given training step. \\nFor example, RNNs are not able to decide which memory to store and which to discard, as the \\nhidden state is forced to be updated at every training step.\\nSpecifically, we will discuss what an LSTM is at a very high level and how the functionality of \\nLSTMs allows them to store long-term dependencies. Then we will go into the actual underlying \\nmathematical framework governing LSTMs and discuss an example to highlight why each com-\\nputation matters. We will also compare LSTMs to vanilla RNNs and see that LSTMs have a much \\nmore sophisticated architecture that allows them to surpass vanilla RNNs in sequential tasks. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41e1f615-a560-4e1b-8dee-5ef639810113', embedding=None, metadata={'page_label': '244', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 244\\nRevisiting the problem of the vanishing gradient and illustrating it through an example will lead \\nus to understand how LSTMs solve the problem.\\nThereafter, we will discuss several techniques that have been introduced to improve the predic -\\ntions produced by a standard LSTM (for example, improving the quality/variety of generated \\ntext in a text generation task). For example, generating several predictions at once instead of \\npredicting them one by one can help to improve the quality of generated predictions. We will \\nalso  look at bidirectional LSTMs (BiLSTMs), which are an extension to the standard LSTM, that \\nhave greater capabilities for capturing the patterns present in a sequence than a standard LSTM.\\nFinally, we will discuss two recent LSTM variants. First, we will look at peephole connections , \\nwhich introduce more parameters and information to the LSTM gates, allowing LSTMs to per -\\nform better. Next, we will discuss Gated Recurrent Units (GRUs ), which are gaining increasing \\npopularity as they have a much simpler structure compared to standard LSTMs and also do not \\ndegrade performance.\\nSpecifically, this chapter will cover the following main topics:\\n• Understanding Long Short-Term Memory Networks\\n• How LSTMs solve the vanishing gradient problem\\n• Improving LSTMs\\n• Other variants of LSTMs\\nUnderstanding Long Short-Term Memory Networks\\nIn this section, we will first explain how an LSTM cell operates. We will see that in addition to the \\nhidden states, a gating mechanism is in place to control information flow inside the cell. Transformer models have emerged as a more powerful alternative for sequence \\nlearning. Transformer models deliver better performance as these models have ac-\\ncess to the full history of the sequence at a given step, whereas LSTM models can \\nonly see the previous output at a given step. We will discuss Transformer models in \\ndetail in Chapter 10, Transformers  and Chapter 11, Image Captioning with Transformers. \\nHowever, it’s still worth learning about LSTMs as they have laid the foundation for \\nnext-generation models like Transformers. Additionally, LSTMs are still used to some \\nextent, especially when working on time-series problems in memory-constrained \\nenvironments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f0c2c5a-87b0-4d9d-8524-27ca417eb393', embedding=None, metadata={'page_label': '245', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 245\\nThen we will work through a detailed example and see how gates and states help at various stages \\nof the example to achieve desired behaviors, finally leading to the desired output. Finally, we will \\ncompare an LSTM against a standard RNN to learn how an LSTM differs from a standard RNN.\\nWhat is an LSTM?\\nLSTMs can be seen as a more complex and capable family of RNNs. Though LSTMs are a compli-\\ncated beast, the underlying principles of LSTMs are as same as of RNNs; they process a sequence \\nof items by working on one input at a time in a sequential order. An LSTM is mainly composed \\nof five different components:\\n• Cell state: This is  the internal cell state (that is, memory) of an LSTM cell\\n• Hidden state: This is the external hidden state exposed to other layers and used to cal-\\nculate predictions\\n• Input gate: This  determines how much of the current input is read into the cell state\\n• Forget gate: This  determines how much of the previous cell state is sent into the current \\ncell state\\n• Output gate: This determines how much of the cell state is output into the hidden state\\nWe can wrap the RNN to a cell architecture as follows: the cell will output some state (with a \\nnonlinear activation function) that is dependent on the previous cell state and the current input. \\nHowever, in RNNs, the cell state is continuously updated with every incoming input. This behavior \\nis quite undesirable for storing long-term dependencies.\\nLSTMs can decide when to add, update, or forget information stored in each neuron in the cell \\nstate. In other words, LSTMs are equipped with a mechanism to keep the cell state unchanged \\n(if warranted for better performance), giving them the ability to store long-term dependencies.\\nThis is achieved by introducing a gating mechanism. LSTMs possess gates for each operation the \\ncell needs to perform. The gates are continuous (often sigmoid functions) between 0 and 1, where \\n0 means no information flows through the gate and 1 means all the information flows through \\nthe gate. An LSTM uses one such gate for each neuron in the cell. As explained in the introduction, \\nthese gates control the following:\\n• How much of the current input is written to the cell state (input gate)\\n• How much information is forgotten from the previous cell state (forget gate)\\n• How much information is output into the final hidden state from the cell state (output gate)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8af6203f-1735-44c4-91bb-1753d050b014', embedding=None, metadata={'page_label': '246', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 246\\nFigure 7.1 illustrates this functionality for a hypothetical scenario. Each gate decides how much of \\nvarious data (for example, the current input, the previous hidden state, or the previous cell state) \\nflows into the states (that is, the final hidden state or the cell state). The thickness of each line \\nrepresents how much information is flowing from/to that gate (in some hypothetical scenarios). \\nFor example, in this figure, you can see that the input gate is allowing more from the current input \\nthan from the previous final hidden state, where the forget gate allows more from the previous \\nfinal hidden state than from the current input:\\nFigure 7.1: An abstract view of the data flow in an LSTM\\nLSTMs in more detail\\nHere we will walk through the actual mechanism of LSTMs. We will first briefly discuss the over -\\nall view of an LSTM cell and then start discussing each of the computations crunched within an \\nLSTM cell, along with an example of text generation.\\nAs we discussed earlier, LSTMs have a gating mechanism composed of the following three gates:\\n• Input gate: A gate that outputs values between 0 (the current input is not written to the \\ncell state) and 1 (the current input is fully written to the cell state). Sigmoid activation is \\nused to squash the output to between 0 and 1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e32a086d-821c-45bd-9c65-e5b651d8296c', embedding=None, metadata={'page_label': '247', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 247\\n• Forget gate: A sigmoidal gate that outputs values between 0 (the previous cell state is \\nfully forgotten for calculating the current cell state) and 1 (the previous cell state is fully \\nread in when calculating the current cell state).\\n• Output gate: A sigmoidal gate that outputs values between 0 (the current cell state is fully \\ndiscarded for calculating the final state) and 1 (the current cell state is fully used when \\ncalculating the final hidden state).\\nThis can be shown as in Figure 7.2 . This is a very high-level diagram, and some details have been \\nomitted in order to avoid clutter. We present LSTMs, both with loops and without loops, to \\nimprove understanding. The figure on the right-hand side depicts an LSTM with loops, and the \\none on the left-hand side shows the same LSTM with the loops unfolded so that no loops are \\npresent in the model:\\nFigure 7.2: An LSTM with recurrent links (that is, loops) (right) and an LSTM with recurrent \\nlinks unfolded (left)\\nNow, to get a better understanding of LSTMs, let’s consider a language modeling example. We \\nwill discuss the actual update rules and equations side by side with the example to ground our \\nunderstanding of LSTMs better.\\nLet’s consider an example of generating text starting from the following sentence:\\nJohn gave Mary a puppy.\\nThe story that we output should be about John, Mary, and the puppy. Let’s assume our LSTM \\noutputs two sentences following the given sentence:\\nJohn gave Mary a puppy. ____________________. _____________________.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e0fee05-f1e2-41bb-bc13-bfedc4117466', embedding=None, metadata={'page_label': '248', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 248\\nThe following is the output given by our LSTM:\\nJohn gave Mary a puppy. It barks very loudly. They named it Luna.\\nWe are still far from outputting realistic phrases such as these. However, LSTMs can learn rela-\\ntionships such as between nouns and pronouns. For example, it is related to the puppy , and they \\nto John and Mary. Then, it should learn the relationship between the noun/pronoun and the verb. \\nFor example, for it, the verb should have an s at the end. We illustrate these relationships/depen -\\ndencies in Figure 7.3. As we can see, both long-term (for example, Luna --> puppy) and short-term \\n(for example, It -->barks ) dependencies are present in this phrase. The solid arrows depict links \\nbetween nouns and pronouns and dashed arrows show links between nouns/pronouns and verbs:\\nFigure 7.3: Sentences given and predicted by the LSTM with various relationships between \\nwords highlighted\\nNow let’s consider how LSTMs, using their various operations, can model such relationships and \\ndependencies to output sensible text, given a starting sentence.\\nThe input gate (i t) takes the current input (x t) and the previous final hidden state (h t-1) as the \\ninput and calculates it, as follows:\\n𝑖𝑖𝑡𝑡=𝜎𝜎(𝑊𝑊𝑖𝑖𝑖𝑖𝑥𝑥𝑡𝑡+𝑊𝑊𝑖𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑖𝑖) \\nThe input gate i t can be understood as the calculation performed at the hidden layer of a sin-\\ngle-hidden-layer standard RNN with the sigmoidal activation. Remember that we calculated the \\nhidden state of a standard RNN as follows:\\nℎ𝑡𝑡= tanh(𝑈𝑈𝑈𝑈𝑡𝑡+𝑊𝑊ℎ𝑡𝑡𝑡𝑡) \\nTherefore, the calculation of i t of the LSTM looks quite analogous to the calculation of h t of a \\nstandard RNN, except for the change in the activation function and the addition of bias.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='609a646c-4d62-4b17-802c-83fc958d68eb', embedding=None, metadata={'page_label': '249', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 249\\nAfter the calculation, a value of 0 for it will mean that no information from the current input will \\nflow to the cell state, where a value of 1 means that all the information from the current input \\nwill flow to the cell state.\\nNext, another value (which is called candidate value) is calculated as follows, which is fed in to \\ncalculate the current cell state later. This value will be treated as a potential candidate for the \\nfinal cell state of this time step:\\n𝑐𝑐𝑐𝑡𝑡= tanh(𝑊𝑊𝑐𝑐𝑐𝑐𝑥𝑥𝑡𝑡+𝑊𝑊𝑐𝑐𝑐ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑐𝑐) \\nWe can visualize these calculations in Figure 7.4:\\nFigure 7.4: Calculation of i t and 𝑐𝑐𝑐𝑡𝑡  (in bold) in the context of all the calculations (grayed out) \\nthat take place in an LSTM\\nIn our example, at the very beginning of the learning, the input gate needs to be highly activated, \\nas the model has no prior knowledge of the task. The first word that the LSTM outputs is it. Also, \\nin order to do so, the LSTM must learn that puppy  is also referred to as it. Let’s assume our LSTM \\nhas five neurons to store the state. We would like the LSTM to store the information that it refers \\nto puppy. Another piece of information we would like the LSTM to learn (in a different neuron) \\nis that the present tense verb should have an s at the end of the verb when the pronoun it  is used. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80ea54ee-f623-4dd1-aa16-e9720d766b81', embedding=None, metadata={'page_label': '250', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 250\\nOne more thing the LSTM needs to know is that the puppy barks loud. Figure 7.5  illustrates how \\nthis knowledge might be encoded in the cell state of the LSTM. Each circle represents a single \\nneuron (that is, a hidden unit) of the cell state:\\nFigure 7.5: The knowledge that should be encoded in the cell state to output the first sentence\\nWith this information, we can output the first new sentence:\\nJohn gave Mary a puppy. It barks very loudly.\\nNext, the forget gate is calculated as follows:\\n𝑓𝑓𝑡𝑡= 𝜎𝜎𝜎𝜎𝜎 𝑓𝑓𝑓𝑓𝑥𝑥𝑡𝑡+𝜎𝜎𝑓𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑓𝑓) \\nThe forget gate does the following. A value of 0 for the forget gate means that no information \\nfrom ct-1 will be passed to calculate c t, and a value of 1 means that all the information of ct-1 will \\npropagate into the calculation of ct. It may sound counter-intuitive, as switching on the forget gate \\ncauses the model to remember from the previous step and vice versa. But to respect the original \\nnaming conventions and design, we’ll continue to use them as they are.\\nNow we will see how the forget gate helps in predicting the next sentence:\\nThey named it Luna.\\nNow, as you can see, the new relationship we are looking at is between John and Mary and they. \\nTherefore, we no longer need information about it and how the verb bark behaves, as the subjects \\nare John and Mary. We can use the forget gate in combination with the current subject they and \\nthe corresponding verb named  to replace the information stored in the Current subject  and Verb \\nfor current subject  neurons (see Figure 7.6):', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2dd17646-6a66-451e-8704-b3e6ea45dd93', embedding=None, metadata={'page_label': '251', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 251\\nFigure 7.6: The knowledge in the third neuron from the left (it --> barks) is replaced with new \\ninformation (they --> named)\\nIn terms of the values of weights, we illustrate this transformation in Figure 7.7. We do not change \\nthe state of the neuron maintaining the it --> puppy  relationship, because puppy  appears as an \\nobject in the last sentence. This is done by setting weights connecting it --> puppy  from ct-1 to ct to \\n1. Then we will replace the neurons maintaining the current subject and verb information with \\na new subject and verb. This is achieved by setting the forget weights of ft, for that neuron, to 0. \\nThen we will set the weights of it, connecting the current subject and verb to the corresponding \\nstate neurons, to 1. We can think of 𝑐𝑐𝑐𝑡𝑡  (the candidate value) as a potential candidate for the cell’s \\nmemory, as it contains information from the current input x t:\\nFigure 7.7: How the cell state c t is calculated with the previous state c t-1 and the candidate \\nvalue 𝑐𝑐𝑐𝑡𝑡 \\nThe current cell state will be updated as follows:\\n𝑐𝑐𝑡𝑡=𝑓𝑓𝑡𝑡𝑐𝑐𝑡𝑡𝑡𝑡+𝑖𝑖𝑡𝑡𝑐𝑐𝑐𝑡𝑡 \\nIn other words, the current state is a combination of the following:\\n• What information to forget/remember from the previous cell state\\n• What information to add/discard to the current input', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc040831-7b80-4a07-9b5e-d532c8f8321a', embedding=None, metadata={'page_label': '252', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 252\\nNext, in Figure 7.8, we highlight what we have calculated so far with respect to all the calculations \\nthat are taking place inside an LSTM:\\nFigure 7.8: Calculations covered so far, including i t, ft, 𝑐𝑐𝑐𝑡𝑡 , and c t\\nAfter learning the full cell state, it would look like Figure 7.9 :\\nFigure 7.9: The full cell state will look like this after outputting both the sentences\\nNext, we will look at how the final state of the LSTM cell ( ht) is computed:\\n𝑜𝑜𝑡𝑡=𝜎𝜎(𝑊𝑊𝑜𝑜𝑜𝑜𝑥𝑥𝑡𝑡+𝑊𝑊𝑜𝑜𝑜ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑜𝑜) \\nℎ𝑡𝑡=𝑜𝑜𝑡𝑡tanh(𝑐𝑐𝑡𝑡) ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8cf9b044-a86a-4948-8b40-49af1bc23a04', embedding=None, metadata={'page_label': '253', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 253\\nIn our example, we want to output the following sentence:\\nThey named it Luna.\\nFor this, we do not need the second to last neuron to compute this sentence, as it contains informa -\\ntion about how the puppy barks, whereas this sentence is about the name of the puppy. Therefore, \\nwe can ignore this neuron (containing the bark -> loud  relationship) during the predictions of the \\nlast sentence. This is exactly what ot does; it ignores the unnecessary memory and only retrieves \\nthe related memory from the cell state when calculating the final output of the LSTM cell. Also, \\nin Figure 7.10, we illustrate what a full LSTM cell would look like at a glance:\\nFigure 7.10: What the full LSTM looks like\\nHere, we summarize all the equations  relating to the operations taking place within an LSTM cell:\\n𝑖𝑖𝑡𝑡=𝜎𝜎(𝑊𝑊𝑖𝑖𝑖𝑖𝑥𝑥𝑡𝑡+𝑊𝑊𝑖𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑖𝑖) \\n𝑓𝑓𝑡𝑡= 𝜎𝜎𝜎𝜎𝜎 𝑓𝑓𝑓𝑓𝑥𝑥𝑡𝑡+𝜎𝜎𝑓𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑓𝑓) \\n𝑐𝑐𝑐𝑡𝑡= tanh(𝑊𝑊𝑐𝑐𝑐𝑐𝑥𝑥𝑡𝑡+𝑊𝑊𝑐𝑐𝑐ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑐𝑐) \\n𝑐𝑐𝑡𝑡=𝑓𝑓𝑡𝑡𝑐𝑐𝑡𝑡𝑡𝑡+𝑖𝑖𝑡𝑡𝑐𝑐𝑐𝑡𝑡 \\n𝑜𝑜𝑡𝑡=𝜎𝜎(𝑊𝑊𝑜𝑜𝑜𝑜𝑥𝑥𝑡𝑡+𝑊𝑊𝑜𝑜𝑜ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑜𝑜) \\nℎ𝑡𝑡=𝑜𝑜𝑡𝑡tanh(𝑐𝑐𝑡𝑡) ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='82f9793c-79d9-4c5c-904d-dec0690dd882', embedding=None, metadata={'page_label': '254', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 254\\nNow in the  bigger picture, for a sequential learning problem, we can unroll the LSTM cells over \\ntime to show how they would link together so that they receive the previous state of the cell to \\ncompute the next state, as shown in Figure 7.11:\\nFigure 7.11: How LSTMs would be linked over time\\nHowever, this is not adequate to do something useful. We typically use machine learning mod-\\nels to solve a task formulated as a classification or regression problem. As you can see, we still \\ndon’t have an output layer to output predictions. But if we want to use what the LSTM actually \\nlearned, we need a way to extract the final output from the LSTM. Therefore, we will fit a  softmax  \\nlayer (with weights W s and bias bs) on top of the LSTM. The final output is obtained using the \\nfollowing equation:\\n𝑦𝑦𝑡𝑡= 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 (𝑊𝑊𝑠𝑠ℎ𝑡𝑡+𝑏𝑏𝑠𝑠) \\nNow the final picture of the LSTM with the softmax layer looks like Figure 7.12 :\\nFigure 7.12: LSTMs with a softmax output layer linked over time', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fad386ad-4f9a-4780-b55f-68afd2de1dca', embedding=None, metadata={'page_label': '255', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 255\\nWith the softmax head attached to the LSTM, it can now perform a given classification task end \\nto end. Now let’s compare and contrast LSTMs and the standard RNN model we discussed in the \\nprevious chapter.\\nHow LSTMs differ from standard RNNs\\nLet’s now investigate how LSTMs compare to standard RNNs. An LSTM has a more intricate \\nstructure compared to a standard RNN. One of the primary differences is that an LSTM has two \\ndifferent states: a cell state ct and a final hidden state ht. However, an RNN only has a single hidden \\nstate ht. The next primary difference is that, since an LSTM has three different gates, an LSTM \\nhas much more control over how the current input and the previous cell state are handled when \\ncomputing the final hidden state h t.\\nHaving the two different states is quite advantageous. With this mechanism, we can decouple the \\nmodel’s short-term and long-term memory. In other words, even when the cell state is changing \\nquickly, the final hidden state will still be changed more slowly. So, while the cell state is learning \\nboth short-term and long-term dependencies, the final hidden state can reflect either only the \\nshort-term dependencies, only the long-term dependencies, or both.\\nNext, the gating mechanism is composed of three gates: the input, forget, and output gates.\\nIt is quite evident that this is a more principled approach (especially compared to the standard \\nRNNs) that permits better control over how much the current input and the previous cell state \\ncontribute to the current cell state. Also, the output gate gives better control over how much the \\ncell state contributes to the final hidden state. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f2cfafb0-2ff9-4d43-a9eb-accaf5ea47ea', embedding=None, metadata={'page_label': '256', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 256\\nIn Figure 7.13, we compare schematic diagrams of a standard RNN and an LSTM to emphasize the \\ndifference in terms of the functionality of the two models:\\nFigure 7.13: A side-by-side comparison of a standard RNN and an LSTM cell\\nIn summary, with the design of maintaining two different states, an LSTM can learn both short-\\nterm and long-term dependencies, which helps solve the problem of the vanishing gradient, \\nwhich we’ll discuss in the following section.\\nHow LSTMs solve the vanishing gradient problem\\nAs we discussed earlier, even though RNNs are theoretically sound, in practice they suffer from \\na serious drawback. That is, when Backpropagation Through Time  (BPTT ) is used, the gradi-\\nent diminishes quickly, which allows us to propagate the information of only a few time steps. \\nConsequently, we can only store the information of very few time steps, thus possessing only \\nshort-term memory. This in turn limits the usefulness of RNNs in real-world sequential tasks.\\nOften, useful and interesting sequential tasks (such as stock market predictions or language \\nmodeling) require the ability to learn and store long-term dependencies. Think of the following \\nexample for predicting the next word:\\nJohn is a talented student. He is an A-grade student and plays rugby and cricket. All the other students \\nenvy ______.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3267046e-9bb9-4ab9-9bdb-98dc59378fd5', embedding=None, metadata={'page_label': '257', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 257\\nFor us, this is a very easy task. The answer would be John. However, for an RNN, this is a difficult \\ntask. We are trying to predict an answer that lies at the very beginning of the text. Also, to solve \\nthis task, we need a way to store long-term dependencies in the state of the RNN. This is exactly \\nthe type of task LSTMs are designed to solve.\\nIn Chapter 6, Recurrent Neural Networks , we discussed how a vanishing/exploding gradient can \\nappear without any nonlinear functions present. We will now see that it could still happen even \\nwith the nonlinear term present. For this, we will derive the term 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   for a standard RNN \\nand 𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   for an LSTM network to understand the differences. This is the crucial term that \\ncauses the vanishing gradient, as we learned in the previous chapter.\\nLet’s assume the hidden state is calculated as follows for a standard RNN:\\nℎ𝑡𝑡=𝜎𝜎(𝑊𝑊𝑥𝑥𝑥𝑥𝑡𝑡+𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡) \\nTo simplify the calculations, we can ignore the current input related terms and focus on the re -\\ncurrent part, which will give us the following equation:\\nℎ𝑡𝑡=𝜎𝜎(𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡) \\nIf we calculate 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   for the preceding equations, we will get the following:\\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄=∏𝑊𝑊 ℎ𝜎𝜎(𝑊𝑊ℎ𝜕𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡)(1−𝜎𝜎(𝑊𝑊ℎ𝜕𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡))𝑡𝑡𝑡𝑘\\n𝑡𝑡𝑖𝑖 \\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄=𝑊𝑊ℎ𝑡𝑡∏𝜎𝜎(𝑊𝑊ℎ𝜕𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡)(1−𝜎𝜎(𝑊𝑊ℎ𝜕𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡))𝑡𝑡𝑡𝑘\\n𝑡𝑡𝑖𝑖 \\nNow  let’s see what happens when 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≪0  or 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≫0  (which will happen as learning \\ncontinues). In both cases, 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   will start to approach 0, giving rise to the vanishing gradient. \\nEven when 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡=0 , where the gradient is maximum (0.25) for sigmoid activation, when \\nmultiplied for many time steps, the overall gradient becomes quite small. Moreover, the term \\n𝑊𝑊ℎ𝑘𝑘  (possibly due to bad initialization) can cause exploding or vanishing of the gradients as well. \\nHowever, compared to the gradient vanishing due to 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≪0  or 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≫0 , the gradient \\nvanishing/explosion caused by the term 𝑊𝑊ℎ𝑘𝑘  is relatively easy to solve (with careful initialization \\nof weights and gradient clipping).\\nNow let’s look at an LSTM cell. More specifically, we’ll look at the cell state, given by the following \\nequation:\\n𝑐𝑐𝑡𝑡=𝑓𝑓𝑡𝑡𝑐𝑐𝑡𝑡𝑡𝑡+𝑖𝑖𝑡𝑡𝑐𝑐𝑐𝑡𝑡 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='49e5db8c-fc96-4a85-885d-34d445c0a500', embedding=None, metadata={'page_label': '258', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 258\\nThis is the product of all the forget gate applications happening in the LSTM. However, if you \\ncalculate 𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄  in a similar way for LSTMs (that is, ignoring the 𝑊𝑊𝑓𝑓𝑓𝑓𝑥𝑥𝑡𝑡  terms and b f, as they \\nare non-recurrent), we get the following:\\n𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡= ∏𝜎𝜎𝜎𝜎𝜎 𝑓𝑓𝑓ℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡)𝑡𝑡𝑡𝑘\\n𝑡𝑡𝑖𝑖⁄  \\nIn this case, though the gradient will vanish if 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≪0 , on the other hand, if 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≫0 , the \\nderivative will decrease much slower than it would in a standard RNN. Therefore, we have one \\nalternative, where the gradient will not vanish. Also, as the squashing function is used, the gra-\\ndients will not explode due to 𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   being large (which is the thing likely to be the cause of \\na gradient explosion). In addition, when 𝑊𝑊ℎℎ𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡≫0 , we get a maximum gradient close to 1, \\nmeaning that the gradients will not rapidly decrease as we saw with RNNs (when the gradient \\nis at maximum). Finally, there is no term such as 𝑊𝑊ℎ𝑘𝑘  in the derivation. However, derivations are \\ntrickier for 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄  . Let’s see if such terms are present in the derivation of 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄  . If you \\ncalculate the derivatives of this, you will get something of the following form:\\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄=𝜕𝜕(𝑜𝑜𝑡𝑡tanh(𝑐𝑐𝑡𝑡))𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄  \\nOnce you solve this, you will get something of this form: \\ntanh(∙)𝜎𝜎(∙)[1−𝜎𝜎(∙)]𝑤𝑤𝑜𝑜𝑜+𝜎𝜎(∙)[1−tanh2(∙)]{𝑐𝑐𝑡𝑡𝑡𝑡𝜎𝜎(∙)[1−𝜎𝜎(∙)]𝑤𝑤𝑓𝑓𝑜+𝜎𝜎(∙)[1−tanh2(∙)]𝑤𝑤𝑐𝑐𝑜+tanh(∙)𝜎𝜎(∙)[1−𝜎𝜎(∙)]𝑤𝑤𝑖𝑖𝑜} \\nWe do not care about the content within 𝜎𝜎(∙)  or tanh(∙) , because no matter the value, it will be \\nbounded by (0,1) or (-1,1). If we further reduce the notation by replacing the 𝜎𝜎(∙) , [1−𝜎𝜎(∙)] , tanh(∙)  \\nand [1−tanh2(∙)]  terms with a common notation such as 𝛾𝛾𝛾𝛾𝛾 , we get something of this form: \\n𝛾𝛾(∙)𝑤𝑤𝑜𝑜𝑜+𝛾𝛾(∙)𝛾𝛾𝛾𝑡𝑡𝑡𝑡𝛾𝛾(∙)𝑤𝑤𝑓𝑓𝑜+𝛾𝛾(∙)𝑤𝑤𝑐𝑐𝑜+𝛾𝛾(∙)𝑤𝑤𝑖𝑖𝑜] \\nAlternatively, we get the following (assuming that the outside 𝛾𝛾(∙)  gets absorbed by each 𝛾𝛾(∙)  \\nterm present within the square brackets): \\n𝛾𝛾(∙)𝑤𝑤𝑜𝑜𝑜+𝑐𝑐𝑡𝑡𝑡𝑡𝛾𝛾(∙)𝑤𝑤𝑓𝑓𝑜+𝛾𝛾(∙)𝑤𝑤𝑐𝑐𝑜+𝛾𝛾(∙)𝑤𝑤𝑖𝑖𝑜 \\nThis will give the following: \\n𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄≈∏𝛾𝛾(∙)𝑤𝑤𝑜𝑜𝑜+𝑐𝑐𝑡𝑡𝑡𝑡𝛾𝛾(∙)𝑤𝑤𝑓𝑓𝑜+𝛾𝛾(∙)𝑤𝑤𝑐𝑐𝑜+𝛾𝛾(∙)𝑤𝑤𝑖𝑖𝑜𝑡𝑡𝑡𝑡\\n𝑖𝑖𝑖𝑖 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='074e7d77-7efa-4469-be47-550a0abd1656', embedding=None, metadata={'page_label': '259', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 259\\nThis means that though the term 𝜕𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   is safe from any 𝑊𝑊ℎ𝑘𝑘  terms, 𝜕𝜕𝜕𝑡𝑡𝜕𝜕𝜕𝑡𝑡𝑡𝑡𝑡⁄   is not. There -\\nfore, we must be careful when initializing the weights of the LSTM and we should use gradient \\nclipping as well.\\nImproving LSTMs\\nHaving a model backed up by solid foundations does not always guarantee pragmatic success \\nwhen used in the real world. Natural language is quite complex. Sometimes seasoned writers \\nstruggle to produce quality content. So we can’t expect LSTMs to magically output meaningful, \\nwell-written content all of a sudden. Having a sophisticated design—allowing for better mod-\\neling of long-term dependencies in the data—does help, but we need more techniques during \\ninference to produce better text. Therefore, numerous extensions have been developed to help \\nLSTMs perform better at the prediction stage. Here we will discuss several such improvements: \\ngreedy sampling, beam search, using word vectors instead of a one-hot-encoded representation of \\nwords, and using bidirectional LSTMs. It is important to note that these optimization techniques \\nare not specific to LSTMs; rather, any sequential model can benefit from them.\\nGreedy sampling\\nIf we try to always  predict the word with the highest probability, the LSTM will tend to produce \\nvery monotonic results. For example, due to the frequent occurrence of stop words (e.g. the), it \\nmay repeat them many times before switching to another word.\\nOne way to get around this is to use greedy sampling, where we pick the predicted best n and \\nsample from that set. This helps to break the monotonic nature of the predictions.\\nLet’s consider the  first sentence of the previous example:\\nJohn gave Mary a puppy.\\nSay, we start with the first word and want to predict the next four words:\\nJohn ____ ____ _ _____.Note\\nHowever, ht of LSTMs being unsafe from vanishing gradient is not as crucial as it is \\nfor RNNs, because c t still can store the long-term dependencies without being af -\\nfected by vanishing gradient, and ht can retrieve the long-term dependencies from \\nct, if required to.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dcbe37a8-0c1d-48c2-bb14-25b17d0c2e74', embedding=None, metadata={'page_label': '260', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 260\\nIf we attempt to choose samples deterministically, the LSTM might output something like the \\nfollowing:\\nJohn gave Mary gave John.\\nHowever, by sampling the next word from a subset of words in the vocabulary (most highly prob -\\nable ones), the LSTM is forced to vary the prediction and might output the following:\\nJohn gave Mary a puppy.\\nAlternatively, it might give the following output:\\nJohn gave puppy a puppy.\\nHowever, even though greedy sampling helps to add more flavor/diversity to the generated text, \\nthis method does not guarantee that the output will always be realistic, especially when output -\\nting longer sequences of text. Now we will see a better search technique that actually looks ahead \\nseveral steps before predictions.\\nBeam search\\nBeam search is a way of helping with the quality of the  predictions produced by the LSTM. In \\nthis, the predictions are found by solving a search problem. Particularly, we predict several steps \\nahead for multiple candidates at each step. This gives rise to a tree-like structure with candidate \\nsequences of words (Figure 7.14). The crucial idea of beam search is to produce the b outputs (that \\nis, 𝑦𝑦𝑡𝑡,𝑦𝑦𝑡𝑡𝑡𝑡,…,𝑦𝑦𝑡𝑡𝑡𝑡𝑡 ) at once instead of a single output y t. Here, b  is known as the length of the beam, \\nand the b  outputs produced are known as the beam. More technically, we pick the beam that \\nhas the highest joint probability 𝑃𝑃(𝑦𝑦𝑡𝑡,𝑦𝑦𝑡𝑡𝑡𝑡,…,𝑦𝑦𝑡𝑡𝑡𝑡𝑡|𝑥𝑥𝑡𝑡)  instead of picking the highest probable \\n𝑃𝑃(𝑦𝑦𝑡𝑡|𝑥𝑥𝑡𝑡) . We are looking farther into the future before making a prediction, which usually leads \\nto better results.\\nLet’s understand beam search through the previous example:\\nJohn gave Mary a puppy.\\nSay, we are predicting word by word and initially we have the following:\\nJohn ____ ____ _ _____.\\nLet’s assume hypothetically that our LSTM produces the example sentence using beam search. \\nThen the probabilities for each word might look like what we see in Figure 7.14. Let’s assume beam \\nlength b = 2, and we will consider the n = 3 best candidates at each stage of the search. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0df67038-9da7-42e5-84c9-fb305d2651f3', embedding=None, metadata={'page_label': '261', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 261\\nThe search tree would look like the following figure:\\nFigure 7.14: The search space of beam search for a b=2 and n=3\\nWe start with the word John and get the probabilities for all the words in the vocabulary. In our \\nexample, as n = 3, we pick the best three candidates for the next level of the tree: gave, Mary, and \\npuppy . (Note that these might not be the candidates found by an actual LSTM and are only used \\nas an example.) Then from these selected candidates, the next level of the tree is grown. And from \\nthat, we will pick the best three candidates, and the search will repeat until we reach a depth of \\nb in the tree.\\nThe path that gives the highest joint probability (that is, 𝑃𝑃(𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 |𝐽𝐽𝐽𝐽ℎ𝑛𝑛)= 0.09 ) is highlight -\\ned with heavier arrows. Also, this is a better prediction mechanism, as it would return a higher \\nprobability, or a reward, for a phrase such as John gave Mary than John Mary John  or John John gave .\\nNote that the outputs produced by both greedy sampling and beam search are identical in our \\nexample, which is a simple sentence containing five words. However, this is not the case when we \\nscale this to output a small paragraph. Then the results produced by beam search will be much \\nmore realistic and meaningful than the ones produced by greedy sampling.\\nUsing word vectors\\nAnother  popular way of improving the performance of LSTMs is to use word vectors instead of \\nusing one-hot-encoded vectors as the input to the LSTM. Let’s understand the value of this meth -\\nod through an example. Let’s assume that we want to generate text starting from some random \\nword. In our case, it would be the following:\\nJohn ____ ____ _ _____.\\nWe have already trained our LSTM on the following sentences:\\nJohn gave Mary a puppy. Mary has sent Bob a kitten.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5a6601b7-1308-49fa-89a5-4ab8fc063517', embedding=None, metadata={'page_label': '262', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 262\\nLet’s also assume that we have the word vectors positioned as shown in Figure 7.15 . Remember \\nthat semantically similar words will have vectors placed close to each other: \\nFigure 7.15: Assumed word vectors’ topology in two-dimensional space\\nThe word embeddings of these words, in their numerical form, might look like the following:\\nkitten: [0.5, 0.3, 0.2]\\npuppy: [0.49, 0.31, 0.25]\\ngave: [0.1, 0.8, 0.9]\\nIt can be seen that distance(kitten, puppy) < distance(kitten, gave) . However, if we use one-hot \\nencoding, they would be as follows:\\nkitten: [ 1, 0, 0, …]\\npuppy: [0, 1, 0, …]\\ngave: [0, 0, 1, …]\\nThen, distance(kitten, puppy) = distance(kitten, gave). As we can already see, one-hot-encoded \\nvectors do not capture the proper relationship between words and see all the words are equally \\ndistanced from each other. However, word vectors are capable of capturing such relationships \\nand are more suitable to represent text for machine learning models.\\nUsing word vectors, the LSTM will learn to exploit relationships between words better. For ex -\\nample, with word vectors, LSTM will learn the following:\\nJohn gave Mary a kitten.\\nThis is quite close to the following:\\nJohn gave Mary a puppy.\\nAlso, it is quite different from the following:\\nJohn gave Mary a gave.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8cc7d13b-2419-4093-a9c7-056debcdf6a2', embedding=None, metadata={'page_label': '263', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 263\\nHowever, this would not be the case if one-hot-encoded vectors are used.\\nBidirectional LSTMs (BiLSTMs)\\nMaking LSTMs bidirectional is  another way of improving the quality of the predictions of an \\nLSTM. By this we mean training the LSTM with text read in both directions: from the beginning \\nto the end and the end to the beginning. So far during the training of the LSTM, we would create \\na dataset as follows.\\nConsider the following two sentences:\\nJohn gave Mary a _____. It barks very loudly.\\nAt this stage, there is data missing in one of the sentences that we would want our LSTM to fill \\nsensibly.\\nIf we read from the beginning up to the missing word, it would be as follows:\\nJohn gave Mary a _____.\\nThis does not provide enough information about the context of the missing word to fill the word \\nproperly. However, if we read in both directions, it would be the following:\\nJohn gave Mary a _____.\\n_____. It barks very loudly.\\nIf we created data with both these pieces, it is adequate to predict that the missing word should \\nbe something like dog  or puppy. Therefore, certain problems can benefit significantly from reading \\ndata from both sides. BiLSTMs also help in multilingual problems as different languages can have \\nvery different sentence structures.\\nAnother application of BiLSTMs is neural machine translation, where we translate a \\nsentence of a source language to a target language. As there is no specific alignment \\nbetween the translation of one language to another, having access to both sides of a \\ngiven token in the source language can greatly help to understand the context bet -\\nter, thus producing better translations. As an example, consider a translation task \\nof translating Filipino to English. In Filipino, sentences are usually written having \\nverb-object-subject in that order, whereas in English, it is subject-verb-object. In this \\ntranslation task, it will be extremely helpful to read sentences both forward and \\nbackward to make a good translation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='631b563a-cf51-4a7f-835a-a18c0d46b7ab', embedding=None, metadata={'page_label': '264', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 264\\nA BiLSTM is essentially two separate LSTM networks. One network learns data from the beginning \\nto the end, and the other network learns data from the end to the beginning. In Figure 7.16 , we \\nillustrate the architecture of a BiLSTM network.\\nTraining occurs in two phases. First, the solid-colored network is trained with data created by \\nreading the text from the beginning to the end. This network represents the normal training \\nprocedure used for standard LSTMs. Secondly, the dashed network is trained with data generated \\nby reading the text in the reversed direction. Then, at the inference phase, we use both the solid \\nand dashed states’ information (by concatenating both states and creating a vector) to predict \\nthe missing word:\\nFigure 7.16: A schematic diagram of a BiLSTM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0c8d935-267e-4560-83d7-28b24cf67114', embedding=None, metadata={'page_label': '265', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 265\\nIn this section, we discussed several different ways to improve the performance of LSTM models. \\nThis involved employing better prediction strategies to introduce structural changes such as \\nword vectors and BiLSTMs.\\nOther variants of LSTMs\\nThough we will mainly focus on the standard LSTM architecture, many variants have emerged \\nthat either simplify the complex architecture found in standard LSTMs, produce better perfor -\\nmance, or both. We will look at two variants that introduce structural modifications to the cell \\narchitecture of LSTMs: peephole connections and GRUs.\\nPeephole connections\\nPeephole connections allow gates to see not only the current input and the previous final hidden \\nstate, but also the previous cell state. This increases the number of weights in the LSTM cell. Having \\nsuch connections has been shown to produce better results. The equations would look like these: \\n𝑖𝑖𝑡𝑡=𝜎𝜎(𝑊𝑊𝑖𝑖𝑖𝑖𝑥𝑥𝑡𝑡+𝑊𝑊𝑖𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑊𝑊𝑖𝑖𝑖𝑖𝑐𝑐𝑡𝑡𝑡𝑡+𝑏𝑏𝑖𝑖) \\n𝑐𝑐𝑐𝑡𝑡= tanh(𝑊𝑊𝑐𝑐𝑐𝑐𝑥𝑥𝑡𝑡+𝑊𝑊𝑐𝑐𝑐ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑐𝑐) \\n𝑓𝑓𝑡𝑡= 𝜎𝜎𝜎𝜎𝜎 𝑓𝑓𝑓𝑓𝑥𝑥𝑡𝑡+𝜎𝜎𝑓𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝜎𝜎𝑓𝑓𝑓𝑓𝑐𝑐𝑡𝑡𝑡𝑡+𝑏𝑏𝑓𝑓) \\n𝑐𝑐𝑡𝑡=𝑓𝑓𝑡𝑡𝑐𝑐𝑡𝑡𝑡𝑡+𝑖𝑖𝑡𝑡𝑐𝑐𝑐𝑡𝑡 \\n𝑜𝑜𝑡𝑡=𝜎𝜎(𝑊𝑊𝑜𝑜𝑜𝑜𝑥𝑥𝑡𝑡+𝑊𝑊𝑜𝑜𝑜ℎ𝑡𝑡𝑡𝑡+𝑊𝑊𝑜𝑜𝑜𝑜𝑐𝑐𝑡𝑡+𝑏𝑏𝑜𝑜) \\nℎ𝑡𝑡=𝑜𝑜𝑡𝑡tanh(𝑐𝑐𝑡𝑡) \\nLet’s briefly look at how this helps the LSTM perform better. So far, the gates see the current \\ninput and final hidden state but not the cell state. However, in this configuration, if the output \\ngate is close to zero, even when the cell state contains information crucial to better performance, \\nthe final hidden state will be close to zero. Thus, the gates will not take the hidden state into \\nconsideration during calculation. Including the cell state directly in the gate calculation equa -\\ntion allows more control over the cell state, and it can perform well even in situations where the \\noutput gate is close to zero.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1d3f28b-6d37-4328-8102-06e61f0dd999', embedding=None, metadata={'page_label': '266', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 266\\nWe illustrate the architecture of the LSTM with peephole connections in Figure 7.17. We have \\ngrayed all the existing connections in a standard LSTM and the newly added connections are \\nshown in black:\\nFigure 7.17: An LSTM with peephole connections (the peephole connections are shown in black \\nwhile the other connections are grayed out)\\nGated Recurrent Units\\nGRUs  can be seen as a simplification of the standard LSTM architecture. As we have seen already, \\nan LSTM has three different gates and two different states. This alone requires a large number of \\nparameters even for a small state size. Therefore, scientists have investigated ways to reduce the \\nnumber of parameters. GRUs are a result of one such endeavor.\\nThere are several main differences in GRUs compared to LSTMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95adef61-a452-4863-a635-de330a0603d2', embedding=None, metadata={'page_label': '267', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 267\\nFirst, GRUs combine two states, the cell state and the final hidden state, into a single hidden \\nstate ht. Now, as a side effect of this simple modification of not having two different states, we \\ncan get rid of the output gate. Remember, the output gate was merely deciding how much of \\nthe cell state is read into the final hidden state. This operation greatly reduces the number of \\nparameters in the cell.\\nNext, GRUs introduce a reset gate that, when it’s close to 1, takes the full previous state informa-\\ntion in when computing the current state. Also, when the reset gate is close to 0, it ignores the \\nprevious state when computing the current state: \\n𝑟𝑟𝑡𝑡=𝜎𝜎(𝑊𝑊𝑟𝑟𝑟𝑟𝑥𝑥𝑡𝑡+𝑊𝑊𝑟𝑟𝑟ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑟𝑟) \\nℎ̃𝑡𝑡= tanh(𝑊𝑊ℎ𝑥𝑥𝑥𝑥𝑡𝑡+𝑊𝑊ℎℎ(𝑟𝑟𝑡𝑡ℎ𝑡𝑡𝑡𝑡)+𝑏𝑏ℎ) \\nThen, GRUs combine the input and forget gates into one update gate. The standard LSTM has two \\ngates known as the input and forget gates. The input gate decides how much of the current input \\nis read into the cell state, and the forget gate determines how much of the previous cell state is \\nread into the current cell state. Mathematically, this can be shown as follows: \\n𝑖𝑖𝑡𝑡=𝜎𝜎(𝑊𝑊𝑖𝑖𝑖𝑖𝑥𝑥𝑡𝑡+𝑊𝑊𝑖𝑖𝑖ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑖𝑖) \\n𝑓𝑓𝑡𝑡= 𝜎𝜎𝜎𝜎𝜎 𝑓𝑓𝑓𝑓𝑥𝑥𝑡𝑡+𝜎𝜎𝑓𝑓𝑓ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑓𝑓) \\nGRUs combine these two operations into a single gate known as the update gate. If the update \\ngate is 0, then the full state information of the previous cell state is pushed into the current cell \\nstate, where none of the current input is read into the state. If the update gate is 1, then all of the \\ncurrent input is read into the current cell state and none of the previous cell state is propagated \\ninto the current cell state. In other words, the input gate i t becomes inverse of the forget gate, \\nthat is, 1−𝑓𝑓𝑡𝑡 : \\n𝑧𝑧𝑡𝑡=𝜎𝜎(𝑊𝑊𝑧𝑧𝑧𝑧𝑥𝑥𝑡𝑡+𝑊𝑊𝑧𝑧𝑧ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑧𝑧) \\nℎ𝑡𝑡=𝑧𝑧𝑡𝑡ℎ̃𝑡𝑡+(1−𝑧𝑧 𝑡𝑡)ℎ𝑡𝑡𝑡𝑡 \\nNow let’s bring all the equations into one place. The GRU computations would look like this: \\n𝑟𝑟𝑡𝑡=𝜎𝜎(𝑊𝑊𝑟𝑟𝑟𝑟𝑥𝑥𝑡𝑡+𝑊𝑊𝑟𝑟𝑟ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑟𝑟) \\nℎ̃𝑡𝑡= tanh(𝑊𝑊ℎ𝑥𝑥𝑥𝑥𝑡𝑡+𝑊𝑊ℎℎ(𝑟𝑟𝑡𝑡ℎ𝑡𝑡𝑡𝑡)+𝑏𝑏ℎ) \\n𝑧𝑧𝑡𝑡=𝜎𝜎(𝑊𝑊𝑧𝑧𝑧𝑧𝑥𝑥𝑡𝑡+𝑊𝑊𝑧𝑧𝑧ℎ𝑡𝑡𝑡𝑡+𝑏𝑏𝑧𝑧) \\nℎ𝑡𝑡=𝑧𝑧𝑡𝑡ℎ̃𝑡𝑡+(1−𝑧𝑧 𝑡𝑡)ℎ𝑡𝑡𝑡𝑡 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7afc6f5-54d6-4877-b87d-d127139b08e3', embedding=None, metadata={'page_label': '268', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 268\\nThis is much more compact than LSTMs. In Figure 7.18, we can visualize a GRU cell (left) and an \\nLSTM cell (right) side by side:\\nFigure 7.18: A side-by-side comparison of a GRU (left) and the standard LSTM (right)\\nIn this section, we learned two variants of the LSTM: LSTMs with peepholes and GRUs. GRUs \\nhave become a popular choice over LSTMs, due to their simplicity and on-par performance with \\nmore complex LSTMs.\\nSummary\\nIn this chapter, you learned about LSTM networks. First, we discussed what an LSTM is and its \\nhigh-level architecture. We also delved into the detailed computations that take place in an LSTM \\nand discussed the computations through an example.\\nWe saw that an LSTM is composed mainly of five different things:\\n• Cell state: The internal cell state of an LSTM cell\\n• Hidden state: The external hidden state used to calculate predictions\\n• Input gate: This determines how much of the current input is read into the cell state', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='000ebd4b-f068-49ee-8779-6bfb9370e1c4', embedding=None, metadata={'page_label': '269', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7 269\\n• Forget gate: This determines how much of the previous cell state is sent into the current \\ncell state\\n• Output gate: This determines how much of the cell state is output into the hidden state\\nHaving such a complex structure allows LSTMs to capture both short-term and long-term de -\\npendencies quite well.\\nWe compared LSTMs to vanilla RNNs and saw that LSTMs are actually capable of learning long-\\nterm dependencies as an inherent part of their structure, whereas RNNs can fail to learn long-\\nterm dependencies. Afterward, we discussed how LSTMs solve the vanishing gradient with its \\ncomplex structure.\\nThen we discussed several extensions that improve the performance of LSTMs. First, a very simple \\ntechnique we called greedy sampling, in which, instead of always outputting the best candidate, \\nwe randomly sample a prediction from a set of best candidates. We saw that this improves the \\ndiversity of the generated text. After that, we looked at a more complex search technique called \\nbeam search. With this, instead of making a prediction for a single time step into the future, we \\npredict several time steps into the future and pick the candidates that produce the best joint \\nprobability. Another improvement involved seeing how word vectors can help improve the quality \\nof the predictions of an LSTM. Using word vectors, LSTMs can learn more effectively to replace \\nsemantically similar words during prediction (for example, instead of outputting dog, LSTM might \\noutput cat), leading to more realism and correctness of the generated text. The final extension \\nwe considered was BiLSTMs or bidirectional LSTMs. A popular application of BiLSTMs is filling \\nmissing words in a phrase. BiLSTMs read the text in both directions, from the beginning to the \\nend and the end to the beginning. This gives more context as we are looking at both the past and \\nfuture before predicting.\\nFinally, we discussed two variants of vanilla LSTMs: peephole connections and GRUs. Vanilla \\nLSTMs, when calculating the gates, only look at the current input and the hidden state. With \\npeephole connections, we make the gate computations dependent on all: the current input, and \\nthe hidden and cell states.\\nGRUs are a much more elegant variant of vanilla LSTMs that simplify LSTMs without compro -\\nmising on performance. GRUs have only two gates and a single state, whereas vanilla LSTMs \\nhave three gates and two states.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54ba9ba8-bd54-4aef-bdf0-8a6cfebc7a36', embedding=None, metadata={'page_label': '270', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Understanding Long Short-Term Memory Networks 270\\nIn the next chapter, we will see all these different architectures in action with implementations \\nof each of them and see how well they perform in text generation tasks.\\nTo access the code files for this book, visit our GitHub page at:  \\nhttps://packt.link/nlpgithub\\nJoin our Discord community to meet like-minded people and learn alongside  \\nmore than 1000 members at: https://packt.link/nlp\\n ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3e2c4ee-036a-4af4-8e72-88f8b8f0e7e8', embedding=None, metadata={'page_label': '271', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8\\nApplications of LSTM – \\nGenerating Text\\nNow that we have a good understanding of the underlying mechanisms of LSTMs, such as how \\nthey solve the problem of the vanishing gradient and update rules, we can look at how to use \\nthem in NLP tasks. LSTMs are employed for tasks such as text generation and image caption \\ngeneration. For example, language modeling is at the core of any NLP task, as the ability to model \\nlanguage effectively leads to effective language understanding. Therefore, this is typically used \\nfor pretraining downstream decision support NLP models. By itself, language modeling can be \\nused to generate songs ( https://towardsdatascience.com/generating-drake-rap-lyrics-\\nusing-language-models-and-lstms-8725d71b1b12 ), movie scripts ( https://builtin.com/\\nmedia-gaming/ai-movie-script ), etc.\\nThe application that we will cover in this chapter is building an LSTM that can write new folk \\nstories. For this task, we will download translations of some folk stories by the Grimm brothers. \\nWe will use these stories to train an LSTM and then ask it to output a fresh new story. We will \\nprocess the text by breaking it into character-level bigrams (n-grams where n=2 ) and make a \\nvocabulary out of the unique bigrams. Note that representing bigrams as one-hot-encoded vec-\\ntors is very ineffective for machine learning models, as it forces the model to treat each bigram \\nas an independent unit of text that is entirely different from other bigrams. But bigrams do share \\nsemantics, where certain bigrams co-occur where certain ones would not. One-hot encoding will \\nignore this important property, which is undesirable. To leverage this property in our modeling, \\nwe will use an embedding layer and jointly train it with the model. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='463d4169-a148-45d9-8f34-c396c2048429', embedding=None, metadata={'page_label': '272', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Applications of LSTM – Generating Text 272\\nWe will also explore ways to implement previously described techniques such as greedy sampling \\nor beam search for improving the quality of predictions. Afterward, we will see how we can im -\\nplement time-series models other than standard LSTMs, such as GRUs.\\nSpecifically, this chapter will cover the following main topics:\\n• Our data\\n• Implementing the language model\\n• Comparing LSTMs to LSTMs with peephole connections and GRUs\\n• Improving sequential models – beam search\\n• Improving LSTMs – generating text with words instead of n-grams\\nOur data\\nFirst, we will discuss  the data we will use for text generation and various preprocessing steps \\nemployed to clean the data.\\nAbout the dataset\\nFirst, we will understand what the dataset looks like so that when we see the generated text, we \\ncan assess whether it makes sense, given the training data. We will download the first 100 books \\nfrom the website https://www.cs.cmu.edu/~spok/grimmtmp/ . These are translations of a set of \\nbooks (from German to English) by the Grimm brothers.\\nInitially, we will download all 209 books from the website with an automated script, as follows:\\nurl = \\'https://www.cs.cmu.edu/~spok/grimmtmp/\\'\\ndir_name = \\'data\\'\\ndef download_data (url, filename, download_dir):\\n    \"\"\"Download a file if not present, and make sure it\\'s the right \\n    size.\"\"\"\\n      \\n    # Create directories if doesn\\'t exist\\n    os.makedirs(download_dir, exist_ok= True)\\n    \\n    # If file doesn\\'t exist download\\n    if not os.path.exists(os.path.join(download_dir,filename)):\\n        filepath, _ = urlretrieve(url + filename, ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83ca7d3b-71a0-4bd1-997a-df164c8226e6', embedding=None, metadata={'page_label': '273', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 8 273\\n        os.path.join(download_dir,filename))\\n    else:\\n        filepath = os.path.join(download_dir, filename)\\n        \\n    return filepath\\n# Number of files and their names to download\\nnum_files = 209\\nfilenames = [ format(i, '03d')+'.txt'  for i in range (1,num_files+ 1)]\\n# Download each file\\nfor fn in filenames:\\n    download_data(url, fn, dir_name)\\n# Check if all files are downloaded\\nfor i in range (len(filenames)):\\n    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\\n    assert file_exists\\nprint('{} files found.' .format(len(filenames)))\\nWe will now show example text snippets extracted from two randomly picked stories. The fol-\\nlowing is the first snippet:\\nThen she said, my dearest benjamin, your father has had these coffins made for you and for \\nyour eleven brothers, for if I bring a little girl into the world, you are all to be killed and buried \\nin them. And as she wept while she was saying this, the son comforted her and said, weep not, \\ndear mother, we will save ourselves, and go hence. But she said, go forth into the forest with \\nyour eleven brothers, and let one sit constantly on the highest tree which can be found, and \\nkeep watch, looking towards the tower here in the castle. If I give birth to a little son, I will put \\nup a white flag, and then you may venture to come back. But if I bear a daughter, I will hoist \\na red flag, and then fly hence as quickly as you are able, and may the good God protect you.\\nThe second text snippet is as follows:\\nRed-cap did not know what a wicked creature he was, and was not at all afraid of him.\\n“Good-day, little red-cap,” said he.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8bc722da-bbd5-4390-b3e8-a9e7295e461b', embedding=None, metadata={'page_label': '274', 'file_name': 'Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_path': 'c:\\\\Users\\\\User\\\\Desktop\\\\Project Anchor\\\\E2E-RAG-LLM-LLAMAIndex-Open-AI\\\\Data\\\\Thushan Ganegedara - Natural Language Processing with TensorFlow_ The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition-Packt Publishing (2022).pdf', 'file_type': 'application/pdf', 'file_size': 17364930, 'creation_date': '2024-04-02', 'last_modified_date': '2024-02-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Applications of LSTM – Generating Text 274\\n“Thank you kindly, wolf.”\\n“Whither away so early, little red-cap?”\\n“To my grandmother’s.”\\n“What have you got in your apron?”\\n“Cake and wine. Yesterday was baking-day, so poor sick grandmother is to have something \\ngood, to make her stronger.”\\n“Where does your grandmother live, little red-cap?”\\n“A good quarter of a league farther on in the wood. Her house stands under the three large \\noak-trees, the nut-trees are just below. You surely must know it,” replied little red-cap.\\nThe wolf thought to himself, what a tender young creature. What a nice plump mouthful, she \\nwill be better to eat than the old woman.\\nWe now understand what our data looks like. With that understanding, let us move on to pro -\\ncessing our data further.\\nGenerating training, validation, and test sets\\nWe will  segregate the stories we downloaded into three sets: training, validation, and test files. \\nWe will use the content in each set of files as the training, validation, and test data. We will use \\nscikit-learn’s train_test_split()  function to do so.\\nfrom sklearn.model_selection import train_test_split\\n# Fix the random seed so we get the same output everytime\\nrandom_state = 54321\\nfilenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\\n# First separate train and valid+test data\\ntrain_filenames, test_and_valid_filenames = train_test_split(filenames, \\ntest_size= 0.2, random_state=random_state)\\n# Separate valid+test data to validation and test data\\nvalid_filenames, test_filenames = train_test_split(test_and_valid_\\nfilenames, test_size= 0.5, random_state=random_state) ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\rag1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1214/1214 [00:02<00:00, 518.47it/s]\n",
      "Generating embeddings: 100%|██████████| 1248/1248 [00:31<00:00, 39.88it/s]\n"
     ]
    }
   ],
   "source": [
    "index= VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x26db63a0af0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine= index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x26db63b8d60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_engine.query(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subdiscipline of artificial intelligence that focuses on teaching computers how to learn without being explicitly programmed for specific tasks. It involves creating algorithms that can learn from data and make predictions based on that data. Machine learning can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1=query_engine.query(\"What is natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of natural language processing is to make machines understand spoken and written languages, enabling them to process and interpret unstructured text and speech data.\n"
     ]
    }
   ],
   "source": [
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Machine learning is a subdiscipline of artificial\n",
      "intelligence that focuses on teaching computers how to learn without\n",
      "being explicitly programmed for specific tasks. It involves creating\n",
      "algorithms that can learn from data and make predictions based on that\n",
      "data. Machine learning can be broadly categorized into supervised\n",
      "learning, unsupervised learning, and reinforcement learning.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 6954faa1-ba18-4feb-8917-6d1d3a4ce333\n",
      "Similarity: 0.841589600094414\n",
      "Text: Preface Deep Learning with TensorFlow and Keras, Third Edition,\n",
      "is a concise yet thorough introduction to modern  neural networks,\n",
      "artificial intelligence, and deep learning technologies designed\n",
      "especially for software  engineers and data scientists. The book is\n",
      "the natural follow-up of the books Deep Learning with Keras  [1] and\n",
      "TensorFlow 1.x...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: ed0ad128-d9f6-46e6-a32b-1dcfb3177d7b\n",
      "Similarity: 0.8389274603857956\n",
      "Text: Neural Network Foundations with TF 10 In short: what are neural\n",
      "networks after all? In one sentence, machine learning models are a way\n",
      "to compute a function that maps some inputs to  their corresponding\n",
      "outputs. The function is nothing more than a number of addition and\n",
      "multiplication  operations. However, when combined with a nonlinear\n",
      "activati...\n",
      "Machine learning is a subdiscipline of artificial intelligence that focuses on teaching computers how to learn without being explicitly programmed for specific tasks. It involves creating algorithms that can learn from data and make predictions based on that data. Machine learning can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "\n",
    "retriever= VectorIndexRetriever(index=index,similarity_top_k=5)\n",
    "postprocessor= SimilarityPostprocessor(similarity_cutoff=0.8)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,\n",
    "                                    node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_engine.query(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Machine learning is a subdiscipline of artificial\n",
      "intelligence that focuses on teaching computers how to learn without\n",
      "being explicitly programmed for specific tasks. It involves creating\n",
      "algorithms that can learn from data and make predictions based on that\n",
      "data. There are three broad categories of machine learning: supervised\n",
      "learning, unsupervised learning, and reinforcement learning.\n",
      "______________________________________________________________________\n",
      "Source Node 1/5\n",
      "Node ID: 6954faa1-ba18-4feb-8917-6d1d3a4ce333\n",
      "Similarity: 0.841589600094414\n",
      "Text: Preface Deep Learning with TensorFlow and Keras, Third Edition,\n",
      "is a concise yet thorough introduction to modern  neural networks,\n",
      "artificial intelligence, and deep learning technologies designed\n",
      "especially for software  engineers and data scientists. The book is\n",
      "the natural follow-up of the books Deep Learning with Keras  [1] and\n",
      "TensorFlow 1.x...\n",
      "______________________________________________________________________\n",
      "Source Node 2/5\n",
      "Node ID: ed0ad128-d9f6-46e6-a32b-1dcfb3177d7b\n",
      "Similarity: 0.8389274603857956\n",
      "Text: Neural Network Foundations with TF 10 In short: what are neural\n",
      "networks after all? In one sentence, machine learning models are a way\n",
      "to compute a function that maps some inputs to  their corresponding\n",
      "outputs. The function is nothing more than a number of addition and\n",
      "multiplication  operations. However, when combined with a nonlinear\n",
      "activati...\n",
      "______________________________________________________________________\n",
      "Source Node 3/5\n",
      "Node ID: 7a32b577-29d4-4434-a108-a441224ecbeb\n",
      "Similarity: 0.8340102845380709\n",
      "Text: 13 An Introduction to AutoML The goal of AutoML is to enable\n",
      "domain experts who are unfamiliar with machine learning technologies\n",
      "to use ML techniques easily. In this chapter, we will go through a\n",
      "practical exercise using Google Cloud Platform and do quite a bit  of\n",
      "hands-on work after briefly discussing the fundamentals. We will\n",
      "cover: • Autom...\n",
      "______________________________________________________________________\n",
      "Source Node 4/5\n",
      "Node ID: 83e71055-72af-4ae2-b657-5b7582f42450\n",
      "Similarity: 0.8299544133256617\n",
      "Text: An Introduction to AutoML 448 Feature mapping aims at altering\n",
      "the original feature space by means of a mapping function. This  can\n",
      "be implemented in multiple ways; for instance, it can use autoencoders\n",
      "(see Chapter 8), PCA (see  Chapter 7), or clustering (see Chapter 7).\n",
      "In short, feature engineering is an art based on intuition, trial and\n",
      "erro...\n",
      "______________________________________________________________________\n",
      "Source Node 5/5\n",
      "Node ID: 0bb4dbcd-8f29-41d8-9ba5-9d11457902ec\n",
      "Similarity: 0.8294406663454646\n",
      "Text: Chapter 13 461 Then it is possible to make online predictions of\n",
      "income by using a REST API (see https://en.wikipedia.\n",
      "org/wiki/Representational_state_transfer ), using this command for the\n",
      "example we’re looking  at in this chapter, as shown in Figure 13.20:\n",
      "Figure 13.20: AutoML Tables – querying the deployed model in\n",
      "production Put simply, we c...\n",
      "Machine learning is a subdiscipline of artificial intelligence that focuses on teaching computers how to learn without being explicitly programmed for specific tasks. It involves creating algorithms that can learn from data and make predictions based on that data. There are three broad categories of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subdiscipline of artificial intelligence that focuses on teaching computers how to learn without being explicitly programmed for specific tasks. It involves creating algorithms that can learn from data and make predictions based on that data. There are three broad categories of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"Data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is machine learning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
